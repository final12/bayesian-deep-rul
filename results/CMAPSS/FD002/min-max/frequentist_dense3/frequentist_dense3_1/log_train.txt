Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_1', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24146
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:04:55.038744 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:04:55.042105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4310.00
 ---- batch: 020 ----
mean loss: 4133.73
 ---- batch: 030 ----
mean loss: 4102.82
 ---- batch: 040 ----
mean loss: 3978.75
 ---- batch: 050 ----
mean loss: 3840.56
 ---- batch: 060 ----
mean loss: 3877.00
 ---- batch: 070 ----
mean loss: 3749.02
 ---- batch: 080 ----
mean loss: 3747.40
 ---- batch: 090 ----
mean loss: 3692.77
train mean loss: 3917.98
epoch train time: 0:00:33.827916
elapsed time: 0:00:33.833687
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:05:28.872486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3533.28
 ---- batch: 020 ----
mean loss: 3571.36
 ---- batch: 030 ----
mean loss: 3504.04
 ---- batch: 040 ----
mean loss: 3459.02
 ---- batch: 050 ----
mean loss: 3339.59
 ---- batch: 060 ----
mean loss: 3344.51
 ---- batch: 070 ----
mean loss: 3287.58
 ---- batch: 080 ----
mean loss: 3208.95
 ---- batch: 090 ----
mean loss: 3185.36
train mean loss: 3366.89
epoch train time: 0:00:00.493107
elapsed time: 0:00:34.327006
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:05:29.365849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3068.82
 ---- batch: 020 ----
mean loss: 3011.88
 ---- batch: 030 ----
mean loss: 3001.11
 ---- batch: 040 ----
mean loss: 2957.03
 ---- batch: 050 ----
mean loss: 2926.51
 ---- batch: 060 ----
mean loss: 2878.15
 ---- batch: 070 ----
mean loss: 2865.81
 ---- batch: 080 ----
mean loss: 2794.38
 ---- batch: 090 ----
mean loss: 2721.07
train mean loss: 2904.25
epoch train time: 0:00:00.502255
elapsed time: 0:00:34.829460
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:05:29.868267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2683.03
 ---- batch: 020 ----
mean loss: 2611.59
 ---- batch: 030 ----
mean loss: 2567.77
 ---- batch: 040 ----
mean loss: 2591.20
 ---- batch: 050 ----
mean loss: 2531.55
 ---- batch: 060 ----
mean loss: 2517.16
 ---- batch: 070 ----
mean loss: 2432.12
 ---- batch: 080 ----
mean loss: 2415.85
 ---- batch: 090 ----
mean loss: 2412.85
train mean loss: 2518.76
epoch train time: 0:00:00.490454
elapsed time: 0:00:35.320082
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:05:30.358874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2315.84
 ---- batch: 020 ----
mean loss: 2308.48
 ---- batch: 030 ----
mean loss: 2251.62
 ---- batch: 040 ----
mean loss: 2217.77
 ---- batch: 050 ----
mean loss: 2235.39
 ---- batch: 060 ----
mean loss: 2146.85
 ---- batch: 070 ----
mean loss: 2139.28
 ---- batch: 080 ----
mean loss: 2142.40
 ---- batch: 090 ----
mean loss: 2085.15
train mean loss: 2194.88
epoch train time: 0:00:00.501956
elapsed time: 0:00:35.822194
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:05:30.860994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2007.97
 ---- batch: 020 ----
mean loss: 1976.57
 ---- batch: 030 ----
mean loss: 1986.44
 ---- batch: 040 ----
mean loss: 1937.53
 ---- batch: 050 ----
mean loss: 1972.99
 ---- batch: 060 ----
mean loss: 1861.47
 ---- batch: 070 ----
mean loss: 1882.21
 ---- batch: 080 ----
mean loss: 1886.30
 ---- batch: 090 ----
mean loss: 1802.70
train mean loss: 1914.64
epoch train time: 0:00:00.500463
elapsed time: 0:00:36.322810
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:05:31.361631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1780.68
 ---- batch: 020 ----
mean loss: 1771.95
 ---- batch: 030 ----
mean loss: 1685.21
 ---- batch: 040 ----
mean loss: 1719.02
 ---- batch: 050 ----
mean loss: 1687.04
 ---- batch: 060 ----
mean loss: 1681.91
 ---- batch: 070 ----
mean loss: 1644.50
 ---- batch: 080 ----
mean loss: 1632.57
 ---- batch: 090 ----
mean loss: 1616.32
train mean loss: 1682.61
epoch train time: 0:00:00.497319
elapsed time: 0:00:36.820303
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:05:31.859100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1554.44
 ---- batch: 020 ----
mean loss: 1561.22
 ---- batch: 030 ----
mean loss: 1559.10
 ---- batch: 040 ----
mean loss: 1481.65
 ---- batch: 050 ----
mean loss: 1522.35
 ---- batch: 060 ----
mean loss: 1499.08
 ---- batch: 070 ----
mean loss: 1446.64
 ---- batch: 080 ----
mean loss: 1469.46
 ---- batch: 090 ----
mean loss: 1426.44
train mean loss: 1496.04
epoch train time: 0:00:00.491915
elapsed time: 0:00:37.312367
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:05:32.351178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1415.35
 ---- batch: 020 ----
mean loss: 1369.40
 ---- batch: 030 ----
mean loss: 1366.56
 ---- batch: 040 ----
mean loss: 1346.41
 ---- batch: 050 ----
mean loss: 1374.94
 ---- batch: 060 ----
mean loss: 1314.74
 ---- batch: 070 ----
mean loss: 1331.31
 ---- batch: 080 ----
mean loss: 1284.18
 ---- batch: 090 ----
mean loss: 1312.08
train mean loss: 1343.02
epoch train time: 0:00:00.505903
elapsed time: 0:00:37.818436
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:05:32.857232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1279.32
 ---- batch: 020 ----
mean loss: 1247.98
 ---- batch: 030 ----
mean loss: 1256.27
 ---- batch: 040 ----
mean loss: 1203.51
 ---- batch: 050 ----
mean loss: 1232.17
 ---- batch: 060 ----
mean loss: 1211.67
 ---- batch: 070 ----
mean loss: 1222.85
 ---- batch: 080 ----
mean loss: 1175.55
 ---- batch: 090 ----
mean loss: 1191.36
train mean loss: 1219.94
epoch train time: 0:00:00.496955
elapsed time: 0:00:38.315543
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:05:33.354339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1141.85
 ---- batch: 020 ----
mean loss: 1161.56
 ---- batch: 030 ----
mean loss: 1157.10
 ---- batch: 040 ----
mean loss: 1121.56
 ---- batch: 050 ----
mean loss: 1121.42
 ---- batch: 060 ----
mean loss: 1135.04
 ---- batch: 070 ----
mean loss: 1117.28
 ---- batch: 080 ----
mean loss: 1085.91
 ---- batch: 090 ----
mean loss: 1109.17
train mean loss: 1126.06
epoch train time: 0:00:00.502227
elapsed time: 0:00:38.817921
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:05:33.856737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1071.88
 ---- batch: 020 ----
mean loss: 1080.98
 ---- batch: 030 ----
mean loss: 1084.24
 ---- batch: 040 ----
mean loss: 1068.70
 ---- batch: 050 ----
mean loss: 1061.66
 ---- batch: 060 ----
mean loss: 1057.13
 ---- batch: 070 ----
mean loss: 1056.37
 ---- batch: 080 ----
mean loss: 1025.77
 ---- batch: 090 ----
mean loss: 1015.34
train mean loss: 1054.63
epoch train time: 0:00:00.508650
elapsed time: 0:00:39.326765
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:05:34.365561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1012.72
 ---- batch: 020 ----
mean loss: 1003.54
 ---- batch: 030 ----
mean loss: 1020.80
 ---- batch: 040 ----
mean loss: 997.86
 ---- batch: 050 ----
mean loss: 1027.02
 ---- batch: 060 ----
mean loss: 996.33
 ---- batch: 070 ----
mean loss: 983.89
 ---- batch: 080 ----
mean loss: 992.04
 ---- batch: 090 ----
mean loss: 987.88
train mean loss: 1001.76
epoch train time: 0:00:00.498830
elapsed time: 0:00:39.825765
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:05:34.864569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 981.96
 ---- batch: 020 ----
mean loss: 971.61
 ---- batch: 030 ----
mean loss: 966.62
 ---- batch: 040 ----
mean loss: 947.90
 ---- batch: 050 ----
mean loss: 962.53
 ---- batch: 060 ----
mean loss: 959.01
 ---- batch: 070 ----
mean loss: 970.02
 ---- batch: 080 ----
mean loss: 956.56
 ---- batch: 090 ----
mean loss: 954.19
train mean loss: 961.95
epoch train time: 0:00:00.499557
elapsed time: 0:00:40.325479
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:05:35.364307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.35
 ---- batch: 020 ----
mean loss: 947.02
 ---- batch: 030 ----
mean loss: 937.36
 ---- batch: 040 ----
mean loss: 918.82
 ---- batch: 050 ----
mean loss: 932.43
 ---- batch: 060 ----
mean loss: 925.83
 ---- batch: 070 ----
mean loss: 925.27
 ---- batch: 080 ----
mean loss: 938.43
 ---- batch: 090 ----
mean loss: 930.58
train mean loss: 934.10
epoch train time: 0:00:00.502666
elapsed time: 0:00:40.828324
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:05:35.867129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.74
 ---- batch: 020 ----
mean loss: 917.53
 ---- batch: 030 ----
mean loss: 917.96
 ---- batch: 040 ----
mean loss: 922.96
 ---- batch: 050 ----
mean loss: 917.05
 ---- batch: 060 ----
mean loss: 905.77
 ---- batch: 070 ----
mean loss: 889.61
 ---- batch: 080 ----
mean loss: 911.55
 ---- batch: 090 ----
mean loss: 908.09
train mean loss: 913.71
epoch train time: 0:00:00.491089
elapsed time: 0:00:41.319571
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:05:36.358369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.06
 ---- batch: 020 ----
mean loss: 887.22
 ---- batch: 030 ----
mean loss: 900.13
 ---- batch: 040 ----
mean loss: 906.36
 ---- batch: 050 ----
mean loss: 883.69
 ---- batch: 060 ----
mean loss: 907.16
 ---- batch: 070 ----
mean loss: 911.76
 ---- batch: 080 ----
mean loss: 911.52
 ---- batch: 090 ----
mean loss: 883.08
train mean loss: 900.15
epoch train time: 0:00:00.486854
elapsed time: 0:00:41.806575
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:05:36.845371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.45
 ---- batch: 020 ----
mean loss: 887.99
 ---- batch: 030 ----
mean loss: 909.88
 ---- batch: 040 ----
mean loss: 899.50
 ---- batch: 050 ----
mean loss: 881.43
 ---- batch: 060 ----
mean loss: 876.93
 ---- batch: 070 ----
mean loss: 886.23
 ---- batch: 080 ----
mean loss: 883.71
 ---- batch: 090 ----
mean loss: 881.12
train mean loss: 890.59
epoch train time: 0:00:00.493157
elapsed time: 0:00:42.299884
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:05:37.338700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.55
 ---- batch: 020 ----
mean loss: 892.07
 ---- batch: 030 ----
mean loss: 875.00
 ---- batch: 040 ----
mean loss: 889.73
 ---- batch: 050 ----
mean loss: 887.23
 ---- batch: 060 ----
mean loss: 880.18
 ---- batch: 070 ----
mean loss: 867.43
 ---- batch: 080 ----
mean loss: 896.75
 ---- batch: 090 ----
mean loss: 890.65
train mean loss: 884.64
epoch train time: 0:00:00.500408
elapsed time: 0:00:42.800462
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:05:37.839258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.31
 ---- batch: 020 ----
mean loss: 894.03
 ---- batch: 030 ----
mean loss: 880.50
 ---- batch: 040 ----
mean loss: 887.34
 ---- batch: 050 ----
mean loss: 871.97
 ---- batch: 060 ----
mean loss: 881.14
 ---- batch: 070 ----
mean loss: 890.16
 ---- batch: 080 ----
mean loss: 868.97
 ---- batch: 090 ----
mean loss: 876.26
train mean loss: 881.01
epoch train time: 0:00:00.491982
elapsed time: 0:00:43.292600
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:05:38.331387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.81
 ---- batch: 020 ----
mean loss: 871.27
 ---- batch: 030 ----
mean loss: 871.06
 ---- batch: 040 ----
mean loss: 879.16
 ---- batch: 050 ----
mean loss: 895.58
 ---- batch: 060 ----
mean loss: 876.22
 ---- batch: 070 ----
mean loss: 861.25
 ---- batch: 080 ----
mean loss: 890.86
 ---- batch: 090 ----
mean loss: 881.46
train mean loss: 877.84
epoch train time: 0:00:00.493493
elapsed time: 0:00:43.786240
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:05:38.825036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.44
 ---- batch: 020 ----
mean loss: 889.53
 ---- batch: 030 ----
mean loss: 869.93
 ---- batch: 040 ----
mean loss: 858.06
 ---- batch: 050 ----
mean loss: 869.49
 ---- batch: 060 ----
mean loss: 892.56
 ---- batch: 070 ----
mean loss: 883.10
 ---- batch: 080 ----
mean loss: 872.70
 ---- batch: 090 ----
mean loss: 877.71
train mean loss: 877.37
epoch train time: 0:00:00.486150
elapsed time: 0:00:44.272546
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:05:39.311342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.71
 ---- batch: 020 ----
mean loss: 872.21
 ---- batch: 030 ----
mean loss: 862.65
 ---- batch: 040 ----
mean loss: 868.69
 ---- batch: 050 ----
mean loss: 879.26
 ---- batch: 060 ----
mean loss: 880.48
 ---- batch: 070 ----
mean loss: 876.56
 ---- batch: 080 ----
mean loss: 879.27
 ---- batch: 090 ----
mean loss: 885.11
train mean loss: 875.88
epoch train time: 0:00:00.500663
elapsed time: 0:00:44.773407
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:05:39.812221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.89
 ---- batch: 020 ----
mean loss: 871.43
 ---- batch: 030 ----
mean loss: 877.58
 ---- batch: 040 ----
mean loss: 859.20
 ---- batch: 050 ----
mean loss: 873.26
 ---- batch: 060 ----
mean loss: 866.70
 ---- batch: 070 ----
mean loss: 881.90
 ---- batch: 080 ----
mean loss: 881.44
 ---- batch: 090 ----
mean loss: 873.38
train mean loss: 876.66
epoch train time: 0:00:00.505409
elapsed time: 0:00:45.278984
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:05:40.317781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.15
 ---- batch: 020 ----
mean loss: 889.47
 ---- batch: 030 ----
mean loss: 883.26
 ---- batch: 040 ----
mean loss: 876.04
 ---- batch: 050 ----
mean loss: 881.15
 ---- batch: 060 ----
mean loss: 877.90
 ---- batch: 070 ----
mean loss: 873.96
 ---- batch: 080 ----
mean loss: 866.25
 ---- batch: 090 ----
mean loss: 883.82
train mean loss: 875.41
epoch train time: 0:00:00.491907
elapsed time: 0:00:45.771042
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:05:40.809854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.52
 ---- batch: 020 ----
mean loss: 869.87
 ---- batch: 030 ----
mean loss: 863.70
 ---- batch: 040 ----
mean loss: 866.69
 ---- batch: 050 ----
mean loss: 864.97
 ---- batch: 060 ----
mean loss: 901.77
 ---- batch: 070 ----
mean loss: 883.69
 ---- batch: 080 ----
mean loss: 878.29
 ---- batch: 090 ----
mean loss: 869.42
train mean loss: 875.00
epoch train time: 0:00:00.501442
elapsed time: 0:00:46.272660
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:05:41.311490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.24
 ---- batch: 020 ----
mean loss: 873.63
 ---- batch: 030 ----
mean loss: 871.90
 ---- batch: 040 ----
mean loss: 873.01
 ---- batch: 050 ----
mean loss: 864.79
 ---- batch: 060 ----
mean loss: 869.97
 ---- batch: 070 ----
mean loss: 879.70
 ---- batch: 080 ----
mean loss: 887.21
 ---- batch: 090 ----
mean loss: 887.94
train mean loss: 875.16
epoch train time: 0:00:00.495881
elapsed time: 0:00:46.768738
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:05:41.807527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.90
 ---- batch: 020 ----
mean loss: 869.20
 ---- batch: 030 ----
mean loss: 878.28
 ---- batch: 040 ----
mean loss: 885.25
 ---- batch: 050 ----
mean loss: 873.32
 ---- batch: 060 ----
mean loss: 871.08
 ---- batch: 070 ----
mean loss: 862.69
 ---- batch: 080 ----
mean loss: 892.02
 ---- batch: 090 ----
mean loss: 862.51
train mean loss: 874.92
epoch train time: 0:00:00.494788
elapsed time: 0:00:47.263715
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:05:42.302541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.85
 ---- batch: 020 ----
mean loss: 875.00
 ---- batch: 030 ----
mean loss: 861.98
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.66
 ---- batch: 060 ----
mean loss: 883.30
 ---- batch: 070 ----
mean loss: 889.91
 ---- batch: 080 ----
mean loss: 863.91
 ---- batch: 090 ----
mean loss: 859.46
train mean loss: 875.84
epoch train time: 0:00:00.496806
elapsed time: 0:00:47.760730
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:05:42.799534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.37
 ---- batch: 030 ----
mean loss: 870.61
 ---- batch: 040 ----
mean loss: 876.63
 ---- batch: 050 ----
mean loss: 875.45
 ---- batch: 060 ----
mean loss: 878.08
 ---- batch: 070 ----
mean loss: 872.70
 ---- batch: 080 ----
mean loss: 870.21
 ---- batch: 090 ----
mean loss: 857.35
train mean loss: 874.97
epoch train time: 0:00:00.501874
elapsed time: 0:00:48.262777
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:05:43.301573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.87
 ---- batch: 020 ----
mean loss: 871.03
 ---- batch: 030 ----
mean loss: 868.38
 ---- batch: 040 ----
mean loss: 878.45
 ---- batch: 050 ----
mean loss: 892.63
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.93
 ---- batch: 090 ----
mean loss: 861.13
train mean loss: 875.07
epoch train time: 0:00:00.497813
elapsed time: 0:00:48.760748
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:05:43.799546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.13
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.04
 ---- batch: 050 ----
mean loss: 876.38
 ---- batch: 060 ----
mean loss: 886.64
 ---- batch: 070 ----
mean loss: 861.71
 ---- batch: 080 ----
mean loss: 882.73
 ---- batch: 090 ----
mean loss: 870.73
train mean loss: 874.91
epoch train time: 0:00:00.495698
elapsed time: 0:00:49.256599
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:05:44.295523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.45
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.21
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.76
train mean loss: 876.11
epoch train time: 0:00:00.492517
elapsed time: 0:00:49.749430
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:05:44.788255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.87
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.29
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.37
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.77
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.493405
elapsed time: 0:00:50.243066
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:05:45.281906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.82
 ---- batch: 020 ----
mean loss: 872.42
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.58
 ---- batch: 050 ----
mean loss: 870.21
 ---- batch: 060 ----
mean loss: 889.56
 ---- batch: 070 ----
mean loss: 878.82
 ---- batch: 080 ----
mean loss: 875.93
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.70
epoch train time: 0:00:00.491380
elapsed time: 0:00:50.734659
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:05:45.773456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.44
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.42
 ---- batch: 070 ----
mean loss: 861.52
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.20
train mean loss: 875.33
epoch train time: 0:00:00.509863
elapsed time: 0:00:51.244678
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:05:46.283465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.85
 ---- batch: 040 ----
mean loss: 896.13
 ---- batch: 050 ----
mean loss: 895.65
 ---- batch: 060 ----
mean loss: 872.60
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.88
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.495249
elapsed time: 0:00:51.740068
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:05:46.778863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.28
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 881.99
 ---- batch: 090 ----
mean loss: 857.68
train mean loss: 875.42
epoch train time: 0:00:00.495590
elapsed time: 0:00:52.235826
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:05:47.274628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.88
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.12
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.70
train mean loss: 875.94
epoch train time: 0:00:00.498039
elapsed time: 0:00:52.734047
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:05:47.772851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.25
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.78
 ---- batch: 080 ----
mean loss: 872.87
 ---- batch: 090 ----
mean loss: 880.57
train mean loss: 875.20
epoch train time: 0:00:00.500351
elapsed time: 0:00:53.234549
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:05:48.273375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.89
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.50
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.94
 ---- batch: 090 ----
mean loss: 867.29
train mean loss: 874.55
epoch train time: 0:00:00.487393
elapsed time: 0:00:53.722134
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:05:48.760948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.82
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.43
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.483997
elapsed time: 0:00:54.206292
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:05:49.245101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.13
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.49
 ---- batch: 060 ----
mean loss: 873.51
 ---- batch: 070 ----
mean loss: 889.66
 ---- batch: 080 ----
mean loss: 893.93
 ---- batch: 090 ----
mean loss: 900.17
train mean loss: 875.66
epoch train time: 0:00:00.493545
elapsed time: 0:00:54.700004
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:05:49.738800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.41
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.90
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.10
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.23
epoch train time: 0:00:00.501276
elapsed time: 0:00:55.201458
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:05:50.240255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.90
 ---- batch: 020 ----
mean loss: 896.01
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.44
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.57
 ---- batch: 080 ----
mean loss: 874.53
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.65
epoch train time: 0:00:00.510539
elapsed time: 0:00:55.712146
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:05:50.750946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.22
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.62
 ---- batch: 080 ----
mean loss: 876.95
 ---- batch: 090 ----
mean loss: 864.57
train mean loss: 874.88
epoch train time: 0:00:00.505349
elapsed time: 0:00:56.217646
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:05:51.256451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.46
 ---- batch: 070 ----
mean loss: 894.00
 ---- batch: 080 ----
mean loss: 865.66
 ---- batch: 090 ----
mean loss: 871.92
train mean loss: 875.10
epoch train time: 0:00:00.505061
elapsed time: 0:00:56.722888
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:05:51.761689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.70
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.70
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.65
 ---- batch: 060 ----
mean loss: 876.80
 ---- batch: 070 ----
mean loss: 871.95
 ---- batch: 080 ----
mean loss: 877.45
 ---- batch: 090 ----
mean loss: 896.57
train mean loss: 874.85
epoch train time: 0:00:00.518206
elapsed time: 0:00:57.241255
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:05:52.280066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.54
 ---- batch: 020 ----
mean loss: 874.88
 ---- batch: 030 ----
mean loss: 842.41
 ---- batch: 040 ----
mean loss: 870.90
 ---- batch: 050 ----
mean loss: 879.94
 ---- batch: 060 ----
mean loss: 870.33
 ---- batch: 070 ----
mean loss: 893.30
 ---- batch: 080 ----
mean loss: 874.30
 ---- batch: 090 ----
mean loss: 901.59
train mean loss: 874.94
epoch train time: 0:00:00.514643
elapsed time: 0:00:57.756080
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:05:52.794876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.79
 ---- batch: 020 ----
mean loss: 885.95
 ---- batch: 030 ----
mean loss: 869.68
 ---- batch: 040 ----
mean loss: 881.87
 ---- batch: 050 ----
mean loss: 866.17
 ---- batch: 060 ----
mean loss: 869.62
 ---- batch: 070 ----
mean loss: 865.49
 ---- batch: 080 ----
mean loss: 885.12
 ---- batch: 090 ----
mean loss: 878.95
train mean loss: 874.96
epoch train time: 0:00:00.510219
elapsed time: 0:00:58.266450
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:05:53.305256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.61
 ---- batch: 020 ----
mean loss: 893.23
 ---- batch: 030 ----
mean loss: 872.98
 ---- batch: 040 ----
mean loss: 888.76
 ---- batch: 050 ----
mean loss: 867.77
 ---- batch: 060 ----
mean loss: 869.07
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 858.03
 ---- batch: 090 ----
mean loss: 875.46
train mean loss: 875.15
epoch train time: 0:00:00.509291
elapsed time: 0:00:58.775903
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:05:53.814701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.42
 ---- batch: 020 ----
mean loss: 873.35
 ---- batch: 030 ----
mean loss: 870.55
 ---- batch: 040 ----
mean loss: 862.72
 ---- batch: 050 ----
mean loss: 871.29
 ---- batch: 060 ----
mean loss: 875.63
 ---- batch: 070 ----
mean loss: 898.82
 ---- batch: 080 ----
mean loss: 874.46
 ---- batch: 090 ----
mean loss: 887.67
train mean loss: 875.69
epoch train time: 0:00:00.518507
elapsed time: 0:00:59.294569
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:05:54.333357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.20
 ---- batch: 020 ----
mean loss: 875.67
 ---- batch: 030 ----
mean loss: 868.29
 ---- batch: 040 ----
mean loss: 876.40
 ---- batch: 050 ----
mean loss: 869.82
 ---- batch: 060 ----
mean loss: 874.43
 ---- batch: 070 ----
mean loss: 877.10
 ---- batch: 080 ----
mean loss: 890.28
 ---- batch: 090 ----
mean loss: 870.38
train mean loss: 875.10
epoch train time: 0:00:00.508116
elapsed time: 0:00:59.802842
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:05:54.841663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.39
 ---- batch: 020 ----
mean loss: 863.20
 ---- batch: 030 ----
mean loss: 884.35
 ---- batch: 040 ----
mean loss: 878.62
 ---- batch: 050 ----
mean loss: 875.71
 ---- batch: 060 ----
mean loss: 870.58
 ---- batch: 070 ----
mean loss: 875.20
 ---- batch: 080 ----
mean loss: 873.82
 ---- batch: 090 ----
mean loss: 883.90
train mean loss: 874.97
epoch train time: 0:00:00.513885
elapsed time: 0:01:00.316913
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:05:55.355701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.10
 ---- batch: 020 ----
mean loss: 853.59
 ---- batch: 030 ----
mean loss: 878.74
 ---- batch: 040 ----
mean loss: 879.11
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 858.45
 ---- batch: 070 ----
mean loss: 882.02
 ---- batch: 080 ----
mean loss: 887.78
 ---- batch: 090 ----
mean loss: 862.44
train mean loss: 875.73
epoch train time: 0:00:00.516870
elapsed time: 0:01:00.833963
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:05:55.872790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.91
 ---- batch: 020 ----
mean loss: 854.59
 ---- batch: 030 ----
mean loss: 865.08
 ---- batch: 040 ----
mean loss: 868.00
 ---- batch: 050 ----
mean loss: 891.58
 ---- batch: 060 ----
mean loss: 872.24
 ---- batch: 070 ----
mean loss: 913.58
 ---- batch: 080 ----
mean loss: 859.98
 ---- batch: 090 ----
mean loss: 881.53
train mean loss: 875.41
epoch train time: 0:00:00.497893
elapsed time: 0:01:01.332035
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:05:56.370849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.25
 ---- batch: 020 ----
mean loss: 870.54
 ---- batch: 030 ----
mean loss: 879.46
 ---- batch: 040 ----
mean loss: 867.02
 ---- batch: 050 ----
mean loss: 886.26
 ---- batch: 060 ----
mean loss: 875.60
 ---- batch: 070 ----
mean loss: 878.38
 ---- batch: 080 ----
mean loss: 871.65
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.60
epoch train time: 0:00:00.502530
elapsed time: 0:01:01.834747
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:05:56.873545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.09
 ---- batch: 020 ----
mean loss: 881.83
 ---- batch: 030 ----
mean loss: 876.41
 ---- batch: 040 ----
mean loss: 875.09
 ---- batch: 050 ----
mean loss: 887.96
 ---- batch: 060 ----
mean loss: 873.75
 ---- batch: 070 ----
mean loss: 876.70
 ---- batch: 080 ----
mean loss: 888.02
 ---- batch: 090 ----
mean loss: 866.74
train mean loss: 874.69
epoch train time: 0:00:00.501640
elapsed time: 0:01:02.336542
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:05:57.375338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.87
 ---- batch: 020 ----
mean loss: 870.56
 ---- batch: 030 ----
mean loss: 889.75
 ---- batch: 040 ----
mean loss: 877.69
 ---- batch: 050 ----
mean loss: 872.85
 ---- batch: 060 ----
mean loss: 868.47
 ---- batch: 070 ----
mean loss: 878.12
 ---- batch: 080 ----
mean loss: 872.49
 ---- batch: 090 ----
mean loss: 884.50
train mean loss: 875.13
epoch train time: 0:00:00.513337
elapsed time: 0:01:02.850038
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:05:57.888844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.68
 ---- batch: 020 ----
mean loss: 873.42
 ---- batch: 030 ----
mean loss: 870.15
 ---- batch: 040 ----
mean loss: 891.02
 ---- batch: 050 ----
mean loss: 876.22
 ---- batch: 060 ----
mean loss: 880.78
 ---- batch: 070 ----
mean loss: 870.26
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 877.87
train mean loss: 875.25
epoch train time: 0:00:00.518233
elapsed time: 0:01:03.368431
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:05:58.407228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.11
 ---- batch: 020 ----
mean loss: 873.65
 ---- batch: 030 ----
mean loss: 879.05
 ---- batch: 040 ----
mean loss: 891.21
 ---- batch: 050 ----
mean loss: 852.61
 ---- batch: 060 ----
mean loss: 841.99
 ---- batch: 070 ----
mean loss: 818.47
 ---- batch: 080 ----
mean loss: 811.82
 ---- batch: 090 ----
mean loss: 799.67
train mean loss: 844.23
epoch train time: 0:00:00.522691
elapsed time: 0:01:03.891271
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:05:58.930068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 783.73
 ---- batch: 020 ----
mean loss: 765.98
 ---- batch: 030 ----
mean loss: 782.34
 ---- batch: 040 ----
mean loss: 759.90
 ---- batch: 050 ----
mean loss: 754.32
 ---- batch: 060 ----
mean loss: 750.38
 ---- batch: 070 ----
mean loss: 753.97
 ---- batch: 080 ----
mean loss: 724.45
 ---- batch: 090 ----
mean loss: 709.32
train mean loss: 747.81
epoch train time: 0:00:00.502769
elapsed time: 0:01:04.394189
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:05:59.433000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 586.42
 ---- batch: 020 ----
mean loss: 481.68
 ---- batch: 030 ----
mean loss: 420.29
 ---- batch: 040 ----
mean loss: 393.50
 ---- batch: 050 ----
mean loss: 389.33
 ---- batch: 060 ----
mean loss: 377.17
 ---- batch: 070 ----
mean loss: 361.69
 ---- batch: 080 ----
mean loss: 361.53
 ---- batch: 090 ----
mean loss: 350.85
train mean loss: 408.65
epoch train time: 0:00:00.513334
elapsed time: 0:01:04.907711
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:05:59.946546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.61
 ---- batch: 020 ----
mean loss: 337.29
 ---- batch: 030 ----
mean loss: 316.12
 ---- batch: 040 ----
mean loss: 317.80
 ---- batch: 050 ----
mean loss: 321.72
 ---- batch: 060 ----
mean loss: 301.66
 ---- batch: 070 ----
mean loss: 294.38
 ---- batch: 080 ----
mean loss: 292.30
 ---- batch: 090 ----
mean loss: 283.80
train mean loss: 308.79
epoch train time: 0:00:00.502271
elapsed time: 0:01:05.410174
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:06:00.448978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.62
 ---- batch: 020 ----
mean loss: 283.32
 ---- batch: 030 ----
mean loss: 286.38
 ---- batch: 040 ----
mean loss: 278.33
 ---- batch: 050 ----
mean loss: 277.28
 ---- batch: 060 ----
mean loss: 273.81
 ---- batch: 070 ----
mean loss: 259.52
 ---- batch: 080 ----
mean loss: 262.55
 ---- batch: 090 ----
mean loss: 264.11
train mean loss: 274.01
epoch train time: 0:00:00.496111
elapsed time: 0:01:05.906464
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:06:00.945248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.88
 ---- batch: 020 ----
mean loss: 255.74
 ---- batch: 030 ----
mean loss: 257.59
 ---- batch: 040 ----
mean loss: 250.67
 ---- batch: 050 ----
mean loss: 259.28
 ---- batch: 060 ----
mean loss: 258.74
 ---- batch: 070 ----
mean loss: 251.02
 ---- batch: 080 ----
mean loss: 257.61
 ---- batch: 090 ----
mean loss: 247.43
train mean loss: 254.94
epoch train time: 0:00:00.494396
elapsed time: 0:01:06.401028
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:06:01.439850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.98
 ---- batch: 020 ----
mean loss: 238.17
 ---- batch: 030 ----
mean loss: 259.03
 ---- batch: 040 ----
mean loss: 252.25
 ---- batch: 050 ----
mean loss: 248.61
 ---- batch: 060 ----
mean loss: 251.77
 ---- batch: 070 ----
mean loss: 239.10
 ---- batch: 080 ----
mean loss: 237.58
 ---- batch: 090 ----
mean loss: 233.18
train mean loss: 243.98
epoch train time: 0:00:00.497096
elapsed time: 0:01:06.898305
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:06:01.937121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.60
 ---- batch: 020 ----
mean loss: 242.72
 ---- batch: 030 ----
mean loss: 231.72
 ---- batch: 040 ----
mean loss: 231.13
 ---- batch: 050 ----
mean loss: 231.52
 ---- batch: 060 ----
mean loss: 231.28
 ---- batch: 070 ----
mean loss: 236.33
 ---- batch: 080 ----
mean loss: 236.63
 ---- batch: 090 ----
mean loss: 231.86
train mean loss: 234.88
epoch train time: 0:00:00.494449
elapsed time: 0:01:07.392944
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:06:02.431750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.07
 ---- batch: 020 ----
mean loss: 235.80
 ---- batch: 030 ----
mean loss: 222.85
 ---- batch: 040 ----
mean loss: 225.86
 ---- batch: 050 ----
mean loss: 225.63
 ---- batch: 060 ----
mean loss: 230.49
 ---- batch: 070 ----
mean loss: 241.21
 ---- batch: 080 ----
mean loss: 230.61
 ---- batch: 090 ----
mean loss: 234.67
train mean loss: 230.96
epoch train time: 0:00:00.489851
elapsed time: 0:01:07.882977
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:06:02.921774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.30
 ---- batch: 020 ----
mean loss: 223.88
 ---- batch: 030 ----
mean loss: 223.94
 ---- batch: 040 ----
mean loss: 224.03
 ---- batch: 050 ----
mean loss: 228.28
 ---- batch: 060 ----
mean loss: 219.31
 ---- batch: 070 ----
mean loss: 217.42
 ---- batch: 080 ----
mean loss: 233.38
 ---- batch: 090 ----
mean loss: 231.60
train mean loss: 227.36
epoch train time: 0:00:00.483216
elapsed time: 0:01:08.366342
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:06:03.405137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.06
 ---- batch: 020 ----
mean loss: 219.17
 ---- batch: 030 ----
mean loss: 226.01
 ---- batch: 040 ----
mean loss: 232.33
 ---- batch: 050 ----
mean loss: 223.05
 ---- batch: 060 ----
mean loss: 218.58
 ---- batch: 070 ----
mean loss: 223.82
 ---- batch: 080 ----
mean loss: 219.09
 ---- batch: 090 ----
mean loss: 223.90
train mean loss: 222.65
epoch train time: 0:00:00.491154
elapsed time: 0:01:08.857643
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:06:03.896468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.28
 ---- batch: 020 ----
mean loss: 217.00
 ---- batch: 030 ----
mean loss: 220.20
 ---- batch: 040 ----
mean loss: 222.92
 ---- batch: 050 ----
mean loss: 222.64
 ---- batch: 060 ----
mean loss: 224.36
 ---- batch: 070 ----
mean loss: 215.06
 ---- batch: 080 ----
mean loss: 225.07
 ---- batch: 090 ----
mean loss: 225.85
train mean loss: 221.26
epoch train time: 0:00:00.492116
elapsed time: 0:01:09.349936
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:06:04.388733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.47
 ---- batch: 020 ----
mean loss: 201.95
 ---- batch: 030 ----
mean loss: 219.97
 ---- batch: 040 ----
mean loss: 215.06
 ---- batch: 050 ----
mean loss: 226.53
 ---- batch: 060 ----
mean loss: 218.72
 ---- batch: 070 ----
mean loss: 221.00
 ---- batch: 080 ----
mean loss: 226.82
 ---- batch: 090 ----
mean loss: 219.76
train mean loss: 217.17
epoch train time: 0:00:00.500737
elapsed time: 0:01:09.850825
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:06:04.889649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.85
 ---- batch: 020 ----
mean loss: 215.87
 ---- batch: 030 ----
mean loss: 223.75
 ---- batch: 040 ----
mean loss: 211.84
 ---- batch: 050 ----
mean loss: 214.12
 ---- batch: 060 ----
mean loss: 219.04
 ---- batch: 070 ----
mean loss: 213.09
 ---- batch: 080 ----
mean loss: 219.58
 ---- batch: 090 ----
mean loss: 217.84
train mean loss: 215.85
epoch train time: 0:00:00.495461
elapsed time: 0:01:10.346462
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:06:05.385260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.74
 ---- batch: 020 ----
mean loss: 212.51
 ---- batch: 030 ----
mean loss: 210.14
 ---- batch: 040 ----
mean loss: 217.37
 ---- batch: 050 ----
mean loss: 211.17
 ---- batch: 060 ----
mean loss: 216.00
 ---- batch: 070 ----
mean loss: 213.22
 ---- batch: 080 ----
mean loss: 212.52
 ---- batch: 090 ----
mean loss: 218.91
train mean loss: 213.98
epoch train time: 0:00:00.501406
elapsed time: 0:01:10.848032
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:06:05.886827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.75
 ---- batch: 020 ----
mean loss: 209.90
 ---- batch: 030 ----
mean loss: 213.58
 ---- batch: 040 ----
mean loss: 216.36
 ---- batch: 050 ----
mean loss: 206.40
 ---- batch: 060 ----
mean loss: 207.81
 ---- batch: 070 ----
mean loss: 215.94
 ---- batch: 080 ----
mean loss: 214.45
 ---- batch: 090 ----
mean loss: 213.45
train mean loss: 211.81
epoch train time: 0:00:00.495893
elapsed time: 0:01:11.344072
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:06:06.382867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.78
 ---- batch: 020 ----
mean loss: 207.84
 ---- batch: 030 ----
mean loss: 196.86
 ---- batch: 040 ----
mean loss: 207.49
 ---- batch: 050 ----
mean loss: 217.36
 ---- batch: 060 ----
mean loss: 208.40
 ---- batch: 070 ----
mean loss: 212.68
 ---- batch: 080 ----
mean loss: 216.95
 ---- batch: 090 ----
mean loss: 206.29
train mean loss: 209.16
epoch train time: 0:00:00.506856
elapsed time: 0:01:11.851075
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:06:06.889869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.75
 ---- batch: 020 ----
mean loss: 207.92
 ---- batch: 030 ----
mean loss: 205.66
 ---- batch: 040 ----
mean loss: 214.00
 ---- batch: 050 ----
mean loss: 196.96
 ---- batch: 060 ----
mean loss: 206.08
 ---- batch: 070 ----
mean loss: 208.42
 ---- batch: 080 ----
mean loss: 206.99
 ---- batch: 090 ----
mean loss: 207.67
train mean loss: 206.34
epoch train time: 0:00:00.493985
elapsed time: 0:01:12.345213
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:06:07.384009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.62
 ---- batch: 020 ----
mean loss: 201.78
 ---- batch: 030 ----
mean loss: 202.51
 ---- batch: 040 ----
mean loss: 217.11
 ---- batch: 050 ----
mean loss: 206.15
 ---- batch: 060 ----
mean loss: 202.03
 ---- batch: 070 ----
mean loss: 216.31
 ---- batch: 080 ----
mean loss: 202.31
 ---- batch: 090 ----
mean loss: 208.35
train mean loss: 205.30
epoch train time: 0:00:00.501852
elapsed time: 0:01:12.847280
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:06:07.886141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.20
 ---- batch: 020 ----
mean loss: 198.96
 ---- batch: 030 ----
mean loss: 200.42
 ---- batch: 040 ----
mean loss: 212.38
 ---- batch: 050 ----
mean loss: 202.84
 ---- batch: 060 ----
mean loss: 208.03
 ---- batch: 070 ----
mean loss: 193.35
 ---- batch: 080 ----
mean loss: 207.96
 ---- batch: 090 ----
mean loss: 205.99
train mean loss: 202.98
epoch train time: 0:00:00.491351
elapsed time: 0:01:13.338850
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:06:08.377686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.58
 ---- batch: 020 ----
mean loss: 196.08
 ---- batch: 030 ----
mean loss: 196.95
 ---- batch: 040 ----
mean loss: 197.32
 ---- batch: 050 ----
mean loss: 204.72
 ---- batch: 060 ----
mean loss: 201.53
 ---- batch: 070 ----
mean loss: 199.53
 ---- batch: 080 ----
mean loss: 201.86
 ---- batch: 090 ----
mean loss: 208.49
train mean loss: 201.62
epoch train time: 0:00:00.509289
elapsed time: 0:01:13.848328
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:06:08.887127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.49
 ---- batch: 020 ----
mean loss: 200.18
 ---- batch: 030 ----
mean loss: 208.20
 ---- batch: 040 ----
mean loss: 201.00
 ---- batch: 050 ----
mean loss: 202.80
 ---- batch: 060 ----
mean loss: 197.65
 ---- batch: 070 ----
mean loss: 196.74
 ---- batch: 080 ----
mean loss: 196.31
 ---- batch: 090 ----
mean loss: 196.32
train mean loss: 199.22
epoch train time: 0:00:00.487924
elapsed time: 0:01:14.336417
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:06:09.375216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.75
 ---- batch: 020 ----
mean loss: 194.87
 ---- batch: 030 ----
mean loss: 191.93
 ---- batch: 040 ----
mean loss: 199.84
 ---- batch: 050 ----
mean loss: 207.06
 ---- batch: 060 ----
mean loss: 203.57
 ---- batch: 070 ----
mean loss: 191.75
 ---- batch: 080 ----
mean loss: 205.60
 ---- batch: 090 ----
mean loss: 197.71
train mean loss: 198.75
epoch train time: 0:00:00.507592
elapsed time: 0:01:14.844171
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:06:09.882968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.02
 ---- batch: 020 ----
mean loss: 186.91
 ---- batch: 030 ----
mean loss: 194.80
 ---- batch: 040 ----
mean loss: 193.21
 ---- batch: 050 ----
mean loss: 200.73
 ---- batch: 060 ----
mean loss: 201.36
 ---- batch: 070 ----
mean loss: 194.36
 ---- batch: 080 ----
mean loss: 202.06
 ---- batch: 090 ----
mean loss: 200.37
train mean loss: 197.09
epoch train time: 0:00:00.487717
elapsed time: 0:01:15.332035
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:06:10.370830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.40
 ---- batch: 020 ----
mean loss: 193.05
 ---- batch: 030 ----
mean loss: 191.10
 ---- batch: 040 ----
mean loss: 192.67
 ---- batch: 050 ----
mean loss: 198.50
 ---- batch: 060 ----
mean loss: 200.85
 ---- batch: 070 ----
mean loss: 187.00
 ---- batch: 080 ----
mean loss: 195.25
 ---- batch: 090 ----
mean loss: 207.31
train mean loss: 196.42
epoch train time: 0:00:00.493621
elapsed time: 0:01:15.825820
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:06:10.864617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.97
 ---- batch: 020 ----
mean loss: 201.99
 ---- batch: 030 ----
mean loss: 191.86
 ---- batch: 040 ----
mean loss: 197.02
 ---- batch: 050 ----
mean loss: 197.18
 ---- batch: 060 ----
mean loss: 192.83
 ---- batch: 070 ----
mean loss: 182.44
 ---- batch: 080 ----
mean loss: 197.67
 ---- batch: 090 ----
mean loss: 200.24
train mean loss: 195.88
epoch train time: 0:00:00.494962
elapsed time: 0:01:16.320927
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:06:11.359756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.53
 ---- batch: 020 ----
mean loss: 190.39
 ---- batch: 030 ----
mean loss: 194.00
 ---- batch: 040 ----
mean loss: 198.13
 ---- batch: 050 ----
mean loss: 192.24
 ---- batch: 060 ----
mean loss: 192.73
 ---- batch: 070 ----
mean loss: 191.17
 ---- batch: 080 ----
mean loss: 194.80
 ---- batch: 090 ----
mean loss: 183.92
train mean loss: 192.23
epoch train time: 0:00:00.499003
elapsed time: 0:01:16.820144
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:06:11.858942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.70
 ---- batch: 020 ----
mean loss: 189.95
 ---- batch: 030 ----
mean loss: 188.72
 ---- batch: 040 ----
mean loss: 192.43
 ---- batch: 050 ----
mean loss: 178.98
 ---- batch: 060 ----
mean loss: 195.41
 ---- batch: 070 ----
mean loss: 195.06
 ---- batch: 080 ----
mean loss: 196.70
 ---- batch: 090 ----
mean loss: 199.43
train mean loss: 191.87
epoch train time: 0:00:00.495992
elapsed time: 0:01:17.316300
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:06:12.355098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.11
 ---- batch: 020 ----
mean loss: 192.49
 ---- batch: 030 ----
mean loss: 188.71
 ---- batch: 040 ----
mean loss: 189.53
 ---- batch: 050 ----
mean loss: 187.37
 ---- batch: 060 ----
mean loss: 195.87
 ---- batch: 070 ----
mean loss: 189.53
 ---- batch: 080 ----
mean loss: 190.89
 ---- batch: 090 ----
mean loss: 187.07
train mean loss: 191.18
epoch train time: 0:00:00.503653
elapsed time: 0:01:17.820106
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:06:12.858901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.27
 ---- batch: 020 ----
mean loss: 192.81
 ---- batch: 030 ----
mean loss: 193.92
 ---- batch: 040 ----
mean loss: 189.83
 ---- batch: 050 ----
mean loss: 191.87
 ---- batch: 060 ----
mean loss: 191.25
 ---- batch: 070 ----
mean loss: 193.75
 ---- batch: 080 ----
mean loss: 193.05
 ---- batch: 090 ----
mean loss: 190.07
train mean loss: 190.30
epoch train time: 0:00:00.486761
elapsed time: 0:01:18.307015
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:06:13.345808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.91
 ---- batch: 020 ----
mean loss: 182.01
 ---- batch: 030 ----
mean loss: 181.02
 ---- batch: 040 ----
mean loss: 187.32
 ---- batch: 050 ----
mean loss: 192.63
 ---- batch: 060 ----
mean loss: 198.05
 ---- batch: 070 ----
mean loss: 188.10
 ---- batch: 080 ----
mean loss: 199.04
 ---- batch: 090 ----
mean loss: 186.62
train mean loss: 189.36
epoch train time: 0:00:00.518993
elapsed time: 0:01:18.826155
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:06:13.864950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.71
 ---- batch: 020 ----
mean loss: 192.52
 ---- batch: 030 ----
mean loss: 183.16
 ---- batch: 040 ----
mean loss: 183.08
 ---- batch: 050 ----
mean loss: 191.74
 ---- batch: 060 ----
mean loss: 193.42
 ---- batch: 070 ----
mean loss: 186.23
 ---- batch: 080 ----
mean loss: 191.40
 ---- batch: 090 ----
mean loss: 183.18
train mean loss: 188.14
epoch train time: 0:00:00.493426
elapsed time: 0:01:19.319749
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:06:14.358545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.14
 ---- batch: 020 ----
mean loss: 191.55
 ---- batch: 030 ----
mean loss: 186.50
 ---- batch: 040 ----
mean loss: 182.43
 ---- batch: 050 ----
mean loss: 176.97
 ---- batch: 060 ----
mean loss: 194.52
 ---- batch: 070 ----
mean loss: 186.10
 ---- batch: 080 ----
mean loss: 192.04
 ---- batch: 090 ----
mean loss: 189.24
train mean loss: 187.35
epoch train time: 0:00:00.495414
elapsed time: 0:01:19.815340
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:06:14.854139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.74
 ---- batch: 020 ----
mean loss: 187.21
 ---- batch: 030 ----
mean loss: 184.22
 ---- batch: 040 ----
mean loss: 188.49
 ---- batch: 050 ----
mean loss: 184.97
 ---- batch: 060 ----
mean loss: 193.95
 ---- batch: 070 ----
mean loss: 190.03
 ---- batch: 080 ----
mean loss: 186.94
 ---- batch: 090 ----
mean loss: 179.32
train mean loss: 185.76
epoch train time: 0:00:00.497712
elapsed time: 0:01:20.313208
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:06:15.352021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.44
 ---- batch: 020 ----
mean loss: 175.96
 ---- batch: 030 ----
mean loss: 189.78
 ---- batch: 040 ----
mean loss: 180.17
 ---- batch: 050 ----
mean loss: 182.17
 ---- batch: 060 ----
mean loss: 191.72
 ---- batch: 070 ----
mean loss: 190.04
 ---- batch: 080 ----
mean loss: 188.65
 ---- batch: 090 ----
mean loss: 188.56
train mean loss: 185.94
epoch train time: 0:00:00.503459
elapsed time: 0:01:20.816861
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:06:15.855657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.29
 ---- batch: 020 ----
mean loss: 181.13
 ---- batch: 030 ----
mean loss: 182.34
 ---- batch: 040 ----
mean loss: 180.73
 ---- batch: 050 ----
mean loss: 190.05
 ---- batch: 060 ----
mean loss: 190.76
 ---- batch: 070 ----
mean loss: 182.56
 ---- batch: 080 ----
mean loss: 185.50
 ---- batch: 090 ----
mean loss: 187.93
train mean loss: 184.94
epoch train time: 0:00:00.492304
elapsed time: 0:01:21.309316
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:06:16.348116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.00
 ---- batch: 020 ----
mean loss: 174.85
 ---- batch: 030 ----
mean loss: 182.67
 ---- batch: 040 ----
mean loss: 183.05
 ---- batch: 050 ----
mean loss: 180.17
 ---- batch: 060 ----
mean loss: 186.36
 ---- batch: 070 ----
mean loss: 193.42
 ---- batch: 080 ----
mean loss: 188.08
 ---- batch: 090 ----
mean loss: 181.23
train mean loss: 183.67
epoch train time: 0:00:00.503144
elapsed time: 0:01:21.812614
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:06:16.851421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.88
 ---- batch: 020 ----
mean loss: 182.34
 ---- batch: 030 ----
mean loss: 177.49
 ---- batch: 040 ----
mean loss: 184.44
 ---- batch: 050 ----
mean loss: 181.78
 ---- batch: 060 ----
mean loss: 184.84
 ---- batch: 070 ----
mean loss: 186.59
 ---- batch: 080 ----
mean loss: 187.30
 ---- batch: 090 ----
mean loss: 190.83
train mean loss: 183.23
epoch train time: 0:00:00.489032
elapsed time: 0:01:22.301812
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:06:17.340607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.15
 ---- batch: 020 ----
mean loss: 183.41
 ---- batch: 030 ----
mean loss: 177.40
 ---- batch: 040 ----
mean loss: 177.30
 ---- batch: 050 ----
mean loss: 182.58
 ---- batch: 060 ----
mean loss: 181.93
 ---- batch: 070 ----
mean loss: 189.83
 ---- batch: 080 ----
mean loss: 185.44
 ---- batch: 090 ----
mean loss: 184.15
train mean loss: 182.03
epoch train time: 0:00:00.492516
elapsed time: 0:01:22.794473
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:06:17.833267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.29
 ---- batch: 020 ----
mean loss: 174.90
 ---- batch: 030 ----
mean loss: 178.12
 ---- batch: 040 ----
mean loss: 174.81
 ---- batch: 050 ----
mean loss: 181.32
 ---- batch: 060 ----
mean loss: 183.50
 ---- batch: 070 ----
mean loss: 181.76
 ---- batch: 080 ----
mean loss: 187.08
 ---- batch: 090 ----
mean loss: 183.54
train mean loss: 181.62
epoch train time: 0:00:00.492043
elapsed time: 0:01:23.286731
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:06:18.325531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.40
 ---- batch: 020 ----
mean loss: 177.48
 ---- batch: 030 ----
mean loss: 177.11
 ---- batch: 040 ----
mean loss: 182.10
 ---- batch: 050 ----
mean loss: 183.04
 ---- batch: 060 ----
mean loss: 172.68
 ---- batch: 070 ----
mean loss: 181.29
 ---- batch: 080 ----
mean loss: 182.70
 ---- batch: 090 ----
mean loss: 185.57
train mean loss: 180.88
epoch train time: 0:00:00.497691
elapsed time: 0:01:23.784577
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:06:18.823398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.75
 ---- batch: 020 ----
mean loss: 180.67
 ---- batch: 030 ----
mean loss: 180.82
 ---- batch: 040 ----
mean loss: 180.11
 ---- batch: 050 ----
mean loss: 182.65
 ---- batch: 060 ----
mean loss: 184.73
 ---- batch: 070 ----
mean loss: 181.53
 ---- batch: 080 ----
mean loss: 177.56
 ---- batch: 090 ----
mean loss: 178.65
train mean loss: 180.88
epoch train time: 0:00:00.484836
elapsed time: 0:01:24.269614
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:06:19.308410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.45
 ---- batch: 020 ----
mean loss: 174.12
 ---- batch: 030 ----
mean loss: 183.31
 ---- batch: 040 ----
mean loss: 176.75
 ---- batch: 050 ----
mean loss: 182.19
 ---- batch: 060 ----
mean loss: 179.72
 ---- batch: 070 ----
mean loss: 177.63
 ---- batch: 080 ----
mean loss: 179.13
 ---- batch: 090 ----
mean loss: 184.17
train mean loss: 179.25
epoch train time: 0:00:00.502449
elapsed time: 0:01:24.772226
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:06:19.811038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.64
 ---- batch: 020 ----
mean loss: 180.11
 ---- batch: 030 ----
mean loss: 178.19
 ---- batch: 040 ----
mean loss: 176.07
 ---- batch: 050 ----
mean loss: 176.94
 ---- batch: 060 ----
mean loss: 176.09
 ---- batch: 070 ----
mean loss: 187.44
 ---- batch: 080 ----
mean loss: 168.16
 ---- batch: 090 ----
mean loss: 179.71
train mean loss: 178.27
epoch train time: 0:00:00.489297
elapsed time: 0:01:25.261714
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:06:20.300525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.46
 ---- batch: 020 ----
mean loss: 177.72
 ---- batch: 030 ----
mean loss: 171.98
 ---- batch: 040 ----
mean loss: 180.69
 ---- batch: 050 ----
mean loss: 180.78
 ---- batch: 060 ----
mean loss: 179.61
 ---- batch: 070 ----
mean loss: 171.41
 ---- batch: 080 ----
mean loss: 174.94
 ---- batch: 090 ----
mean loss: 174.61
train mean loss: 177.51
epoch train time: 0:00:00.494463
elapsed time: 0:01:25.756341
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:06:20.795152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.75
 ---- batch: 020 ----
mean loss: 168.37
 ---- batch: 030 ----
mean loss: 185.62
 ---- batch: 040 ----
mean loss: 169.69
 ---- batch: 050 ----
mean loss: 169.73
 ---- batch: 060 ----
mean loss: 179.83
 ---- batch: 070 ----
mean loss: 182.59
 ---- batch: 080 ----
mean loss: 179.39
 ---- batch: 090 ----
mean loss: 182.40
train mean loss: 177.17
epoch train time: 0:00:00.491477
elapsed time: 0:01:26.247983
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:06:21.286793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.88
 ---- batch: 020 ----
mean loss: 179.93
 ---- batch: 030 ----
mean loss: 169.18
 ---- batch: 040 ----
mean loss: 176.62
 ---- batch: 050 ----
mean loss: 184.69
 ---- batch: 060 ----
mean loss: 179.58
 ---- batch: 070 ----
mean loss: 176.78
 ---- batch: 080 ----
mean loss: 178.04
 ---- batch: 090 ----
mean loss: 178.11
train mean loss: 178.11
epoch train time: 0:00:00.496818
elapsed time: 0:01:26.744979
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:06:21.783762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.07
 ---- batch: 020 ----
mean loss: 179.06
 ---- batch: 030 ----
mean loss: 171.36
 ---- batch: 040 ----
mean loss: 176.94
 ---- batch: 050 ----
mean loss: 174.93
 ---- batch: 060 ----
mean loss: 177.34
 ---- batch: 070 ----
mean loss: 178.80
 ---- batch: 080 ----
mean loss: 181.33
 ---- batch: 090 ----
mean loss: 175.72
train mean loss: 175.96
epoch train time: 0:00:00.491366
elapsed time: 0:01:27.236494
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:06:22.275291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.65
 ---- batch: 020 ----
mean loss: 177.87
 ---- batch: 030 ----
mean loss: 167.91
 ---- batch: 040 ----
mean loss: 175.72
 ---- batch: 050 ----
mean loss: 171.50
 ---- batch: 060 ----
mean loss: 178.88
 ---- batch: 070 ----
mean loss: 173.43
 ---- batch: 080 ----
mean loss: 174.12
 ---- batch: 090 ----
mean loss: 172.54
train mean loss: 174.69
epoch train time: 0:00:00.501343
elapsed time: 0:01:27.737989
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:06:22.776803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.14
 ---- batch: 020 ----
mean loss: 173.70
 ---- batch: 030 ----
mean loss: 177.49
 ---- batch: 040 ----
mean loss: 175.55
 ---- batch: 050 ----
mean loss: 170.65
 ---- batch: 060 ----
mean loss: 176.70
 ---- batch: 070 ----
mean loss: 181.11
 ---- batch: 080 ----
mean loss: 175.90
 ---- batch: 090 ----
mean loss: 172.59
train mean loss: 175.37
epoch train time: 0:00:00.499844
elapsed time: 0:01:28.238001
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:06:23.276801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.58
 ---- batch: 020 ----
mean loss: 172.22
 ---- batch: 030 ----
mean loss: 176.52
 ---- batch: 040 ----
mean loss: 176.53
 ---- batch: 050 ----
mean loss: 170.06
 ---- batch: 060 ----
mean loss: 178.53
 ---- batch: 070 ----
mean loss: 175.23
 ---- batch: 080 ----
mean loss: 176.01
 ---- batch: 090 ----
mean loss: 177.98
train mean loss: 175.44
epoch train time: 0:00:00.499277
elapsed time: 0:01:28.737447
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:06:23.776243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.35
 ---- batch: 020 ----
mean loss: 169.38
 ---- batch: 030 ----
mean loss: 179.67
 ---- batch: 040 ----
mean loss: 177.80
 ---- batch: 050 ----
mean loss: 174.75
 ---- batch: 060 ----
mean loss: 172.16
 ---- batch: 070 ----
mean loss: 177.54
 ---- batch: 080 ----
mean loss: 170.43
 ---- batch: 090 ----
mean loss: 174.06
train mean loss: 173.19
epoch train time: 0:00:00.494195
elapsed time: 0:01:29.231793
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:06:24.270590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.66
 ---- batch: 020 ----
mean loss: 176.26
 ---- batch: 030 ----
mean loss: 173.39
 ---- batch: 040 ----
mean loss: 165.95
 ---- batch: 050 ----
mean loss: 170.77
 ---- batch: 060 ----
mean loss: 177.24
 ---- batch: 070 ----
mean loss: 177.94
 ---- batch: 080 ----
mean loss: 172.67
 ---- batch: 090 ----
mean loss: 175.80
train mean loss: 173.47
epoch train time: 0:00:00.500569
elapsed time: 0:01:29.732516
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:06:24.771314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.97
 ---- batch: 020 ----
mean loss: 169.01
 ---- batch: 030 ----
mean loss: 168.02
 ---- batch: 040 ----
mean loss: 175.37
 ---- batch: 050 ----
mean loss: 175.04
 ---- batch: 060 ----
mean loss: 175.57
 ---- batch: 070 ----
mean loss: 167.29
 ---- batch: 080 ----
mean loss: 174.84
 ---- batch: 090 ----
mean loss: 171.17
train mean loss: 172.84
epoch train time: 0:00:00.497464
elapsed time: 0:01:30.230146
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:06:25.268968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.02
 ---- batch: 020 ----
mean loss: 172.28
 ---- batch: 030 ----
mean loss: 167.18
 ---- batch: 040 ----
mean loss: 172.93
 ---- batch: 050 ----
mean loss: 173.87
 ---- batch: 060 ----
mean loss: 179.65
 ---- batch: 070 ----
mean loss: 177.84
 ---- batch: 080 ----
mean loss: 162.89
 ---- batch: 090 ----
mean loss: 178.20
train mean loss: 173.00
epoch train time: 0:00:00.498138
elapsed time: 0:01:30.728484
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:06:25.767291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.21
 ---- batch: 020 ----
mean loss: 169.69
 ---- batch: 030 ----
mean loss: 168.21
 ---- batch: 040 ----
mean loss: 163.49
 ---- batch: 050 ----
mean loss: 176.39
 ---- batch: 060 ----
mean loss: 170.05
 ---- batch: 070 ----
mean loss: 176.23
 ---- batch: 080 ----
mean loss: 174.61
 ---- batch: 090 ----
mean loss: 177.99
train mean loss: 171.85
epoch train time: 0:00:00.494567
elapsed time: 0:01:31.223220
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:06:26.262034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.78
 ---- batch: 020 ----
mean loss: 162.08
 ---- batch: 030 ----
mean loss: 168.17
 ---- batch: 040 ----
mean loss: 168.65
 ---- batch: 050 ----
mean loss: 174.53
 ---- batch: 060 ----
mean loss: 173.69
 ---- batch: 070 ----
mean loss: 175.72
 ---- batch: 080 ----
mean loss: 178.61
 ---- batch: 090 ----
mean loss: 182.24
train mean loss: 171.59
epoch train time: 0:00:00.495318
elapsed time: 0:01:31.718718
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:06:26.757515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.47
 ---- batch: 020 ----
mean loss: 158.40
 ---- batch: 030 ----
mean loss: 170.03
 ---- batch: 040 ----
mean loss: 168.27
 ---- batch: 050 ----
mean loss: 169.88
 ---- batch: 060 ----
mean loss: 171.17
 ---- batch: 070 ----
mean loss: 172.98
 ---- batch: 080 ----
mean loss: 175.91
 ---- batch: 090 ----
mean loss: 183.83
train mean loss: 171.27
epoch train time: 0:00:00.487959
elapsed time: 0:01:32.206822
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:06:27.245646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.94
 ---- batch: 020 ----
mean loss: 165.73
 ---- batch: 030 ----
mean loss: 174.23
 ---- batch: 040 ----
mean loss: 172.76
 ---- batch: 050 ----
mean loss: 174.52
 ---- batch: 060 ----
mean loss: 168.19
 ---- batch: 070 ----
mean loss: 170.52
 ---- batch: 080 ----
mean loss: 171.38
 ---- batch: 090 ----
mean loss: 164.15
train mean loss: 169.88
epoch train time: 0:00:00.491689
elapsed time: 0:01:32.698703
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:06:27.737514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.24
 ---- batch: 020 ----
mean loss: 163.93
 ---- batch: 030 ----
mean loss: 166.47
 ---- batch: 040 ----
mean loss: 171.38
 ---- batch: 050 ----
mean loss: 169.21
 ---- batch: 060 ----
mean loss: 173.91
 ---- batch: 070 ----
mean loss: 170.28
 ---- batch: 080 ----
mean loss: 171.68
 ---- batch: 090 ----
mean loss: 166.34
train mean loss: 169.53
epoch train time: 0:00:00.501134
elapsed time: 0:01:33.200003
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:06:28.238800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.09
 ---- batch: 020 ----
mean loss: 166.92
 ---- batch: 030 ----
mean loss: 167.27
 ---- batch: 040 ----
mean loss: 170.87
 ---- batch: 050 ----
mean loss: 172.35
 ---- batch: 060 ----
mean loss: 170.15
 ---- batch: 070 ----
mean loss: 165.53
 ---- batch: 080 ----
mean loss: 170.12
 ---- batch: 090 ----
mean loss: 169.52
train mean loss: 168.42
epoch train time: 0:00:00.500196
elapsed time: 0:01:33.700346
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:06:28.739141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.60
 ---- batch: 020 ----
mean loss: 162.66
 ---- batch: 030 ----
mean loss: 165.99
 ---- batch: 040 ----
mean loss: 169.00
 ---- batch: 050 ----
mean loss: 171.53
 ---- batch: 060 ----
mean loss: 164.30
 ---- batch: 070 ----
mean loss: 170.04
 ---- batch: 080 ----
mean loss: 178.28
 ---- batch: 090 ----
mean loss: 172.60
train mean loss: 168.12
epoch train time: 0:00:00.487471
elapsed time: 0:01:34.187994
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:06:29.226788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.05
 ---- batch: 020 ----
mean loss: 168.61
 ---- batch: 030 ----
mean loss: 163.35
 ---- batch: 040 ----
mean loss: 170.08
 ---- batch: 050 ----
mean loss: 170.16
 ---- batch: 060 ----
mean loss: 165.59
 ---- batch: 070 ----
mean loss: 166.48
 ---- batch: 080 ----
mean loss: 170.31
 ---- batch: 090 ----
mean loss: 167.11
train mean loss: 167.76
epoch train time: 0:00:00.497605
elapsed time: 0:01:34.685750
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:06:29.724563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.00
 ---- batch: 020 ----
mean loss: 160.18
 ---- batch: 030 ----
mean loss: 162.43
 ---- batch: 040 ----
mean loss: 169.65
 ---- batch: 050 ----
mean loss: 173.80
 ---- batch: 060 ----
mean loss: 163.96
 ---- batch: 070 ----
mean loss: 170.85
 ---- batch: 080 ----
mean loss: 169.70
 ---- batch: 090 ----
mean loss: 169.78
train mean loss: 166.92
epoch train time: 0:00:00.487639
elapsed time: 0:01:35.173551
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:06:30.212363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.27
 ---- batch: 020 ----
mean loss: 166.35
 ---- batch: 030 ----
mean loss: 160.48
 ---- batch: 040 ----
mean loss: 170.02
 ---- batch: 050 ----
mean loss: 167.17
 ---- batch: 060 ----
mean loss: 173.92
 ---- batch: 070 ----
mean loss: 168.64
 ---- batch: 080 ----
mean loss: 166.68
 ---- batch: 090 ----
mean loss: 167.28
train mean loss: 167.43
epoch train time: 0:00:00.489822
elapsed time: 0:01:35.663569
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:06:30.702370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.41
 ---- batch: 020 ----
mean loss: 166.45
 ---- batch: 030 ----
mean loss: 162.93
 ---- batch: 040 ----
mean loss: 163.51
 ---- batch: 050 ----
mean loss: 165.10
 ---- batch: 060 ----
mean loss: 165.31
 ---- batch: 070 ----
mean loss: 169.87
 ---- batch: 080 ----
mean loss: 167.42
 ---- batch: 090 ----
mean loss: 172.99
train mean loss: 166.14
epoch train time: 0:00:00.493829
elapsed time: 0:01:36.157551
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:06:31.196347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.63
 ---- batch: 020 ----
mean loss: 163.44
 ---- batch: 030 ----
mean loss: 162.72
 ---- batch: 040 ----
mean loss: 168.74
 ---- batch: 050 ----
mean loss: 165.42
 ---- batch: 060 ----
mean loss: 169.20
 ---- batch: 070 ----
mean loss: 170.04
 ---- batch: 080 ----
mean loss: 169.12
 ---- batch: 090 ----
mean loss: 173.32
train mean loss: 165.89
epoch train time: 0:00:00.501026
elapsed time: 0:01:36.658772
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:06:31.697586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.65
 ---- batch: 020 ----
mean loss: 159.76
 ---- batch: 030 ----
mean loss: 163.27
 ---- batch: 040 ----
mean loss: 166.55
 ---- batch: 050 ----
mean loss: 172.27
 ---- batch: 060 ----
mean loss: 165.76
 ---- batch: 070 ----
mean loss: 165.71
 ---- batch: 080 ----
mean loss: 164.45
 ---- batch: 090 ----
mean loss: 180.94
train mean loss: 166.35
epoch train time: 0:00:00.495654
elapsed time: 0:01:37.154590
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:06:32.193385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.12
 ---- batch: 020 ----
mean loss: 154.78
 ---- batch: 030 ----
mean loss: 161.31
 ---- batch: 040 ----
mean loss: 161.81
 ---- batch: 050 ----
mean loss: 166.84
 ---- batch: 060 ----
mean loss: 160.84
 ---- batch: 070 ----
mean loss: 169.90
 ---- batch: 080 ----
mean loss: 174.10
 ---- batch: 090 ----
mean loss: 171.44
train mean loss: 164.66
epoch train time: 0:00:00.487482
elapsed time: 0:01:37.642220
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:06:32.681018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.36
 ---- batch: 020 ----
mean loss: 159.85
 ---- batch: 030 ----
mean loss: 160.34
 ---- batch: 040 ----
mean loss: 166.99
 ---- batch: 050 ----
mean loss: 162.68
 ---- batch: 060 ----
mean loss: 157.29
 ---- batch: 070 ----
mean loss: 164.71
 ---- batch: 080 ----
mean loss: 168.66
 ---- batch: 090 ----
mean loss: 172.61
train mean loss: 164.54
epoch train time: 0:00:00.488774
elapsed time: 0:01:38.131144
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:06:33.169940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.59
 ---- batch: 020 ----
mean loss: 163.06
 ---- batch: 030 ----
mean loss: 164.85
 ---- batch: 040 ----
mean loss: 160.62
 ---- batch: 050 ----
mean loss: 162.73
 ---- batch: 060 ----
mean loss: 162.03
 ---- batch: 070 ----
mean loss: 167.35
 ---- batch: 080 ----
mean loss: 164.82
 ---- batch: 090 ----
mean loss: 171.53
train mean loss: 163.94
epoch train time: 0:00:00.489009
elapsed time: 0:01:38.620302
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:06:33.659116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.26
 ---- batch: 020 ----
mean loss: 157.36
 ---- batch: 030 ----
mean loss: 169.50
 ---- batch: 040 ----
mean loss: 158.86
 ---- batch: 050 ----
mean loss: 160.74
 ---- batch: 060 ----
mean loss: 165.03
 ---- batch: 070 ----
mean loss: 164.96
 ---- batch: 080 ----
mean loss: 171.49
 ---- batch: 090 ----
mean loss: 161.50
train mean loss: 164.52
epoch train time: 0:00:00.508173
elapsed time: 0:01:39.128644
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:06:34.167451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.54
 ---- batch: 020 ----
mean loss: 164.23
 ---- batch: 030 ----
mean loss: 159.37
 ---- batch: 040 ----
mean loss: 158.56
 ---- batch: 050 ----
mean loss: 154.57
 ---- batch: 060 ----
mean loss: 165.06
 ---- batch: 070 ----
mean loss: 166.07
 ---- batch: 080 ----
mean loss: 164.12
 ---- batch: 090 ----
mean loss: 168.90
train mean loss: 163.27
epoch train time: 0:00:00.497868
elapsed time: 0:01:39.626675
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:06:34.665472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.49
 ---- batch: 020 ----
mean loss: 151.64
 ---- batch: 030 ----
mean loss: 161.02
 ---- batch: 040 ----
mean loss: 161.52
 ---- batch: 050 ----
mean loss: 161.44
 ---- batch: 060 ----
mean loss: 159.02
 ---- batch: 070 ----
mean loss: 166.42
 ---- batch: 080 ----
mean loss: 171.38
 ---- batch: 090 ----
mean loss: 169.01
train mean loss: 162.85
epoch train time: 0:00:00.500519
elapsed time: 0:01:40.127357
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:06:35.166153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.34
 ---- batch: 020 ----
mean loss: 161.92
 ---- batch: 030 ----
mean loss: 158.67
 ---- batch: 040 ----
mean loss: 160.24
 ---- batch: 050 ----
mean loss: 160.15
 ---- batch: 060 ----
mean loss: 166.66
 ---- batch: 070 ----
mean loss: 166.54
 ---- batch: 080 ----
mean loss: 166.78
 ---- batch: 090 ----
mean loss: 158.22
train mean loss: 161.87
epoch train time: 0:00:00.503662
elapsed time: 0:01:40.631172
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:06:35.669968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.85
 ---- batch: 020 ----
mean loss: 158.39
 ---- batch: 030 ----
mean loss: 157.99
 ---- batch: 040 ----
mean loss: 161.22
 ---- batch: 050 ----
mean loss: 155.25
 ---- batch: 060 ----
mean loss: 158.46
 ---- batch: 070 ----
mean loss: 160.95
 ---- batch: 080 ----
mean loss: 172.90
 ---- batch: 090 ----
mean loss: 168.56
train mean loss: 162.28
epoch train time: 0:00:00.513146
elapsed time: 0:01:41.144474
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:06:36.183271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.80
 ---- batch: 020 ----
mean loss: 158.17
 ---- batch: 030 ----
mean loss: 154.33
 ---- batch: 040 ----
mean loss: 165.30
 ---- batch: 050 ----
mean loss: 164.36
 ---- batch: 060 ----
mean loss: 163.26
 ---- batch: 070 ----
mean loss: 163.74
 ---- batch: 080 ----
mean loss: 167.35
 ---- batch: 090 ----
mean loss: 164.18
train mean loss: 161.95
epoch train time: 0:00:00.506684
elapsed time: 0:01:41.651312
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:06:36.690113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.68
 ---- batch: 020 ----
mean loss: 158.77
 ---- batch: 030 ----
mean loss: 157.77
 ---- batch: 040 ----
mean loss: 168.51
 ---- batch: 050 ----
mean loss: 156.60
 ---- batch: 060 ----
mean loss: 168.77
 ---- batch: 070 ----
mean loss: 160.66
 ---- batch: 080 ----
mean loss: 159.19
 ---- batch: 090 ----
mean loss: 163.66
train mean loss: 160.98
epoch train time: 0:00:00.504230
elapsed time: 0:01:42.155718
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:06:37.194506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.31
 ---- batch: 020 ----
mean loss: 154.56
 ---- batch: 030 ----
mean loss: 163.24
 ---- batch: 040 ----
mean loss: 159.56
 ---- batch: 050 ----
mean loss: 162.96
 ---- batch: 060 ----
mean loss: 156.36
 ---- batch: 070 ----
mean loss: 162.00
 ---- batch: 080 ----
mean loss: 165.30
 ---- batch: 090 ----
mean loss: 169.06
train mean loss: 160.84
epoch train time: 0:00:00.492353
elapsed time: 0:01:42.648242
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:06:37.687046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.44
 ---- batch: 020 ----
mean loss: 153.19
 ---- batch: 030 ----
mean loss: 163.01
 ---- batch: 040 ----
mean loss: 160.60
 ---- batch: 050 ----
mean loss: 155.30
 ---- batch: 060 ----
mean loss: 162.26
 ---- batch: 070 ----
mean loss: 167.45
 ---- batch: 080 ----
mean loss: 168.92
 ---- batch: 090 ----
mean loss: 161.42
train mean loss: 160.45
epoch train time: 0:00:00.497572
elapsed time: 0:01:43.145972
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:06:38.184783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.47
 ---- batch: 020 ----
mean loss: 162.53
 ---- batch: 030 ----
mean loss: 160.99
 ---- batch: 040 ----
mean loss: 157.60
 ---- batch: 050 ----
mean loss: 160.30
 ---- batch: 060 ----
mean loss: 159.12
 ---- batch: 070 ----
mean loss: 158.00
 ---- batch: 080 ----
mean loss: 160.09
 ---- batch: 090 ----
mean loss: 164.96
train mean loss: 160.19
epoch train time: 0:00:00.494585
elapsed time: 0:01:43.640732
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:06:38.679529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.02
 ---- batch: 020 ----
mean loss: 155.74
 ---- batch: 030 ----
mean loss: 166.12
 ---- batch: 040 ----
mean loss: 149.77
 ---- batch: 050 ----
mean loss: 163.75
 ---- batch: 060 ----
mean loss: 155.67
 ---- batch: 070 ----
mean loss: 168.36
 ---- batch: 080 ----
mean loss: 155.19
 ---- batch: 090 ----
mean loss: 165.05
train mean loss: 159.51
epoch train time: 0:00:00.502730
elapsed time: 0:01:44.143662
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:06:39.182476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.74
 ---- batch: 020 ----
mean loss: 154.84
 ---- batch: 030 ----
mean loss: 159.43
 ---- batch: 040 ----
mean loss: 163.77
 ---- batch: 050 ----
mean loss: 161.97
 ---- batch: 060 ----
mean loss: 160.49
 ---- batch: 070 ----
mean loss: 163.54
 ---- batch: 080 ----
mean loss: 158.82
 ---- batch: 090 ----
mean loss: 158.03
train mean loss: 159.26
epoch train time: 0:00:00.496679
elapsed time: 0:01:44.640511
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:06:39.679306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.54
 ---- batch: 020 ----
mean loss: 158.24
 ---- batch: 030 ----
mean loss: 152.64
 ---- batch: 040 ----
mean loss: 164.59
 ---- batch: 050 ----
mean loss: 153.72
 ---- batch: 060 ----
mean loss: 157.57
 ---- batch: 070 ----
mean loss: 162.56
 ---- batch: 080 ----
mean loss: 151.52
 ---- batch: 090 ----
mean loss: 162.21
train mean loss: 158.61
epoch train time: 0:00:00.507128
elapsed time: 0:01:45.147821
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:06:40.186615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.73
 ---- batch: 020 ----
mean loss: 163.78
 ---- batch: 030 ----
mean loss: 163.80
 ---- batch: 040 ----
mean loss: 157.64
 ---- batch: 050 ----
mean loss: 153.93
 ---- batch: 060 ----
mean loss: 162.07
 ---- batch: 070 ----
mean loss: 160.29
 ---- batch: 080 ----
mean loss: 159.42
 ---- batch: 090 ----
mean loss: 157.66
train mean loss: 158.40
epoch train time: 0:00:00.496131
elapsed time: 0:01:45.644102
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:06:40.682907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.00
 ---- batch: 020 ----
mean loss: 159.07
 ---- batch: 030 ----
mean loss: 150.85
 ---- batch: 040 ----
mean loss: 158.75
 ---- batch: 050 ----
mean loss: 166.03
 ---- batch: 060 ----
mean loss: 164.42
 ---- batch: 070 ----
mean loss: 158.93
 ---- batch: 080 ----
mean loss: 161.21
 ---- batch: 090 ----
mean loss: 163.82
train mean loss: 159.11
epoch train time: 0:00:00.491995
elapsed time: 0:01:46.136260
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:06:41.175087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.74
 ---- batch: 020 ----
mean loss: 152.47
 ---- batch: 030 ----
mean loss: 159.28
 ---- batch: 040 ----
mean loss: 158.43
 ---- batch: 050 ----
mean loss: 159.84
 ---- batch: 060 ----
mean loss: 160.90
 ---- batch: 070 ----
mean loss: 156.94
 ---- batch: 080 ----
mean loss: 160.40
 ---- batch: 090 ----
mean loss: 157.32
train mean loss: 158.30
epoch train time: 0:00:00.491777
elapsed time: 0:01:46.628219
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:06:41.667017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.04
 ---- batch: 020 ----
mean loss: 158.81
 ---- batch: 030 ----
mean loss: 153.82
 ---- batch: 040 ----
mean loss: 154.10
 ---- batch: 050 ----
mean loss: 159.03
 ---- batch: 060 ----
mean loss: 160.33
 ---- batch: 070 ----
mean loss: 155.68
 ---- batch: 080 ----
mean loss: 165.84
 ---- batch: 090 ----
mean loss: 160.35
train mean loss: 157.97
epoch train time: 0:00:00.497897
elapsed time: 0:01:47.126281
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:06:42.165077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.32
 ---- batch: 020 ----
mean loss: 151.75
 ---- batch: 030 ----
mean loss: 154.54
 ---- batch: 040 ----
mean loss: 159.38
 ---- batch: 050 ----
mean loss: 150.61
 ---- batch: 060 ----
mean loss: 157.11
 ---- batch: 070 ----
mean loss: 161.80
 ---- batch: 080 ----
mean loss: 160.89
 ---- batch: 090 ----
mean loss: 164.90
train mean loss: 157.47
epoch train time: 0:00:00.492114
elapsed time: 0:01:47.618560
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:06:42.657344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.50
 ---- batch: 020 ----
mean loss: 154.82
 ---- batch: 030 ----
mean loss: 152.85
 ---- batch: 040 ----
mean loss: 154.43
 ---- batch: 050 ----
mean loss: 157.69
 ---- batch: 060 ----
mean loss: 159.55
 ---- batch: 070 ----
mean loss: 164.37
 ---- batch: 080 ----
mean loss: 153.06
 ---- batch: 090 ----
mean loss: 159.72
train mean loss: 157.03
epoch train time: 0:00:00.497190
elapsed time: 0:01:48.115887
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:06:43.154683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.17
 ---- batch: 020 ----
mean loss: 160.35
 ---- batch: 030 ----
mean loss: 150.55
 ---- batch: 040 ----
mean loss: 161.66
 ---- batch: 050 ----
mean loss: 145.25
 ---- batch: 060 ----
mean loss: 158.49
 ---- batch: 070 ----
mean loss: 148.14
 ---- batch: 080 ----
mean loss: 167.57
 ---- batch: 090 ----
mean loss: 159.72
train mean loss: 156.59
epoch train time: 0:00:00.484490
elapsed time: 0:01:48.600540
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:06:43.639351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.21
 ---- batch: 020 ----
mean loss: 153.68
 ---- batch: 030 ----
mean loss: 152.35
 ---- batch: 040 ----
mean loss: 157.58
 ---- batch: 050 ----
mean loss: 154.12
 ---- batch: 060 ----
mean loss: 158.19
 ---- batch: 070 ----
mean loss: 153.05
 ---- batch: 080 ----
mean loss: 155.62
 ---- batch: 090 ----
mean loss: 163.97
train mean loss: 156.22
epoch train time: 0:00:00.491855
elapsed time: 0:01:49.092559
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:06:44.131355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.15
 ---- batch: 020 ----
mean loss: 156.40
 ---- batch: 030 ----
mean loss: 160.34
 ---- batch: 040 ----
mean loss: 155.02
 ---- batch: 050 ----
mean loss: 152.36
 ---- batch: 060 ----
mean loss: 153.74
 ---- batch: 070 ----
mean loss: 159.37
 ---- batch: 080 ----
mean loss: 151.90
 ---- batch: 090 ----
mean loss: 157.65
train mean loss: 154.93
epoch train time: 0:00:00.482855
elapsed time: 0:01:49.575563
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:06:44.614357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.15
 ---- batch: 020 ----
mean loss: 150.16
 ---- batch: 030 ----
mean loss: 156.47
 ---- batch: 040 ----
mean loss: 159.79
 ---- batch: 050 ----
mean loss: 153.09
 ---- batch: 060 ----
mean loss: 151.87
 ---- batch: 070 ----
mean loss: 160.57
 ---- batch: 080 ----
mean loss: 151.41
 ---- batch: 090 ----
mean loss: 158.42
train mean loss: 155.53
epoch train time: 0:00:00.489991
elapsed time: 0:01:50.065702
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:06:45.104536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.50
 ---- batch: 020 ----
mean loss: 152.62
 ---- batch: 030 ----
mean loss: 149.85
 ---- batch: 040 ----
mean loss: 154.33
 ---- batch: 050 ----
mean loss: 157.52
 ---- batch: 060 ----
mean loss: 162.10
 ---- batch: 070 ----
mean loss: 160.46
 ---- batch: 080 ----
mean loss: 158.39
 ---- batch: 090 ----
mean loss: 159.22
train mean loss: 155.95
epoch train time: 0:00:00.489806
elapsed time: 0:01:50.555710
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:06:45.594506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.53
 ---- batch: 020 ----
mean loss: 154.64
 ---- batch: 030 ----
mean loss: 151.23
 ---- batch: 040 ----
mean loss: 152.12
 ---- batch: 050 ----
mean loss: 155.01
 ---- batch: 060 ----
mean loss: 163.03
 ---- batch: 070 ----
mean loss: 154.60
 ---- batch: 080 ----
mean loss: 152.85
 ---- batch: 090 ----
mean loss: 156.57
train mean loss: 154.60
epoch train time: 0:00:00.508640
elapsed time: 0:01:51.064496
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:06:46.103291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.99
 ---- batch: 020 ----
mean loss: 150.45
 ---- batch: 030 ----
mean loss: 160.03
 ---- batch: 040 ----
mean loss: 157.38
 ---- batch: 050 ----
mean loss: 146.46
 ---- batch: 060 ----
mean loss: 155.56
 ---- batch: 070 ----
mean loss: 157.53
 ---- batch: 080 ----
mean loss: 162.56
 ---- batch: 090 ----
mean loss: 152.33
train mean loss: 154.73
epoch train time: 0:00:00.497713
elapsed time: 0:01:51.562358
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:06:46.601167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.09
 ---- batch: 020 ----
mean loss: 155.44
 ---- batch: 030 ----
mean loss: 149.30
 ---- batch: 040 ----
mean loss: 152.35
 ---- batch: 050 ----
mean loss: 159.38
 ---- batch: 060 ----
mean loss: 155.52
 ---- batch: 070 ----
mean loss: 149.81
 ---- batch: 080 ----
mean loss: 154.36
 ---- batch: 090 ----
mean loss: 155.93
train mean loss: 154.18
epoch train time: 0:00:00.500110
elapsed time: 0:01:52.062633
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:06:47.101430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.82
 ---- batch: 020 ----
mean loss: 146.29
 ---- batch: 030 ----
mean loss: 156.17
 ---- batch: 040 ----
mean loss: 155.16
 ---- batch: 050 ----
mean loss: 145.85
 ---- batch: 060 ----
mean loss: 149.52
 ---- batch: 070 ----
mean loss: 154.81
 ---- batch: 080 ----
mean loss: 157.89
 ---- batch: 090 ----
mean loss: 155.59
train mean loss: 153.32
epoch train time: 0:00:00.492509
elapsed time: 0:01:52.555293
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:06:47.594087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.18
 ---- batch: 020 ----
mean loss: 145.61
 ---- batch: 030 ----
mean loss: 150.91
 ---- batch: 040 ----
mean loss: 159.08
 ---- batch: 050 ----
mean loss: 162.87
 ---- batch: 060 ----
mean loss: 154.89
 ---- batch: 070 ----
mean loss: 161.42
 ---- batch: 080 ----
mean loss: 151.99
 ---- batch: 090 ----
mean loss: 155.53
train mean loss: 153.59
epoch train time: 0:00:00.496462
elapsed time: 0:01:53.051915
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:06:48.090717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.05
 ---- batch: 020 ----
mean loss: 149.21
 ---- batch: 030 ----
mean loss: 153.44
 ---- batch: 040 ----
mean loss: 154.79
 ---- batch: 050 ----
mean loss: 151.97
 ---- batch: 060 ----
mean loss: 155.15
 ---- batch: 070 ----
mean loss: 156.82
 ---- batch: 080 ----
mean loss: 158.74
 ---- batch: 090 ----
mean loss: 150.29
train mean loss: 153.12
epoch train time: 0:00:00.489090
elapsed time: 0:01:53.541162
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:06:48.579957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.76
 ---- batch: 020 ----
mean loss: 145.15
 ---- batch: 030 ----
mean loss: 150.59
 ---- batch: 040 ----
mean loss: 151.45
 ---- batch: 050 ----
mean loss: 149.87
 ---- batch: 060 ----
mean loss: 151.49
 ---- batch: 070 ----
mean loss: 160.61
 ---- batch: 080 ----
mean loss: 149.84
 ---- batch: 090 ----
mean loss: 155.00
train mean loss: 152.62
epoch train time: 0:00:00.493694
elapsed time: 0:01:54.035004
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:06:49.073834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.83
 ---- batch: 020 ----
mean loss: 157.02
 ---- batch: 030 ----
mean loss: 158.68
 ---- batch: 040 ----
mean loss: 158.20
 ---- batch: 050 ----
mean loss: 147.62
 ---- batch: 060 ----
mean loss: 151.49
 ---- batch: 070 ----
mean loss: 152.83
 ---- batch: 080 ----
mean loss: 156.35
 ---- batch: 090 ----
mean loss: 150.34
train mean loss: 152.42
epoch train time: 0:00:00.489186
elapsed time: 0:01:54.524441
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:06:49.563263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.62
 ---- batch: 020 ----
mean loss: 146.93
 ---- batch: 030 ----
mean loss: 151.23
 ---- batch: 040 ----
mean loss: 153.93
 ---- batch: 050 ----
mean loss: 152.84
 ---- batch: 060 ----
mean loss: 156.35
 ---- batch: 070 ----
mean loss: 153.20
 ---- batch: 080 ----
mean loss: 147.95
 ---- batch: 090 ----
mean loss: 153.24
train mean loss: 152.31
epoch train time: 0:00:00.506890
elapsed time: 0:01:55.031537
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:06:50.070334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.77
 ---- batch: 020 ----
mean loss: 147.51
 ---- batch: 030 ----
mean loss: 148.25
 ---- batch: 040 ----
mean loss: 145.49
 ---- batch: 050 ----
mean loss: 153.12
 ---- batch: 060 ----
mean loss: 155.76
 ---- batch: 070 ----
mean loss: 154.04
 ---- batch: 080 ----
mean loss: 153.42
 ---- batch: 090 ----
mean loss: 156.17
train mean loss: 151.84
epoch train time: 0:00:00.499430
elapsed time: 0:01:55.531118
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:06:50.569939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.39
 ---- batch: 020 ----
mean loss: 152.90
 ---- batch: 030 ----
mean loss: 145.47
 ---- batch: 040 ----
mean loss: 158.21
 ---- batch: 050 ----
mean loss: 152.78
 ---- batch: 060 ----
mean loss: 152.47
 ---- batch: 070 ----
mean loss: 151.67
 ---- batch: 080 ----
mean loss: 146.79
 ---- batch: 090 ----
mean loss: 153.74
train mean loss: 151.28
epoch train time: 0:00:00.500913
elapsed time: 0:01:56.032217
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:06:51.071017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.92
 ---- batch: 020 ----
mean loss: 150.53
 ---- batch: 030 ----
mean loss: 152.70
 ---- batch: 040 ----
mean loss: 152.35
 ---- batch: 050 ----
mean loss: 148.93
 ---- batch: 060 ----
mean loss: 152.47
 ---- batch: 070 ----
mean loss: 146.55
 ---- batch: 080 ----
mean loss: 150.96
 ---- batch: 090 ----
mean loss: 153.92
train mean loss: 151.52
epoch train time: 0:00:00.496611
elapsed time: 0:01:56.528991
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:06:51.567786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.94
 ---- batch: 020 ----
mean loss: 147.59
 ---- batch: 030 ----
mean loss: 157.35
 ---- batch: 040 ----
mean loss: 140.05
 ---- batch: 050 ----
mean loss: 148.20
 ---- batch: 060 ----
mean loss: 146.97
 ---- batch: 070 ----
mean loss: 154.51
 ---- batch: 080 ----
mean loss: 156.55
 ---- batch: 090 ----
mean loss: 153.64
train mean loss: 151.06
epoch train time: 0:00:00.497646
elapsed time: 0:01:57.026796
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:06:52.065591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.48
 ---- batch: 020 ----
mean loss: 151.21
 ---- batch: 030 ----
mean loss: 145.65
 ---- batch: 040 ----
mean loss: 147.23
 ---- batch: 050 ----
mean loss: 153.20
 ---- batch: 060 ----
mean loss: 158.16
 ---- batch: 070 ----
mean loss: 149.58
 ---- batch: 080 ----
mean loss: 155.54
 ---- batch: 090 ----
mean loss: 152.38
train mean loss: 151.52
epoch train time: 0:00:00.496230
elapsed time: 0:01:57.523173
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:06:52.561987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.01
 ---- batch: 020 ----
mean loss: 146.80
 ---- batch: 030 ----
mean loss: 147.79
 ---- batch: 040 ----
mean loss: 153.84
 ---- batch: 050 ----
mean loss: 146.31
 ---- batch: 060 ----
mean loss: 149.45
 ---- batch: 070 ----
mean loss: 151.88
 ---- batch: 080 ----
mean loss: 156.95
 ---- batch: 090 ----
mean loss: 153.55
train mean loss: 150.16
epoch train time: 0:00:00.502186
elapsed time: 0:01:58.025525
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:06:53.064320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.03
 ---- batch: 020 ----
mean loss: 152.66
 ---- batch: 030 ----
mean loss: 152.08
 ---- batch: 040 ----
mean loss: 149.34
 ---- batch: 050 ----
mean loss: 153.78
 ---- batch: 060 ----
mean loss: 148.53
 ---- batch: 070 ----
mean loss: 150.03
 ---- batch: 080 ----
mean loss: 150.19
 ---- batch: 090 ----
mean loss: 147.33
train mean loss: 150.34
epoch train time: 0:00:00.496595
elapsed time: 0:01:58.522265
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:06:53.561064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.07
 ---- batch: 020 ----
mean loss: 141.00
 ---- batch: 030 ----
mean loss: 147.03
 ---- batch: 040 ----
mean loss: 150.68
 ---- batch: 050 ----
mean loss: 147.26
 ---- batch: 060 ----
mean loss: 151.78
 ---- batch: 070 ----
mean loss: 155.05
 ---- batch: 080 ----
mean loss: 150.27
 ---- batch: 090 ----
mean loss: 158.73
train mean loss: 149.77
epoch train time: 0:00:00.502629
elapsed time: 0:01:59.025052
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:06:54.063846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.86
 ---- batch: 020 ----
mean loss: 145.44
 ---- batch: 030 ----
mean loss: 146.84
 ---- batch: 040 ----
mean loss: 152.00
 ---- batch: 050 ----
mean loss: 151.26
 ---- batch: 060 ----
mean loss: 145.42
 ---- batch: 070 ----
mean loss: 151.92
 ---- batch: 080 ----
mean loss: 155.83
 ---- batch: 090 ----
mean loss: 149.18
train mean loss: 149.45
epoch train time: 0:00:00.492609
elapsed time: 0:01:59.517809
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:06:54.556606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.73
 ---- batch: 020 ----
mean loss: 147.14
 ---- batch: 030 ----
mean loss: 153.78
 ---- batch: 040 ----
mean loss: 142.67
 ---- batch: 050 ----
mean loss: 144.05
 ---- batch: 060 ----
mean loss: 155.26
 ---- batch: 070 ----
mean loss: 153.60
 ---- batch: 080 ----
mean loss: 150.67
 ---- batch: 090 ----
mean loss: 148.87
train mean loss: 148.89
epoch train time: 0:00:00.510994
elapsed time: 0:02:00.029015
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:06:55.067846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.23
 ---- batch: 020 ----
mean loss: 151.28
 ---- batch: 030 ----
mean loss: 151.74
 ---- batch: 040 ----
mean loss: 143.57
 ---- batch: 050 ----
mean loss: 148.97
 ---- batch: 060 ----
mean loss: 150.48
 ---- batch: 070 ----
mean loss: 147.77
 ---- batch: 080 ----
mean loss: 151.23
 ---- batch: 090 ----
mean loss: 149.45
train mean loss: 149.10
epoch train time: 0:00:00.497316
elapsed time: 0:02:00.526523
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:06:55.565307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.33
 ---- batch: 020 ----
mean loss: 141.39
 ---- batch: 030 ----
mean loss: 150.68
 ---- batch: 040 ----
mean loss: 153.23
 ---- batch: 050 ----
mean loss: 146.62
 ---- batch: 060 ----
mean loss: 152.33
 ---- batch: 070 ----
mean loss: 150.78
 ---- batch: 080 ----
mean loss: 151.77
 ---- batch: 090 ----
mean loss: 146.43
train mean loss: 148.77
epoch train time: 0:00:00.497425
elapsed time: 0:02:01.024096
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:06:56.062910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.77
 ---- batch: 020 ----
mean loss: 144.38
 ---- batch: 030 ----
mean loss: 142.82
 ---- batch: 040 ----
mean loss: 152.35
 ---- batch: 050 ----
mean loss: 153.22
 ---- batch: 060 ----
mean loss: 156.46
 ---- batch: 070 ----
mean loss: 147.57
 ---- batch: 080 ----
mean loss: 146.63
 ---- batch: 090 ----
mean loss: 156.05
train mean loss: 149.42
epoch train time: 0:00:00.497597
elapsed time: 0:02:01.521856
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:06:56.560684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.55
 ---- batch: 020 ----
mean loss: 141.27
 ---- batch: 030 ----
mean loss: 152.47
 ---- batch: 040 ----
mean loss: 144.11
 ---- batch: 050 ----
mean loss: 143.93
 ---- batch: 060 ----
mean loss: 149.53
 ---- batch: 070 ----
mean loss: 150.06
 ---- batch: 080 ----
mean loss: 148.74
 ---- batch: 090 ----
mean loss: 146.11
train mean loss: 147.07
epoch train time: 0:00:00.490101
elapsed time: 0:02:02.012136
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:06:57.050941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.66
 ---- batch: 020 ----
mean loss: 143.61
 ---- batch: 030 ----
mean loss: 147.95
 ---- batch: 040 ----
mean loss: 148.86
 ---- batch: 050 ----
mean loss: 152.51
 ---- batch: 060 ----
mean loss: 151.47
 ---- batch: 070 ----
mean loss: 144.66
 ---- batch: 080 ----
mean loss: 145.24
 ---- batch: 090 ----
mean loss: 144.08
train mean loss: 147.62
epoch train time: 0:00:00.487893
elapsed time: 0:02:02.500193
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:06:57.539005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.98
 ---- batch: 020 ----
mean loss: 145.25
 ---- batch: 030 ----
mean loss: 148.72
 ---- batch: 040 ----
mean loss: 149.87
 ---- batch: 050 ----
mean loss: 148.75
 ---- batch: 060 ----
mean loss: 147.21
 ---- batch: 070 ----
mean loss: 142.66
 ---- batch: 080 ----
mean loss: 148.39
 ---- batch: 090 ----
mean loss: 151.49
train mean loss: 146.99
epoch train time: 0:00:00.506254
elapsed time: 0:02:03.006623
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:06:58.045438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.37
 ---- batch: 020 ----
mean loss: 146.48
 ---- batch: 030 ----
mean loss: 146.58
 ---- batch: 040 ----
mean loss: 150.47
 ---- batch: 050 ----
mean loss: 146.25
 ---- batch: 060 ----
mean loss: 144.12
 ---- batch: 070 ----
mean loss: 150.07
 ---- batch: 080 ----
mean loss: 144.40
 ---- batch: 090 ----
mean loss: 151.52
train mean loss: 147.07
epoch train time: 0:00:00.502329
elapsed time: 0:02:03.509117
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:06:58.547910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.65
 ---- batch: 020 ----
mean loss: 143.94
 ---- batch: 030 ----
mean loss: 143.24
 ---- batch: 040 ----
mean loss: 148.59
 ---- batch: 050 ----
mean loss: 144.70
 ---- batch: 060 ----
mean loss: 151.35
 ---- batch: 070 ----
mean loss: 146.96
 ---- batch: 080 ----
mean loss: 149.72
 ---- batch: 090 ----
mean loss: 150.96
train mean loss: 146.62
epoch train time: 0:00:00.496434
elapsed time: 0:02:04.005713
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:06:59.044525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.27
 ---- batch: 020 ----
mean loss: 144.99
 ---- batch: 030 ----
mean loss: 148.00
 ---- batch: 040 ----
mean loss: 147.26
 ---- batch: 050 ----
mean loss: 146.47
 ---- batch: 060 ----
mean loss: 145.78
 ---- batch: 070 ----
mean loss: 141.68
 ---- batch: 080 ----
mean loss: 147.03
 ---- batch: 090 ----
mean loss: 145.00
train mean loss: 146.14
epoch train time: 0:00:00.499990
elapsed time: 0:02:04.505887
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:06:59.544683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.15
 ---- batch: 020 ----
mean loss: 142.36
 ---- batch: 030 ----
mean loss: 144.27
 ---- batch: 040 ----
mean loss: 143.65
 ---- batch: 050 ----
mean loss: 146.25
 ---- batch: 060 ----
mean loss: 152.16
 ---- batch: 070 ----
mean loss: 145.60
 ---- batch: 080 ----
mean loss: 145.67
 ---- batch: 090 ----
mean loss: 146.52
train mean loss: 146.58
epoch train time: 0:00:00.499409
elapsed time: 0:02:05.005440
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:07:00.044251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.07
 ---- batch: 020 ----
mean loss: 146.23
 ---- batch: 030 ----
mean loss: 141.88
 ---- batch: 040 ----
mean loss: 151.90
 ---- batch: 050 ----
mean loss: 150.26
 ---- batch: 060 ----
mean loss: 151.94
 ---- batch: 070 ----
mean loss: 143.55
 ---- batch: 080 ----
mean loss: 141.02
 ---- batch: 090 ----
mean loss: 149.08
train mean loss: 146.60
epoch train time: 0:00:00.498470
elapsed time: 0:02:05.504095
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:07:00.542892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.80
 ---- batch: 020 ----
mean loss: 138.57
 ---- batch: 030 ----
mean loss: 142.16
 ---- batch: 040 ----
mean loss: 149.84
 ---- batch: 050 ----
mean loss: 154.99
 ---- batch: 060 ----
mean loss: 151.46
 ---- batch: 070 ----
mean loss: 150.10
 ---- batch: 080 ----
mean loss: 145.06
 ---- batch: 090 ----
mean loss: 148.77
train mean loss: 147.03
epoch train time: 0:00:00.503334
elapsed time: 0:02:06.007581
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:07:01.046393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.09
 ---- batch: 020 ----
mean loss: 141.66
 ---- batch: 030 ----
mean loss: 147.16
 ---- batch: 040 ----
mean loss: 143.74
 ---- batch: 050 ----
mean loss: 146.42
 ---- batch: 060 ----
mean loss: 152.29
 ---- batch: 070 ----
mean loss: 157.13
 ---- batch: 080 ----
mean loss: 135.57
 ---- batch: 090 ----
mean loss: 151.12
train mean loss: 146.62
epoch train time: 0:00:00.507619
elapsed time: 0:02:06.515374
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:07:01.554173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.01
 ---- batch: 020 ----
mean loss: 145.81
 ---- batch: 030 ----
mean loss: 141.92
 ---- batch: 040 ----
mean loss: 145.78
 ---- batch: 050 ----
mean loss: 148.89
 ---- batch: 060 ----
mean loss: 148.28
 ---- batch: 070 ----
mean loss: 141.10
 ---- batch: 080 ----
mean loss: 153.50
 ---- batch: 090 ----
mean loss: 148.85
train mean loss: 146.26
epoch train time: 0:00:00.503586
elapsed time: 0:02:07.019174
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:07:02.057976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.23
 ---- batch: 020 ----
mean loss: 144.80
 ---- batch: 030 ----
mean loss: 139.93
 ---- batch: 040 ----
mean loss: 144.61
 ---- batch: 050 ----
mean loss: 147.57
 ---- batch: 060 ----
mean loss: 146.64
 ---- batch: 070 ----
mean loss: 149.07
 ---- batch: 080 ----
mean loss: 146.72
 ---- batch: 090 ----
mean loss: 145.87
train mean loss: 145.28
epoch train time: 0:00:00.488281
elapsed time: 0:02:07.507610
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:07:02.546406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.78
 ---- batch: 020 ----
mean loss: 139.95
 ---- batch: 030 ----
mean loss: 148.28
 ---- batch: 040 ----
mean loss: 142.86
 ---- batch: 050 ----
mean loss: 142.96
 ---- batch: 060 ----
mean loss: 143.31
 ---- batch: 070 ----
mean loss: 143.25
 ---- batch: 080 ----
mean loss: 154.17
 ---- batch: 090 ----
mean loss: 151.49
train mean loss: 145.01
epoch train time: 0:00:00.500077
elapsed time: 0:02:08.007850
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:07:03.046678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.10
 ---- batch: 020 ----
mean loss: 140.87
 ---- batch: 030 ----
mean loss: 148.63
 ---- batch: 040 ----
mean loss: 145.66
 ---- batch: 050 ----
mean loss: 145.48
 ---- batch: 060 ----
mean loss: 147.97
 ---- batch: 070 ----
mean loss: 147.05
 ---- batch: 080 ----
mean loss: 148.65
 ---- batch: 090 ----
mean loss: 137.12
train mean loss: 144.66
epoch train time: 0:00:00.499099
elapsed time: 0:02:08.507132
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:07:03.545934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.99
 ---- batch: 020 ----
mean loss: 144.83
 ---- batch: 030 ----
mean loss: 145.00
 ---- batch: 040 ----
mean loss: 142.36
 ---- batch: 050 ----
mean loss: 141.84
 ---- batch: 060 ----
mean loss: 143.84
 ---- batch: 070 ----
mean loss: 149.62
 ---- batch: 080 ----
mean loss: 145.57
 ---- batch: 090 ----
mean loss: 145.03
train mean loss: 144.43
epoch train time: 0:00:00.500937
elapsed time: 0:02:09.008244
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:07:04.047061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.43
 ---- batch: 020 ----
mean loss: 145.47
 ---- batch: 030 ----
mean loss: 146.62
 ---- batch: 040 ----
mean loss: 141.35
 ---- batch: 050 ----
mean loss: 143.32
 ---- batch: 060 ----
mean loss: 147.38
 ---- batch: 070 ----
mean loss: 146.04
 ---- batch: 080 ----
mean loss: 137.51
 ---- batch: 090 ----
mean loss: 147.89
train mean loss: 143.90
epoch train time: 0:00:00.503175
elapsed time: 0:02:09.511608
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:07:04.550407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.08
 ---- batch: 020 ----
mean loss: 141.40
 ---- batch: 030 ----
mean loss: 142.73
 ---- batch: 040 ----
mean loss: 140.70
 ---- batch: 050 ----
mean loss: 144.41
 ---- batch: 060 ----
mean loss: 140.42
 ---- batch: 070 ----
mean loss: 146.18
 ---- batch: 080 ----
mean loss: 146.43
 ---- batch: 090 ----
mean loss: 155.05
train mean loss: 143.91
epoch train time: 0:00:00.517445
elapsed time: 0:02:10.029212
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:07:05.068007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.24
 ---- batch: 020 ----
mean loss: 143.48
 ---- batch: 030 ----
mean loss: 136.63
 ---- batch: 040 ----
mean loss: 144.45
 ---- batch: 050 ----
mean loss: 142.58
 ---- batch: 060 ----
mean loss: 147.90
 ---- batch: 070 ----
mean loss: 143.47
 ---- batch: 080 ----
mean loss: 146.36
 ---- batch: 090 ----
mean loss: 147.32
train mean loss: 143.95
epoch train time: 0:00:00.497427
elapsed time: 0:02:10.526789
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:07:05.565586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.86
 ---- batch: 020 ----
mean loss: 143.73
 ---- batch: 030 ----
mean loss: 149.59
 ---- batch: 040 ----
mean loss: 136.89
 ---- batch: 050 ----
mean loss: 137.64
 ---- batch: 060 ----
mean loss: 146.16
 ---- batch: 070 ----
mean loss: 143.69
 ---- batch: 080 ----
mean loss: 145.89
 ---- batch: 090 ----
mean loss: 147.78
train mean loss: 143.78
epoch train time: 0:00:00.501906
elapsed time: 0:02:11.028847
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:07:06.067644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.10
 ---- batch: 020 ----
mean loss: 146.08
 ---- batch: 030 ----
mean loss: 147.40
 ---- batch: 040 ----
mean loss: 144.29
 ---- batch: 050 ----
mean loss: 138.35
 ---- batch: 060 ----
mean loss: 142.56
 ---- batch: 070 ----
mean loss: 140.79
 ---- batch: 080 ----
mean loss: 135.79
 ---- batch: 090 ----
mean loss: 151.43
train mean loss: 143.74
epoch train time: 0:00:00.494775
elapsed time: 0:02:11.523776
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:07:06.562573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.05
 ---- batch: 020 ----
mean loss: 141.58
 ---- batch: 030 ----
mean loss: 141.28
 ---- batch: 040 ----
mean loss: 136.60
 ---- batch: 050 ----
mean loss: 143.80
 ---- batch: 060 ----
mean loss: 145.74
 ---- batch: 070 ----
mean loss: 140.76
 ---- batch: 080 ----
mean loss: 148.51
 ---- batch: 090 ----
mean loss: 150.72
train mean loss: 143.55
epoch train time: 0:00:00.502729
elapsed time: 0:02:12.026655
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:07:07.065452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.56
 ---- batch: 020 ----
mean loss: 136.01
 ---- batch: 030 ----
mean loss: 144.04
 ---- batch: 040 ----
mean loss: 144.82
 ---- batch: 050 ----
mean loss: 146.65
 ---- batch: 060 ----
mean loss: 147.75
 ---- batch: 070 ----
mean loss: 141.99
 ---- batch: 080 ----
mean loss: 149.99
 ---- batch: 090 ----
mean loss: 146.43
train mean loss: 143.12
epoch train time: 0:00:00.496101
elapsed time: 0:02:12.522928
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:07:07.561742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.54
 ---- batch: 020 ----
mean loss: 136.96
 ---- batch: 030 ----
mean loss: 140.48
 ---- batch: 040 ----
mean loss: 137.63
 ---- batch: 050 ----
mean loss: 144.67
 ---- batch: 060 ----
mean loss: 150.32
 ---- batch: 070 ----
mean loss: 151.00
 ---- batch: 080 ----
mean loss: 148.06
 ---- batch: 090 ----
mean loss: 143.25
train mean loss: 143.40
epoch train time: 0:00:00.499432
elapsed time: 0:02:13.022526
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:07:08.061322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.66
 ---- batch: 020 ----
mean loss: 144.88
 ---- batch: 030 ----
mean loss: 128.26
 ---- batch: 040 ----
mean loss: 142.63
 ---- batch: 050 ----
mean loss: 139.19
 ---- batch: 060 ----
mean loss: 144.64
 ---- batch: 070 ----
mean loss: 146.15
 ---- batch: 080 ----
mean loss: 150.78
 ---- batch: 090 ----
mean loss: 149.31
train mean loss: 142.92
epoch train time: 0:00:00.494638
elapsed time: 0:02:13.517313
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:07:08.556110
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 140.65
 ---- batch: 020 ----
mean loss: 135.04
 ---- batch: 030 ----
mean loss: 133.88
 ---- batch: 040 ----
mean loss: 139.79
 ---- batch: 050 ----
mean loss: 133.13
 ---- batch: 060 ----
mean loss: 129.01
 ---- batch: 070 ----
mean loss: 138.18
 ---- batch: 080 ----
mean loss: 141.48
 ---- batch: 090 ----
mean loss: 135.78
train mean loss: 136.57
epoch train time: 0:00:00.496669
elapsed time: 0:02:14.014143
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:07:09.052926
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.91
 ---- batch: 020 ----
mean loss: 128.52
 ---- batch: 030 ----
mean loss: 139.66
 ---- batch: 040 ----
mean loss: 130.40
 ---- batch: 050 ----
mean loss: 136.80
 ---- batch: 060 ----
mean loss: 137.63
 ---- batch: 070 ----
mean loss: 133.47
 ---- batch: 080 ----
mean loss: 137.87
 ---- batch: 090 ----
mean loss: 133.51
train mean loss: 135.15
epoch train time: 0:00:00.494428
elapsed time: 0:02:14.508703
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:07:09.547530
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.03
 ---- batch: 020 ----
mean loss: 136.70
 ---- batch: 030 ----
mean loss: 129.51
 ---- batch: 040 ----
mean loss: 138.75
 ---- batch: 050 ----
mean loss: 136.13
 ---- batch: 060 ----
mean loss: 128.92
 ---- batch: 070 ----
mean loss: 135.41
 ---- batch: 080 ----
mean loss: 135.99
 ---- batch: 090 ----
mean loss: 136.36
train mean loss: 134.78
epoch train time: 0:00:00.505799
elapsed time: 0:02:15.014719
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:07:10.053516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.61
 ---- batch: 020 ----
mean loss: 132.43
 ---- batch: 030 ----
mean loss: 142.02
 ---- batch: 040 ----
mean loss: 130.64
 ---- batch: 050 ----
mean loss: 136.32
 ---- batch: 060 ----
mean loss: 132.57
 ---- batch: 070 ----
mean loss: 131.48
 ---- batch: 080 ----
mean loss: 143.30
 ---- batch: 090 ----
mean loss: 131.44
train mean loss: 134.50
epoch train time: 0:00:00.500334
elapsed time: 0:02:15.515204
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:07:10.554000
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.47
 ---- batch: 020 ----
mean loss: 135.19
 ---- batch: 030 ----
mean loss: 140.50
 ---- batch: 040 ----
mean loss: 125.24
 ---- batch: 050 ----
mean loss: 139.03
 ---- batch: 060 ----
mean loss: 138.08
 ---- batch: 070 ----
mean loss: 131.21
 ---- batch: 080 ----
mean loss: 140.43
 ---- batch: 090 ----
mean loss: 133.35
train mean loss: 134.70
epoch train time: 0:00:00.498254
elapsed time: 0:02:16.013608
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:07:11.052405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.89
 ---- batch: 020 ----
mean loss: 129.45
 ---- batch: 030 ----
mean loss: 132.39
 ---- batch: 040 ----
mean loss: 138.43
 ---- batch: 050 ----
mean loss: 136.64
 ---- batch: 060 ----
mean loss: 131.34
 ---- batch: 070 ----
mean loss: 137.05
 ---- batch: 080 ----
mean loss: 134.03
 ---- batch: 090 ----
mean loss: 136.70
train mean loss: 134.47
epoch train time: 0:00:00.499731
elapsed time: 0:02:16.513503
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:07:11.552354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.65
 ---- batch: 020 ----
mean loss: 133.76
 ---- batch: 030 ----
mean loss: 135.08
 ---- batch: 040 ----
mean loss: 133.46
 ---- batch: 050 ----
mean loss: 133.98
 ---- batch: 060 ----
mean loss: 132.30
 ---- batch: 070 ----
mean loss: 133.61
 ---- batch: 080 ----
mean loss: 135.33
 ---- batch: 090 ----
mean loss: 137.01
train mean loss: 134.66
epoch train time: 0:00:00.498393
elapsed time: 0:02:17.012102
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:07:12.050899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.58
 ---- batch: 020 ----
mean loss: 137.27
 ---- batch: 030 ----
mean loss: 140.11
 ---- batch: 040 ----
mean loss: 138.14
 ---- batch: 050 ----
mean loss: 132.59
 ---- batch: 060 ----
mean loss: 130.82
 ---- batch: 070 ----
mean loss: 132.90
 ---- batch: 080 ----
mean loss: 139.25
 ---- batch: 090 ----
mean loss: 127.59
train mean loss: 134.38
epoch train time: 0:00:00.492488
elapsed time: 0:02:17.504751
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:07:12.543546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.62
 ---- batch: 020 ----
mean loss: 137.33
 ---- batch: 030 ----
mean loss: 134.27
 ---- batch: 040 ----
mean loss: 134.00
 ---- batch: 050 ----
mean loss: 138.16
 ---- batch: 060 ----
mean loss: 137.48
 ---- batch: 070 ----
mean loss: 131.42
 ---- batch: 080 ----
mean loss: 129.46
 ---- batch: 090 ----
mean loss: 136.28
train mean loss: 134.40
epoch train time: 0:00:00.492052
elapsed time: 0:02:17.996950
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:07:13.035746
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.21
 ---- batch: 020 ----
mean loss: 133.13
 ---- batch: 030 ----
mean loss: 134.27
 ---- batch: 040 ----
mean loss: 138.02
 ---- batch: 050 ----
mean loss: 128.63
 ---- batch: 060 ----
mean loss: 136.03
 ---- batch: 070 ----
mean loss: 134.19
 ---- batch: 080 ----
mean loss: 137.78
 ---- batch: 090 ----
mean loss: 133.51
train mean loss: 134.30
epoch train time: 0:00:00.487340
elapsed time: 0:02:18.484437
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:07:13.523232
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.33
 ---- batch: 020 ----
mean loss: 126.30
 ---- batch: 030 ----
mean loss: 139.12
 ---- batch: 040 ----
mean loss: 136.05
 ---- batch: 050 ----
mean loss: 131.01
 ---- batch: 060 ----
mean loss: 136.05
 ---- batch: 070 ----
mean loss: 136.12
 ---- batch: 080 ----
mean loss: 133.52
 ---- batch: 090 ----
mean loss: 138.03
train mean loss: 134.19
epoch train time: 0:00:00.492749
elapsed time: 0:02:18.977336
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:07:14.016132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.57
 ---- batch: 020 ----
mean loss: 129.73
 ---- batch: 030 ----
mean loss: 132.64
 ---- batch: 040 ----
mean loss: 138.65
 ---- batch: 050 ----
mean loss: 131.65
 ---- batch: 060 ----
mean loss: 130.69
 ---- batch: 070 ----
mean loss: 139.13
 ---- batch: 080 ----
mean loss: 132.43
 ---- batch: 090 ----
mean loss: 135.95
train mean loss: 134.33
epoch train time: 0:00:00.489615
elapsed time: 0:02:19.467102
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:07:14.505943
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.66
 ---- batch: 020 ----
mean loss: 133.97
 ---- batch: 030 ----
mean loss: 127.08
 ---- batch: 040 ----
mean loss: 137.45
 ---- batch: 050 ----
mean loss: 138.02
 ---- batch: 060 ----
mean loss: 137.37
 ---- batch: 070 ----
mean loss: 136.26
 ---- batch: 080 ----
mean loss: 129.93
 ---- batch: 090 ----
mean loss: 138.30
train mean loss: 134.17
epoch train time: 0:00:00.496493
elapsed time: 0:02:19.963795
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:07:15.002609
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.32
 ---- batch: 020 ----
mean loss: 122.89
 ---- batch: 030 ----
mean loss: 133.09
 ---- batch: 040 ----
mean loss: 133.01
 ---- batch: 050 ----
mean loss: 135.36
 ---- batch: 060 ----
mean loss: 136.06
 ---- batch: 070 ----
mean loss: 140.32
 ---- batch: 080 ----
mean loss: 136.75
 ---- batch: 090 ----
mean loss: 139.16
train mean loss: 134.26
epoch train time: 0:00:00.485966
elapsed time: 0:02:20.449935
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:07:15.488722
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.89
 ---- batch: 020 ----
mean loss: 135.13
 ---- batch: 030 ----
mean loss: 133.84
 ---- batch: 040 ----
mean loss: 128.98
 ---- batch: 050 ----
mean loss: 135.13
 ---- batch: 060 ----
mean loss: 135.09
 ---- batch: 070 ----
mean loss: 127.43
 ---- batch: 080 ----
mean loss: 142.14
 ---- batch: 090 ----
mean loss: 133.43
train mean loss: 134.17
epoch train time: 0:00:00.495853
elapsed time: 0:02:20.945928
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:07:15.984723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.02
 ---- batch: 020 ----
mean loss: 133.55
 ---- batch: 030 ----
mean loss: 133.91
 ---- batch: 040 ----
mean loss: 132.71
 ---- batch: 050 ----
mean loss: 132.18
 ---- batch: 060 ----
mean loss: 134.18
 ---- batch: 070 ----
mean loss: 134.46
 ---- batch: 080 ----
mean loss: 136.63
 ---- batch: 090 ----
mean loss: 136.23
train mean loss: 134.12
epoch train time: 0:00:00.487576
elapsed time: 0:02:21.433661
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:07:16.472458
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.44
 ---- batch: 020 ----
mean loss: 136.37
 ---- batch: 030 ----
mean loss: 137.94
 ---- batch: 040 ----
mean loss: 129.37
 ---- batch: 050 ----
mean loss: 134.16
 ---- batch: 060 ----
mean loss: 134.42
 ---- batch: 070 ----
mean loss: 134.45
 ---- batch: 080 ----
mean loss: 137.64
 ---- batch: 090 ----
mean loss: 134.68
train mean loss: 134.09
epoch train time: 0:00:00.499313
elapsed time: 0:02:21.933134
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:07:16.971929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.76
 ---- batch: 020 ----
mean loss: 130.53
 ---- batch: 030 ----
mean loss: 137.30
 ---- batch: 040 ----
mean loss: 132.76
 ---- batch: 050 ----
mean loss: 133.40
 ---- batch: 060 ----
mean loss: 136.42
 ---- batch: 070 ----
mean loss: 132.12
 ---- batch: 080 ----
mean loss: 134.82
 ---- batch: 090 ----
mean loss: 140.37
train mean loss: 133.82
epoch train time: 0:00:00.488424
elapsed time: 0:02:22.421713
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:07:17.460508
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.67
 ---- batch: 020 ----
mean loss: 130.59
 ---- batch: 030 ----
mean loss: 140.40
 ---- batch: 040 ----
mean loss: 137.28
 ---- batch: 050 ----
mean loss: 137.48
 ---- batch: 060 ----
mean loss: 127.78
 ---- batch: 070 ----
mean loss: 127.99
 ---- batch: 080 ----
mean loss: 140.07
 ---- batch: 090 ----
mean loss: 129.09
train mean loss: 133.94
epoch train time: 0:00:00.489286
elapsed time: 0:02:22.911146
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:07:17.949941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.87
 ---- batch: 020 ----
mean loss: 130.97
 ---- batch: 030 ----
mean loss: 129.18
 ---- batch: 040 ----
mean loss: 135.86
 ---- batch: 050 ----
mean loss: 139.76
 ---- batch: 060 ----
mean loss: 132.03
 ---- batch: 070 ----
mean loss: 133.92
 ---- batch: 080 ----
mean loss: 140.30
 ---- batch: 090 ----
mean loss: 133.42
train mean loss: 133.84
epoch train time: 0:00:00.500361
elapsed time: 0:02:23.411656
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:07:18.450475
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.65
 ---- batch: 020 ----
mean loss: 131.87
 ---- batch: 030 ----
mean loss: 131.68
 ---- batch: 040 ----
mean loss: 133.35
 ---- batch: 050 ----
mean loss: 132.67
 ---- batch: 060 ----
mean loss: 137.87
 ---- batch: 070 ----
mean loss: 131.33
 ---- batch: 080 ----
mean loss: 140.25
 ---- batch: 090 ----
mean loss: 130.51
train mean loss: 133.84
epoch train time: 0:00:00.497471
elapsed time: 0:02:23.909309
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:07:18.948123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.85
 ---- batch: 020 ----
mean loss: 135.28
 ---- batch: 030 ----
mean loss: 133.74
 ---- batch: 040 ----
mean loss: 130.59
 ---- batch: 050 ----
mean loss: 128.12
 ---- batch: 060 ----
mean loss: 135.42
 ---- batch: 070 ----
mean loss: 134.94
 ---- batch: 080 ----
mean loss: 138.87
 ---- batch: 090 ----
mean loss: 134.72
train mean loss: 134.14
epoch train time: 0:00:00.488078
elapsed time: 0:02:24.397551
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:07:19.436345
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.88
 ---- batch: 020 ----
mean loss: 134.46
 ---- batch: 030 ----
mean loss: 133.63
 ---- batch: 040 ----
mean loss: 132.19
 ---- batch: 050 ----
mean loss: 136.39
 ---- batch: 060 ----
mean loss: 131.71
 ---- batch: 070 ----
mean loss: 133.72
 ---- batch: 080 ----
mean loss: 135.66
 ---- batch: 090 ----
mean loss: 138.28
train mean loss: 133.79
epoch train time: 0:00:00.497414
elapsed time: 0:02:24.895113
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:07:19.933909
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.52
 ---- batch: 020 ----
mean loss: 127.13
 ---- batch: 030 ----
mean loss: 132.85
 ---- batch: 040 ----
mean loss: 132.62
 ---- batch: 050 ----
mean loss: 131.27
 ---- batch: 060 ----
mean loss: 133.35
 ---- batch: 070 ----
mean loss: 136.15
 ---- batch: 080 ----
mean loss: 142.92
 ---- batch: 090 ----
mean loss: 138.09
train mean loss: 134.01
epoch train time: 0:00:00.497189
elapsed time: 0:02:25.392467
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:07:20.431263
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.66
 ---- batch: 020 ----
mean loss: 138.96
 ---- batch: 030 ----
mean loss: 132.65
 ---- batch: 040 ----
mean loss: 134.84
 ---- batch: 050 ----
mean loss: 133.22
 ---- batch: 060 ----
mean loss: 131.82
 ---- batch: 070 ----
mean loss: 131.84
 ---- batch: 080 ----
mean loss: 133.60
 ---- batch: 090 ----
mean loss: 131.38
train mean loss: 133.86
epoch train time: 0:00:00.493396
elapsed time: 0:02:25.886016
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:07:20.924813
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.21
 ---- batch: 020 ----
mean loss: 136.31
 ---- batch: 030 ----
mean loss: 133.02
 ---- batch: 040 ----
mean loss: 134.17
 ---- batch: 050 ----
mean loss: 131.35
 ---- batch: 060 ----
mean loss: 132.41
 ---- batch: 070 ----
mean loss: 132.10
 ---- batch: 080 ----
mean loss: 134.06
 ---- batch: 090 ----
mean loss: 138.32
train mean loss: 133.84
epoch train time: 0:00:00.496022
elapsed time: 0:02:26.382194
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:07:21.420990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.37
 ---- batch: 020 ----
mean loss: 141.39
 ---- batch: 030 ----
mean loss: 128.31
 ---- batch: 040 ----
mean loss: 141.13
 ---- batch: 050 ----
mean loss: 139.55
 ---- batch: 060 ----
mean loss: 133.12
 ---- batch: 070 ----
mean loss: 130.55
 ---- batch: 080 ----
mean loss: 132.92
 ---- batch: 090 ----
mean loss: 128.45
train mean loss: 134.05
epoch train time: 0:00:00.493186
elapsed time: 0:02:26.875545
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:07:21.914344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.33
 ---- batch: 020 ----
mean loss: 129.98
 ---- batch: 030 ----
mean loss: 136.93
 ---- batch: 040 ----
mean loss: 139.23
 ---- batch: 050 ----
mean loss: 134.25
 ---- batch: 060 ----
mean loss: 130.75
 ---- batch: 070 ----
mean loss: 133.83
 ---- batch: 080 ----
mean loss: 130.80
 ---- batch: 090 ----
mean loss: 135.73
train mean loss: 133.53
epoch train time: 0:00:00.494328
elapsed time: 0:02:27.370023
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:07:22.408840
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.22
 ---- batch: 020 ----
mean loss: 133.68
 ---- batch: 030 ----
mean loss: 131.68
 ---- batch: 040 ----
mean loss: 134.51
 ---- batch: 050 ----
mean loss: 135.36
 ---- batch: 060 ----
mean loss: 135.07
 ---- batch: 070 ----
mean loss: 133.07
 ---- batch: 080 ----
mean loss: 131.50
 ---- batch: 090 ----
mean loss: 138.53
train mean loss: 133.65
epoch train time: 0:00:00.493692
elapsed time: 0:02:27.863904
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:07:22.902702
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.93
 ---- batch: 020 ----
mean loss: 132.86
 ---- batch: 030 ----
mean loss: 134.58
 ---- batch: 040 ----
mean loss: 132.12
 ---- batch: 050 ----
mean loss: 131.26
 ---- batch: 060 ----
mean loss: 132.00
 ---- batch: 070 ----
mean loss: 133.46
 ---- batch: 080 ----
mean loss: 140.17
 ---- batch: 090 ----
mean loss: 131.65
train mean loss: 133.76
epoch train time: 0:00:00.492050
elapsed time: 0:02:28.356114
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:07:23.394911
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.96
 ---- batch: 020 ----
mean loss: 133.22
 ---- batch: 030 ----
mean loss: 134.93
 ---- batch: 040 ----
mean loss: 139.28
 ---- batch: 050 ----
mean loss: 128.01
 ---- batch: 060 ----
mean loss: 125.80
 ---- batch: 070 ----
mean loss: 132.95
 ---- batch: 080 ----
mean loss: 136.22
 ---- batch: 090 ----
mean loss: 133.15
train mean loss: 133.54
epoch train time: 0:00:00.493681
elapsed time: 0:02:28.849946
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:07:23.888744
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.06
 ---- batch: 020 ----
mean loss: 132.72
 ---- batch: 030 ----
mean loss: 130.96
 ---- batch: 040 ----
mean loss: 134.28
 ---- batch: 050 ----
mean loss: 136.48
 ---- batch: 060 ----
mean loss: 133.07
 ---- batch: 070 ----
mean loss: 136.39
 ---- batch: 080 ----
mean loss: 129.66
 ---- batch: 090 ----
mean loss: 132.15
train mean loss: 133.48
epoch train time: 0:00:00.495164
elapsed time: 0:02:29.345261
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:07:24.384057
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.96
 ---- batch: 020 ----
mean loss: 131.00
 ---- batch: 030 ----
mean loss: 132.43
 ---- batch: 040 ----
mean loss: 133.99
 ---- batch: 050 ----
mean loss: 132.44
 ---- batch: 060 ----
mean loss: 134.85
 ---- batch: 070 ----
mean loss: 127.54
 ---- batch: 080 ----
mean loss: 134.98
 ---- batch: 090 ----
mean loss: 132.33
train mean loss: 133.71
epoch train time: 0:00:00.497650
elapsed time: 0:02:29.843076
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:07:24.881860
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.65
 ---- batch: 020 ----
mean loss: 140.24
 ---- batch: 030 ----
mean loss: 128.62
 ---- batch: 040 ----
mean loss: 135.15
 ---- batch: 050 ----
mean loss: 137.53
 ---- batch: 060 ----
mean loss: 134.39
 ---- batch: 070 ----
mean loss: 133.62
 ---- batch: 080 ----
mean loss: 130.10
 ---- batch: 090 ----
mean loss: 136.82
train mean loss: 133.45
epoch train time: 0:00:00.495560
elapsed time: 0:02:30.338770
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:07:25.377564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.90
 ---- batch: 020 ----
mean loss: 136.16
 ---- batch: 030 ----
mean loss: 128.67
 ---- batch: 040 ----
mean loss: 125.58
 ---- batch: 050 ----
mean loss: 134.71
 ---- batch: 060 ----
mean loss: 135.05
 ---- batch: 070 ----
mean loss: 136.14
 ---- batch: 080 ----
mean loss: 133.66
 ---- batch: 090 ----
mean loss: 133.64
train mean loss: 133.43
epoch train time: 0:00:00.501980
elapsed time: 0:02:30.840895
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:07:25.879692
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.05
 ---- batch: 020 ----
mean loss: 131.87
 ---- batch: 030 ----
mean loss: 134.75
 ---- batch: 040 ----
mean loss: 136.24
 ---- batch: 050 ----
mean loss: 134.03
 ---- batch: 060 ----
mean loss: 128.49
 ---- batch: 070 ----
mean loss: 134.34
 ---- batch: 080 ----
mean loss: 134.69
 ---- batch: 090 ----
mean loss: 133.42
train mean loss: 133.31
epoch train time: 0:00:00.489594
elapsed time: 0:02:31.330652
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:07:26.369479
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.19
 ---- batch: 020 ----
mean loss: 132.22
 ---- batch: 030 ----
mean loss: 140.14
 ---- batch: 040 ----
mean loss: 131.33
 ---- batch: 050 ----
mean loss: 133.88
 ---- batch: 060 ----
mean loss: 131.00
 ---- batch: 070 ----
mean loss: 135.58
 ---- batch: 080 ----
mean loss: 136.01
 ---- batch: 090 ----
mean loss: 131.99
train mean loss: 133.49
epoch train time: 0:00:00.498906
elapsed time: 0:02:31.829790
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:07:26.868590
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.84
 ---- batch: 020 ----
mean loss: 127.81
 ---- batch: 030 ----
mean loss: 132.93
 ---- batch: 040 ----
mean loss: 131.85
 ---- batch: 050 ----
mean loss: 138.89
 ---- batch: 060 ----
mean loss: 130.40
 ---- batch: 070 ----
mean loss: 135.00
 ---- batch: 080 ----
mean loss: 134.75
 ---- batch: 090 ----
mean loss: 130.64
train mean loss: 133.14
epoch train time: 0:00:00.491107
elapsed time: 0:02:32.321048
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:07:27.359842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.49
 ---- batch: 020 ----
mean loss: 132.17
 ---- batch: 030 ----
mean loss: 129.68
 ---- batch: 040 ----
mean loss: 128.54
 ---- batch: 050 ----
mean loss: 135.84
 ---- batch: 060 ----
mean loss: 135.11
 ---- batch: 070 ----
mean loss: 131.78
 ---- batch: 080 ----
mean loss: 135.17
 ---- batch: 090 ----
mean loss: 136.12
train mean loss: 133.54
epoch train time: 0:00:00.489995
elapsed time: 0:02:32.811200
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:07:27.849997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 140.09
 ---- batch: 020 ----
mean loss: 137.55
 ---- batch: 030 ----
mean loss: 129.32
 ---- batch: 040 ----
mean loss: 124.19
 ---- batch: 050 ----
mean loss: 132.83
 ---- batch: 060 ----
mean loss: 132.46
 ---- batch: 070 ----
mean loss: 132.81
 ---- batch: 080 ----
mean loss: 132.30
 ---- batch: 090 ----
mean loss: 131.81
train mean loss: 133.46
epoch train time: 0:00:00.491959
elapsed time: 0:02:33.303312
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:07:28.342124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.73
 ---- batch: 020 ----
mean loss: 131.03
 ---- batch: 030 ----
mean loss: 138.22
 ---- batch: 040 ----
mean loss: 129.04
 ---- batch: 050 ----
mean loss: 135.94
 ---- batch: 060 ----
mean loss: 126.54
 ---- batch: 070 ----
mean loss: 135.23
 ---- batch: 080 ----
mean loss: 130.39
 ---- batch: 090 ----
mean loss: 134.47
train mean loss: 133.24
epoch train time: 0:00:00.500668
elapsed time: 0:02:33.804151
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:07:28.842955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.22
 ---- batch: 020 ----
mean loss: 129.10
 ---- batch: 030 ----
mean loss: 133.50
 ---- batch: 040 ----
mean loss: 135.18
 ---- batch: 050 ----
mean loss: 128.80
 ---- batch: 060 ----
mean loss: 137.59
 ---- batch: 070 ----
mean loss: 129.75
 ---- batch: 080 ----
mean loss: 132.87
 ---- batch: 090 ----
mean loss: 136.02
train mean loss: 133.21
epoch train time: 0:00:00.492447
elapsed time: 0:02:34.296753
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:07:29.335546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.64
 ---- batch: 020 ----
mean loss: 132.58
 ---- batch: 030 ----
mean loss: 128.70
 ---- batch: 040 ----
mean loss: 128.27
 ---- batch: 050 ----
mean loss: 131.13
 ---- batch: 060 ----
mean loss: 140.00
 ---- batch: 070 ----
mean loss: 137.93
 ---- batch: 080 ----
mean loss: 129.19
 ---- batch: 090 ----
mean loss: 138.92
train mean loss: 133.23
epoch train time: 0:00:00.505964
elapsed time: 0:02:34.802893
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:07:29.841713
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.32
 ---- batch: 020 ----
mean loss: 138.90
 ---- batch: 030 ----
mean loss: 130.89
 ---- batch: 040 ----
mean loss: 130.33
 ---- batch: 050 ----
mean loss: 132.87
 ---- batch: 060 ----
mean loss: 133.01
 ---- batch: 070 ----
mean loss: 133.73
 ---- batch: 080 ----
mean loss: 130.59
 ---- batch: 090 ----
mean loss: 138.06
train mean loss: 133.30
epoch train time: 0:00:00.490580
elapsed time: 0:02:35.293697
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:07:30.332549
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.96
 ---- batch: 020 ----
mean loss: 133.93
 ---- batch: 030 ----
mean loss: 130.28
 ---- batch: 040 ----
mean loss: 131.45
 ---- batch: 050 ----
mean loss: 134.21
 ---- batch: 060 ----
mean loss: 130.69
 ---- batch: 070 ----
mean loss: 134.10
 ---- batch: 080 ----
mean loss: 137.95
 ---- batch: 090 ----
mean loss: 134.62
train mean loss: 132.90
epoch train time: 0:00:00.509707
elapsed time: 0:02:35.803654
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:07:30.842452
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.55
 ---- batch: 020 ----
mean loss: 128.78
 ---- batch: 030 ----
mean loss: 131.12
 ---- batch: 040 ----
mean loss: 134.99
 ---- batch: 050 ----
mean loss: 131.81
 ---- batch: 060 ----
mean loss: 132.08
 ---- batch: 070 ----
mean loss: 138.71
 ---- batch: 080 ----
mean loss: 130.09
 ---- batch: 090 ----
mean loss: 133.24
train mean loss: 133.13
epoch train time: 0:00:00.486197
elapsed time: 0:02:36.290002
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:07:31.328797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.62
 ---- batch: 020 ----
mean loss: 134.45
 ---- batch: 030 ----
mean loss: 131.15
 ---- batch: 040 ----
mean loss: 139.71
 ---- batch: 050 ----
mean loss: 137.50
 ---- batch: 060 ----
mean loss: 132.20
 ---- batch: 070 ----
mean loss: 128.99
 ---- batch: 080 ----
mean loss: 132.71
 ---- batch: 090 ----
mean loss: 131.30
train mean loss: 133.05
epoch train time: 0:00:00.487780
elapsed time: 0:02:36.777932
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:07:31.816731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.19
 ---- batch: 020 ----
mean loss: 128.15
 ---- batch: 030 ----
mean loss: 129.19
 ---- batch: 040 ----
mean loss: 134.94
 ---- batch: 050 ----
mean loss: 139.04
 ---- batch: 060 ----
mean loss: 134.83
 ---- batch: 070 ----
mean loss: 138.58
 ---- batch: 080 ----
mean loss: 132.89
 ---- batch: 090 ----
mean loss: 134.08
train mean loss: 133.15
epoch train time: 0:00:00.486971
elapsed time: 0:02:37.265063
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:07:32.303859
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.09
 ---- batch: 020 ----
mean loss: 137.58
 ---- batch: 030 ----
mean loss: 129.65
 ---- batch: 040 ----
mean loss: 128.15
 ---- batch: 050 ----
mean loss: 134.12
 ---- batch: 060 ----
mean loss: 132.38
 ---- batch: 070 ----
mean loss: 131.99
 ---- batch: 080 ----
mean loss: 133.76
 ---- batch: 090 ----
mean loss: 136.66
train mean loss: 132.75
epoch train time: 0:00:00.496130
elapsed time: 0:02:37.764754
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_1/checkpoint.pth.tar
**** end time: 2019-09-25 23:07:32.803531 ****
