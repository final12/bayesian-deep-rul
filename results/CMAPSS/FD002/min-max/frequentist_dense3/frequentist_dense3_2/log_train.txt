Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24238
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:07:49.208051 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:07:49.211282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4327.44
 ---- batch: 020 ----
mean loss: 4155.49
 ---- batch: 030 ----
mean loss: 4131.97
 ---- batch: 040 ----
mean loss: 4011.99
 ---- batch: 050 ----
mean loss: 3877.03
 ---- batch: 060 ----
mean loss: 3918.47
 ---- batch: 070 ----
mean loss: 3793.32
 ---- batch: 080 ----
mean loss: 3793.71
 ---- batch: 090 ----
mean loss: 3738.78
train mean loss: 3953.77
epoch train time: 0:00:33.647220
elapsed time: 0:00:33.652973
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:08:22.861081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3577.66
 ---- batch: 020 ----
mean loss: 3615.09
 ---- batch: 030 ----
mean loss: 3546.08
 ---- batch: 040 ----
mean loss: 3499.78
 ---- batch: 050 ----
mean loss: 3378.10
 ---- batch: 060 ----
mean loss: 3383.27
 ---- batch: 070 ----
mean loss: 3326.60
 ---- batch: 080 ----
mean loss: 3248.19
 ---- batch: 090 ----
mean loss: 3226.02
train mean loss: 3407.82
epoch train time: 0:00:00.496471
elapsed time: 0:00:34.149619
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:08:23.357734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3113.11
 ---- batch: 020 ----
mean loss: 3059.06
 ---- batch: 030 ----
mean loss: 3052.26
 ---- batch: 040 ----
mean loss: 3011.35
 ---- batch: 050 ----
mean loss: 2983.96
 ---- batch: 060 ----
mean loss: 2938.06
 ---- batch: 070 ----
mean loss: 2926.77
 ---- batch: 080 ----
mean loss: 2855.48
 ---- batch: 090 ----
mean loss: 2781.62
train mean loss: 2959.90
epoch train time: 0:00:00.499455
elapsed time: 0:00:34.649245
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:08:23.857340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2744.41
 ---- batch: 020 ----
mean loss: 2673.14
 ---- batch: 030 ----
mean loss: 2629.50
 ---- batch: 040 ----
mean loss: 2654.28
 ---- batch: 050 ----
mean loss: 2594.71
 ---- batch: 060 ----
mean loss: 2579.72
 ---- batch: 070 ----
mean loss: 2491.49
 ---- batch: 080 ----
mean loss: 2474.13
 ---- batch: 090 ----
mean loss: 2470.66
train mean loss: 2579.45
epoch train time: 0:00:00.495876
elapsed time: 0:00:35.145261
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:08:24.353364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2370.64
 ---- batch: 020 ----
mean loss: 2363.21
 ---- batch: 030 ----
mean loss: 2303.82
 ---- batch: 040 ----
mean loss: 2268.53
 ---- batch: 050 ----
mean loss: 2285.03
 ---- batch: 060 ----
mean loss: 2192.70
 ---- batch: 070 ----
mean loss: 2183.10
 ---- batch: 080 ----
mean loss: 2185.01
 ---- batch: 090 ----
mean loss: 2125.86
train mean loss: 2242.69
epoch train time: 0:00:00.505121
elapsed time: 0:00:35.650552
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:08:24.858656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2045.67
 ---- batch: 020 ----
mean loss: 2013.08
 ---- batch: 030 ----
mean loss: 2022.82
 ---- batch: 040 ----
mean loss: 1974.60
 ---- batch: 050 ----
mean loss: 2012.01
 ---- batch: 060 ----
mean loss: 1899.90
 ---- batch: 070 ----
mean loss: 1921.23
 ---- batch: 080 ----
mean loss: 1925.83
 ---- batch: 090 ----
mean loss: 1839.55
train mean loss: 1952.30
epoch train time: 0:00:00.496099
elapsed time: 0:00:36.146816
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:08:25.354919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1815.42
 ---- batch: 020 ----
mean loss: 1805.18
 ---- batch: 030 ----
mean loss: 1715.86
 ---- batch: 040 ----
mean loss: 1749.59
 ---- batch: 050 ----
mean loss: 1716.38
 ---- batch: 060 ----
mean loss: 1710.84
 ---- batch: 070 ----
mean loss: 1672.23
 ---- batch: 080 ----
mean loss: 1659.63
 ---- batch: 090 ----
mean loss: 1642.52
train mean loss: 1712.07
epoch train time: 0:00:00.507659
elapsed time: 0:00:36.654627
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:08:25.862728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1576.96
 ---- batch: 020 ----
mean loss: 1581.87
 ---- batch: 030 ----
mean loss: 1577.60
 ---- batch: 040 ----
mean loss: 1498.13
 ---- batch: 050 ----
mean loss: 1538.20
 ---- batch: 060 ----
mean loss: 1513.49
 ---- batch: 070 ----
mean loss: 1459.16
 ---- batch: 080 ----
mean loss: 1481.49
 ---- batch: 090 ----
mean loss: 1437.39
train mean loss: 1511.67
epoch train time: 0:00:00.501190
elapsed time: 0:00:37.155967
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:08:26.364084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1425.36
 ---- batch: 020 ----
mean loss: 1379.40
 ---- batch: 030 ----
mean loss: 1376.90
 ---- batch: 040 ----
mean loss: 1356.65
 ---- batch: 050 ----
mean loss: 1386.41
 ---- batch: 060 ----
mean loss: 1325.71
 ---- batch: 070 ----
mean loss: 1342.76
 ---- batch: 080 ----
mean loss: 1295.38
 ---- batch: 090 ----
mean loss: 1323.54
train mean loss: 1353.86
epoch train time: 0:00:00.517843
elapsed time: 0:00:37.673992
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:08:26.882090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1290.69
 ---- batch: 020 ----
mean loss: 1258.97
 ---- batch: 030 ----
mean loss: 1267.91
 ---- batch: 040 ----
mean loss: 1214.13
 ---- batch: 050 ----
mean loss: 1243.05
 ---- batch: 060 ----
mean loss: 1222.30
 ---- batch: 070 ----
mean loss: 1233.77
 ---- batch: 080 ----
mean loss: 1185.83
 ---- batch: 090 ----
mean loss: 1201.18
train mean loss: 1230.57
epoch train time: 0:00:00.503615
elapsed time: 0:00:38.177754
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:08:27.385856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1149.38
 ---- batch: 020 ----
mean loss: 1168.39
 ---- batch: 030 ----
mean loss: 1163.23
 ---- batch: 040 ----
mean loss: 1126.57
 ---- batch: 050 ----
mean loss: 1125.92
 ---- batch: 060 ----
mean loss: 1139.03
 ---- batch: 070 ----
mean loss: 1120.65
 ---- batch: 080 ----
mean loss: 1088.73
 ---- batch: 090 ----
mean loss: 1111.59
train mean loss: 1130.63
epoch train time: 0:00:00.509238
elapsed time: 0:00:38.687150
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:08:27.895252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1073.73
 ---- batch: 020 ----
mean loss: 1082.42
 ---- batch: 030 ----
mean loss: 1085.34
 ---- batch: 040 ----
mean loss: 1069.44
 ---- batch: 050 ----
mean loss: 1062.15
 ---- batch: 060 ----
mean loss: 1057.40
 ---- batch: 070 ----
mean loss: 1056.43
 ---- batch: 080 ----
mean loss: 1025.63
 ---- batch: 090 ----
mean loss: 1015.02
train mean loss: 1055.17
epoch train time: 0:00:00.491801
elapsed time: 0:00:39.179090
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:08:28.387187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1012.16
 ---- batch: 020 ----
mean loss: 1002.88
 ---- batch: 030 ----
mean loss: 1019.96
 ---- batch: 040 ----
mean loss: 996.90
 ---- batch: 050 ----
mean loss: 1025.91
 ---- batch: 060 ----
mean loss: 995.25
 ---- batch: 070 ----
mean loss: 982.60
 ---- batch: 080 ----
mean loss: 990.70
 ---- batch: 090 ----
mean loss: 986.56
train mean loss: 1000.71
epoch train time: 0:00:00.497453
elapsed time: 0:00:39.676684
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:08:28.884784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.51
 ---- batch: 020 ----
mean loss: 970.15
 ---- batch: 030 ----
mean loss: 965.09
 ---- batch: 040 ----
mean loss: 946.27
 ---- batch: 050 ----
mean loss: 960.99
 ---- batch: 060 ----
mean loss: 957.33
 ---- batch: 070 ----
mean loss: 968.45
 ---- batch: 080 ----
mean loss: 954.87
 ---- batch: 090 ----
mean loss: 952.49
train mean loss: 960.37
epoch train time: 0:00:00.496347
elapsed time: 0:00:40.173179
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:08:29.381298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 953.73
 ---- batch: 020 ----
mean loss: 945.31
 ---- batch: 030 ----
mean loss: 935.77
 ---- batch: 040 ----
mean loss: 917.39
 ---- batch: 050 ----
mean loss: 930.67
 ---- batch: 060 ----
mean loss: 924.08
 ---- batch: 070 ----
mean loss: 923.63
 ---- batch: 080 ----
mean loss: 936.86
 ---- batch: 090 ----
mean loss: 928.96
train mean loss: 932.48
epoch train time: 0:00:00.506365
elapsed time: 0:00:40.679717
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:08:29.887820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.97
 ---- batch: 020 ----
mean loss: 916.11
 ---- batch: 030 ----
mean loss: 916.38
 ---- batch: 040 ----
mean loss: 921.60
 ---- batch: 050 ----
mean loss: 915.64
 ---- batch: 060 ----
mean loss: 904.28
 ---- batch: 070 ----
mean loss: 888.04
 ---- batch: 080 ----
mean loss: 909.65
 ---- batch: 090 ----
mean loss: 906.36
train mean loss: 912.09
epoch train time: 0:00:00.503439
elapsed time: 0:00:41.183313
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:08:30.391434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.06
 ---- batch: 020 ----
mean loss: 884.66
 ---- batch: 030 ----
mean loss: 897.72
 ---- batch: 040 ----
mean loss: 903.96
 ---- batch: 050 ----
mean loss: 881.02
 ---- batch: 060 ----
mean loss: 904.64
 ---- batch: 070 ----
mean loss: 909.31
 ---- batch: 080 ----
mean loss: 909.23
 ---- batch: 090 ----
mean loss: 880.52
train mean loss: 897.73
epoch train time: 0:00:00.509375
elapsed time: 0:00:41.692865
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:08:30.900968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.02
 ---- batch: 020 ----
mean loss: 885.47
 ---- batch: 030 ----
mean loss: 907.25
 ---- batch: 040 ----
mean loss: 896.99
 ---- batch: 050 ----
mean loss: 878.86
 ---- batch: 060 ----
mean loss: 874.08
 ---- batch: 070 ----
mean loss: 884.21
 ---- batch: 080 ----
mean loss: 881.54
 ---- batch: 090 ----
mean loss: 878.98
train mean loss: 888.14
epoch train time: 0:00:00.501687
elapsed time: 0:00:42.194708
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:08:31.402815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.43
 ---- batch: 020 ----
mean loss: 890.41
 ---- batch: 030 ----
mean loss: 872.26
 ---- batch: 040 ----
mean loss: 887.85
 ---- batch: 050 ----
mean loss: 885.29
 ---- batch: 060 ----
mean loss: 878.43
 ---- batch: 070 ----
mean loss: 865.26
 ---- batch: 080 ----
mean loss: 894.95
 ---- batch: 090 ----
mean loss: 888.95
train mean loss: 882.67
epoch train time: 0:00:00.509442
elapsed time: 0:00:42.704311
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:08:31.912402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.72
 ---- batch: 020 ----
mean loss: 892.42
 ---- batch: 030 ----
mean loss: 879.10
 ---- batch: 040 ----
mean loss: 886.12
 ---- batch: 050 ----
mean loss: 870.33
 ---- batch: 060 ----
mean loss: 880.07
 ---- batch: 070 ----
mean loss: 888.77
 ---- batch: 080 ----
mean loss: 867.33
 ---- batch: 090 ----
mean loss: 874.85
train mean loss: 879.57
epoch train time: 0:00:00.503094
elapsed time: 0:00:43.207545
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:08:32.415646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.69
 ---- batch: 020 ----
mean loss: 870.28
 ---- batch: 030 ----
mean loss: 869.91
 ---- batch: 040 ----
mean loss: 877.96
 ---- batch: 050 ----
mean loss: 894.93
 ---- batch: 060 ----
mean loss: 875.25
 ---- batch: 070 ----
mean loss: 860.00
 ---- batch: 080 ----
mean loss: 890.28
 ---- batch: 090 ----
mean loss: 880.61
train mean loss: 876.85
epoch train time: 0:00:00.508082
elapsed time: 0:00:43.715770
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:08:32.923869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.94
 ---- batch: 020 ----
mean loss: 888.90
 ---- batch: 030 ----
mean loss: 868.88
 ---- batch: 040 ----
mean loss: 857.44
 ---- batch: 050 ----
mean loss: 868.89
 ---- batch: 060 ----
mean loss: 891.95
 ---- batch: 070 ----
mean loss: 882.65
 ---- batch: 080 ----
mean loss: 872.09
 ---- batch: 090 ----
mean loss: 877.21
train mean loss: 876.76
epoch train time: 0:00:00.498383
elapsed time: 0:00:44.214301
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:08:33.422401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.36
 ---- batch: 020 ----
mean loss: 871.55
 ---- batch: 030 ----
mean loss: 862.00
 ---- batch: 040 ----
mean loss: 868.48
 ---- batch: 050 ----
mean loss: 879.01
 ---- batch: 060 ----
mean loss: 880.01
 ---- batch: 070 ----
mean loss: 876.21
 ---- batch: 080 ----
mean loss: 878.89
 ---- batch: 090 ----
mean loss: 885.00
train mean loss: 875.52
epoch train time: 0:00:00.510562
elapsed time: 0:00:44.725012
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:08:33.933133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.71
 ---- batch: 020 ----
mean loss: 871.24
 ---- batch: 030 ----
mean loss: 877.36
 ---- batch: 040 ----
mean loss: 858.69
 ---- batch: 050 ----
mean loss: 873.11
 ---- batch: 060 ----
mean loss: 866.24
 ---- batch: 070 ----
mean loss: 881.68
 ---- batch: 080 ----
mean loss: 881.43
 ---- batch: 090 ----
mean loss: 873.35
train mean loss: 876.46
epoch train time: 0:00:00.501497
elapsed time: 0:00:45.226677
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:08:34.434779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.96
 ---- batch: 020 ----
mean loss: 889.25
 ---- batch: 030 ----
mean loss: 883.27
 ---- batch: 040 ----
mean loss: 876.03
 ---- batch: 050 ----
mean loss: 881.07
 ---- batch: 060 ----
mean loss: 877.85
 ---- batch: 070 ----
mean loss: 873.91
 ---- batch: 080 ----
mean loss: 865.97
 ---- batch: 090 ----
mean loss: 883.83
train mean loss: 875.30
epoch train time: 0:00:00.507960
elapsed time: 0:00:45.734793
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:08:34.942895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.49
 ---- batch: 020 ----
mean loss: 869.80
 ---- batch: 030 ----
mean loss: 863.51
 ---- batch: 040 ----
mean loss: 866.49
 ---- batch: 050 ----
mean loss: 864.77
 ---- batch: 060 ----
mean loss: 901.99
 ---- batch: 070 ----
mean loss: 883.64
 ---- batch: 080 ----
mean loss: 878.32
 ---- batch: 090 ----
mean loss: 869.28
train mean loss: 874.95
epoch train time: 0:00:00.499574
elapsed time: 0:00:46.234512
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:08:35.442643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.24
 ---- batch: 020 ----
mean loss: 873.56
 ---- batch: 030 ----
mean loss: 871.84
 ---- batch: 040 ----
mean loss: 872.94
 ---- batch: 050 ----
mean loss: 864.76
 ---- batch: 060 ----
mean loss: 869.91
 ---- batch: 070 ----
mean loss: 879.66
 ---- batch: 080 ----
mean loss: 887.22
 ---- batch: 090 ----
mean loss: 887.98
train mean loss: 875.13
epoch train time: 0:00:00.497642
elapsed time: 0:00:46.732344
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:08:35.940453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.91
 ---- batch: 020 ----
mean loss: 869.19
 ---- batch: 030 ----
mean loss: 878.20
 ---- batch: 040 ----
mean loss: 885.30
 ---- batch: 050 ----
mean loss: 873.31
 ---- batch: 060 ----
mean loss: 871.07
 ---- batch: 070 ----
mean loss: 862.65
 ---- batch: 080 ----
mean loss: 892.07
 ---- batch: 090 ----
mean loss: 862.45
train mean loss: 874.90
epoch train time: 0:00:00.496104
elapsed time: 0:00:47.228601
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:08:36.436701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.85
 ---- batch: 020 ----
mean loss: 874.98
 ---- batch: 030 ----
mean loss: 861.94
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.67
 ---- batch: 060 ----
mean loss: 883.31
 ---- batch: 070 ----
mean loss: 889.94
 ---- batch: 080 ----
mean loss: 863.93
 ---- batch: 090 ----
mean loss: 859.38
train mean loss: 875.83
epoch train time: 0:00:00.501787
elapsed time: 0:00:47.730539
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:08:36.938639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.41
 ---- batch: 030 ----
mean loss: 870.58
 ---- batch: 040 ----
mean loss: 876.62
 ---- batch: 050 ----
mean loss: 875.44
 ---- batch: 060 ----
mean loss: 878.09
 ---- batch: 070 ----
mean loss: 872.72
 ---- batch: 080 ----
mean loss: 870.18
 ---- batch: 090 ----
mean loss: 857.34
train mean loss: 874.97
epoch train time: 0:00:00.499765
elapsed time: 0:00:48.230488
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:08:37.438591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.87
 ---- batch: 020 ----
mean loss: 871.06
 ---- batch: 030 ----
mean loss: 868.38
 ---- batch: 040 ----
mean loss: 878.45
 ---- batch: 050 ----
mean loss: 892.62
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.92
 ---- batch: 090 ----
mean loss: 861.12
train mean loss: 875.07
epoch train time: 0:00:00.513180
elapsed time: 0:00:48.743845
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:08:37.951962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.12
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.03
 ---- batch: 050 ----
mean loss: 876.38
 ---- batch: 060 ----
mean loss: 886.67
 ---- batch: 070 ----
mean loss: 861.70
 ---- batch: 080 ----
mean loss: 882.75
 ---- batch: 090 ----
mean loss: 870.73
train mean loss: 874.91
epoch train time: 0:00:00.511329
elapsed time: 0:00:49.255368
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:08:38.463463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.45
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.21
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.77
train mean loss: 876.11
epoch train time: 0:00:00.503254
elapsed time: 0:00:49.758764
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:08:38.966864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.87
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.28
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.77
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.496917
elapsed time: 0:00:50.255832
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:08:39.463925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.82
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.57
 ---- batch: 050 ----
mean loss: 870.22
 ---- batch: 060 ----
mean loss: 889.57
 ---- batch: 070 ----
mean loss: 878.82
 ---- batch: 080 ----
mean loss: 875.94
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.70
epoch train time: 0:00:00.504987
elapsed time: 0:00:50.760955
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:08:39.969054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.45
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.42
 ---- batch: 070 ----
mean loss: 861.53
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.21
train mean loss: 875.33
epoch train time: 0:00:00.507316
elapsed time: 0:00:51.268416
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:08:40.476519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.85
 ---- batch: 040 ----
mean loss: 896.13
 ---- batch: 050 ----
mean loss: 895.65
 ---- batch: 060 ----
mean loss: 872.60
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.88
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.503682
elapsed time: 0:00:51.772244
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:08:40.980345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.28
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 881.99
 ---- batch: 090 ----
mean loss: 857.68
train mean loss: 875.42
epoch train time: 0:00:00.494086
elapsed time: 0:00:52.266503
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:08:41.474621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.88
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.12
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.70
train mean loss: 875.94
epoch train time: 0:00:00.502167
elapsed time: 0:00:52.768836
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:08:41.976939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.24
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.78
 ---- batch: 080 ----
mean loss: 872.87
 ---- batch: 090 ----
mean loss: 880.57
train mean loss: 875.20
epoch train time: 0:00:00.513106
elapsed time: 0:00:53.282095
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:08:42.490197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.89
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.50
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.94
 ---- batch: 090 ----
mean loss: 867.29
train mean loss: 874.55
epoch train time: 0:00:00.509983
elapsed time: 0:00:53.792223
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:08:43.000322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.83
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.44
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.495984
elapsed time: 0:00:54.288349
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:08:43.496448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.13
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.49
 ---- batch: 060 ----
mean loss: 873.51
 ---- batch: 070 ----
mean loss: 889.66
 ---- batch: 080 ----
mean loss: 893.93
 ---- batch: 090 ----
mean loss: 900.17
train mean loss: 875.66
epoch train time: 0:00:00.503946
elapsed time: 0:00:54.792454
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:08:44.000554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.41
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.91
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.11
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.23
epoch train time: 0:00:00.497621
elapsed time: 0:00:55.290234
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:08:44.498332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.91
 ---- batch: 020 ----
mean loss: 896.00
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.44
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.57
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.65
epoch train time: 0:00:00.506182
elapsed time: 0:00:55.796558
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:08:45.004658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.23
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.62
 ---- batch: 080 ----
mean loss: 876.95
 ---- batch: 090 ----
mean loss: 864.58
train mean loss: 874.88
epoch train time: 0:00:00.497895
elapsed time: 0:00:56.294608
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:08:45.502725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.46
 ---- batch: 070 ----
mean loss: 894.00
 ---- batch: 080 ----
mean loss: 865.67
 ---- batch: 090 ----
mean loss: 871.93
train mean loss: 875.10
epoch train time: 0:00:00.499794
elapsed time: 0:00:56.794573
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:08:46.002673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.70
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.70
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.65
 ---- batch: 060 ----
mean loss: 876.81
 ---- batch: 070 ----
mean loss: 871.95
 ---- batch: 080 ----
mean loss: 877.45
 ---- batch: 090 ----
mean loss: 896.57
train mean loss: 874.85
epoch train time: 0:00:00.493730
elapsed time: 0:00:57.288444
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:08:46.496558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.54
 ---- batch: 020 ----
mean loss: 874.88
 ---- batch: 030 ----
mean loss: 842.41
 ---- batch: 040 ----
mean loss: 870.90
 ---- batch: 050 ----
mean loss: 879.94
 ---- batch: 060 ----
mean loss: 870.33
 ---- batch: 070 ----
mean loss: 893.30
 ---- batch: 080 ----
mean loss: 874.30
 ---- batch: 090 ----
mean loss: 901.59
train mean loss: 874.94
epoch train time: 0:00:00.509718
elapsed time: 0:00:57.798353
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:08:47.006454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.79
 ---- batch: 020 ----
mean loss: 885.95
 ---- batch: 030 ----
mean loss: 869.68
 ---- batch: 040 ----
mean loss: 881.87
 ---- batch: 050 ----
mean loss: 866.17
 ---- batch: 060 ----
mean loss: 869.62
 ---- batch: 070 ----
mean loss: 865.49
 ---- batch: 080 ----
mean loss: 885.12
 ---- batch: 090 ----
mean loss: 878.95
train mean loss: 874.96
epoch train time: 0:00:00.507728
elapsed time: 0:00:58.306259
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:08:47.514359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.61
 ---- batch: 020 ----
mean loss: 893.23
 ---- batch: 030 ----
mean loss: 872.98
 ---- batch: 040 ----
mean loss: 888.76
 ---- batch: 050 ----
mean loss: 867.77
 ---- batch: 060 ----
mean loss: 869.08
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 858.03
 ---- batch: 090 ----
mean loss: 875.46
train mean loss: 875.15
epoch train time: 0:00:00.515866
elapsed time: 0:00:58.822278
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:08:48.030368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.42
 ---- batch: 020 ----
mean loss: 873.35
 ---- batch: 030 ----
mean loss: 870.55
 ---- batch: 040 ----
mean loss: 862.72
 ---- batch: 050 ----
mean loss: 871.30
 ---- batch: 060 ----
mean loss: 875.63
 ---- batch: 070 ----
mean loss: 898.82
 ---- batch: 080 ----
mean loss: 874.46
 ---- batch: 090 ----
mean loss: 887.67
train mean loss: 875.69
epoch train time: 0:00:00.501222
elapsed time: 0:00:59.323634
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:08:48.531740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.20
 ---- batch: 020 ----
mean loss: 875.67
 ---- batch: 030 ----
mean loss: 868.29
 ---- batch: 040 ----
mean loss: 876.40
 ---- batch: 050 ----
mean loss: 869.82
 ---- batch: 060 ----
mean loss: 874.43
 ---- batch: 070 ----
mean loss: 877.11
 ---- batch: 080 ----
mean loss: 890.28
 ---- batch: 090 ----
mean loss: 870.38
train mean loss: 875.10
epoch train time: 0:00:00.517919
elapsed time: 0:00:59.841707
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:08:49.049828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.39
 ---- batch: 020 ----
mean loss: 863.20
 ---- batch: 030 ----
mean loss: 884.34
 ---- batch: 040 ----
mean loss: 878.62
 ---- batch: 050 ----
mean loss: 875.71
 ---- batch: 060 ----
mean loss: 870.58
 ---- batch: 070 ----
mean loss: 875.20
 ---- batch: 080 ----
mean loss: 873.82
 ---- batch: 090 ----
mean loss: 883.90
train mean loss: 874.97
epoch train time: 0:00:00.506115
elapsed time: 0:01:00.347995
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:08:49.556093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.10
 ---- batch: 020 ----
mean loss: 853.59
 ---- batch: 030 ----
mean loss: 878.74
 ---- batch: 040 ----
mean loss: 879.11
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 858.45
 ---- batch: 070 ----
mean loss: 882.02
 ---- batch: 080 ----
mean loss: 887.78
 ---- batch: 090 ----
mean loss: 862.44
train mean loss: 875.73
epoch train time: 0:00:00.508222
elapsed time: 0:01:00.856361
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:08:50.064462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.91
 ---- batch: 020 ----
mean loss: 854.59
 ---- batch: 030 ----
mean loss: 865.08
 ---- batch: 040 ----
mean loss: 868.00
 ---- batch: 050 ----
mean loss: 891.59
 ---- batch: 060 ----
mean loss: 872.24
 ---- batch: 070 ----
mean loss: 913.58
 ---- batch: 080 ----
mean loss: 859.98
 ---- batch: 090 ----
mean loss: 881.53
train mean loss: 875.41
epoch train time: 0:00:00.506713
elapsed time: 0:01:01.363221
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:08:50.571326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.25
 ---- batch: 020 ----
mean loss: 870.54
 ---- batch: 030 ----
mean loss: 879.46
 ---- batch: 040 ----
mean loss: 867.01
 ---- batch: 050 ----
mean loss: 886.26
 ---- batch: 060 ----
mean loss: 875.59
 ---- batch: 070 ----
mean loss: 878.31
 ---- batch: 080 ----
mean loss: 871.45
 ---- batch: 090 ----
mean loss: 869.87
train mean loss: 875.50
epoch train time: 0:00:00.502398
elapsed time: 0:01:01.865773
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:08:51.073874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.84
 ---- batch: 020 ----
mean loss: 877.43
 ---- batch: 030 ----
mean loss: 858.26
 ---- batch: 040 ----
mean loss: 818.74
 ---- batch: 050 ----
mean loss: 799.26
 ---- batch: 060 ----
mean loss: 769.60
 ---- batch: 070 ----
mean loss: 736.17
 ---- batch: 080 ----
mean loss: 699.22
 ---- batch: 090 ----
mean loss: 627.98
train mean loss: 768.11
epoch train time: 0:00:00.503888
elapsed time: 0:01:02.369811
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:08:51.577920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.83
 ---- batch: 020 ----
mean loss: 462.78
 ---- batch: 030 ----
mean loss: 438.10
 ---- batch: 040 ----
mean loss: 422.02
 ---- batch: 050 ----
mean loss: 407.74
 ---- batch: 060 ----
mean loss: 391.43
 ---- batch: 070 ----
mean loss: 383.13
 ---- batch: 080 ----
mean loss: 370.84
 ---- batch: 090 ----
mean loss: 366.90
train mean loss: 413.54
epoch train time: 0:00:00.509898
elapsed time: 0:01:02.879866
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:08:52.087966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.11
 ---- batch: 020 ----
mean loss: 346.27
 ---- batch: 030 ----
mean loss: 338.75
 ---- batch: 040 ----
mean loss: 328.19
 ---- batch: 050 ----
mean loss: 320.67
 ---- batch: 060 ----
mean loss: 318.32
 ---- batch: 070 ----
mean loss: 310.64
 ---- batch: 080 ----
mean loss: 302.93
 ---- batch: 090 ----
mean loss: 300.42
train mean loss: 322.90
epoch train time: 0:00:00.512173
elapsed time: 0:01:03.392188
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:08:52.600289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.21
 ---- batch: 020 ----
mean loss: 285.51
 ---- batch: 030 ----
mean loss: 279.79
 ---- batch: 040 ----
mean loss: 282.49
 ---- batch: 050 ----
mean loss: 274.83
 ---- batch: 060 ----
mean loss: 271.87
 ---- batch: 070 ----
mean loss: 281.36
 ---- batch: 080 ----
mean loss: 277.47
 ---- batch: 090 ----
mean loss: 277.23
train mean loss: 278.79
epoch train time: 0:00:00.502083
elapsed time: 0:01:03.894419
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:08:53.102538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.03
 ---- batch: 020 ----
mean loss: 255.86
 ---- batch: 030 ----
mean loss: 253.66
 ---- batch: 040 ----
mean loss: 249.35
 ---- batch: 050 ----
mean loss: 256.89
 ---- batch: 060 ----
mean loss: 265.58
 ---- batch: 070 ----
mean loss: 253.74
 ---- batch: 080 ----
mean loss: 248.68
 ---- batch: 090 ----
mean loss: 264.44
train mean loss: 257.02
epoch train time: 0:00:00.496397
elapsed time: 0:01:04.390980
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:08:53.599081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.33
 ---- batch: 020 ----
mean loss: 241.72
 ---- batch: 030 ----
mean loss: 249.87
 ---- batch: 040 ----
mean loss: 237.86
 ---- batch: 050 ----
mean loss: 245.22
 ---- batch: 060 ----
mean loss: 249.40
 ---- batch: 070 ----
mean loss: 249.32
 ---- batch: 080 ----
mean loss: 247.14
 ---- batch: 090 ----
mean loss: 243.44
train mean loss: 245.34
epoch train time: 0:00:00.517467
elapsed time: 0:01:04.908593
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:08:54.116693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.10
 ---- batch: 020 ----
mean loss: 244.81
 ---- batch: 030 ----
mean loss: 237.77
 ---- batch: 040 ----
mean loss: 241.18
 ---- batch: 050 ----
mean loss: 252.03
 ---- batch: 060 ----
mean loss: 233.33
 ---- batch: 070 ----
mean loss: 230.95
 ---- batch: 080 ----
mean loss: 232.63
 ---- batch: 090 ----
mean loss: 228.12
train mean loss: 237.33
epoch train time: 0:00:00.501351
elapsed time: 0:01:05.410095
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:08:54.618197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.33
 ---- batch: 020 ----
mean loss: 230.78
 ---- batch: 030 ----
mean loss: 241.39
 ---- batch: 040 ----
mean loss: 234.84
 ---- batch: 050 ----
mean loss: 237.51
 ---- batch: 060 ----
mean loss: 235.96
 ---- batch: 070 ----
mean loss: 224.91
 ---- batch: 080 ----
mean loss: 226.07
 ---- batch: 090 ----
mean loss: 229.63
train mean loss: 232.70
epoch train time: 0:00:00.510041
elapsed time: 0:01:05.920302
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:08:55.128402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.55
 ---- batch: 020 ----
mean loss: 224.14
 ---- batch: 030 ----
mean loss: 229.79
 ---- batch: 040 ----
mean loss: 224.32
 ---- batch: 050 ----
mean loss: 228.36
 ---- batch: 060 ----
mean loss: 228.56
 ---- batch: 070 ----
mean loss: 225.31
 ---- batch: 080 ----
mean loss: 231.55
 ---- batch: 090 ----
mean loss: 224.09
train mean loss: 226.86
epoch train time: 0:00:00.507347
elapsed time: 0:01:06.427796
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:08:55.635899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.17
 ---- batch: 020 ----
mean loss: 216.71
 ---- batch: 030 ----
mean loss: 235.00
 ---- batch: 040 ----
mean loss: 232.16
 ---- batch: 050 ----
mean loss: 226.91
 ---- batch: 060 ----
mean loss: 233.85
 ---- batch: 070 ----
mean loss: 219.89
 ---- batch: 080 ----
mean loss: 221.18
 ---- batch: 090 ----
mean loss: 217.06
train mean loss: 224.26
epoch train time: 0:00:00.507553
elapsed time: 0:01:06.935496
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:08:56.143596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.22
 ---- batch: 020 ----
mean loss: 222.42
 ---- batch: 030 ----
mean loss: 217.12
 ---- batch: 040 ----
mean loss: 212.89
 ---- batch: 050 ----
mean loss: 215.86
 ---- batch: 060 ----
mean loss: 215.02
 ---- batch: 070 ----
mean loss: 220.70
 ---- batch: 080 ----
mean loss: 220.33
 ---- batch: 090 ----
mean loss: 213.72
train mean loss: 217.89
epoch train time: 0:00:00.502989
elapsed time: 0:01:07.438635
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:08:56.646751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.40
 ---- batch: 020 ----
mean loss: 221.57
 ---- batch: 030 ----
mean loss: 206.34
 ---- batch: 040 ----
mean loss: 211.67
 ---- batch: 050 ----
mean loss: 210.99
 ---- batch: 060 ----
mean loss: 215.18
 ---- batch: 070 ----
mean loss: 224.87
 ---- batch: 080 ----
mean loss: 213.72
 ---- batch: 090 ----
mean loss: 220.01
train mean loss: 215.51
epoch train time: 0:00:00.502283
elapsed time: 0:01:07.941079
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:08:57.149191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.43
 ---- batch: 020 ----
mean loss: 210.07
 ---- batch: 030 ----
mean loss: 209.23
 ---- batch: 040 ----
mean loss: 210.79
 ---- batch: 050 ----
mean loss: 212.94
 ---- batch: 060 ----
mean loss: 205.75
 ---- batch: 070 ----
mean loss: 201.56
 ---- batch: 080 ----
mean loss: 220.32
 ---- batch: 090 ----
mean loss: 217.99
train mean loss: 212.73
epoch train time: 0:00:00.497411
elapsed time: 0:01:08.438682
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:08:57.646817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.11
 ---- batch: 020 ----
mean loss: 202.74
 ---- batch: 030 ----
mean loss: 210.09
 ---- batch: 040 ----
mean loss: 218.05
 ---- batch: 050 ----
mean loss: 212.92
 ---- batch: 060 ----
mean loss: 209.82
 ---- batch: 070 ----
mean loss: 209.43
 ---- batch: 080 ----
mean loss: 205.57
 ---- batch: 090 ----
mean loss: 208.60
train mean loss: 208.79
epoch train time: 0:00:00.507916
elapsed time: 0:01:08.946781
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:08:58.154883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.53
 ---- batch: 020 ----
mean loss: 205.32
 ---- batch: 030 ----
mean loss: 206.89
 ---- batch: 040 ----
mean loss: 209.53
 ---- batch: 050 ----
mean loss: 210.00
 ---- batch: 060 ----
mean loss: 210.94
 ---- batch: 070 ----
mean loss: 203.76
 ---- batch: 080 ----
mean loss: 211.89
 ---- batch: 090 ----
mean loss: 213.62
train mean loss: 208.37
epoch train time: 0:00:00.503476
elapsed time: 0:01:09.450426
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:08:58.658526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.41
 ---- batch: 020 ----
mean loss: 191.94
 ---- batch: 030 ----
mean loss: 207.17
 ---- batch: 040 ----
mean loss: 204.96
 ---- batch: 050 ----
mean loss: 213.02
 ---- batch: 060 ----
mean loss: 206.10
 ---- batch: 070 ----
mean loss: 206.48
 ---- batch: 080 ----
mean loss: 212.14
 ---- batch: 090 ----
mean loss: 206.47
train mean loss: 204.46
epoch train time: 0:00:00.496022
elapsed time: 0:01:09.946608
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:08:59.154709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.12
 ---- batch: 020 ----
mean loss: 201.11
 ---- batch: 030 ----
mean loss: 208.76
 ---- batch: 040 ----
mean loss: 196.46
 ---- batch: 050 ----
mean loss: 200.95
 ---- batch: 060 ----
mean loss: 204.11
 ---- batch: 070 ----
mean loss: 199.18
 ---- batch: 080 ----
mean loss: 208.15
 ---- batch: 090 ----
mean loss: 204.82
train mean loss: 202.74
epoch train time: 0:00:00.497869
elapsed time: 0:01:10.444626
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:08:59.652727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.52
 ---- batch: 020 ----
mean loss: 200.16
 ---- batch: 030 ----
mean loss: 198.37
 ---- batch: 040 ----
mean loss: 204.35
 ---- batch: 050 ----
mean loss: 198.44
 ---- batch: 060 ----
mean loss: 203.77
 ---- batch: 070 ----
mean loss: 197.27
 ---- batch: 080 ----
mean loss: 199.98
 ---- batch: 090 ----
mean loss: 207.67
train mean loss: 201.55
epoch train time: 0:00:00.503110
elapsed time: 0:01:10.947884
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:09:00.155985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.89
 ---- batch: 020 ----
mean loss: 197.53
 ---- batch: 030 ----
mean loss: 201.37
 ---- batch: 040 ----
mean loss: 203.45
 ---- batch: 050 ----
mean loss: 193.38
 ---- batch: 060 ----
mean loss: 196.87
 ---- batch: 070 ----
mean loss: 206.17
 ---- batch: 080 ----
mean loss: 200.69
 ---- batch: 090 ----
mean loss: 198.01
train mean loss: 199.19
epoch train time: 0:00:00.497028
elapsed time: 0:01:11.445058
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:09:00.653185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.24
 ---- batch: 020 ----
mean loss: 198.04
 ---- batch: 030 ----
mean loss: 185.89
 ---- batch: 040 ----
mean loss: 194.63
 ---- batch: 050 ----
mean loss: 202.53
 ---- batch: 060 ----
mean loss: 195.27
 ---- batch: 070 ----
mean loss: 200.83
 ---- batch: 080 ----
mean loss: 203.80
 ---- batch: 090 ----
mean loss: 197.00
train mean loss: 197.14
epoch train time: 0:00:00.506133
elapsed time: 0:01:11.951362
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:09:01.159462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.63
 ---- batch: 020 ----
mean loss: 193.58
 ---- batch: 030 ----
mean loss: 193.01
 ---- batch: 040 ----
mean loss: 199.53
 ---- batch: 050 ----
mean loss: 184.13
 ---- batch: 060 ----
mean loss: 194.79
 ---- batch: 070 ----
mean loss: 196.26
 ---- batch: 080 ----
mean loss: 196.76
 ---- batch: 090 ----
mean loss: 195.29
train mean loss: 193.70
epoch train time: 0:00:00.500116
elapsed time: 0:01:12.451639
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:09:01.659740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.25
 ---- batch: 020 ----
mean loss: 190.86
 ---- batch: 030 ----
mean loss: 190.56
 ---- batch: 040 ----
mean loss: 201.14
 ---- batch: 050 ----
mean loss: 190.25
 ---- batch: 060 ----
mean loss: 189.36
 ---- batch: 070 ----
mean loss: 206.07
 ---- batch: 080 ----
mean loss: 195.40
 ---- batch: 090 ----
mean loss: 193.01
train mean loss: 192.73
epoch train time: 0:00:00.510629
elapsed time: 0:01:12.962478
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:09:02.170594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.88
 ---- batch: 020 ----
mean loss: 185.99
 ---- batch: 030 ----
mean loss: 187.45
 ---- batch: 040 ----
mean loss: 201.52
 ---- batch: 050 ----
mean loss: 192.45
 ---- batch: 060 ----
mean loss: 197.48
 ---- batch: 070 ----
mean loss: 184.81
 ---- batch: 080 ----
mean loss: 192.61
 ---- batch: 090 ----
mean loss: 192.87
train mean loss: 190.52
epoch train time: 0:00:00.506622
elapsed time: 0:01:13.469279
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:09:02.677401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.74
 ---- batch: 020 ----
mean loss: 183.26
 ---- batch: 030 ----
mean loss: 182.45
 ---- batch: 040 ----
mean loss: 186.66
 ---- batch: 050 ----
mean loss: 193.05
 ---- batch: 060 ----
mean loss: 191.13
 ---- batch: 070 ----
mean loss: 187.81
 ---- batch: 080 ----
mean loss: 190.00
 ---- batch: 090 ----
mean loss: 193.05
train mean loss: 189.04
epoch train time: 0:00:00.502846
elapsed time: 0:01:13.972292
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:09:03.180409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.36
 ---- batch: 020 ----
mean loss: 184.03
 ---- batch: 030 ----
mean loss: 194.49
 ---- batch: 040 ----
mean loss: 191.57
 ---- batch: 050 ----
mean loss: 189.39
 ---- batch: 060 ----
mean loss: 183.63
 ---- batch: 070 ----
mean loss: 185.06
 ---- batch: 080 ----
mean loss: 186.14
 ---- batch: 090 ----
mean loss: 189.23
train mean loss: 187.24
epoch train time: 0:00:00.500741
elapsed time: 0:01:14.473202
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:09:03.681314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.11
 ---- batch: 020 ----
mean loss: 183.36
 ---- batch: 030 ----
mean loss: 177.82
 ---- batch: 040 ----
mean loss: 185.25
 ---- batch: 050 ----
mean loss: 193.65
 ---- batch: 060 ----
mean loss: 187.84
 ---- batch: 070 ----
mean loss: 181.10
 ---- batch: 080 ----
mean loss: 195.47
 ---- batch: 090 ----
mean loss: 187.22
train mean loss: 186.44
epoch train time: 0:00:00.506997
elapsed time: 0:01:14.980355
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:09:04.188474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.52
 ---- batch: 020 ----
mean loss: 176.78
 ---- batch: 030 ----
mean loss: 187.36
 ---- batch: 040 ----
mean loss: 180.99
 ---- batch: 050 ----
mean loss: 189.00
 ---- batch: 060 ----
mean loss: 191.60
 ---- batch: 070 ----
mean loss: 184.03
 ---- batch: 080 ----
mean loss: 188.61
 ---- batch: 090 ----
mean loss: 189.50
train mean loss: 186.29
epoch train time: 0:00:00.498538
elapsed time: 0:01:15.479059
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:09:04.687160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.31
 ---- batch: 020 ----
mean loss: 177.20
 ---- batch: 030 ----
mean loss: 184.19
 ---- batch: 040 ----
mean loss: 180.45
 ---- batch: 050 ----
mean loss: 184.80
 ---- batch: 060 ----
mean loss: 192.18
 ---- batch: 070 ----
mean loss: 175.75
 ---- batch: 080 ----
mean loss: 186.14
 ---- batch: 090 ----
mean loss: 194.48
train mean loss: 184.53
epoch train time: 0:00:00.500982
elapsed time: 0:01:15.980197
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:09:05.188295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.06
 ---- batch: 020 ----
mean loss: 188.42
 ---- batch: 030 ----
mean loss: 177.28
 ---- batch: 040 ----
mean loss: 186.14
 ---- batch: 050 ----
mean loss: 185.50
 ---- batch: 060 ----
mean loss: 177.64
 ---- batch: 070 ----
mean loss: 175.52
 ---- batch: 080 ----
mean loss: 185.19
 ---- batch: 090 ----
mean loss: 189.18
train mean loss: 183.73
epoch train time: 0:00:00.508601
elapsed time: 0:01:16.488950
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:09:05.697054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.30
 ---- batch: 020 ----
mean loss: 177.39
 ---- batch: 030 ----
mean loss: 184.93
 ---- batch: 040 ----
mean loss: 182.94
 ---- batch: 050 ----
mean loss: 181.33
 ---- batch: 060 ----
mean loss: 181.79
 ---- batch: 070 ----
mean loss: 177.94
 ---- batch: 080 ----
mean loss: 182.80
 ---- batch: 090 ----
mean loss: 172.06
train mean loss: 180.26
epoch train time: 0:00:00.510406
elapsed time: 0:01:16.999513
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:09:06.207615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.89
 ---- batch: 020 ----
mean loss: 180.73
 ---- batch: 030 ----
mean loss: 174.85
 ---- batch: 040 ----
mean loss: 181.17
 ---- batch: 050 ----
mean loss: 169.64
 ---- batch: 060 ----
mean loss: 181.81
 ---- batch: 070 ----
mean loss: 181.77
 ---- batch: 080 ----
mean loss: 182.33
 ---- batch: 090 ----
mean loss: 187.77
train mean loss: 180.53
epoch train time: 0:00:00.506144
elapsed time: 0:01:17.505808
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:09:06.713918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.47
 ---- batch: 020 ----
mean loss: 179.63
 ---- batch: 030 ----
mean loss: 175.40
 ---- batch: 040 ----
mean loss: 177.84
 ---- batch: 050 ----
mean loss: 176.91
 ---- batch: 060 ----
mean loss: 185.73
 ---- batch: 070 ----
mean loss: 177.50
 ---- batch: 080 ----
mean loss: 182.27
 ---- batch: 090 ----
mean loss: 181.16
train mean loss: 180.26
epoch train time: 0:00:00.499163
elapsed time: 0:01:18.005129
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:09:07.213231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.11
 ---- batch: 020 ----
mean loss: 179.62
 ---- batch: 030 ----
mean loss: 182.95
 ---- batch: 040 ----
mean loss: 179.04
 ---- batch: 050 ----
mean loss: 181.82
 ---- batch: 060 ----
mean loss: 179.16
 ---- batch: 070 ----
mean loss: 182.76
 ---- batch: 080 ----
mean loss: 182.05
 ---- batch: 090 ----
mean loss: 180.68
train mean loss: 180.00
epoch train time: 0:00:00.499062
elapsed time: 0:01:18.504354
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:09:07.712469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.25
 ---- batch: 020 ----
mean loss: 174.45
 ---- batch: 030 ----
mean loss: 167.27
 ---- batch: 040 ----
mean loss: 176.44
 ---- batch: 050 ----
mean loss: 180.32
 ---- batch: 060 ----
mean loss: 186.47
 ---- batch: 070 ----
mean loss: 176.79
 ---- batch: 080 ----
mean loss: 184.28
 ---- batch: 090 ----
mean loss: 179.72
train mean loss: 177.61
epoch train time: 0:00:00.508490
elapsed time: 0:01:19.013006
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:09:08.221125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.96
 ---- batch: 020 ----
mean loss: 180.94
 ---- batch: 030 ----
mean loss: 172.61
 ---- batch: 040 ----
mean loss: 173.71
 ---- batch: 050 ----
mean loss: 178.08
 ---- batch: 060 ----
mean loss: 182.90
 ---- batch: 070 ----
mean loss: 178.41
 ---- batch: 080 ----
mean loss: 179.66
 ---- batch: 090 ----
mean loss: 176.55
train mean loss: 177.66
epoch train time: 0:00:00.502789
elapsed time: 0:01:19.515969
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:09:08.724067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.62
 ---- batch: 020 ----
mean loss: 179.55
 ---- batch: 030 ----
mean loss: 175.86
 ---- batch: 040 ----
mean loss: 173.47
 ---- batch: 050 ----
mean loss: 166.53
 ---- batch: 060 ----
mean loss: 180.71
 ---- batch: 070 ----
mean loss: 174.31
 ---- batch: 080 ----
mean loss: 180.09
 ---- batch: 090 ----
mean loss: 177.41
train mean loss: 176.04
epoch train time: 0:00:00.498729
elapsed time: 0:01:20.014844
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:09:09.222955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.76
 ---- batch: 020 ----
mean loss: 176.53
 ---- batch: 030 ----
mean loss: 173.60
 ---- batch: 040 ----
mean loss: 174.45
 ---- batch: 050 ----
mean loss: 177.95
 ---- batch: 060 ----
mean loss: 184.25
 ---- batch: 070 ----
mean loss: 180.73
 ---- batch: 080 ----
mean loss: 176.83
 ---- batch: 090 ----
mean loss: 171.49
train mean loss: 175.29
epoch train time: 0:00:00.496855
elapsed time: 0:01:20.511892
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:09:09.720009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.13
 ---- batch: 020 ----
mean loss: 167.58
 ---- batch: 030 ----
mean loss: 178.99
 ---- batch: 040 ----
mean loss: 168.88
 ---- batch: 050 ----
mean loss: 169.01
 ---- batch: 060 ----
mean loss: 184.35
 ---- batch: 070 ----
mean loss: 175.44
 ---- batch: 080 ----
mean loss: 179.68
 ---- batch: 090 ----
mean loss: 181.55
train mean loss: 175.02
epoch train time: 0:00:00.501364
elapsed time: 0:01:21.013454
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:09:10.221555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.16
 ---- batch: 020 ----
mean loss: 170.42
 ---- batch: 030 ----
mean loss: 171.20
 ---- batch: 040 ----
mean loss: 169.04
 ---- batch: 050 ----
mean loss: 179.78
 ---- batch: 060 ----
mean loss: 178.69
 ---- batch: 070 ----
mean loss: 173.24
 ---- batch: 080 ----
mean loss: 174.22
 ---- batch: 090 ----
mean loss: 177.74
train mean loss: 173.99
epoch train time: 0:00:00.509088
elapsed time: 0:01:21.522695
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:09:10.730800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.98
 ---- batch: 020 ----
mean loss: 165.49
 ---- batch: 030 ----
mean loss: 171.88
 ---- batch: 040 ----
mean loss: 174.41
 ---- batch: 050 ----
mean loss: 174.05
 ---- batch: 060 ----
mean loss: 173.72
 ---- batch: 070 ----
mean loss: 181.36
 ---- batch: 080 ----
mean loss: 176.68
 ---- batch: 090 ----
mean loss: 169.47
train mean loss: 173.13
epoch train time: 0:00:00.505816
elapsed time: 0:01:22.028689
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:09:11.236804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.65
 ---- batch: 020 ----
mean loss: 167.35
 ---- batch: 030 ----
mean loss: 169.68
 ---- batch: 040 ----
mean loss: 171.75
 ---- batch: 050 ----
mean loss: 172.17
 ---- batch: 060 ----
mean loss: 171.85
 ---- batch: 070 ----
mean loss: 174.33
 ---- batch: 080 ----
mean loss: 175.51
 ---- batch: 090 ----
mean loss: 177.07
train mean loss: 171.79
epoch train time: 0:00:00.505752
elapsed time: 0:01:22.534609
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:09:11.742713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.28
 ---- batch: 020 ----
mean loss: 174.71
 ---- batch: 030 ----
mean loss: 167.28
 ---- batch: 040 ----
mean loss: 171.50
 ---- batch: 050 ----
mean loss: 171.16
 ---- batch: 060 ----
mean loss: 172.32
 ---- batch: 070 ----
mean loss: 177.53
 ---- batch: 080 ----
mean loss: 172.83
 ---- batch: 090 ----
mean loss: 169.20
train mean loss: 171.31
epoch train time: 0:00:00.510283
elapsed time: 0:01:23.045039
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:09:12.253158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.55
 ---- batch: 020 ----
mean loss: 162.03
 ---- batch: 030 ----
mean loss: 167.34
 ---- batch: 040 ----
mean loss: 166.39
 ---- batch: 050 ----
mean loss: 177.65
 ---- batch: 060 ----
mean loss: 171.84
 ---- batch: 070 ----
mean loss: 169.28
 ---- batch: 080 ----
mean loss: 181.93
 ---- batch: 090 ----
mean loss: 173.76
train mean loss: 171.54
epoch train time: 0:00:00.503162
elapsed time: 0:01:23.548367
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:09:12.756468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.97
 ---- batch: 020 ----
mean loss: 164.39
 ---- batch: 030 ----
mean loss: 168.05
 ---- batch: 040 ----
mean loss: 167.83
 ---- batch: 050 ----
mean loss: 175.04
 ---- batch: 060 ----
mean loss: 161.13
 ---- batch: 070 ----
mean loss: 170.24
 ---- batch: 080 ----
mean loss: 171.13
 ---- batch: 090 ----
mean loss: 173.35
train mean loss: 169.67
epoch train time: 0:00:00.507419
elapsed time: 0:01:24.055951
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:09:13.264053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.13
 ---- batch: 020 ----
mean loss: 167.73
 ---- batch: 030 ----
mean loss: 168.75
 ---- batch: 040 ----
mean loss: 172.00
 ---- batch: 050 ----
mean loss: 174.41
 ---- batch: 060 ----
mean loss: 174.04
 ---- batch: 070 ----
mean loss: 167.42
 ---- batch: 080 ----
mean loss: 170.26
 ---- batch: 090 ----
mean loss: 166.36
train mean loss: 169.80
epoch train time: 0:00:00.505596
elapsed time: 0:01:24.561694
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:09:13.769793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.30
 ---- batch: 020 ----
mean loss: 165.37
 ---- batch: 030 ----
mean loss: 169.56
 ---- batch: 040 ----
mean loss: 166.26
 ---- batch: 050 ----
mean loss: 169.85
 ---- batch: 060 ----
mean loss: 168.70
 ---- batch: 070 ----
mean loss: 169.72
 ---- batch: 080 ----
mean loss: 169.14
 ---- batch: 090 ----
mean loss: 170.56
train mean loss: 169.43
epoch train time: 0:00:00.513319
elapsed time: 0:01:25.075174
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:09:14.283277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.37
 ---- batch: 020 ----
mean loss: 168.76
 ---- batch: 030 ----
mean loss: 160.24
 ---- batch: 040 ----
mean loss: 165.42
 ---- batch: 050 ----
mean loss: 166.70
 ---- batch: 060 ----
mean loss: 162.84
 ---- batch: 070 ----
mean loss: 173.58
 ---- batch: 080 ----
mean loss: 160.51
 ---- batch: 090 ----
mean loss: 170.98
train mean loss: 166.92
epoch train time: 0:00:00.501490
elapsed time: 0:01:25.576812
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:09:14.784911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.39
 ---- batch: 020 ----
mean loss: 166.40
 ---- batch: 030 ----
mean loss: 161.25
 ---- batch: 040 ----
mean loss: 169.41
 ---- batch: 050 ----
mean loss: 167.26
 ---- batch: 060 ----
mean loss: 168.13
 ---- batch: 070 ----
mean loss: 163.76
 ---- batch: 080 ----
mean loss: 166.87
 ---- batch: 090 ----
mean loss: 160.60
train mean loss: 166.81
epoch train time: 0:00:00.503485
elapsed time: 0:01:26.080449
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:09:15.288559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.65
 ---- batch: 020 ----
mean loss: 157.47
 ---- batch: 030 ----
mean loss: 172.38
 ---- batch: 040 ----
mean loss: 160.88
 ---- batch: 050 ----
mean loss: 160.07
 ---- batch: 060 ----
mean loss: 171.12
 ---- batch: 070 ----
mean loss: 171.04
 ---- batch: 080 ----
mean loss: 168.32
 ---- batch: 090 ----
mean loss: 169.88
train mean loss: 165.99
epoch train time: 0:00:00.501473
elapsed time: 0:01:26.582080
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:09:15.790201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.40
 ---- batch: 020 ----
mean loss: 168.39
 ---- batch: 030 ----
mean loss: 161.28
 ---- batch: 040 ----
mean loss: 161.45
 ---- batch: 050 ----
mean loss: 173.90
 ---- batch: 060 ----
mean loss: 168.89
 ---- batch: 070 ----
mean loss: 170.11
 ---- batch: 080 ----
mean loss: 166.88
 ---- batch: 090 ----
mean loss: 167.11
train mean loss: 166.83
epoch train time: 0:00:00.501734
elapsed time: 0:01:27.084002
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:09:16.292090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.41
 ---- batch: 020 ----
mean loss: 167.89
 ---- batch: 030 ----
mean loss: 161.57
 ---- batch: 040 ----
mean loss: 164.91
 ---- batch: 050 ----
mean loss: 164.64
 ---- batch: 060 ----
mean loss: 165.33
 ---- batch: 070 ----
mean loss: 167.86
 ---- batch: 080 ----
mean loss: 172.30
 ---- batch: 090 ----
mean loss: 167.67
train mean loss: 165.09
epoch train time: 0:00:00.498168
elapsed time: 0:01:27.582311
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:09:16.790414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.33
 ---- batch: 020 ----
mean loss: 164.80
 ---- batch: 030 ----
mean loss: 157.65
 ---- batch: 040 ----
mean loss: 165.79
 ---- batch: 050 ----
mean loss: 162.83
 ---- batch: 060 ----
mean loss: 167.72
 ---- batch: 070 ----
mean loss: 165.95
 ---- batch: 080 ----
mean loss: 166.12
 ---- batch: 090 ----
mean loss: 162.49
train mean loss: 164.82
epoch train time: 0:00:00.508304
elapsed time: 0:01:28.090774
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:09:17.298876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.43
 ---- batch: 020 ----
mean loss: 162.07
 ---- batch: 030 ----
mean loss: 166.40
 ---- batch: 040 ----
mean loss: 162.59
 ---- batch: 050 ----
mean loss: 162.91
 ---- batch: 060 ----
mean loss: 165.25
 ---- batch: 070 ----
mean loss: 168.44
 ---- batch: 080 ----
mean loss: 165.78
 ---- batch: 090 ----
mean loss: 160.30
train mean loss: 163.99
epoch train time: 0:00:00.507241
elapsed time: 0:01:28.598197
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:09:17.806297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.21
 ---- batch: 020 ----
mean loss: 160.74
 ---- batch: 030 ----
mean loss: 162.06
 ---- batch: 040 ----
mean loss: 163.20
 ---- batch: 050 ----
mean loss: 163.97
 ---- batch: 060 ----
mean loss: 167.85
 ---- batch: 070 ----
mean loss: 165.14
 ---- batch: 080 ----
mean loss: 164.47
 ---- batch: 090 ----
mean loss: 169.44
train mean loss: 165.09
epoch train time: 0:00:00.505494
elapsed time: 0:01:29.103849
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:09:18.311975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.85
 ---- batch: 020 ----
mean loss: 158.91
 ---- batch: 030 ----
mean loss: 171.91
 ---- batch: 040 ----
mean loss: 164.96
 ---- batch: 050 ----
mean loss: 163.78
 ---- batch: 060 ----
mean loss: 164.35
 ---- batch: 070 ----
mean loss: 169.31
 ---- batch: 080 ----
mean loss: 158.89
 ---- batch: 090 ----
mean loss: 160.16
train mean loss: 162.89
epoch train time: 0:00:00.504714
elapsed time: 0:01:29.608735
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:09:18.816836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.28
 ---- batch: 020 ----
mean loss: 166.49
 ---- batch: 030 ----
mean loss: 164.22
 ---- batch: 040 ----
mean loss: 155.89
 ---- batch: 050 ----
mean loss: 159.44
 ---- batch: 060 ----
mean loss: 168.80
 ---- batch: 070 ----
mean loss: 169.35
 ---- batch: 080 ----
mean loss: 159.97
 ---- batch: 090 ----
mean loss: 162.79
train mean loss: 163.40
epoch train time: 0:00:00.503364
elapsed time: 0:01:30.112250
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:09:19.320352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.86
 ---- batch: 020 ----
mean loss: 159.77
 ---- batch: 030 ----
mean loss: 164.04
 ---- batch: 040 ----
mean loss: 165.43
 ---- batch: 050 ----
mean loss: 165.96
 ---- batch: 060 ----
mean loss: 163.09
 ---- batch: 070 ----
mean loss: 155.54
 ---- batch: 080 ----
mean loss: 162.09
 ---- batch: 090 ----
mean loss: 164.19
train mean loss: 162.85
epoch train time: 0:00:00.528396
elapsed time: 0:01:30.640794
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:09:19.848927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.15
 ---- batch: 020 ----
mean loss: 162.66
 ---- batch: 030 ----
mean loss: 158.05
 ---- batch: 040 ----
mean loss: 159.86
 ---- batch: 050 ----
mean loss: 161.25
 ---- batch: 060 ----
mean loss: 167.30
 ---- batch: 070 ----
mean loss: 168.36
 ---- batch: 080 ----
mean loss: 153.58
 ---- batch: 090 ----
mean loss: 174.67
train mean loss: 163.15
epoch train time: 0:00:00.521384
elapsed time: 0:01:31.162370
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:09:20.370502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.33
 ---- batch: 020 ----
mean loss: 157.77
 ---- batch: 030 ----
mean loss: 158.79
 ---- batch: 040 ----
mean loss: 152.74
 ---- batch: 050 ----
mean loss: 166.10
 ---- batch: 060 ----
mean loss: 161.16
 ---- batch: 070 ----
mean loss: 163.93
 ---- batch: 080 ----
mean loss: 159.32
 ---- batch: 090 ----
mean loss: 168.61
train mean loss: 161.18
epoch train time: 0:00:00.519675
elapsed time: 0:01:31.682215
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:09:20.890327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.09
 ---- batch: 020 ----
mean loss: 151.46
 ---- batch: 030 ----
mean loss: 160.43
 ---- batch: 040 ----
mean loss: 157.55
 ---- batch: 050 ----
mean loss: 161.64
 ---- batch: 060 ----
mean loss: 165.52
 ---- batch: 070 ----
mean loss: 162.28
 ---- batch: 080 ----
mean loss: 167.42
 ---- batch: 090 ----
mean loss: 167.95
train mean loss: 160.23
epoch train time: 0:00:00.514537
elapsed time: 0:01:32.196915
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:09:21.405017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.27
 ---- batch: 020 ----
mean loss: 147.51
 ---- batch: 030 ----
mean loss: 156.85
 ---- batch: 040 ----
mean loss: 160.33
 ---- batch: 050 ----
mean loss: 159.17
 ---- batch: 060 ----
mean loss: 157.24
 ---- batch: 070 ----
mean loss: 157.76
 ---- batch: 080 ----
mean loss: 163.77
 ---- batch: 090 ----
mean loss: 172.66
train mean loss: 159.84
epoch train time: 0:00:00.517576
elapsed time: 0:01:32.714643
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:09:21.922744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.45
 ---- batch: 020 ----
mean loss: 155.87
 ---- batch: 030 ----
mean loss: 162.39
 ---- batch: 040 ----
mean loss: 165.38
 ---- batch: 050 ----
mean loss: 162.24
 ---- batch: 060 ----
mean loss: 161.07
 ---- batch: 070 ----
mean loss: 159.35
 ---- batch: 080 ----
mean loss: 160.07
 ---- batch: 090 ----
mean loss: 155.82
train mean loss: 159.66
epoch train time: 0:00:00.510100
elapsed time: 0:01:33.224915
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:09:22.433020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.11
 ---- batch: 020 ----
mean loss: 156.72
 ---- batch: 030 ----
mean loss: 158.27
 ---- batch: 040 ----
mean loss: 157.89
 ---- batch: 050 ----
mean loss: 160.86
 ---- batch: 060 ----
mean loss: 164.95
 ---- batch: 070 ----
mean loss: 158.05
 ---- batch: 080 ----
mean loss: 163.09
 ---- batch: 090 ----
mean loss: 156.99
train mean loss: 159.26
epoch train time: 0:00:00.522533
elapsed time: 0:01:33.747603
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:09:22.955706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.05
 ---- batch: 020 ----
mean loss: 158.54
 ---- batch: 030 ----
mean loss: 156.56
 ---- batch: 040 ----
mean loss: 159.89
 ---- batch: 050 ----
mean loss: 162.20
 ---- batch: 060 ----
mean loss: 157.93
 ---- batch: 070 ----
mean loss: 153.36
 ---- batch: 080 ----
mean loss: 162.74
 ---- batch: 090 ----
mean loss: 160.85
train mean loss: 158.70
epoch train time: 0:00:00.517224
elapsed time: 0:01:34.264998
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:09:23.473115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.16
 ---- batch: 020 ----
mean loss: 154.74
 ---- batch: 030 ----
mean loss: 157.59
 ---- batch: 040 ----
mean loss: 156.62
 ---- batch: 050 ----
mean loss: 160.84
 ---- batch: 060 ----
mean loss: 157.27
 ---- batch: 070 ----
mean loss: 162.16
 ---- batch: 080 ----
mean loss: 165.78
 ---- batch: 090 ----
mean loss: 157.85
train mean loss: 158.82
epoch train time: 0:00:00.525404
elapsed time: 0:01:34.790582
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:09:23.998703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.83
 ---- batch: 020 ----
mean loss: 163.78
 ---- batch: 030 ----
mean loss: 153.18
 ---- batch: 040 ----
mean loss: 157.73
 ---- batch: 050 ----
mean loss: 158.53
 ---- batch: 060 ----
mean loss: 159.92
 ---- batch: 070 ----
mean loss: 154.02
 ---- batch: 080 ----
mean loss: 163.47
 ---- batch: 090 ----
mean loss: 159.68
train mean loss: 158.43
epoch train time: 0:00:00.522065
elapsed time: 0:01:35.312820
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:09:24.520923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.06
 ---- batch: 020 ----
mean loss: 153.13
 ---- batch: 030 ----
mean loss: 150.13
 ---- batch: 040 ----
mean loss: 156.97
 ---- batch: 050 ----
mean loss: 162.14
 ---- batch: 060 ----
mean loss: 156.04
 ---- batch: 070 ----
mean loss: 161.84
 ---- batch: 080 ----
mean loss: 161.32
 ---- batch: 090 ----
mean loss: 160.15
train mean loss: 157.01
epoch train time: 0:00:00.519881
elapsed time: 0:01:35.832859
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:09:25.040965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.41
 ---- batch: 020 ----
mean loss: 155.71
 ---- batch: 030 ----
mean loss: 148.00
 ---- batch: 040 ----
mean loss: 161.35
 ---- batch: 050 ----
mean loss: 154.67
 ---- batch: 060 ----
mean loss: 165.24
 ---- batch: 070 ----
mean loss: 161.20
 ---- batch: 080 ----
mean loss: 156.71
 ---- batch: 090 ----
mean loss: 153.05
train mean loss: 157.04
epoch train time: 0:00:00.507658
elapsed time: 0:01:36.340673
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:09:25.548807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.23
 ---- batch: 020 ----
mean loss: 157.49
 ---- batch: 030 ----
mean loss: 151.44
 ---- batch: 040 ----
mean loss: 156.76
 ---- batch: 050 ----
mean loss: 154.18
 ---- batch: 060 ----
mean loss: 156.41
 ---- batch: 070 ----
mean loss: 156.36
 ---- batch: 080 ----
mean loss: 158.95
 ---- batch: 090 ----
mean loss: 164.34
train mean loss: 156.46
epoch train time: 0:00:00.504467
elapsed time: 0:01:36.845319
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:09:26.053422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.44
 ---- batch: 020 ----
mean loss: 153.51
 ---- batch: 030 ----
mean loss: 151.94
 ---- batch: 040 ----
mean loss: 159.19
 ---- batch: 050 ----
mean loss: 152.05
 ---- batch: 060 ----
mean loss: 160.00
 ---- batch: 070 ----
mean loss: 156.47
 ---- batch: 080 ----
mean loss: 163.07
 ---- batch: 090 ----
mean loss: 164.38
train mean loss: 156.08
epoch train time: 0:00:00.510508
elapsed time: 0:01:37.356022
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:09:26.564129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.38
 ---- batch: 020 ----
mean loss: 155.08
 ---- batch: 030 ----
mean loss: 152.86
 ---- batch: 040 ----
mean loss: 163.08
 ---- batch: 050 ----
mean loss: 157.57
 ---- batch: 060 ----
mean loss: 159.99
 ---- batch: 070 ----
mean loss: 153.11
 ---- batch: 080 ----
mean loss: 153.98
 ---- batch: 090 ----
mean loss: 169.42
train mean loss: 157.17
epoch train time: 0:00:00.505568
elapsed time: 0:01:37.861742
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:09:27.069843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.55
 ---- batch: 020 ----
mean loss: 146.52
 ---- batch: 030 ----
mean loss: 151.81
 ---- batch: 040 ----
mean loss: 154.62
 ---- batch: 050 ----
mean loss: 153.95
 ---- batch: 060 ----
mean loss: 152.20
 ---- batch: 070 ----
mean loss: 158.79
 ---- batch: 080 ----
mean loss: 162.08
 ---- batch: 090 ----
mean loss: 164.56
train mean loss: 155.15
epoch train time: 0:00:00.497315
elapsed time: 0:01:38.359215
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:09:27.567347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.51
 ---- batch: 020 ----
mean loss: 149.86
 ---- batch: 030 ----
mean loss: 148.17
 ---- batch: 040 ----
mean loss: 156.32
 ---- batch: 050 ----
mean loss: 155.06
 ---- batch: 060 ----
mean loss: 150.96
 ---- batch: 070 ----
mean loss: 153.53
 ---- batch: 080 ----
mean loss: 160.60
 ---- batch: 090 ----
mean loss: 158.23
train mean loss: 154.61
epoch train time: 0:00:00.507299
elapsed time: 0:01:38.866694
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:09:28.074805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.66
 ---- batch: 020 ----
mean loss: 150.73
 ---- batch: 030 ----
mean loss: 152.49
 ---- batch: 040 ----
mean loss: 157.89
 ---- batch: 050 ----
mean loss: 152.54
 ---- batch: 060 ----
mean loss: 154.17
 ---- batch: 070 ----
mean loss: 159.02
 ---- batch: 080 ----
mean loss: 154.35
 ---- batch: 090 ----
mean loss: 160.52
train mean loss: 154.44
epoch train time: 0:00:00.510579
elapsed time: 0:01:39.377429
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:09:28.585532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.21
 ---- batch: 020 ----
mean loss: 146.49
 ---- batch: 030 ----
mean loss: 156.74
 ---- batch: 040 ----
mean loss: 148.11
 ---- batch: 050 ----
mean loss: 153.93
 ---- batch: 060 ----
mean loss: 154.89
 ---- batch: 070 ----
mean loss: 158.24
 ---- batch: 080 ----
mean loss: 165.43
 ---- batch: 090 ----
mean loss: 156.03
train mean loss: 155.09
epoch train time: 0:00:00.520818
elapsed time: 0:01:39.898399
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:09:29.106500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.94
 ---- batch: 020 ----
mean loss: 151.40
 ---- batch: 030 ----
mean loss: 152.67
 ---- batch: 040 ----
mean loss: 148.11
 ---- batch: 050 ----
mean loss: 146.37
 ---- batch: 060 ----
mean loss: 156.49
 ---- batch: 070 ----
mean loss: 153.08
 ---- batch: 080 ----
mean loss: 155.25
 ---- batch: 090 ----
mean loss: 153.60
train mean loss: 153.30
epoch train time: 0:00:00.504413
elapsed time: 0:01:40.402959
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:09:29.611059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.91
 ---- batch: 020 ----
mean loss: 141.73
 ---- batch: 030 ----
mean loss: 151.18
 ---- batch: 040 ----
mean loss: 149.48
 ---- batch: 050 ----
mean loss: 153.82
 ---- batch: 060 ----
mean loss: 147.99
 ---- batch: 070 ----
mean loss: 158.88
 ---- batch: 080 ----
mean loss: 160.88
 ---- batch: 090 ----
mean loss: 163.23
train mean loss: 153.84
epoch train time: 0:00:00.513903
elapsed time: 0:01:40.917005
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:09:30.125106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.31
 ---- batch: 020 ----
mean loss: 153.84
 ---- batch: 030 ----
mean loss: 151.34
 ---- batch: 040 ----
mean loss: 149.17
 ---- batch: 050 ----
mean loss: 150.40
 ---- batch: 060 ----
mean loss: 158.47
 ---- batch: 070 ----
mean loss: 155.39
 ---- batch: 080 ----
mean loss: 151.03
 ---- batch: 090 ----
mean loss: 151.72
train mean loss: 152.36
epoch train time: 0:00:00.504204
elapsed time: 0:01:41.421377
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:09:30.629482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.66
 ---- batch: 020 ----
mean loss: 145.05
 ---- batch: 030 ----
mean loss: 148.01
 ---- batch: 040 ----
mean loss: 154.57
 ---- batch: 050 ----
mean loss: 147.28
 ---- batch: 060 ----
mean loss: 149.14
 ---- batch: 070 ----
mean loss: 152.71
 ---- batch: 080 ----
mean loss: 158.91
 ---- batch: 090 ----
mean loss: 162.42
train mean loss: 153.10
epoch train time: 0:00:00.501483
elapsed time: 0:01:41.923021
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:09:31.131144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.53
 ---- batch: 020 ----
mean loss: 147.41
 ---- batch: 030 ----
mean loss: 146.41
 ---- batch: 040 ----
mean loss: 153.42
 ---- batch: 050 ----
mean loss: 149.86
 ---- batch: 060 ----
mean loss: 155.19
 ---- batch: 070 ----
mean loss: 153.60
 ---- batch: 080 ----
mean loss: 154.05
 ---- batch: 090 ----
mean loss: 158.44
train mean loss: 152.03
epoch train time: 0:00:00.495012
elapsed time: 0:01:42.418247
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:09:31.626348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.61
 ---- batch: 020 ----
mean loss: 155.19
 ---- batch: 030 ----
mean loss: 151.36
 ---- batch: 040 ----
mean loss: 152.21
 ---- batch: 050 ----
mean loss: 147.67
 ---- batch: 060 ----
mean loss: 152.73
 ---- batch: 070 ----
mean loss: 150.31
 ---- batch: 080 ----
mean loss: 151.40
 ---- batch: 090 ----
mean loss: 153.21
train mean loss: 151.77
epoch train time: 0:00:00.507281
elapsed time: 0:01:42.925681
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:09:32.133783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.51
 ---- batch: 020 ----
mean loss: 150.23
 ---- batch: 030 ----
mean loss: 148.08
 ---- batch: 040 ----
mean loss: 151.41
 ---- batch: 050 ----
mean loss: 151.54
 ---- batch: 060 ----
mean loss: 147.82
 ---- batch: 070 ----
mean loss: 147.37
 ---- batch: 080 ----
mean loss: 157.05
 ---- batch: 090 ----
mean loss: 164.65
train mean loss: 151.33
epoch train time: 0:00:00.504702
elapsed time: 0:01:43.430532
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:09:32.638634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.46
 ---- batch: 020 ----
mean loss: 146.64
 ---- batch: 030 ----
mean loss: 155.46
 ---- batch: 040 ----
mean loss: 150.30
 ---- batch: 050 ----
mean loss: 144.51
 ---- batch: 060 ----
mean loss: 151.80
 ---- batch: 070 ----
mean loss: 155.68
 ---- batch: 080 ----
mean loss: 159.08
 ---- batch: 090 ----
mean loss: 153.18
train mean loss: 150.95
epoch train time: 0:00:00.509741
elapsed time: 0:01:43.940428
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:09:33.148534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.33
 ---- batch: 020 ----
mean loss: 157.54
 ---- batch: 030 ----
mean loss: 149.61
 ---- batch: 040 ----
mean loss: 148.18
 ---- batch: 050 ----
mean loss: 152.44
 ---- batch: 060 ----
mean loss: 148.44
 ---- batch: 070 ----
mean loss: 152.50
 ---- batch: 080 ----
mean loss: 154.17
 ---- batch: 090 ----
mean loss: 152.60
train mean loss: 151.28
epoch train time: 0:00:00.505393
elapsed time: 0:01:44.445988
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:09:33.654090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.00
 ---- batch: 020 ----
mean loss: 147.53
 ---- batch: 030 ----
mean loss: 156.96
 ---- batch: 040 ----
mean loss: 144.03
 ---- batch: 050 ----
mean loss: 152.23
 ---- batch: 060 ----
mean loss: 144.43
 ---- batch: 070 ----
mean loss: 157.09
 ---- batch: 080 ----
mean loss: 151.14
 ---- batch: 090 ----
mean loss: 157.40
train mean loss: 150.64
epoch train time: 0:00:00.504820
elapsed time: 0:01:44.950952
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:09:34.159052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.66
 ---- batch: 020 ----
mean loss: 145.73
 ---- batch: 030 ----
mean loss: 145.79
 ---- batch: 040 ----
mean loss: 150.66
 ---- batch: 050 ----
mean loss: 158.98
 ---- batch: 060 ----
mean loss: 154.31
 ---- batch: 070 ----
mean loss: 151.54
 ---- batch: 080 ----
mean loss: 153.17
 ---- batch: 090 ----
mean loss: 146.02
train mean loss: 150.68
epoch train time: 0:00:00.507887
elapsed time: 0:01:45.458998
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:09:34.667113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.76
 ---- batch: 020 ----
mean loss: 153.59
 ---- batch: 030 ----
mean loss: 144.14
 ---- batch: 040 ----
mean loss: 155.64
 ---- batch: 050 ----
mean loss: 148.44
 ---- batch: 060 ----
mean loss: 146.16
 ---- batch: 070 ----
mean loss: 150.01
 ---- batch: 080 ----
mean loss: 147.78
 ---- batch: 090 ----
mean loss: 147.96
train mean loss: 150.12
epoch train time: 0:00:00.503383
elapsed time: 0:01:45.962550
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:09:35.170681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.41
 ---- batch: 020 ----
mean loss: 154.16
 ---- batch: 030 ----
mean loss: 151.34
 ---- batch: 040 ----
mean loss: 152.34
 ---- batch: 050 ----
mean loss: 145.51
 ---- batch: 060 ----
mean loss: 150.98
 ---- batch: 070 ----
mean loss: 151.05
 ---- batch: 080 ----
mean loss: 146.42
 ---- batch: 090 ----
mean loss: 152.02
train mean loss: 149.33
epoch train time: 0:00:00.493448
elapsed time: 0:01:46.456176
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:09:35.664280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.27
 ---- batch: 020 ----
mean loss: 152.24
 ---- batch: 030 ----
mean loss: 141.52
 ---- batch: 040 ----
mean loss: 149.43
 ---- batch: 050 ----
mean loss: 151.53
 ---- batch: 060 ----
mean loss: 151.50
 ---- batch: 070 ----
mean loss: 150.07
 ---- batch: 080 ----
mean loss: 153.61
 ---- batch: 090 ----
mean loss: 155.17
train mean loss: 149.67
epoch train time: 0:00:00.503115
elapsed time: 0:01:46.959438
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:09:36.167535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.35
 ---- batch: 020 ----
mean loss: 144.52
 ---- batch: 030 ----
mean loss: 149.65
 ---- batch: 040 ----
mean loss: 154.41
 ---- batch: 050 ----
mean loss: 152.40
 ---- batch: 060 ----
mean loss: 154.01
 ---- batch: 070 ----
mean loss: 144.98
 ---- batch: 080 ----
mean loss: 146.34
 ---- batch: 090 ----
mean loss: 150.06
train mean loss: 149.64
epoch train time: 0:00:00.498455
elapsed time: 0:01:47.458036
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:09:36.666152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.87
 ---- batch: 020 ----
mean loss: 151.66
 ---- batch: 030 ----
mean loss: 143.21
 ---- batch: 040 ----
mean loss: 147.79
 ---- batch: 050 ----
mean loss: 148.62
 ---- batch: 060 ----
mean loss: 147.89
 ---- batch: 070 ----
mean loss: 145.71
 ---- batch: 080 ----
mean loss: 156.65
 ---- batch: 090 ----
mean loss: 153.56
train mean loss: 148.60
epoch train time: 0:00:00.513753
elapsed time: 0:01:47.972017
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:09:37.180117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.50
 ---- batch: 020 ----
mean loss: 145.77
 ---- batch: 030 ----
mean loss: 145.74
 ---- batch: 040 ----
mean loss: 148.41
 ---- batch: 050 ----
mean loss: 141.30
 ---- batch: 060 ----
mean loss: 142.70
 ---- batch: 070 ----
mean loss: 152.63
 ---- batch: 080 ----
mean loss: 150.82
 ---- batch: 090 ----
mean loss: 151.40
train mean loss: 147.83
epoch train time: 0:00:00.505271
elapsed time: 0:01:48.477460
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:09:37.685549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.16
 ---- batch: 020 ----
mean loss: 141.41
 ---- batch: 030 ----
mean loss: 144.97
 ---- batch: 040 ----
mean loss: 148.95
 ---- batch: 050 ----
mean loss: 147.21
 ---- batch: 060 ----
mean loss: 152.44
 ---- batch: 070 ----
mean loss: 158.58
 ---- batch: 080 ----
mean loss: 148.50
 ---- batch: 090 ----
mean loss: 153.71
train mean loss: 149.09
epoch train time: 0:00:00.511083
elapsed time: 0:01:48.988696
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:09:38.196798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.28
 ---- batch: 020 ----
mean loss: 150.27
 ---- batch: 030 ----
mean loss: 146.15
 ---- batch: 040 ----
mean loss: 153.40
 ---- batch: 050 ----
mean loss: 145.61
 ---- batch: 060 ----
mean loss: 146.34
 ---- batch: 070 ----
mean loss: 138.59
 ---- batch: 080 ----
mean loss: 155.61
 ---- batch: 090 ----
mean loss: 145.70
train mean loss: 147.49
epoch train time: 0:00:00.515532
elapsed time: 0:01:49.504395
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:09:38.712528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.13
 ---- batch: 020 ----
mean loss: 143.13
 ---- batch: 030 ----
mean loss: 137.73
 ---- batch: 040 ----
mean loss: 149.89
 ---- batch: 050 ----
mean loss: 144.99
 ---- batch: 060 ----
mean loss: 150.92
 ---- batch: 070 ----
mean loss: 146.30
 ---- batch: 080 ----
mean loss: 150.65
 ---- batch: 090 ----
mean loss: 153.93
train mean loss: 147.17
epoch train time: 0:00:00.521275
elapsed time: 0:01:50.025867
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:09:39.233968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.58
 ---- batch: 020 ----
mean loss: 146.03
 ---- batch: 030 ----
mean loss: 154.07
 ---- batch: 040 ----
mean loss: 147.85
 ---- batch: 050 ----
mean loss: 145.27
 ---- batch: 060 ----
mean loss: 145.65
 ---- batch: 070 ----
mean loss: 147.57
 ---- batch: 080 ----
mean loss: 146.56
 ---- batch: 090 ----
mean loss: 149.09
train mean loss: 146.45
epoch train time: 0:00:00.510062
elapsed time: 0:01:50.536108
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:09:39.744212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.13
 ---- batch: 020 ----
mean loss: 141.08
 ---- batch: 030 ----
mean loss: 144.84
 ---- batch: 040 ----
mean loss: 151.42
 ---- batch: 050 ----
mean loss: 143.32
 ---- batch: 060 ----
mean loss: 144.31
 ---- batch: 070 ----
mean loss: 146.84
 ---- batch: 080 ----
mean loss: 144.88
 ---- batch: 090 ----
mean loss: 149.73
train mean loss: 145.97
epoch train time: 0:00:00.498553
elapsed time: 0:01:51.034811
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:09:40.242912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.83
 ---- batch: 020 ----
mean loss: 148.32
 ---- batch: 030 ----
mean loss: 142.34
 ---- batch: 040 ----
mean loss: 150.90
 ---- batch: 050 ----
mean loss: 151.31
 ---- batch: 060 ----
mean loss: 151.23
 ---- batch: 070 ----
mean loss: 152.18
 ---- batch: 080 ----
mean loss: 157.80
 ---- batch: 090 ----
mean loss: 154.21
train mean loss: 149.74
epoch train time: 0:00:00.508663
elapsed time: 0:01:51.543628
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:09:40.751741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.33
 ---- batch: 020 ----
mean loss: 146.09
 ---- batch: 030 ----
mean loss: 145.05
 ---- batch: 040 ----
mean loss: 138.32
 ---- batch: 050 ----
mean loss: 143.93
 ---- batch: 060 ----
mean loss: 148.97
 ---- batch: 070 ----
mean loss: 142.69
 ---- batch: 080 ----
mean loss: 146.27
 ---- batch: 090 ----
mean loss: 147.88
train mean loss: 145.62
epoch train time: 0:00:00.496616
elapsed time: 0:01:52.040398
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:09:41.248497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.24
 ---- batch: 020 ----
mean loss: 140.88
 ---- batch: 030 ----
mean loss: 145.94
 ---- batch: 040 ----
mean loss: 145.73
 ---- batch: 050 ----
mean loss: 138.69
 ---- batch: 060 ----
mean loss: 147.54
 ---- batch: 070 ----
mean loss: 150.85
 ---- batch: 080 ----
mean loss: 155.36
 ---- batch: 090 ----
mean loss: 147.84
train mean loss: 145.74
epoch train time: 0:00:00.499363
elapsed time: 0:01:52.539906
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:09:41.748008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.43
 ---- batch: 020 ----
mean loss: 146.07
 ---- batch: 030 ----
mean loss: 147.06
 ---- batch: 040 ----
mean loss: 147.68
 ---- batch: 050 ----
mean loss: 152.59
 ---- batch: 060 ----
mean loss: 146.53
 ---- batch: 070 ----
mean loss: 138.18
 ---- batch: 080 ----
mean loss: 146.44
 ---- batch: 090 ----
mean loss: 145.12
train mean loss: 145.74
epoch train time: 0:00:00.508026
elapsed time: 0:01:53.048079
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:09:42.256182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.70
 ---- batch: 020 ----
mean loss: 139.98
 ---- batch: 030 ----
mean loss: 144.16
 ---- batch: 040 ----
mean loss: 143.28
 ---- batch: 050 ----
mean loss: 146.94
 ---- batch: 060 ----
mean loss: 143.26
 ---- batch: 070 ----
mean loss: 150.01
 ---- batch: 080 ----
mean loss: 147.09
 ---- batch: 090 ----
mean loss: 148.85
train mean loss: 145.69
epoch train time: 0:00:00.510072
elapsed time: 0:01:53.558303
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:09:42.766431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.20
 ---- batch: 020 ----
mean loss: 137.93
 ---- batch: 030 ----
mean loss: 140.88
 ---- batch: 040 ----
mean loss: 148.83
 ---- batch: 050 ----
mean loss: 152.21
 ---- batch: 060 ----
mean loss: 143.81
 ---- batch: 070 ----
mean loss: 150.37
 ---- batch: 080 ----
mean loss: 149.46
 ---- batch: 090 ----
mean loss: 149.16
train mean loss: 145.39
epoch train time: 0:00:00.511703
elapsed time: 0:01:54.070191
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:09:43.278290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.87
 ---- batch: 020 ----
mean loss: 143.29
 ---- batch: 030 ----
mean loss: 140.95
 ---- batch: 040 ----
mean loss: 143.43
 ---- batch: 050 ----
mean loss: 145.24
 ---- batch: 060 ----
mean loss: 146.04
 ---- batch: 070 ----
mean loss: 150.88
 ---- batch: 080 ----
mean loss: 153.74
 ---- batch: 090 ----
mean loss: 140.33
train mean loss: 144.77
epoch train time: 0:00:00.506015
elapsed time: 0:01:54.576375
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:09:43.784476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.08
 ---- batch: 020 ----
mean loss: 135.50
 ---- batch: 030 ----
mean loss: 142.00
 ---- batch: 040 ----
mean loss: 145.96
 ---- batch: 050 ----
mean loss: 138.16
 ---- batch: 060 ----
mean loss: 144.84
 ---- batch: 070 ----
mean loss: 153.10
 ---- batch: 080 ----
mean loss: 147.51
 ---- batch: 090 ----
mean loss: 142.53
train mean loss: 143.96
epoch train time: 0:00:00.508579
elapsed time: 0:01:55.085102
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:09:44.293203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.14
 ---- batch: 020 ----
mean loss: 146.61
 ---- batch: 030 ----
mean loss: 149.30
 ---- batch: 040 ----
mean loss: 149.49
 ---- batch: 050 ----
mean loss: 138.70
 ---- batch: 060 ----
mean loss: 143.70
 ---- batch: 070 ----
mean loss: 151.06
 ---- batch: 080 ----
mean loss: 148.32
 ---- batch: 090 ----
mean loss: 139.71
train mean loss: 144.64
epoch train time: 0:00:00.502546
elapsed time: 0:01:55.587799
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:09:44.795921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.83
 ---- batch: 020 ----
mean loss: 138.14
 ---- batch: 030 ----
mean loss: 142.78
 ---- batch: 040 ----
mean loss: 146.83
 ---- batch: 050 ----
mean loss: 144.18
 ---- batch: 060 ----
mean loss: 146.89
 ---- batch: 070 ----
mean loss: 143.48
 ---- batch: 080 ----
mean loss: 138.35
 ---- batch: 090 ----
mean loss: 152.94
train mean loss: 144.36
epoch train time: 0:00:00.492782
elapsed time: 0:01:56.080778
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:09:45.288874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.81
 ---- batch: 020 ----
mean loss: 140.62
 ---- batch: 030 ----
mean loss: 139.61
 ---- batch: 040 ----
mean loss: 136.00
 ---- batch: 050 ----
mean loss: 144.77
 ---- batch: 060 ----
mean loss: 147.79
 ---- batch: 070 ----
mean loss: 146.41
 ---- batch: 080 ----
mean loss: 142.73
 ---- batch: 090 ----
mean loss: 147.26
train mean loss: 143.68
epoch train time: 0:00:00.503427
elapsed time: 0:01:56.584360
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:09:45.792467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.25
 ---- batch: 020 ----
mean loss: 144.35
 ---- batch: 030 ----
mean loss: 138.51
 ---- batch: 040 ----
mean loss: 148.32
 ---- batch: 050 ----
mean loss: 146.52
 ---- batch: 060 ----
mean loss: 146.70
 ---- batch: 070 ----
mean loss: 144.92
 ---- batch: 080 ----
mean loss: 141.31
 ---- batch: 090 ----
mean loss: 144.93
train mean loss: 143.81
epoch train time: 0:00:00.508602
elapsed time: 0:01:57.093118
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:09:46.301237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.95
 ---- batch: 020 ----
mean loss: 141.72
 ---- batch: 030 ----
mean loss: 143.86
 ---- batch: 040 ----
mean loss: 141.63
 ---- batch: 050 ----
mean loss: 139.39
 ---- batch: 060 ----
mean loss: 138.70
 ---- batch: 070 ----
mean loss: 137.66
 ---- batch: 080 ----
mean loss: 146.47
 ---- batch: 090 ----
mean loss: 149.50
train mean loss: 142.99
epoch train time: 0:00:00.504801
elapsed time: 0:01:57.598102
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:09:46.806219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.18
 ---- batch: 020 ----
mean loss: 141.71
 ---- batch: 030 ----
mean loss: 149.08
 ---- batch: 040 ----
mean loss: 132.76
 ---- batch: 050 ----
mean loss: 144.92
 ---- batch: 060 ----
mean loss: 139.42
 ---- batch: 070 ----
mean loss: 147.71
 ---- batch: 080 ----
mean loss: 142.37
 ---- batch: 090 ----
mean loss: 144.01
train mean loss: 142.82
epoch train time: 0:00:00.501869
elapsed time: 0:01:58.100132
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:09:47.308232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.21
 ---- batch: 020 ----
mean loss: 144.01
 ---- batch: 030 ----
mean loss: 136.39
 ---- batch: 040 ----
mean loss: 139.02
 ---- batch: 050 ----
mean loss: 146.05
 ---- batch: 060 ----
mean loss: 148.31
 ---- batch: 070 ----
mean loss: 143.94
 ---- batch: 080 ----
mean loss: 145.33
 ---- batch: 090 ----
mean loss: 145.46
train mean loss: 143.13
epoch train time: 0:00:00.497057
elapsed time: 0:01:58.597335
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:09:47.805435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.03
 ---- batch: 020 ----
mean loss: 137.95
 ---- batch: 030 ----
mean loss: 134.95
 ---- batch: 040 ----
mean loss: 146.14
 ---- batch: 050 ----
mean loss: 135.68
 ---- batch: 060 ----
mean loss: 146.43
 ---- batch: 070 ----
mean loss: 144.16
 ---- batch: 080 ----
mean loss: 148.04
 ---- batch: 090 ----
mean loss: 142.68
train mean loss: 142.10
epoch train time: 0:00:00.498348
elapsed time: 0:01:59.095843
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:09:48.303947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.41
 ---- batch: 020 ----
mean loss: 142.26
 ---- batch: 030 ----
mean loss: 142.29
 ---- batch: 040 ----
mean loss: 141.38
 ---- batch: 050 ----
mean loss: 147.11
 ---- batch: 060 ----
mean loss: 138.54
 ---- batch: 070 ----
mean loss: 144.91
 ---- batch: 080 ----
mean loss: 141.07
 ---- batch: 090 ----
mean loss: 143.16
train mean loss: 141.99
epoch train time: 0:00:00.498148
elapsed time: 0:01:59.594144
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:09:48.802245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.20
 ---- batch: 020 ----
mean loss: 140.00
 ---- batch: 030 ----
mean loss: 139.10
 ---- batch: 040 ----
mean loss: 136.51
 ---- batch: 050 ----
mean loss: 142.64
 ---- batch: 060 ----
mean loss: 141.19
 ---- batch: 070 ----
mean loss: 145.11
 ---- batch: 080 ----
mean loss: 141.64
 ---- batch: 090 ----
mean loss: 147.55
train mean loss: 141.38
epoch train time: 0:00:00.500433
elapsed time: 0:02:00.094752
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:09:49.302850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.32
 ---- batch: 020 ----
mean loss: 139.16
 ---- batch: 030 ----
mean loss: 142.48
 ---- batch: 040 ----
mean loss: 145.40
 ---- batch: 050 ----
mean loss: 138.38
 ---- batch: 060 ----
mean loss: 137.29
 ---- batch: 070 ----
mean loss: 146.11
 ---- batch: 080 ----
mean loss: 144.65
 ---- batch: 090 ----
mean loss: 144.20
train mean loss: 141.92
epoch train time: 0:00:00.510101
elapsed time: 0:02:00.604997
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:09:49.813100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.03
 ---- batch: 020 ----
mean loss: 137.59
 ---- batch: 030 ----
mean loss: 144.56
 ---- batch: 040 ----
mean loss: 137.62
 ---- batch: 050 ----
mean loss: 138.30
 ---- batch: 060 ----
mean loss: 143.65
 ---- batch: 070 ----
mean loss: 144.29
 ---- batch: 080 ----
mean loss: 143.48
 ---- batch: 090 ----
mean loss: 140.34
train mean loss: 141.09
epoch train time: 0:00:00.505100
elapsed time: 0:02:01.110279
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:09:50.318394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.71
 ---- batch: 020 ----
mean loss: 141.26
 ---- batch: 030 ----
mean loss: 143.41
 ---- batch: 040 ----
mean loss: 139.50
 ---- batch: 050 ----
mean loss: 139.22
 ---- batch: 060 ----
mean loss: 147.38
 ---- batch: 070 ----
mean loss: 140.69
 ---- batch: 080 ----
mean loss: 140.10
 ---- batch: 090 ----
mean loss: 144.42
train mean loss: 141.70
epoch train time: 0:00:00.513302
elapsed time: 0:02:01.623751
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:09:50.831863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.46
 ---- batch: 020 ----
mean loss: 136.35
 ---- batch: 030 ----
mean loss: 139.25
 ---- batch: 040 ----
mean loss: 143.39
 ---- batch: 050 ----
mean loss: 139.85
 ---- batch: 060 ----
mean loss: 144.35
 ---- batch: 070 ----
mean loss: 147.59
 ---- batch: 080 ----
mean loss: 142.07
 ---- batch: 090 ----
mean loss: 137.04
train mean loss: 140.42
epoch train time: 0:00:00.498180
elapsed time: 0:02:02.122094
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:09:51.330196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.03
 ---- batch: 020 ----
mean loss: 137.74
 ---- batch: 030 ----
mean loss: 135.27
 ---- batch: 040 ----
mean loss: 142.38
 ---- batch: 050 ----
mean loss: 147.09
 ---- batch: 060 ----
mean loss: 144.94
 ---- batch: 070 ----
mean loss: 139.45
 ---- batch: 080 ----
mean loss: 134.05
 ---- batch: 090 ----
mean loss: 145.01
train mean loss: 140.30
epoch train time: 0:00:00.503932
elapsed time: 0:02:02.626174
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:09:51.834274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.15
 ---- batch: 020 ----
mean loss: 136.78
 ---- batch: 030 ----
mean loss: 143.60
 ---- batch: 040 ----
mean loss: 134.97
 ---- batch: 050 ----
mean loss: 134.77
 ---- batch: 060 ----
mean loss: 145.21
 ---- batch: 070 ----
mean loss: 145.95
 ---- batch: 080 ----
mean loss: 136.80
 ---- batch: 090 ----
mean loss: 139.90
train mean loss: 139.85
epoch train time: 0:00:00.495994
elapsed time: 0:02:03.122333
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:09:52.330432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.85
 ---- batch: 020 ----
mean loss: 137.98
 ---- batch: 030 ----
mean loss: 140.92
 ---- batch: 040 ----
mean loss: 138.55
 ---- batch: 050 ----
mean loss: 145.60
 ---- batch: 060 ----
mean loss: 139.92
 ---- batch: 070 ----
mean loss: 139.03
 ---- batch: 080 ----
mean loss: 138.72
 ---- batch: 090 ----
mean loss: 137.66
train mean loss: 139.37
epoch train time: 0:00:00.498107
elapsed time: 0:02:03.620597
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:09:52.828695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.98
 ---- batch: 020 ----
mean loss: 134.64
 ---- batch: 030 ----
mean loss: 142.02
 ---- batch: 040 ----
mean loss: 138.68
 ---- batch: 050 ----
mean loss: 140.93
 ---- batch: 060 ----
mean loss: 139.95
 ---- batch: 070 ----
mean loss: 139.77
 ---- batch: 080 ----
mean loss: 141.37
 ---- batch: 090 ----
mean loss: 144.19
train mean loss: 139.66
epoch train time: 0:00:00.505558
elapsed time: 0:02:04.126316
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:09:53.334431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.55
 ---- batch: 020 ----
mean loss: 139.10
 ---- batch: 030 ----
mean loss: 135.41
 ---- batch: 040 ----
mean loss: 140.35
 ---- batch: 050 ----
mean loss: 140.34
 ---- batch: 060 ----
mean loss: 138.82
 ---- batch: 070 ----
mean loss: 142.71
 ---- batch: 080 ----
mean loss: 139.34
 ---- batch: 090 ----
mean loss: 140.08
train mean loss: 139.40
epoch train time: 0:00:00.506985
elapsed time: 0:02:04.633462
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:09:53.841562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.15
 ---- batch: 020 ----
mean loss: 136.01
 ---- batch: 030 ----
mean loss: 138.33
 ---- batch: 040 ----
mean loss: 140.68
 ---- batch: 050 ----
mean loss: 136.84
 ---- batch: 060 ----
mean loss: 142.36
 ---- batch: 070 ----
mean loss: 141.17
 ---- batch: 080 ----
mean loss: 142.75
 ---- batch: 090 ----
mean loss: 137.51
train mean loss: 138.89
epoch train time: 0:00:00.498574
elapsed time: 0:02:05.132177
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:09:54.340294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.66
 ---- batch: 020 ----
mean loss: 136.86
 ---- batch: 030 ----
mean loss: 138.76
 ---- batch: 040 ----
mean loss: 144.84
 ---- batch: 050 ----
mean loss: 137.07
 ---- batch: 060 ----
mean loss: 142.28
 ---- batch: 070 ----
mean loss: 133.82
 ---- batch: 080 ----
mean loss: 140.56
 ---- batch: 090 ----
mean loss: 135.20
train mean loss: 138.55
epoch train time: 0:00:00.500095
elapsed time: 0:02:05.632438
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:09:54.840541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.05
 ---- batch: 020 ----
mean loss: 140.03
 ---- batch: 030 ----
mean loss: 135.95
 ---- batch: 040 ----
mean loss: 138.71
 ---- batch: 050 ----
mean loss: 137.85
 ---- batch: 060 ----
mean loss: 140.55
 ---- batch: 070 ----
mean loss: 140.39
 ---- batch: 080 ----
mean loss: 134.29
 ---- batch: 090 ----
mean loss: 137.56
train mean loss: 138.91
epoch train time: 0:00:00.502819
elapsed time: 0:02:06.135412
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:09:55.343503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.12
 ---- batch: 020 ----
mean loss: 137.64
 ---- batch: 030 ----
mean loss: 131.54
 ---- batch: 040 ----
mean loss: 141.22
 ---- batch: 050 ----
mean loss: 139.84
 ---- batch: 060 ----
mean loss: 144.19
 ---- batch: 070 ----
mean loss: 136.29
 ---- batch: 080 ----
mean loss: 136.79
 ---- batch: 090 ----
mean loss: 137.95
train mean loss: 138.27
epoch train time: 0:00:00.502357
elapsed time: 0:02:06.637915
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:09:55.846034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.39
 ---- batch: 020 ----
mean loss: 132.92
 ---- batch: 030 ----
mean loss: 132.02
 ---- batch: 040 ----
mean loss: 143.59
 ---- batch: 050 ----
mean loss: 148.96
 ---- batch: 060 ----
mean loss: 142.38
 ---- batch: 070 ----
mean loss: 140.23
 ---- batch: 080 ----
mean loss: 138.24
 ---- batch: 090 ----
mean loss: 140.62
train mean loss: 138.65
epoch train time: 0:00:00.507625
elapsed time: 0:02:07.145707
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:09:56.353808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.09
 ---- batch: 020 ----
mean loss: 130.83
 ---- batch: 030 ----
mean loss: 139.21
 ---- batch: 040 ----
mean loss: 143.46
 ---- batch: 050 ----
mean loss: 139.02
 ---- batch: 060 ----
mean loss: 142.72
 ---- batch: 070 ----
mean loss: 146.78
 ---- batch: 080 ----
mean loss: 136.27
 ---- batch: 090 ----
mean loss: 144.49
train mean loss: 139.89
epoch train time: 0:00:00.506165
elapsed time: 0:02:07.652016
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:09:56.860114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.86
 ---- batch: 020 ----
mean loss: 139.41
 ---- batch: 030 ----
mean loss: 134.32
 ---- batch: 040 ----
mean loss: 136.77
 ---- batch: 050 ----
mean loss: 138.12
 ---- batch: 060 ----
mean loss: 138.86
 ---- batch: 070 ----
mean loss: 135.17
 ---- batch: 080 ----
mean loss: 139.99
 ---- batch: 090 ----
mean loss: 144.17
train mean loss: 137.81
epoch train time: 0:00:00.501130
elapsed time: 0:02:08.153290
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:09:57.361390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.79
 ---- batch: 020 ----
mean loss: 140.29
 ---- batch: 030 ----
mean loss: 131.34
 ---- batch: 040 ----
mean loss: 136.06
 ---- batch: 050 ----
mean loss: 137.13
 ---- batch: 060 ----
mean loss: 138.62
 ---- batch: 070 ----
mean loss: 138.28
 ---- batch: 080 ----
mean loss: 142.72
 ---- batch: 090 ----
mean loss: 141.80
train mean loss: 137.96
epoch train time: 0:00:00.503569
elapsed time: 0:02:08.657010
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:09:57.865128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.06
 ---- batch: 020 ----
mean loss: 131.22
 ---- batch: 030 ----
mean loss: 136.00
 ---- batch: 040 ----
mean loss: 135.59
 ---- batch: 050 ----
mean loss: 136.81
 ---- batch: 060 ----
mean loss: 133.65
 ---- batch: 070 ----
mean loss: 140.84
 ---- batch: 080 ----
mean loss: 143.79
 ---- batch: 090 ----
mean loss: 147.39
train mean loss: 138.10
epoch train time: 0:00:00.497360
elapsed time: 0:02:09.154535
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:09:58.362636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.82
 ---- batch: 020 ----
mean loss: 134.62
 ---- batch: 030 ----
mean loss: 134.75
 ---- batch: 040 ----
mean loss: 141.38
 ---- batch: 050 ----
mean loss: 137.26
 ---- batch: 060 ----
mean loss: 140.61
 ---- batch: 070 ----
mean loss: 138.97
 ---- batch: 080 ----
mean loss: 143.97
 ---- batch: 090 ----
mean loss: 133.52
train mean loss: 137.54
epoch train time: 0:00:00.500417
elapsed time: 0:02:09.655128
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:09:58.863242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.07
 ---- batch: 020 ----
mean loss: 133.44
 ---- batch: 030 ----
mean loss: 135.01
 ---- batch: 040 ----
mean loss: 132.54
 ---- batch: 050 ----
mean loss: 139.82
 ---- batch: 060 ----
mean loss: 140.15
 ---- batch: 070 ----
mean loss: 140.84
 ---- batch: 080 ----
mean loss: 141.00
 ---- batch: 090 ----
mean loss: 137.04
train mean loss: 136.77
epoch train time: 0:00:00.503827
elapsed time: 0:02:10.159113
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:09:59.367214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.20
 ---- batch: 020 ----
mean loss: 139.56
 ---- batch: 030 ----
mean loss: 135.20
 ---- batch: 040 ----
mean loss: 135.74
 ---- batch: 050 ----
mean loss: 137.50
 ---- batch: 060 ----
mean loss: 139.14
 ---- batch: 070 ----
mean loss: 131.71
 ---- batch: 080 ----
mean loss: 135.44
 ---- batch: 090 ----
mean loss: 140.73
train mean loss: 136.66
epoch train time: 0:00:00.504181
elapsed time: 0:02:10.663457
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:09:59.871558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.63
 ---- batch: 020 ----
mean loss: 132.77
 ---- batch: 030 ----
mean loss: 132.64
 ---- batch: 040 ----
mean loss: 132.95
 ---- batch: 050 ----
mean loss: 139.34
 ---- batch: 060 ----
mean loss: 134.05
 ---- batch: 070 ----
mean loss: 137.56
 ---- batch: 080 ----
mean loss: 137.82
 ---- batch: 090 ----
mean loss: 146.11
train mean loss: 136.02
epoch train time: 0:00:00.507454
elapsed time: 0:02:11.171056
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:10:00.379197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.31
 ---- batch: 020 ----
mean loss: 131.77
 ---- batch: 030 ----
mean loss: 132.36
 ---- batch: 040 ----
mean loss: 138.93
 ---- batch: 050 ----
mean loss: 134.97
 ---- batch: 060 ----
mean loss: 141.70
 ---- batch: 070 ----
mean loss: 136.32
 ---- batch: 080 ----
mean loss: 139.84
 ---- batch: 090 ----
mean loss: 136.36
train mean loss: 136.44
epoch train time: 0:00:00.505080
elapsed time: 0:02:11.676336
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:10:00.884433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.34
 ---- batch: 020 ----
mean loss: 134.64
 ---- batch: 030 ----
mean loss: 140.91
 ---- batch: 040 ----
mean loss: 129.16
 ---- batch: 050 ----
mean loss: 133.27
 ---- batch: 060 ----
mean loss: 136.13
 ---- batch: 070 ----
mean loss: 140.40
 ---- batch: 080 ----
mean loss: 137.24
 ---- batch: 090 ----
mean loss: 137.64
train mean loss: 135.46
epoch train time: 0:00:00.497281
elapsed time: 0:02:12.173756
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:10:01.381871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.14
 ---- batch: 020 ----
mean loss: 135.42
 ---- batch: 030 ----
mean loss: 135.51
 ---- batch: 040 ----
mean loss: 133.68
 ---- batch: 050 ----
mean loss: 134.24
 ---- batch: 060 ----
mean loss: 138.21
 ---- batch: 070 ----
mean loss: 139.04
 ---- batch: 080 ----
mean loss: 131.50
 ---- batch: 090 ----
mean loss: 140.04
train mean loss: 136.07
epoch train time: 0:00:00.506694
elapsed time: 0:02:12.680619
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:10:01.888729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.02
 ---- batch: 020 ----
mean loss: 134.44
 ---- batch: 030 ----
mean loss: 133.48
 ---- batch: 040 ----
mean loss: 133.22
 ---- batch: 050 ----
mean loss: 134.07
 ---- batch: 060 ----
mean loss: 138.18
 ---- batch: 070 ----
mean loss: 135.37
 ---- batch: 080 ----
mean loss: 134.06
 ---- batch: 090 ----
mean loss: 145.92
train mean loss: 135.89
epoch train time: 0:00:00.499492
elapsed time: 0:02:13.180300
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:10:02.388415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.47
 ---- batch: 020 ----
mean loss: 130.27
 ---- batch: 030 ----
mean loss: 135.67
 ---- batch: 040 ----
mean loss: 137.34
 ---- batch: 050 ----
mean loss: 137.93
 ---- batch: 060 ----
mean loss: 142.87
 ---- batch: 070 ----
mean loss: 137.76
 ---- batch: 080 ----
mean loss: 139.99
 ---- batch: 090 ----
mean loss: 135.89
train mean loss: 135.45
epoch train time: 0:00:00.504249
elapsed time: 0:02:13.684717
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:10:02.892820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.45
 ---- batch: 020 ----
mean loss: 130.39
 ---- batch: 030 ----
mean loss: 132.37
 ---- batch: 040 ----
mean loss: 130.35
 ---- batch: 050 ----
mean loss: 136.03
 ---- batch: 060 ----
mean loss: 141.99
 ---- batch: 070 ----
mean loss: 135.64
 ---- batch: 080 ----
mean loss: 141.35
 ---- batch: 090 ----
mean loss: 138.66
train mean loss: 135.94
epoch train time: 0:00:00.512507
elapsed time: 0:02:14.197372
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:10:03.405472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.93
 ---- batch: 020 ----
mean loss: 138.51
 ---- batch: 030 ----
mean loss: 129.44
 ---- batch: 040 ----
mean loss: 134.75
 ---- batch: 050 ----
mean loss: 133.52
 ---- batch: 060 ----
mean loss: 134.95
 ---- batch: 070 ----
mean loss: 139.12
 ---- batch: 080 ----
mean loss: 139.41
 ---- batch: 090 ----
mean loss: 141.25
train mean loss: 135.89
epoch train time: 0:00:00.517401
elapsed time: 0:02:14.714931
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:10:03.923032
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.82
 ---- batch: 020 ----
mean loss: 129.26
 ---- batch: 030 ----
mean loss: 126.33
 ---- batch: 040 ----
mean loss: 135.00
 ---- batch: 050 ----
mean loss: 124.73
 ---- batch: 060 ----
mean loss: 124.66
 ---- batch: 070 ----
mean loss: 126.44
 ---- batch: 080 ----
mean loss: 130.56
 ---- batch: 090 ----
mean loss: 128.42
train mean loss: 129.05
epoch train time: 0:00:00.514557
elapsed time: 0:02:15.229672
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:10:04.437778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.30
 ---- batch: 020 ----
mean loss: 123.88
 ---- batch: 030 ----
mean loss: 127.66
 ---- batch: 040 ----
mean loss: 123.03
 ---- batch: 050 ----
mean loss: 128.66
 ---- batch: 060 ----
mean loss: 129.71
 ---- batch: 070 ----
mean loss: 126.21
 ---- batch: 080 ----
mean loss: 132.59
 ---- batch: 090 ----
mean loss: 125.20
train mean loss: 127.62
epoch train time: 0:00:00.508462
elapsed time: 0:02:15.738296
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:10:04.946388
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.34
 ---- batch: 020 ----
mean loss: 125.92
 ---- batch: 030 ----
mean loss: 123.64
 ---- batch: 040 ----
mean loss: 131.33
 ---- batch: 050 ----
mean loss: 128.68
 ---- batch: 060 ----
mean loss: 122.25
 ---- batch: 070 ----
mean loss: 128.95
 ---- batch: 080 ----
mean loss: 128.56
 ---- batch: 090 ----
mean loss: 126.20
train mean loss: 126.98
epoch train time: 0:00:00.515835
elapsed time: 0:02:16.254273
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:10:05.462395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.54
 ---- batch: 020 ----
mean loss: 124.25
 ---- batch: 030 ----
mean loss: 131.02
 ---- batch: 040 ----
mean loss: 120.11
 ---- batch: 050 ----
mean loss: 128.24
 ---- batch: 060 ----
mean loss: 125.75
 ---- batch: 070 ----
mean loss: 129.11
 ---- batch: 080 ----
mean loss: 133.15
 ---- batch: 090 ----
mean loss: 125.36
train mean loss: 126.61
epoch train time: 0:00:00.501147
elapsed time: 0:02:16.755587
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:10:05.963691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.34
 ---- batch: 020 ----
mean loss: 128.67
 ---- batch: 030 ----
mean loss: 130.91
 ---- batch: 040 ----
mean loss: 118.61
 ---- batch: 050 ----
mean loss: 130.08
 ---- batch: 060 ----
mean loss: 126.93
 ---- batch: 070 ----
mean loss: 124.85
 ---- batch: 080 ----
mean loss: 134.18
 ---- batch: 090 ----
mean loss: 123.50
train mean loss: 126.80
epoch train time: 0:00:00.497594
elapsed time: 0:02:17.253363
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:10:06.461461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.48
 ---- batch: 020 ----
mean loss: 125.05
 ---- batch: 030 ----
mean loss: 129.60
 ---- batch: 040 ----
mean loss: 127.02
 ---- batch: 050 ----
mean loss: 128.99
 ---- batch: 060 ----
mean loss: 123.29
 ---- batch: 070 ----
mean loss: 128.88
 ---- batch: 080 ----
mean loss: 127.51
 ---- batch: 090 ----
mean loss: 127.82
train mean loss: 126.62
epoch train time: 0:00:00.508129
elapsed time: 0:02:17.761657
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:10:06.969770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.60
 ---- batch: 020 ----
mean loss: 123.45
 ---- batch: 030 ----
mean loss: 131.61
 ---- batch: 040 ----
mean loss: 127.59
 ---- batch: 050 ----
mean loss: 131.60
 ---- batch: 060 ----
mean loss: 121.35
 ---- batch: 070 ----
mean loss: 122.58
 ---- batch: 080 ----
mean loss: 126.59
 ---- batch: 090 ----
mean loss: 128.84
train mean loss: 126.61
epoch train time: 0:00:00.508254
elapsed time: 0:02:18.270089
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:10:07.478189
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.44
 ---- batch: 020 ----
mean loss: 128.56
 ---- batch: 030 ----
mean loss: 129.63
 ---- batch: 040 ----
mean loss: 126.47
 ---- batch: 050 ----
mean loss: 127.59
 ---- batch: 060 ----
mean loss: 125.84
 ---- batch: 070 ----
mean loss: 126.83
 ---- batch: 080 ----
mean loss: 126.95
 ---- batch: 090 ----
mean loss: 122.47
train mean loss: 126.40
epoch train time: 0:00:00.502487
elapsed time: 0:02:18.772739
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:10:07.980839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.03
 ---- batch: 020 ----
mean loss: 133.12
 ---- batch: 030 ----
mean loss: 123.99
 ---- batch: 040 ----
mean loss: 124.35
 ---- batch: 050 ----
mean loss: 133.21
 ---- batch: 060 ----
mean loss: 129.84
 ---- batch: 070 ----
mean loss: 122.18
 ---- batch: 080 ----
mean loss: 125.06
 ---- batch: 090 ----
mean loss: 126.79
train mean loss: 126.49
epoch train time: 0:00:00.499762
elapsed time: 0:02:19.272721
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:10:08.480836
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.96
 ---- batch: 020 ----
mean loss: 124.96
 ---- batch: 030 ----
mean loss: 130.08
 ---- batch: 040 ----
mean loss: 129.07
 ---- batch: 050 ----
mean loss: 120.67
 ---- batch: 060 ----
mean loss: 131.08
 ---- batch: 070 ----
mean loss: 125.01
 ---- batch: 080 ----
mean loss: 127.53
 ---- batch: 090 ----
mean loss: 124.44
train mean loss: 126.31
epoch train time: 0:00:00.513042
elapsed time: 0:02:19.785928
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:10:08.994028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.41
 ---- batch: 020 ----
mean loss: 119.68
 ---- batch: 030 ----
mean loss: 131.72
 ---- batch: 040 ----
mean loss: 129.27
 ---- batch: 050 ----
mean loss: 124.96
 ---- batch: 060 ----
mean loss: 127.22
 ---- batch: 070 ----
mean loss: 130.78
 ---- batch: 080 ----
mean loss: 123.08
 ---- batch: 090 ----
mean loss: 128.14
train mean loss: 126.36
epoch train time: 0:00:00.499521
elapsed time: 0:02:20.285620
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:10:09.493742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.59
 ---- batch: 020 ----
mean loss: 122.19
 ---- batch: 030 ----
mean loss: 126.94
 ---- batch: 040 ----
mean loss: 132.86
 ---- batch: 050 ----
mean loss: 128.29
 ---- batch: 060 ----
mean loss: 124.49
 ---- batch: 070 ----
mean loss: 128.72
 ---- batch: 080 ----
mean loss: 123.10
 ---- batch: 090 ----
mean loss: 124.20
train mean loss: 126.45
epoch train time: 0:00:00.502184
elapsed time: 0:02:20.787971
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:10:09.996087
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.02
 ---- batch: 020 ----
mean loss: 129.04
 ---- batch: 030 ----
mean loss: 123.48
 ---- batch: 040 ----
mean loss: 126.71
 ---- batch: 050 ----
mean loss: 128.76
 ---- batch: 060 ----
mean loss: 126.81
 ---- batch: 070 ----
mean loss: 125.06
 ---- batch: 080 ----
mean loss: 125.80
 ---- batch: 090 ----
mean loss: 125.38
train mean loss: 126.32
epoch train time: 0:00:00.497466
elapsed time: 0:02:21.285618
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:10:10.493737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.40
 ---- batch: 020 ----
mean loss: 116.46
 ---- batch: 030 ----
mean loss: 124.27
 ---- batch: 040 ----
mean loss: 124.21
 ---- batch: 050 ----
mean loss: 126.56
 ---- batch: 060 ----
mean loss: 127.53
 ---- batch: 070 ----
mean loss: 130.82
 ---- batch: 080 ----
mean loss: 130.75
 ---- batch: 090 ----
mean loss: 132.32
train mean loss: 126.22
epoch train time: 0:00:00.513990
elapsed time: 0:02:21.799778
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:10:11.007899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.30
 ---- batch: 020 ----
mean loss: 125.95
 ---- batch: 030 ----
mean loss: 125.43
 ---- batch: 040 ----
mean loss: 123.86
 ---- batch: 050 ----
mean loss: 125.06
 ---- batch: 060 ----
mean loss: 127.40
 ---- batch: 070 ----
mean loss: 128.54
 ---- batch: 080 ----
mean loss: 131.64
 ---- batch: 090 ----
mean loss: 125.28
train mean loss: 126.05
epoch train time: 0:00:00.505270
elapsed time: 0:02:22.305216
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:10:11.513317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.28
 ---- batch: 020 ----
mean loss: 120.87
 ---- batch: 030 ----
mean loss: 127.03
 ---- batch: 040 ----
mean loss: 124.65
 ---- batch: 050 ----
mean loss: 126.19
 ---- batch: 060 ----
mean loss: 124.36
 ---- batch: 070 ----
mean loss: 127.41
 ---- batch: 080 ----
mean loss: 127.79
 ---- batch: 090 ----
mean loss: 129.74
train mean loss: 126.12
epoch train time: 0:00:00.506877
elapsed time: 0:02:22.812292
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:10:12.020392
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.51
 ---- batch: 020 ----
mean loss: 126.36
 ---- batch: 030 ----
mean loss: 129.95
 ---- batch: 040 ----
mean loss: 120.34
 ---- batch: 050 ----
mean loss: 125.85
 ---- batch: 060 ----
mean loss: 125.24
 ---- batch: 070 ----
mean loss: 128.00
 ---- batch: 080 ----
mean loss: 127.85
 ---- batch: 090 ----
mean loss: 125.88
train mean loss: 125.92
epoch train time: 0:00:00.500659
elapsed time: 0:02:23.313100
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:10:12.521202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.71
 ---- batch: 020 ----
mean loss: 126.61
 ---- batch: 030 ----
mean loss: 127.66
 ---- batch: 040 ----
mean loss: 126.07
 ---- batch: 050 ----
mean loss: 123.27
 ---- batch: 060 ----
mean loss: 128.09
 ---- batch: 070 ----
mean loss: 123.50
 ---- batch: 080 ----
mean loss: 128.12
 ---- batch: 090 ----
mean loss: 132.71
train mean loss: 126.02
epoch train time: 0:00:00.503767
elapsed time: 0:02:23.817037
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:10:13.025157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.74
 ---- batch: 020 ----
mean loss: 126.94
 ---- batch: 030 ----
mean loss: 129.20
 ---- batch: 040 ----
mean loss: 128.74
 ---- batch: 050 ----
mean loss: 129.57
 ---- batch: 060 ----
mean loss: 121.37
 ---- batch: 070 ----
mean loss: 120.90
 ---- batch: 080 ----
mean loss: 130.18
 ---- batch: 090 ----
mean loss: 123.42
train mean loss: 126.05
epoch train time: 0:00:00.498971
elapsed time: 0:02:24.316178
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:10:13.524280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.08
 ---- batch: 020 ----
mean loss: 124.51
 ---- batch: 030 ----
mean loss: 123.25
 ---- batch: 040 ----
mean loss: 127.41
 ---- batch: 050 ----
mean loss: 126.84
 ---- batch: 060 ----
mean loss: 128.25
 ---- batch: 070 ----
mean loss: 126.65
 ---- batch: 080 ----
mean loss: 126.44
 ---- batch: 090 ----
mean loss: 127.62
train mean loss: 125.95
epoch train time: 0:00:00.505097
elapsed time: 0:02:24.821442
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:10:14.029540
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.01
 ---- batch: 020 ----
mean loss: 125.05
 ---- batch: 030 ----
mean loss: 125.99
 ---- batch: 040 ----
mean loss: 123.58
 ---- batch: 050 ----
mean loss: 124.99
 ---- batch: 060 ----
mean loss: 130.42
 ---- batch: 070 ----
mean loss: 126.70
 ---- batch: 080 ----
mean loss: 128.59
 ---- batch: 090 ----
mean loss: 123.96
train mean loss: 125.85
epoch train time: 0:00:00.495768
elapsed time: 0:02:25.317355
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:10:14.525468
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.40
 ---- batch: 020 ----
mean loss: 123.86
 ---- batch: 030 ----
mean loss: 123.45
 ---- batch: 040 ----
mean loss: 122.38
 ---- batch: 050 ----
mean loss: 122.91
 ---- batch: 060 ----
mean loss: 124.70
 ---- batch: 070 ----
mean loss: 129.01
 ---- batch: 080 ----
mean loss: 131.80
 ---- batch: 090 ----
mean loss: 129.55
train mean loss: 125.98
epoch train time: 0:00:00.512436
elapsed time: 0:02:25.829958
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:10:15.038061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.81
 ---- batch: 020 ----
mean loss: 127.19
 ---- batch: 030 ----
mean loss: 124.89
 ---- batch: 040 ----
mean loss: 125.62
 ---- batch: 050 ----
mean loss: 128.55
 ---- batch: 060 ----
mean loss: 121.19
 ---- batch: 070 ----
mean loss: 127.43
 ---- batch: 080 ----
mean loss: 125.93
 ---- batch: 090 ----
mean loss: 128.98
train mean loss: 125.88
epoch train time: 0:00:00.494276
elapsed time: 0:02:26.324382
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:10:15.532518
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.64
 ---- batch: 020 ----
mean loss: 118.35
 ---- batch: 030 ----
mean loss: 125.89
 ---- batch: 040 ----
mean loss: 119.12
 ---- batch: 050 ----
mean loss: 126.63
 ---- batch: 060 ----
mean loss: 125.25
 ---- batch: 070 ----
mean loss: 127.58
 ---- batch: 080 ----
mean loss: 132.49
 ---- batch: 090 ----
mean loss: 129.81
train mean loss: 125.83
epoch train time: 0:00:00.504628
elapsed time: 0:02:26.829194
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:10:16.037299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.79
 ---- batch: 020 ----
mean loss: 130.31
 ---- batch: 030 ----
mean loss: 130.28
 ---- batch: 040 ----
mean loss: 125.94
 ---- batch: 050 ----
mean loss: 124.80
 ---- batch: 060 ----
mean loss: 123.88
 ---- batch: 070 ----
mean loss: 121.19
 ---- batch: 080 ----
mean loss: 121.03
 ---- batch: 090 ----
mean loss: 126.80
train mean loss: 125.79
epoch train time: 0:00:00.496555
elapsed time: 0:02:27.325906
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:10:16.534009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.57
 ---- batch: 020 ----
mean loss: 134.72
 ---- batch: 030 ----
mean loss: 123.80
 ---- batch: 040 ----
mean loss: 124.11
 ---- batch: 050 ----
mean loss: 121.03
 ---- batch: 060 ----
mean loss: 120.72
 ---- batch: 070 ----
mean loss: 119.74
 ---- batch: 080 ----
mean loss: 128.67
 ---- batch: 090 ----
mean loss: 131.65
train mean loss: 125.90
epoch train time: 0:00:00.502348
elapsed time: 0:02:27.828414
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:10:17.036538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.58
 ---- batch: 020 ----
mean loss: 131.40
 ---- batch: 030 ----
mean loss: 123.52
 ---- batch: 040 ----
mean loss: 133.40
 ---- batch: 050 ----
mean loss: 130.19
 ---- batch: 060 ----
mean loss: 122.28
 ---- batch: 070 ----
mean loss: 121.90
 ---- batch: 080 ----
mean loss: 128.79
 ---- batch: 090 ----
mean loss: 120.25
train mean loss: 125.76
epoch train time: 0:00:00.505434
elapsed time: 0:02:28.334048
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:10:17.542147
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.14
 ---- batch: 020 ----
mean loss: 122.99
 ---- batch: 030 ----
mean loss: 130.52
 ---- batch: 040 ----
mean loss: 129.55
 ---- batch: 050 ----
mean loss: 126.32
 ---- batch: 060 ----
mean loss: 123.67
 ---- batch: 070 ----
mean loss: 125.74
 ---- batch: 080 ----
mean loss: 124.53
 ---- batch: 090 ----
mean loss: 126.51
train mean loss: 125.55
epoch train time: 0:00:00.511066
elapsed time: 0:02:28.845262
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:10:18.053364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.67
 ---- batch: 020 ----
mean loss: 122.28
 ---- batch: 030 ----
mean loss: 125.96
 ---- batch: 040 ----
mean loss: 128.17
 ---- batch: 050 ----
mean loss: 127.77
 ---- batch: 060 ----
mean loss: 128.49
 ---- batch: 070 ----
mean loss: 123.38
 ---- batch: 080 ----
mean loss: 121.90
 ---- batch: 090 ----
mean loss: 129.42
train mean loss: 125.59
epoch train time: 0:00:00.506339
elapsed time: 0:02:29.351746
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:10:18.559882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.32
 ---- batch: 020 ----
mean loss: 126.30
 ---- batch: 030 ----
mean loss: 127.82
 ---- batch: 040 ----
mean loss: 123.18
 ---- batch: 050 ----
mean loss: 123.96
 ---- batch: 060 ----
mean loss: 127.02
 ---- batch: 070 ----
mean loss: 125.98
 ---- batch: 080 ----
mean loss: 127.03
 ---- batch: 090 ----
mean loss: 122.90
train mean loss: 125.66
epoch train time: 0:00:00.504731
elapsed time: 0:02:29.856674
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:10:19.064777
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.50
 ---- batch: 020 ----
mean loss: 123.31
 ---- batch: 030 ----
mean loss: 123.88
 ---- batch: 040 ----
mean loss: 131.74
 ---- batch: 050 ----
mean loss: 124.40
 ---- batch: 060 ----
mean loss: 118.77
 ---- batch: 070 ----
mean loss: 126.08
 ---- batch: 080 ----
mean loss: 129.92
 ---- batch: 090 ----
mean loss: 124.41
train mean loss: 125.62
epoch train time: 0:00:00.501908
elapsed time: 0:02:30.358744
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:10:19.566846
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.84
 ---- batch: 020 ----
mean loss: 128.38
 ---- batch: 030 ----
mean loss: 122.68
 ---- batch: 040 ----
mean loss: 124.62
 ---- batch: 050 ----
mean loss: 122.43
 ---- batch: 060 ----
mean loss: 125.63
 ---- batch: 070 ----
mean loss: 127.72
 ---- batch: 080 ----
mean loss: 123.94
 ---- batch: 090 ----
mean loss: 127.18
train mean loss: 125.28
epoch train time: 0:00:00.504139
elapsed time: 0:02:30.863031
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:10:20.071134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.05
 ---- batch: 020 ----
mean loss: 127.93
 ---- batch: 030 ----
mean loss: 121.32
 ---- batch: 040 ----
mean loss: 125.29
 ---- batch: 050 ----
mean loss: 126.02
 ---- batch: 060 ----
mean loss: 124.53
 ---- batch: 070 ----
mean loss: 119.78
 ---- batch: 080 ----
mean loss: 128.49
 ---- batch: 090 ----
mean loss: 125.12
train mean loss: 125.81
epoch train time: 0:00:00.510685
elapsed time: 0:02:31.373885
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:10:20.581991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.40
 ---- batch: 020 ----
mean loss: 132.19
 ---- batch: 030 ----
mean loss: 121.86
 ---- batch: 040 ----
mean loss: 125.97
 ---- batch: 050 ----
mean loss: 125.11
 ---- batch: 060 ----
mean loss: 126.52
 ---- batch: 070 ----
mean loss: 125.66
 ---- batch: 080 ----
mean loss: 126.35
 ---- batch: 090 ----
mean loss: 127.02
train mean loss: 125.47
epoch train time: 0:00:00.506372
elapsed time: 0:02:31.880414
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:10:21.088518
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.39
 ---- batch: 020 ----
mean loss: 124.53
 ---- batch: 030 ----
mean loss: 123.61
 ---- batch: 040 ----
mean loss: 123.66
 ---- batch: 050 ----
mean loss: 125.12
 ---- batch: 060 ----
mean loss: 127.73
 ---- batch: 070 ----
mean loss: 127.41
 ---- batch: 080 ----
mean loss: 125.76
 ---- batch: 090 ----
mean loss: 123.29
train mean loss: 125.41
epoch train time: 0:00:00.497633
elapsed time: 0:02:32.378195
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:10:21.586294
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.67
 ---- batch: 020 ----
mean loss: 124.11
 ---- batch: 030 ----
mean loss: 126.11
 ---- batch: 040 ----
mean loss: 127.63
 ---- batch: 050 ----
mean loss: 124.68
 ---- batch: 060 ----
mean loss: 123.42
 ---- batch: 070 ----
mean loss: 121.13
 ---- batch: 080 ----
mean loss: 127.73
 ---- batch: 090 ----
mean loss: 128.99
train mean loss: 125.36
epoch train time: 0:00:00.501992
elapsed time: 0:02:32.880328
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:10:22.088440
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.96
 ---- batch: 020 ----
mean loss: 121.85
 ---- batch: 030 ----
mean loss: 132.73
 ---- batch: 040 ----
mean loss: 124.30
 ---- batch: 050 ----
mean loss: 125.57
 ---- batch: 060 ----
mean loss: 122.01
 ---- batch: 070 ----
mean loss: 122.97
 ---- batch: 080 ----
mean loss: 130.89
 ---- batch: 090 ----
mean loss: 124.27
train mean loss: 125.35
epoch train time: 0:00:00.498717
elapsed time: 0:02:33.379203
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:10:22.587321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.20
 ---- batch: 020 ----
mean loss: 124.24
 ---- batch: 030 ----
mean loss: 125.53
 ---- batch: 040 ----
mean loss: 123.35
 ---- batch: 050 ----
mean loss: 130.01
 ---- batch: 060 ----
mean loss: 119.77
 ---- batch: 070 ----
mean loss: 129.06
 ---- batch: 080 ----
mean loss: 126.61
 ---- batch: 090 ----
mean loss: 124.63
train mean loss: 125.52
epoch train time: 0:00:00.510459
elapsed time: 0:02:33.889826
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:10:23.097935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.41
 ---- batch: 020 ----
mean loss: 125.40
 ---- batch: 030 ----
mean loss: 123.85
 ---- batch: 040 ----
mean loss: 120.43
 ---- batch: 050 ----
mean loss: 132.50
 ---- batch: 060 ----
mean loss: 125.35
 ---- batch: 070 ----
mean loss: 117.24
 ---- batch: 080 ----
mean loss: 127.66
 ---- batch: 090 ----
mean loss: 128.71
train mean loss: 125.62
epoch train time: 0:00:00.503200
elapsed time: 0:02:34.393183
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:10:23.601285
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.26
 ---- batch: 020 ----
mean loss: 130.07
 ---- batch: 030 ----
mean loss: 122.43
 ---- batch: 040 ----
mean loss: 116.96
 ---- batch: 050 ----
mean loss: 124.95
 ---- batch: 060 ----
mean loss: 124.20
 ---- batch: 070 ----
mean loss: 129.70
 ---- batch: 080 ----
mean loss: 126.86
 ---- batch: 090 ----
mean loss: 118.68
train mean loss: 125.40
epoch train time: 0:00:00.509917
elapsed time: 0:02:34.903250
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:10:24.111366
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.57
 ---- batch: 020 ----
mean loss: 121.35
 ---- batch: 030 ----
mean loss: 125.28
 ---- batch: 040 ----
mean loss: 123.91
 ---- batch: 050 ----
mean loss: 125.69
 ---- batch: 060 ----
mean loss: 122.25
 ---- batch: 070 ----
mean loss: 128.29
 ---- batch: 080 ----
mean loss: 123.47
 ---- batch: 090 ----
mean loss: 125.26
train mean loss: 125.31
epoch train time: 0:00:00.500560
elapsed time: 0:02:35.403971
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:10:24.612105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.97
 ---- batch: 020 ----
mean loss: 118.01
 ---- batch: 030 ----
mean loss: 123.78
 ---- batch: 040 ----
mean loss: 128.77
 ---- batch: 050 ----
mean loss: 120.44
 ---- batch: 060 ----
mean loss: 124.00
 ---- batch: 070 ----
mean loss: 124.71
 ---- batch: 080 ----
mean loss: 126.45
 ---- batch: 090 ----
mean loss: 129.62
train mean loss: 125.21
epoch train time: 0:00:00.514886
elapsed time: 0:02:35.919037
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:10:25.127137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.52
 ---- batch: 020 ----
mean loss: 126.51
 ---- batch: 030 ----
mean loss: 123.68
 ---- batch: 040 ----
mean loss: 124.77
 ---- batch: 050 ----
mean loss: 120.72
 ---- batch: 060 ----
mean loss: 128.66
 ---- batch: 070 ----
mean loss: 126.39
 ---- batch: 080 ----
mean loss: 123.23
 ---- batch: 090 ----
mean loss: 128.02
train mean loss: 125.25
epoch train time: 0:00:00.503662
elapsed time: 0:02:36.422844
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:10:25.630968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.91
 ---- batch: 020 ----
mean loss: 133.35
 ---- batch: 030 ----
mean loss: 121.32
 ---- batch: 040 ----
mean loss: 122.14
 ---- batch: 050 ----
mean loss: 123.29
 ---- batch: 060 ----
mean loss: 122.23
 ---- batch: 070 ----
mean loss: 124.46
 ---- batch: 080 ----
mean loss: 123.45
 ---- batch: 090 ----
mean loss: 131.79
train mean loss: 125.16
epoch train time: 0:00:00.503097
elapsed time: 0:02:36.926130
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:10:26.134232
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.16
 ---- batch: 020 ----
mean loss: 123.98
 ---- batch: 030 ----
mean loss: 119.60
 ---- batch: 040 ----
mean loss: 124.95
 ---- batch: 050 ----
mean loss: 129.50
 ---- batch: 060 ----
mean loss: 127.00
 ---- batch: 070 ----
mean loss: 126.70
 ---- batch: 080 ----
mean loss: 128.38
 ---- batch: 090 ----
mean loss: 126.23
train mean loss: 124.98
epoch train time: 0:00:00.500436
elapsed time: 0:02:37.426745
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:10:26.634842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.97
 ---- batch: 020 ----
mean loss: 120.72
 ---- batch: 030 ----
mean loss: 119.35
 ---- batch: 040 ----
mean loss: 123.23
 ---- batch: 050 ----
mean loss: 124.36
 ---- batch: 060 ----
mean loss: 125.00
 ---- batch: 070 ----
mean loss: 128.46
 ---- batch: 080 ----
mean loss: 124.75
 ---- batch: 090 ----
mean loss: 129.23
train mean loss: 125.20
epoch train time: 0:00:00.504393
elapsed time: 0:02:37.931292
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:10:27.139390
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.48
 ---- batch: 020 ----
mean loss: 120.61
 ---- batch: 030 ----
mean loss: 121.37
 ---- batch: 040 ----
mean loss: 128.60
 ---- batch: 050 ----
mean loss: 130.61
 ---- batch: 060 ----
mean loss: 129.31
 ---- batch: 070 ----
mean loss: 123.76
 ---- batch: 080 ----
mean loss: 124.15
 ---- batch: 090 ----
mean loss: 127.12
train mean loss: 125.05
epoch train time: 0:00:00.493674
elapsed time: 0:02:38.425106
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:10:27.633205
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.42
 ---- batch: 020 ----
mean loss: 119.59
 ---- batch: 030 ----
mean loss: 120.46
 ---- batch: 040 ----
mean loss: 124.82
 ---- batch: 050 ----
mean loss: 128.52
 ---- batch: 060 ----
mean loss: 129.22
 ---- batch: 070 ----
mean loss: 135.44
 ---- batch: 080 ----
mean loss: 124.76
 ---- batch: 090 ----
mean loss: 119.24
train mean loss: 125.01
epoch train time: 0:00:00.506267
elapsed time: 0:02:38.931523
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:10:28.139641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.16
 ---- batch: 020 ----
mean loss: 129.89
 ---- batch: 030 ----
mean loss: 123.20
 ---- batch: 040 ----
mean loss: 113.87
 ---- batch: 050 ----
mean loss: 128.59
 ---- batch: 060 ----
mean loss: 125.27
 ---- batch: 070 ----
mean loss: 126.40
 ---- batch: 080 ----
mean loss: 126.48
 ---- batch: 090 ----
mean loss: 127.26
train mean loss: 124.74
epoch train time: 0:00:00.492307
elapsed time: 0:02:39.427291
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_2/checkpoint.pth.tar
**** end time: 2019-09-25 23:10:28.635359 ****
