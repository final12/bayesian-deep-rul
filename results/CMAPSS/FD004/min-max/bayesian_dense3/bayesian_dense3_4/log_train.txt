Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 14601
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianDense3...
Done.
**** start time: 2019-09-26 22:54:47.748826 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
    BayesianLinear-2                  [-1, 100]          72,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 112,200
Trainable params: 112,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 22:54:47.758624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4882.60
 ---- batch: 020 ----
mean loss: 4695.12
 ---- batch: 030 ----
mean loss: 4432.22
 ---- batch: 040 ----
mean loss: 4132.78
 ---- batch: 050 ----
mean loss: 3871.83
 ---- batch: 060 ----
mean loss: 3618.63
 ---- batch: 070 ----
mean loss: 3457.96
 ---- batch: 080 ----
mean loss: 3291.96
 ---- batch: 090 ----
mean loss: 3162.41
 ---- batch: 100 ----
mean loss: 3079.27
 ---- batch: 110 ----
mean loss: 2996.43
train mean loss: 3761.34
epoch train time: 0:00:34.149862
elapsed time: 0:00:34.166260
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 22:55:21.915185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2876.26
 ---- batch: 020 ----
mean loss: 2779.99
 ---- batch: 030 ----
mean loss: 2745.82
 ---- batch: 040 ----
mean loss: 2676.66
 ---- batch: 050 ----
mean loss: 2663.38
 ---- batch: 060 ----
mean loss: 2589.99
 ---- batch: 070 ----
mean loss: 2537.56
 ---- batch: 080 ----
mean loss: 2534.66
 ---- batch: 090 ----
mean loss: 2462.58
 ---- batch: 100 ----
mean loss: 2412.78
 ---- batch: 110 ----
mean loss: 2366.64
train mean loss: 2599.06
epoch train time: 0:00:01.944040
elapsed time: 0:00:36.110599
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 22:55:23.859772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2350.03
 ---- batch: 020 ----
mean loss: 2324.68
 ---- batch: 030 ----
mean loss: 2310.38
 ---- batch: 040 ----
mean loss: 2275.33
 ---- batch: 050 ----
mean loss: 2238.49
 ---- batch: 060 ----
mean loss: 2198.03
 ---- batch: 070 ----
mean loss: 2192.39
 ---- batch: 080 ----
mean loss: 2144.26
 ---- batch: 090 ----
mean loss: 2105.83
 ---- batch: 100 ----
mean loss: 2104.12
 ---- batch: 110 ----
mean loss: 2035.34
train mean loss: 2204.33
epoch train time: 0:00:01.956619
elapsed time: 0:00:38.067778
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 22:55:25.816982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2031.68
 ---- batch: 020 ----
mean loss: 1979.72
 ---- batch: 030 ----
mean loss: 2015.90
 ---- batch: 040 ----
mean loss: 1981.22
 ---- batch: 050 ----
mean loss: 1935.62
 ---- batch: 060 ----
mean loss: 1931.63
 ---- batch: 070 ----
mean loss: 1909.13
 ---- batch: 080 ----
mean loss: 1908.42
 ---- batch: 090 ----
mean loss: 1855.17
 ---- batch: 100 ----
mean loss: 1852.58
 ---- batch: 110 ----
mean loss: 1828.13
train mean loss: 1926.79
epoch train time: 0:00:01.910887
elapsed time: 0:00:39.979266
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 22:55:27.728532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1820.72
 ---- batch: 020 ----
mean loss: 1787.76
 ---- batch: 030 ----
mean loss: 1752.45
 ---- batch: 040 ----
mean loss: 1736.87
 ---- batch: 050 ----
mean loss: 1724.06
 ---- batch: 060 ----
mean loss: 1699.38
 ---- batch: 070 ----
mean loss: 1663.35
 ---- batch: 080 ----
mean loss: 1645.15
 ---- batch: 090 ----
mean loss: 1653.05
 ---- batch: 100 ----
mean loss: 1640.32
 ---- batch: 110 ----
mean loss: 1634.81
train mean loss: 1702.14
epoch train time: 0:00:01.925024
elapsed time: 0:00:41.904948
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 22:55:29.654128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1598.31
 ---- batch: 020 ----
mean loss: 1576.73
 ---- batch: 030 ----
mean loss: 1572.88
 ---- batch: 040 ----
mean loss: 1575.61
 ---- batch: 050 ----
mean loss: 1520.84
 ---- batch: 060 ----
mean loss: 1513.81
 ---- batch: 070 ----
mean loss: 1494.39
 ---- batch: 080 ----
mean loss: 1472.09
 ---- batch: 090 ----
mean loss: 1470.29
 ---- batch: 100 ----
mean loss: 1472.96
 ---- batch: 110 ----
mean loss: 1454.15
train mean loss: 1518.87
epoch train time: 0:00:01.901877
elapsed time: 0:00:43.807375
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 22:55:31.556580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1441.80
 ---- batch: 020 ----
mean loss: 1423.46
 ---- batch: 030 ----
mean loss: 1397.68
 ---- batch: 040 ----
mean loss: 1398.36
 ---- batch: 050 ----
mean loss: 1418.84
 ---- batch: 060 ----
mean loss: 1347.69
 ---- batch: 070 ----
mean loss: 1352.88
 ---- batch: 080 ----
mean loss: 1334.04
 ---- batch: 090 ----
mean loss: 1336.89
 ---- batch: 100 ----
mean loss: 1313.62
 ---- batch: 110 ----
mean loss: 1332.18
train mean loss: 1371.09
epoch train time: 0:00:01.925164
elapsed time: 0:00:45.733109
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 22:55:33.482329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1288.06
 ---- batch: 020 ----
mean loss: 1293.07
 ---- batch: 030 ----
mean loss: 1262.88
 ---- batch: 040 ----
mean loss: 1260.92
 ---- batch: 050 ----
mean loss: 1259.26
 ---- batch: 060 ----
mean loss: 1248.11
 ---- batch: 070 ----
mean loss: 1241.11
 ---- batch: 080 ----
mean loss: 1232.07
 ---- batch: 090 ----
mean loss: 1243.58
 ---- batch: 100 ----
mean loss: 1234.66
 ---- batch: 110 ----
mean loss: 1186.89
train mean loss: 1249.23
epoch train time: 0:00:01.873749
elapsed time: 0:00:47.607474
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 22:55:35.356724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1183.68
 ---- batch: 020 ----
mean loss: 1187.68
 ---- batch: 030 ----
mean loss: 1168.95
 ---- batch: 040 ----
mean loss: 1159.32
 ---- batch: 050 ----
mean loss: 1161.15
 ---- batch: 060 ----
mean loss: 1153.12
 ---- batch: 070 ----
mean loss: 1155.82
 ---- batch: 080 ----
mean loss: 1139.13
 ---- batch: 090 ----
mean loss: 1122.61
 ---- batch: 100 ----
mean loss: 1130.13
 ---- batch: 110 ----
mean loss: 1126.82
train mean loss: 1151.68
epoch train time: 0:00:01.863986
elapsed time: 0:00:49.472080
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 22:55:37.221673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1101.44
 ---- batch: 020 ----
mean loss: 1110.03
 ---- batch: 030 ----
mean loss: 1083.22
 ---- batch: 040 ----
mean loss: 1078.57
 ---- batch: 050 ----
mean loss: 1082.77
 ---- batch: 060 ----
mean loss: 1076.23
 ---- batch: 070 ----
mean loss: 1082.90
 ---- batch: 080 ----
mean loss: 1067.37
 ---- batch: 090 ----
mean loss: 1063.52
 ---- batch: 100 ----
mean loss: 1035.72
 ---- batch: 110 ----
mean loss: 1058.25
train mean loss: 1075.83
epoch train time: 0:00:01.910187
elapsed time: 0:00:51.383239
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 22:55:39.132494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1040.84
 ---- batch: 020 ----
mean loss: 1027.85
 ---- batch: 030 ----
mean loss: 1033.56
 ---- batch: 040 ----
mean loss: 1023.70
 ---- batch: 050 ----
mean loss: 1019.99
 ---- batch: 060 ----
mean loss: 1016.65
 ---- batch: 070 ----
mean loss: 995.93
 ---- batch: 080 ----
mean loss: 1003.44
 ---- batch: 090 ----
mean loss: 1002.79
 ---- batch: 100 ----
mean loss: 1013.62
 ---- batch: 110 ----
mean loss: 989.52
train mean loss: 1014.56
epoch train time: 0:00:01.890675
elapsed time: 0:00:53.274535
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 22:55:41.023587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 999.16
 ---- batch: 020 ----
mean loss: 982.41
 ---- batch: 030 ----
mean loss: 980.43
 ---- batch: 040 ----
mean loss: 968.94
 ---- batch: 050 ----
mean loss: 957.98
 ---- batch: 060 ----
mean loss: 966.49
 ---- batch: 070 ----
mean loss: 972.38
 ---- batch: 080 ----
mean loss: 950.28
 ---- batch: 090 ----
mean loss: 968.11
 ---- batch: 100 ----
mean loss: 956.93
 ---- batch: 110 ----
mean loss: 952.17
train mean loss: 968.71
epoch train time: 0:00:01.899458
elapsed time: 0:00:55.174418
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 22:55:42.923596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 962.93
 ---- batch: 020 ----
mean loss: 954.04
 ---- batch: 030 ----
mean loss: 944.96
 ---- batch: 040 ----
mean loss: 940.29
 ---- batch: 050 ----
mean loss: 929.22
 ---- batch: 060 ----
mean loss: 912.85
 ---- batch: 070 ----
mean loss: 934.40
 ---- batch: 080 ----
mean loss: 913.73
 ---- batch: 090 ----
mean loss: 922.57
 ---- batch: 100 ----
mean loss: 928.65
 ---- batch: 110 ----
mean loss: 903.46
train mean loss: 930.97
epoch train time: 0:00:01.862705
elapsed time: 0:00:57.037708
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 22:55:44.786947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.07
 ---- batch: 020 ----
mean loss: 908.35
 ---- batch: 030 ----
mean loss: 904.90
 ---- batch: 040 ----
mean loss: 901.50
 ---- batch: 050 ----
mean loss: 902.52
 ---- batch: 060 ----
mean loss: 912.35
 ---- batch: 070 ----
mean loss: 906.21
 ---- batch: 080 ----
mean loss: 909.07
 ---- batch: 090 ----
mean loss: 897.05
 ---- batch: 100 ----
mean loss: 905.79
 ---- batch: 110 ----
mean loss: 907.96
train mean loss: 905.93
epoch train time: 0:00:01.924596
elapsed time: 0:00:58.962905
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 22:55:46.712109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.79
 ---- batch: 020 ----
mean loss: 893.93
 ---- batch: 030 ----
mean loss: 903.60
 ---- batch: 040 ----
mean loss: 899.91
 ---- batch: 050 ----
mean loss: 886.09
 ---- batch: 060 ----
mean loss: 884.57
 ---- batch: 070 ----
mean loss: 873.54
 ---- batch: 080 ----
mean loss: 876.16
 ---- batch: 090 ----
mean loss: 881.86
 ---- batch: 100 ----
mean loss: 876.83
 ---- batch: 110 ----
mean loss: 892.82
train mean loss: 886.71
epoch train time: 0:00:01.915369
elapsed time: 0:01:00.878859
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 22:55:48.628061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.01
 ---- batch: 020 ----
mean loss: 882.20
 ---- batch: 030 ----
mean loss: 869.83
 ---- batch: 040 ----
mean loss: 862.15
 ---- batch: 050 ----
mean loss: 874.87
 ---- batch: 060 ----
mean loss: 873.81
 ---- batch: 070 ----
mean loss: 888.62
 ---- batch: 080 ----
mean loss: 867.25
 ---- batch: 090 ----
mean loss: 869.13
 ---- batch: 100 ----
mean loss: 879.06
 ---- batch: 110 ----
mean loss: 854.20
train mean loss: 874.16
epoch train time: 0:00:01.890147
elapsed time: 0:01:02.769613
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 22:55:50.518787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.57
 ---- batch: 020 ----
mean loss: 844.50
 ---- batch: 030 ----
mean loss: 866.38
 ---- batch: 040 ----
mean loss: 884.42
 ---- batch: 050 ----
mean loss: 882.51
 ---- batch: 060 ----
mean loss: 885.25
 ---- batch: 070 ----
mean loss: 871.16
 ---- batch: 080 ----
mean loss: 863.59
 ---- batch: 090 ----
mean loss: 852.76
 ---- batch: 100 ----
mean loss: 858.73
 ---- batch: 110 ----
mean loss: 860.75
train mean loss: 866.63
epoch train time: 0:00:01.906626
elapsed time: 0:01:04.676803
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 22:55:52.426165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.73
 ---- batch: 020 ----
mean loss: 867.58
 ---- batch: 030 ----
mean loss: 851.19
 ---- batch: 040 ----
mean loss: 859.11
 ---- batch: 050 ----
mean loss: 876.04
 ---- batch: 060 ----
mean loss: 843.76
 ---- batch: 070 ----
mean loss: 873.07
 ---- batch: 080 ----
mean loss: 858.30
 ---- batch: 090 ----
mean loss: 855.30
 ---- batch: 100 ----
mean loss: 869.03
 ---- batch: 110 ----
mean loss: 872.43
train mean loss: 861.08
epoch train time: 0:00:01.906426
elapsed time: 0:01:06.583970
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 22:55:54.333154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.60
 ---- batch: 020 ----
mean loss: 871.11
 ---- batch: 030 ----
mean loss: 855.84
 ---- batch: 040 ----
mean loss: 835.27
 ---- batch: 050 ----
mean loss: 849.78
 ---- batch: 060 ----
mean loss: 868.67
 ---- batch: 070 ----
mean loss: 857.29
 ---- batch: 080 ----
mean loss: 856.37
 ---- batch: 090 ----
mean loss: 869.16
 ---- batch: 100 ----
mean loss: 850.12
 ---- batch: 110 ----
mean loss: 876.62
train mean loss: 858.31
epoch train time: 0:00:01.888787
elapsed time: 0:01:08.473349
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 22:55:56.222606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.70
 ---- batch: 020 ----
mean loss: 867.53
 ---- batch: 030 ----
mean loss: 865.63
 ---- batch: 040 ----
mean loss: 852.13
 ---- batch: 050 ----
mean loss: 842.90
 ---- batch: 060 ----
mean loss: 857.61
 ---- batch: 070 ----
mean loss: 854.85
 ---- batch: 080 ----
mean loss: 859.29
 ---- batch: 090 ----
mean loss: 850.51
 ---- batch: 100 ----
mean loss: 853.51
 ---- batch: 110 ----
mean loss: 863.60
train mean loss: 855.24
epoch train time: 0:00:01.933769
elapsed time: 0:01:10.407803
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 22:55:58.157044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.99
 ---- batch: 020 ----
mean loss: 890.92
 ---- batch: 030 ----
mean loss: 853.68
 ---- batch: 040 ----
mean loss: 877.82
 ---- batch: 050 ----
mean loss: 861.92
 ---- batch: 060 ----
mean loss: 871.58
 ---- batch: 070 ----
mean loss: 853.88
 ---- batch: 080 ----
mean loss: 866.87
 ---- batch: 090 ----
mean loss: 838.57
 ---- batch: 100 ----
mean loss: 835.39
 ---- batch: 110 ----
mean loss: 843.00
train mean loss: 855.15
epoch train time: 0:00:01.922540
elapsed time: 0:01:12.330958
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 22:56:00.079970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.32
 ---- batch: 020 ----
mean loss: 864.50
 ---- batch: 030 ----
mean loss: 840.06
 ---- batch: 040 ----
mean loss: 867.03
 ---- batch: 050 ----
mean loss: 862.56
 ---- batch: 060 ----
mean loss: 849.49
 ---- batch: 070 ----
mean loss: 864.52
 ---- batch: 080 ----
mean loss: 846.25
 ---- batch: 090 ----
mean loss: 842.96
 ---- batch: 100 ----
mean loss: 839.55
 ---- batch: 110 ----
mean loss: 864.06
train mean loss: 853.10
epoch train time: 0:00:01.932771
elapsed time: 0:01:14.264122
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 22:56:02.013380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.97
 ---- batch: 020 ----
mean loss: 841.82
 ---- batch: 030 ----
mean loss: 830.54
 ---- batch: 040 ----
mean loss: 853.43
 ---- batch: 050 ----
mean loss: 877.90
 ---- batch: 060 ----
mean loss: 845.36
 ---- batch: 070 ----
mean loss: 873.58
 ---- batch: 080 ----
mean loss: 840.38
 ---- batch: 090 ----
mean loss: 854.95
 ---- batch: 100 ----
mean loss: 864.29
 ---- batch: 110 ----
mean loss: 851.94
train mean loss: 852.42
epoch train time: 0:00:01.940545
elapsed time: 0:01:16.205373
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 22:56:03.954621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.90
 ---- batch: 020 ----
mean loss: 851.39
 ---- batch: 030 ----
mean loss: 864.15
 ---- batch: 040 ----
mean loss: 853.95
 ---- batch: 050 ----
mean loss: 867.13
 ---- batch: 060 ----
mean loss: 841.13
 ---- batch: 070 ----
mean loss: 845.23
 ---- batch: 080 ----
mean loss: 865.56
 ---- batch: 090 ----
mean loss: 846.82
 ---- batch: 100 ----
mean loss: 860.28
 ---- batch: 110 ----
mean loss: 838.74
train mean loss: 852.31
epoch train time: 0:00:01.932427
elapsed time: 0:01:18.138428
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 22:56:05.887643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.52
 ---- batch: 020 ----
mean loss: 852.29
 ---- batch: 030 ----
mean loss: 835.48
 ---- batch: 040 ----
mean loss: 856.90
 ---- batch: 050 ----
mean loss: 852.67
 ---- batch: 060 ----
mean loss: 868.32
 ---- batch: 070 ----
mean loss: 827.30
 ---- batch: 080 ----
mean loss: 853.33
 ---- batch: 090 ----
mean loss: 857.90
 ---- batch: 100 ----
mean loss: 842.75
 ---- batch: 110 ----
mean loss: 860.38
train mean loss: 851.02
epoch train time: 0:00:01.901511
elapsed time: 0:01:20.040548
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 22:56:07.789765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.67
 ---- batch: 020 ----
mean loss: 848.78
 ---- batch: 030 ----
mean loss: 851.06
 ---- batch: 040 ----
mean loss: 845.61
 ---- batch: 050 ----
mean loss: 841.81
 ---- batch: 060 ----
mean loss: 861.09
 ---- batch: 070 ----
mean loss: 875.46
 ---- batch: 080 ----
mean loss: 839.34
 ---- batch: 090 ----
mean loss: 855.09
 ---- batch: 100 ----
mean loss: 843.85
 ---- batch: 110 ----
mean loss: 845.30
train mean loss: 850.83
epoch train time: 0:00:01.894654
elapsed time: 0:01:21.935801
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 22:56:09.685035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.33
 ---- batch: 020 ----
mean loss: 860.69
 ---- batch: 030 ----
mean loss: 864.50
 ---- batch: 040 ----
mean loss: 850.52
 ---- batch: 050 ----
mean loss: 842.29
 ---- batch: 060 ----
mean loss: 829.13
 ---- batch: 070 ----
mean loss: 838.16
 ---- batch: 080 ----
mean loss: 864.39
 ---- batch: 090 ----
mean loss: 866.39
 ---- batch: 100 ----
mean loss: 841.00
 ---- batch: 110 ----
mean loss: 841.78
train mean loss: 849.68
epoch train time: 0:00:01.901402
elapsed time: 0:01:23.837787
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 22:56:11.586995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.85
 ---- batch: 020 ----
mean loss: 841.53
 ---- batch: 030 ----
mean loss: 857.57
 ---- batch: 040 ----
mean loss: 873.86
 ---- batch: 050 ----
mean loss: 844.35
 ---- batch: 060 ----
mean loss: 842.91
 ---- batch: 070 ----
mean loss: 837.19
 ---- batch: 080 ----
mean loss: 854.82
 ---- batch: 090 ----
mean loss: 858.01
 ---- batch: 100 ----
mean loss: 842.22
 ---- batch: 110 ----
mean loss: 852.93
train mean loss: 849.59
epoch train time: 0:00:01.920525
elapsed time: 0:01:25.758907
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 22:56:13.508104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.78
 ---- batch: 020 ----
mean loss: 845.02
 ---- batch: 030 ----
mean loss: 861.07
 ---- batch: 040 ----
mean loss: 866.47
 ---- batch: 050 ----
mean loss: 861.71
 ---- batch: 060 ----
mean loss: 845.91
 ---- batch: 070 ----
mean loss: 858.75
 ---- batch: 080 ----
mean loss: 851.75
 ---- batch: 090 ----
mean loss: 827.08
 ---- batch: 100 ----
mean loss: 861.63
 ---- batch: 110 ----
mean loss: 843.00
train mean loss: 849.36
epoch train time: 0:00:01.895994
elapsed time: 0:01:27.655463
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 22:56:15.404683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.36
 ---- batch: 020 ----
mean loss: 828.34
 ---- batch: 030 ----
mean loss: 840.10
 ---- batch: 040 ----
mean loss: 848.78
 ---- batch: 050 ----
mean loss: 857.22
 ---- batch: 060 ----
mean loss: 846.34
 ---- batch: 070 ----
mean loss: 852.31
 ---- batch: 080 ----
mean loss: 864.20
 ---- batch: 090 ----
mean loss: 851.95
 ---- batch: 100 ----
mean loss: 853.02
 ---- batch: 110 ----
mean loss: 847.68
train mean loss: 849.69
epoch train time: 0:00:01.920422
elapsed time: 0:01:29.576536
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 22:56:17.325778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.25
 ---- batch: 020 ----
mean loss: 844.94
 ---- batch: 030 ----
mean loss: 853.10
 ---- batch: 040 ----
mean loss: 867.03
 ---- batch: 050 ----
mean loss: 848.93
 ---- batch: 060 ----
mean loss: 843.44
 ---- batch: 070 ----
mean loss: 813.61
 ---- batch: 080 ----
mean loss: 854.00
 ---- batch: 090 ----
mean loss: 853.18
 ---- batch: 100 ----
mean loss: 860.55
 ---- batch: 110 ----
mean loss: 861.17
train mean loss: 849.30
epoch train time: 0:00:01.888941
elapsed time: 0:01:31.466126
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 22:56:19.215352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.18
 ---- batch: 020 ----
mean loss: 836.95
 ---- batch: 030 ----
mean loss: 868.46
 ---- batch: 040 ----
mean loss: 872.38
 ---- batch: 050 ----
mean loss: 822.83
 ---- batch: 060 ----
mean loss: 838.31
 ---- batch: 070 ----
mean loss: 858.36
 ---- batch: 080 ----
mean loss: 850.78
 ---- batch: 090 ----
mean loss: 849.21
 ---- batch: 100 ----
mean loss: 849.72
 ---- batch: 110 ----
mean loss: 834.51
train mean loss: 848.14
epoch train time: 0:00:01.910893
elapsed time: 0:01:33.377621
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 22:56:21.126816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.63
 ---- batch: 020 ----
mean loss: 835.40
 ---- batch: 030 ----
mean loss: 861.43
 ---- batch: 040 ----
mean loss: 868.38
 ---- batch: 050 ----
mean loss: 854.96
 ---- batch: 060 ----
mean loss: 859.78
 ---- batch: 070 ----
mean loss: 831.20
 ---- batch: 080 ----
mean loss: 854.51
 ---- batch: 090 ----
mean loss: 844.37
 ---- batch: 100 ----
mean loss: 856.84
 ---- batch: 110 ----
mean loss: 835.41
train mean loss: 848.13
epoch train time: 0:00:01.862126
elapsed time: 0:01:35.240375
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 22:56:22.989592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.69
 ---- batch: 020 ----
mean loss: 852.84
 ---- batch: 030 ----
mean loss: 844.04
 ---- batch: 040 ----
mean loss: 847.10
 ---- batch: 050 ----
mean loss: 830.34
 ---- batch: 060 ----
mean loss: 842.91
 ---- batch: 070 ----
mean loss: 849.34
 ---- batch: 080 ----
mean loss: 862.22
 ---- batch: 090 ----
mean loss: 857.94
 ---- batch: 100 ----
mean loss: 854.13
 ---- batch: 110 ----
mean loss: 859.85
train mean loss: 848.64
epoch train time: 0:00:01.923884
elapsed time: 0:01:37.164876
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 22:56:24.913837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.58
 ---- batch: 020 ----
mean loss: 817.19
 ---- batch: 030 ----
mean loss: 840.09
 ---- batch: 040 ----
mean loss: 855.03
 ---- batch: 050 ----
mean loss: 854.98
 ---- batch: 060 ----
mean loss: 857.55
 ---- batch: 070 ----
mean loss: 839.45
 ---- batch: 080 ----
mean loss: 856.27
 ---- batch: 090 ----
mean loss: 856.48
 ---- batch: 100 ----
mean loss: 858.44
 ---- batch: 110 ----
mean loss: 843.40
train mean loss: 847.64
epoch train time: 0:00:01.895464
elapsed time: 0:01:39.060660
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 22:56:26.809967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.90
 ---- batch: 020 ----
mean loss: 862.77
 ---- batch: 030 ----
mean loss: 842.31
 ---- batch: 040 ----
mean loss: 846.94
 ---- batch: 050 ----
mean loss: 840.48
 ---- batch: 060 ----
mean loss: 837.09
 ---- batch: 070 ----
mean loss: 843.72
 ---- batch: 080 ----
mean loss: 857.96
 ---- batch: 090 ----
mean loss: 838.93
 ---- batch: 100 ----
mean loss: 838.17
 ---- batch: 110 ----
mean loss: 836.19
train mean loss: 847.31
epoch train time: 0:00:01.925813
elapsed time: 0:01:40.987282
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 22:56:28.736518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.97
 ---- batch: 020 ----
mean loss: 846.05
 ---- batch: 030 ----
mean loss: 858.83
 ---- batch: 040 ----
mean loss: 847.11
 ---- batch: 050 ----
mean loss: 829.05
 ---- batch: 060 ----
mean loss: 856.57
 ---- batch: 070 ----
mean loss: 838.72
 ---- batch: 080 ----
mean loss: 829.27
 ---- batch: 090 ----
mean loss: 867.58
 ---- batch: 100 ----
mean loss: 857.37
 ---- batch: 110 ----
mean loss: 830.53
train mean loss: 847.43
epoch train time: 0:00:01.889948
elapsed time: 0:01:42.877832
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 22:56:30.627058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.32
 ---- batch: 020 ----
mean loss: 859.14
 ---- batch: 030 ----
mean loss: 834.25
 ---- batch: 040 ----
mean loss: 843.76
 ---- batch: 050 ----
mean loss: 870.87
 ---- batch: 060 ----
mean loss: 838.42
 ---- batch: 070 ----
mean loss: 839.48
 ---- batch: 080 ----
mean loss: 833.74
 ---- batch: 090 ----
mean loss: 833.98
 ---- batch: 100 ----
mean loss: 856.82
 ---- batch: 110 ----
mean loss: 859.27
train mean loss: 847.67
epoch train time: 0:00:01.911094
elapsed time: 0:01:44.789538
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 22:56:32.538747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.70
 ---- batch: 020 ----
mean loss: 837.42
 ---- batch: 030 ----
mean loss: 859.81
 ---- batch: 040 ----
mean loss: 861.39
 ---- batch: 050 ----
mean loss: 821.83
 ---- batch: 060 ----
mean loss: 836.07
 ---- batch: 070 ----
mean loss: 858.58
 ---- batch: 080 ----
mean loss: 857.77
 ---- batch: 090 ----
mean loss: 829.81
 ---- batch: 100 ----
mean loss: 846.49
 ---- batch: 110 ----
mean loss: 844.27
train mean loss: 847.07
epoch train time: 0:00:01.912506
elapsed time: 0:01:46.702633
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 22:56:34.451839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.33
 ---- batch: 020 ----
mean loss: 847.84
 ---- batch: 030 ----
mean loss: 844.31
 ---- batch: 040 ----
mean loss: 847.24
 ---- batch: 050 ----
mean loss: 836.61
 ---- batch: 060 ----
mean loss: 837.97
 ---- batch: 070 ----
mean loss: 844.36
 ---- batch: 080 ----
mean loss: 855.57
 ---- batch: 090 ----
mean loss: 847.44
 ---- batch: 100 ----
mean loss: 854.13
 ---- batch: 110 ----
mean loss: 853.53
train mean loss: 846.84
epoch train time: 0:00:01.928881
elapsed time: 0:01:48.632101
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 22:56:36.381288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.16
 ---- batch: 020 ----
mean loss: 857.92
 ---- batch: 030 ----
mean loss: 836.85
 ---- batch: 040 ----
mean loss: 862.41
 ---- batch: 050 ----
mean loss: 857.69
 ---- batch: 060 ----
mean loss: 848.35
 ---- batch: 070 ----
mean loss: 833.01
 ---- batch: 080 ----
mean loss: 848.00
 ---- batch: 090 ----
mean loss: 839.72
 ---- batch: 100 ----
mean loss: 841.28
 ---- batch: 110 ----
mean loss: 847.49
train mean loss: 846.45
epoch train time: 0:00:01.895102
elapsed time: 0:01:50.527766
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 22:56:38.277013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.90
 ---- batch: 020 ----
mean loss: 826.26
 ---- batch: 030 ----
mean loss: 842.73
 ---- batch: 040 ----
mean loss: 852.19
 ---- batch: 050 ----
mean loss: 840.01
 ---- batch: 060 ----
mean loss: 851.35
 ---- batch: 070 ----
mean loss: 848.75
 ---- batch: 080 ----
mean loss: 869.63
 ---- batch: 090 ----
mean loss: 859.02
 ---- batch: 100 ----
mean loss: 846.95
 ---- batch: 110 ----
mean loss: 825.74
train mean loss: 846.65
epoch train time: 0:00:01.900083
elapsed time: 0:01:52.428449
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 22:56:40.177664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.43
 ---- batch: 020 ----
mean loss: 840.19
 ---- batch: 030 ----
mean loss: 849.07
 ---- batch: 040 ----
mean loss: 838.33
 ---- batch: 050 ----
mean loss: 870.70
 ---- batch: 060 ----
mean loss: 833.47
 ---- batch: 070 ----
mean loss: 846.09
 ---- batch: 080 ----
mean loss: 860.08
 ---- batch: 090 ----
mean loss: 845.61
 ---- batch: 100 ----
mean loss: 843.79
 ---- batch: 110 ----
mean loss: 842.77
train mean loss: 847.14
epoch train time: 0:00:01.896160
elapsed time: 0:01:54.325242
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 22:56:42.074441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.17
 ---- batch: 020 ----
mean loss: 852.44
 ---- batch: 030 ----
mean loss: 856.11
 ---- batch: 040 ----
mean loss: 840.00
 ---- batch: 050 ----
mean loss: 846.73
 ---- batch: 060 ----
mean loss: 843.75
 ---- batch: 070 ----
mean loss: 811.31
 ---- batch: 080 ----
mean loss: 859.29
 ---- batch: 090 ----
mean loss: 847.97
 ---- batch: 100 ----
mean loss: 852.07
 ---- batch: 110 ----
mean loss: 852.80
train mean loss: 846.61
epoch train time: 0:00:01.896851
elapsed time: 0:01:56.222666
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 22:56:43.971888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.03
 ---- batch: 020 ----
mean loss: 838.55
 ---- batch: 030 ----
mean loss: 851.42
 ---- batch: 040 ----
mean loss: 861.83
 ---- batch: 050 ----
mean loss: 835.94
 ---- batch: 060 ----
mean loss: 867.86
 ---- batch: 070 ----
mean loss: 854.39
 ---- batch: 080 ----
mean loss: 833.81
 ---- batch: 090 ----
mean loss: 823.75
 ---- batch: 100 ----
mean loss: 829.78
 ---- batch: 110 ----
mean loss: 844.50
train mean loss: 845.66
epoch train time: 0:00:01.908167
elapsed time: 0:01:58.131447
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 22:56:45.880629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 815.86
 ---- batch: 020 ----
mean loss: 869.31
 ---- batch: 030 ----
mean loss: 852.45
 ---- batch: 040 ----
mean loss: 844.61
 ---- batch: 050 ----
mean loss: 820.43
 ---- batch: 060 ----
mean loss: 821.10
 ---- batch: 070 ----
mean loss: 800.85
 ---- batch: 080 ----
mean loss: 785.48
 ---- batch: 090 ----
mean loss: 757.87
 ---- batch: 100 ----
mean loss: 728.44
 ---- batch: 110 ----
mean loss: 694.70
train mean loss: 795.51
epoch train time: 0:00:01.925233
elapsed time: 0:02:00.057252
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 22:56:47.806485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 628.39
 ---- batch: 020 ----
mean loss: 588.90
 ---- batch: 030 ----
mean loss: 557.42
 ---- batch: 040 ----
mean loss: 532.63
 ---- batch: 050 ----
mean loss: 496.70
 ---- batch: 060 ----
mean loss: 465.89
 ---- batch: 070 ----
mean loss: 460.31
 ---- batch: 080 ----
mean loss: 443.35
 ---- batch: 090 ----
mean loss: 417.09
 ---- batch: 100 ----
mean loss: 417.58
 ---- batch: 110 ----
mean loss: 411.28
train mean loss: 490.74
epoch train time: 0:00:01.889607
elapsed time: 0:02:01.947497
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 22:56:49.696728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.54
 ---- batch: 020 ----
mean loss: 399.07
 ---- batch: 030 ----
mean loss: 379.75
 ---- batch: 040 ----
mean loss: 389.80
 ---- batch: 050 ----
mean loss: 368.44
 ---- batch: 060 ----
mean loss: 366.45
 ---- batch: 070 ----
mean loss: 374.96
 ---- batch: 080 ----
mean loss: 365.73
 ---- batch: 090 ----
mean loss: 358.93
 ---- batch: 100 ----
mean loss: 335.28
 ---- batch: 110 ----
mean loss: 341.88
train mean loss: 368.45
epoch train time: 0:00:01.905263
elapsed time: 0:02:03.853361
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 22:56:51.602555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.42
 ---- batch: 020 ----
mean loss: 339.77
 ---- batch: 030 ----
mean loss: 329.67
 ---- batch: 040 ----
mean loss: 336.47
 ---- batch: 050 ----
mean loss: 327.80
 ---- batch: 060 ----
mean loss: 342.41
 ---- batch: 070 ----
mean loss: 333.52
 ---- batch: 080 ----
mean loss: 338.19
 ---- batch: 090 ----
mean loss: 325.29
 ---- batch: 100 ----
mean loss: 328.82
 ---- batch: 110 ----
mean loss: 323.12
train mean loss: 333.18
epoch train time: 0:00:01.891092
elapsed time: 0:02:05.745034
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 22:56:53.494275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.62
 ---- batch: 020 ----
mean loss: 324.79
 ---- batch: 030 ----
mean loss: 304.79
 ---- batch: 040 ----
mean loss: 314.12
 ---- batch: 050 ----
mean loss: 311.04
 ---- batch: 060 ----
mean loss: 311.43
 ---- batch: 070 ----
mean loss: 308.53
 ---- batch: 080 ----
mean loss: 311.29
 ---- batch: 090 ----
mean loss: 308.60
 ---- batch: 100 ----
mean loss: 308.29
 ---- batch: 110 ----
mean loss: 293.86
train mean loss: 311.51
epoch train time: 0:00:01.891389
elapsed time: 0:02:07.637050
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 22:56:55.386250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.07
 ---- batch: 020 ----
mean loss: 293.76
 ---- batch: 030 ----
mean loss: 290.35
 ---- batch: 040 ----
mean loss: 297.62
 ---- batch: 050 ----
mean loss: 297.94
 ---- batch: 060 ----
mean loss: 298.47
 ---- batch: 070 ----
mean loss: 297.12
 ---- batch: 080 ----
mean loss: 288.77
 ---- batch: 090 ----
mean loss: 306.09
 ---- batch: 100 ----
mean loss: 295.27
 ---- batch: 110 ----
mean loss: 304.50
train mean loss: 296.58
epoch train time: 0:00:01.870278
elapsed time: 0:02:09.507911
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 22:56:57.257112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.54
 ---- batch: 020 ----
mean loss: 292.27
 ---- batch: 030 ----
mean loss: 299.55
 ---- batch: 040 ----
mean loss: 277.50
 ---- batch: 050 ----
mean loss: 276.09
 ---- batch: 060 ----
mean loss: 285.29
 ---- batch: 070 ----
mean loss: 285.94
 ---- batch: 080 ----
mean loss: 292.28
 ---- batch: 090 ----
mean loss: 282.92
 ---- batch: 100 ----
mean loss: 279.90
 ---- batch: 110 ----
mean loss: 280.64
train mean loss: 284.91
epoch train time: 0:00:01.877372
elapsed time: 0:02:11.385890
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 22:56:59.135144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.76
 ---- batch: 020 ----
mean loss: 278.53
 ---- batch: 030 ----
mean loss: 289.57
 ---- batch: 040 ----
mean loss: 277.64
 ---- batch: 050 ----
mean loss: 270.62
 ---- batch: 060 ----
mean loss: 266.26
 ---- batch: 070 ----
mean loss: 279.54
 ---- batch: 080 ----
mean loss: 276.51
 ---- batch: 090 ----
mean loss: 269.56
 ---- batch: 100 ----
mean loss: 269.68
 ---- batch: 110 ----
mean loss: 287.10
train mean loss: 277.62
epoch train time: 0:00:01.930320
elapsed time: 0:02:13.316829
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 22:57:01.066156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.63
 ---- batch: 020 ----
mean loss: 276.17
 ---- batch: 030 ----
mean loss: 275.10
 ---- batch: 040 ----
mean loss: 264.15
 ---- batch: 050 ----
mean loss: 271.47
 ---- batch: 060 ----
mean loss: 271.27
 ---- batch: 070 ----
mean loss: 261.59
 ---- batch: 080 ----
mean loss: 263.28
 ---- batch: 090 ----
mean loss: 266.01
 ---- batch: 100 ----
mean loss: 265.90
 ---- batch: 110 ----
mean loss: 268.59
train mean loss: 269.44
epoch train time: 0:00:01.925913
elapsed time: 0:02:15.243420
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 22:57:02.992624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.08
 ---- batch: 020 ----
mean loss: 252.67
 ---- batch: 030 ----
mean loss: 272.42
 ---- batch: 040 ----
mean loss: 269.34
 ---- batch: 050 ----
mean loss: 258.06
 ---- batch: 060 ----
mean loss: 253.72
 ---- batch: 070 ----
mean loss: 254.15
 ---- batch: 080 ----
mean loss: 262.39
 ---- batch: 090 ----
mean loss: 259.77
 ---- batch: 100 ----
mean loss: 266.01
 ---- batch: 110 ----
mean loss: 264.36
train mean loss: 261.04
epoch train time: 0:00:01.894201
elapsed time: 0:02:17.138239
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 22:57:04.887511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.59
 ---- batch: 020 ----
mean loss: 255.45
 ---- batch: 030 ----
mean loss: 269.95
 ---- batch: 040 ----
mean loss: 259.77
 ---- batch: 050 ----
mean loss: 258.52
 ---- batch: 060 ----
mean loss: 253.13
 ---- batch: 070 ----
mean loss: 259.83
 ---- batch: 080 ----
mean loss: 249.58
 ---- batch: 090 ----
mean loss: 256.26
 ---- batch: 100 ----
mean loss: 251.18
 ---- batch: 110 ----
mean loss: 256.32
train mean loss: 257.50
epoch train time: 0:00:01.900271
elapsed time: 0:02:19.039172
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 22:57:06.788082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.15
 ---- batch: 020 ----
mean loss: 251.32
 ---- batch: 030 ----
mean loss: 265.07
 ---- batch: 040 ----
mean loss: 242.64
 ---- batch: 050 ----
mean loss: 258.91
 ---- batch: 060 ----
mean loss: 253.27
 ---- batch: 070 ----
mean loss: 254.62
 ---- batch: 080 ----
mean loss: 240.48
 ---- batch: 090 ----
mean loss: 252.67
 ---- batch: 100 ----
mean loss: 240.12
 ---- batch: 110 ----
mean loss: 242.56
train mean loss: 250.39
epoch train time: 0:00:01.899568
elapsed time: 0:02:20.939042
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 22:57:08.688254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.88
 ---- batch: 020 ----
mean loss: 242.68
 ---- batch: 030 ----
mean loss: 242.93
 ---- batch: 040 ----
mean loss: 241.91
 ---- batch: 050 ----
mean loss: 255.75
 ---- batch: 060 ----
mean loss: 245.60
 ---- batch: 070 ----
mean loss: 255.24
 ---- batch: 080 ----
mean loss: 242.99
 ---- batch: 090 ----
mean loss: 245.58
 ---- batch: 100 ----
mean loss: 245.03
 ---- batch: 110 ----
mean loss: 249.52
train mean loss: 247.57
epoch train time: 0:00:01.880225
elapsed time: 0:02:22.819871
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 22:57:10.569057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.35
 ---- batch: 020 ----
mean loss: 249.30
 ---- batch: 030 ----
mean loss: 246.61
 ---- batch: 040 ----
mean loss: 238.54
 ---- batch: 050 ----
mean loss: 239.76
 ---- batch: 060 ----
mean loss: 232.99
 ---- batch: 070 ----
mean loss: 237.61
 ---- batch: 080 ----
mean loss: 251.63
 ---- batch: 090 ----
mean loss: 239.90
 ---- batch: 100 ----
mean loss: 249.74
 ---- batch: 110 ----
mean loss: 243.94
train mean loss: 242.43
epoch train time: 0:00:01.886032
elapsed time: 0:02:24.706483
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 22:57:12.455687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.04
 ---- batch: 020 ----
mean loss: 233.26
 ---- batch: 030 ----
mean loss: 244.79
 ---- batch: 040 ----
mean loss: 247.97
 ---- batch: 050 ----
mean loss: 236.52
 ---- batch: 060 ----
mean loss: 232.50
 ---- batch: 070 ----
mean loss: 226.69
 ---- batch: 080 ----
mean loss: 242.75
 ---- batch: 090 ----
mean loss: 247.36
 ---- batch: 100 ----
mean loss: 243.38
 ---- batch: 110 ----
mean loss: 242.16
train mean loss: 239.16
epoch train time: 0:00:01.873154
elapsed time: 0:02:26.580245
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 22:57:14.329416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.96
 ---- batch: 020 ----
mean loss: 223.37
 ---- batch: 030 ----
mean loss: 243.19
 ---- batch: 040 ----
mean loss: 231.68
 ---- batch: 050 ----
mean loss: 235.64
 ---- batch: 060 ----
mean loss: 238.72
 ---- batch: 070 ----
mean loss: 230.76
 ---- batch: 080 ----
mean loss: 239.53
 ---- batch: 090 ----
mean loss: 234.59
 ---- batch: 100 ----
mean loss: 230.10
 ---- batch: 110 ----
mean loss: 231.66
train mean loss: 233.99
epoch train time: 0:00:01.909400
elapsed time: 0:02:28.490194
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 22:57:16.239424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.30
 ---- batch: 020 ----
mean loss: 236.28
 ---- batch: 030 ----
mean loss: 225.15
 ---- batch: 040 ----
mean loss: 221.02
 ---- batch: 050 ----
mean loss: 234.02
 ---- batch: 060 ----
mean loss: 229.42
 ---- batch: 070 ----
mean loss: 235.49
 ---- batch: 080 ----
mean loss: 224.15
 ---- batch: 090 ----
mean loss: 241.65
 ---- batch: 100 ----
mean loss: 228.04
 ---- batch: 110 ----
mean loss: 236.52
train mean loss: 230.32
epoch train time: 0:00:01.902110
elapsed time: 0:02:30.392935
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 22:57:18.142128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.19
 ---- batch: 020 ----
mean loss: 216.81
 ---- batch: 030 ----
mean loss: 224.98
 ---- batch: 040 ----
mean loss: 234.18
 ---- batch: 050 ----
mean loss: 218.76
 ---- batch: 060 ----
mean loss: 228.50
 ---- batch: 070 ----
mean loss: 212.71
 ---- batch: 080 ----
mean loss: 236.46
 ---- batch: 090 ----
mean loss: 229.38
 ---- batch: 100 ----
mean loss: 231.17
 ---- batch: 110 ----
mean loss: 222.87
train mean loss: 226.35
epoch train time: 0:00:01.952567
elapsed time: 0:02:32.346077
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 22:57:20.095275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.87
 ---- batch: 020 ----
mean loss: 219.41
 ---- batch: 030 ----
mean loss: 219.24
 ---- batch: 040 ----
mean loss: 226.78
 ---- batch: 050 ----
mean loss: 220.89
 ---- batch: 060 ----
mean loss: 218.27
 ---- batch: 070 ----
mean loss: 230.51
 ---- batch: 080 ----
mean loss: 221.81
 ---- batch: 090 ----
mean loss: 215.41
 ---- batch: 100 ----
mean loss: 216.77
 ---- batch: 110 ----
mean loss: 240.12
train mean loss: 222.49
epoch train time: 0:00:01.886495
elapsed time: 0:02:34.233145
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 22:57:21.982402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.90
 ---- batch: 020 ----
mean loss: 218.61
 ---- batch: 030 ----
mean loss: 215.48
 ---- batch: 040 ----
mean loss: 204.02
 ---- batch: 050 ----
mean loss: 211.95
 ---- batch: 060 ----
mean loss: 221.65
 ---- batch: 070 ----
mean loss: 227.65
 ---- batch: 080 ----
mean loss: 222.56
 ---- batch: 090 ----
mean loss: 227.98
 ---- batch: 100 ----
mean loss: 216.66
 ---- batch: 110 ----
mean loss: 208.29
train mean loss: 217.83
epoch train time: 0:00:01.899236
elapsed time: 0:02:36.133019
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 22:57:23.882238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.28
 ---- batch: 020 ----
mean loss: 207.66
 ---- batch: 030 ----
mean loss: 223.85
 ---- batch: 040 ----
mean loss: 217.05
 ---- batch: 050 ----
mean loss: 220.21
 ---- batch: 060 ----
mean loss: 215.80
 ---- batch: 070 ----
mean loss: 224.18
 ---- batch: 080 ----
mean loss: 211.31
 ---- batch: 090 ----
mean loss: 205.24
 ---- batch: 100 ----
mean loss: 210.62
 ---- batch: 110 ----
mean loss: 218.12
train mean loss: 215.77
epoch train time: 0:00:01.905180
elapsed time: 0:02:38.038837
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 22:57:25.788022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.47
 ---- batch: 020 ----
mean loss: 212.22
 ---- batch: 030 ----
mean loss: 211.24
 ---- batch: 040 ----
mean loss: 213.51
 ---- batch: 050 ----
mean loss: 204.79
 ---- batch: 060 ----
mean loss: 224.48
 ---- batch: 070 ----
mean loss: 213.58
 ---- batch: 080 ----
mean loss: 218.48
 ---- batch: 090 ----
mean loss: 209.96
 ---- batch: 100 ----
mean loss: 213.76
 ---- batch: 110 ----
mean loss: 212.61
train mean loss: 213.40
epoch train time: 0:00:01.878619
elapsed time: 0:02:39.918039
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 22:57:27.667208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.13
 ---- batch: 020 ----
mean loss: 210.85
 ---- batch: 030 ----
mean loss: 196.48
 ---- batch: 040 ----
mean loss: 217.24
 ---- batch: 050 ----
mean loss: 209.47
 ---- batch: 060 ----
mean loss: 200.99
 ---- batch: 070 ----
mean loss: 214.26
 ---- batch: 080 ----
mean loss: 204.97
 ---- batch: 090 ----
mean loss: 216.96
 ---- batch: 100 ----
mean loss: 210.93
 ---- batch: 110 ----
mean loss: 214.03
train mean loss: 210.25
epoch train time: 0:00:01.905981
elapsed time: 0:02:41.824534
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 22:57:29.573770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.40
 ---- batch: 020 ----
mean loss: 209.41
 ---- batch: 030 ----
mean loss: 199.37
 ---- batch: 040 ----
mean loss: 207.96
 ---- batch: 050 ----
mean loss: 214.56
 ---- batch: 060 ----
mean loss: 212.17
 ---- batch: 070 ----
mean loss: 210.29
 ---- batch: 080 ----
mean loss: 205.27
 ---- batch: 090 ----
mean loss: 209.38
 ---- batch: 100 ----
mean loss: 197.51
 ---- batch: 110 ----
mean loss: 209.26
train mean loss: 207.20
epoch train time: 0:00:01.884101
elapsed time: 0:02:43.709232
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 22:57:31.458424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.23
 ---- batch: 020 ----
mean loss: 209.41
 ---- batch: 030 ----
mean loss: 200.85
 ---- batch: 040 ----
mean loss: 197.48
 ---- batch: 050 ----
mean loss: 204.07
 ---- batch: 060 ----
mean loss: 205.93
 ---- batch: 070 ----
mean loss: 201.28
 ---- batch: 080 ----
mean loss: 198.05
 ---- batch: 090 ----
mean loss: 205.78
 ---- batch: 100 ----
mean loss: 201.63
 ---- batch: 110 ----
mean loss: 208.61
train mean loss: 203.96
epoch train time: 0:00:01.927711
elapsed time: 0:02:45.637505
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 22:57:33.386693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.65
 ---- batch: 020 ----
mean loss: 200.80
 ---- batch: 030 ----
mean loss: 211.15
 ---- batch: 040 ----
mean loss: 202.07
 ---- batch: 050 ----
mean loss: 191.49
 ---- batch: 060 ----
mean loss: 195.00
 ---- batch: 070 ----
mean loss: 204.23
 ---- batch: 080 ----
mean loss: 199.15
 ---- batch: 090 ----
mean loss: 203.01
 ---- batch: 100 ----
mean loss: 202.20
 ---- batch: 110 ----
mean loss: 205.29
train mean loss: 201.54
epoch train time: 0:00:01.898392
elapsed time: 0:02:47.536446
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 22:57:35.285655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.18
 ---- batch: 020 ----
mean loss: 190.34
 ---- batch: 030 ----
mean loss: 201.15
 ---- batch: 040 ----
mean loss: 198.60
 ---- batch: 050 ----
mean loss: 185.07
 ---- batch: 060 ----
mean loss: 201.00
 ---- batch: 070 ----
mean loss: 205.99
 ---- batch: 080 ----
mean loss: 210.96
 ---- batch: 090 ----
mean loss: 196.89
 ---- batch: 100 ----
mean loss: 196.67
 ---- batch: 110 ----
mean loss: 202.10
train mean loss: 199.08
epoch train time: 0:00:01.922868
elapsed time: 0:02:49.459898
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 22:57:37.209116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.52
 ---- batch: 020 ----
mean loss: 192.32
 ---- batch: 030 ----
mean loss: 195.98
 ---- batch: 040 ----
mean loss: 201.24
 ---- batch: 050 ----
mean loss: 201.01
 ---- batch: 060 ----
mean loss: 194.14
 ---- batch: 070 ----
mean loss: 196.03
 ---- batch: 080 ----
mean loss: 196.82
 ---- batch: 090 ----
mean loss: 195.16
 ---- batch: 100 ----
mean loss: 203.36
 ---- batch: 110 ----
mean loss: 196.40
train mean loss: 196.69
epoch train time: 0:00:01.923061
elapsed time: 0:02:51.383572
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 22:57:39.132760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.30
 ---- batch: 020 ----
mean loss: 193.71
 ---- batch: 030 ----
mean loss: 192.92
 ---- batch: 040 ----
mean loss: 189.46
 ---- batch: 050 ----
mean loss: 192.54
 ---- batch: 060 ----
mean loss: 201.20
 ---- batch: 070 ----
mean loss: 193.42
 ---- batch: 080 ----
mean loss: 197.79
 ---- batch: 090 ----
mean loss: 198.29
 ---- batch: 100 ----
mean loss: 204.61
 ---- batch: 110 ----
mean loss: 194.44
train mean loss: 195.71
epoch train time: 0:00:01.913403
elapsed time: 0:02:53.297600
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 22:57:41.046876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.15
 ---- batch: 020 ----
mean loss: 187.47
 ---- batch: 030 ----
mean loss: 188.85
 ---- batch: 040 ----
mean loss: 186.40
 ---- batch: 050 ----
mean loss: 191.75
 ---- batch: 060 ----
mean loss: 198.81
 ---- batch: 070 ----
mean loss: 195.73
 ---- batch: 080 ----
mean loss: 198.62
 ---- batch: 090 ----
mean loss: 193.29
 ---- batch: 100 ----
mean loss: 191.06
 ---- batch: 110 ----
mean loss: 189.34
train mean loss: 192.38
epoch train time: 0:00:01.950043
elapsed time: 0:02:55.248333
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 22:57:42.997574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.56
 ---- batch: 020 ----
mean loss: 184.35
 ---- batch: 030 ----
mean loss: 196.73
 ---- batch: 040 ----
mean loss: 196.25
 ---- batch: 050 ----
mean loss: 195.69
 ---- batch: 060 ----
mean loss: 181.73
 ---- batch: 070 ----
mean loss: 186.02
 ---- batch: 080 ----
mean loss: 195.26
 ---- batch: 090 ----
mean loss: 193.65
 ---- batch: 100 ----
mean loss: 188.57
 ---- batch: 110 ----
mean loss: 196.09
train mean loss: 190.58
epoch train time: 0:00:01.935858
elapsed time: 0:02:57.184843
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 22:57:44.934082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.59
 ---- batch: 020 ----
mean loss: 180.85
 ---- batch: 030 ----
mean loss: 189.26
 ---- batch: 040 ----
mean loss: 195.21
 ---- batch: 050 ----
mean loss: 189.96
 ---- batch: 060 ----
mean loss: 192.10
 ---- batch: 070 ----
mean loss: 189.31
 ---- batch: 080 ----
mean loss: 187.55
 ---- batch: 090 ----
mean loss: 189.84
 ---- batch: 100 ----
mean loss: 181.82
 ---- batch: 110 ----
mean loss: 190.58
train mean loss: 188.42
epoch train time: 0:00:01.899877
elapsed time: 0:02:59.085334
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 22:57:46.834532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.37
 ---- batch: 020 ----
mean loss: 193.58
 ---- batch: 030 ----
mean loss: 181.45
 ---- batch: 040 ----
mean loss: 185.26
 ---- batch: 050 ----
mean loss: 191.55
 ---- batch: 060 ----
mean loss: 190.84
 ---- batch: 070 ----
mean loss: 187.10
 ---- batch: 080 ----
mean loss: 186.83
 ---- batch: 090 ----
mean loss: 190.89
 ---- batch: 100 ----
mean loss: 185.49
 ---- batch: 110 ----
mean loss: 178.85
train mean loss: 187.02
epoch train time: 0:00:01.904464
elapsed time: 0:03:00.990361
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 22:57:48.739537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.65
 ---- batch: 020 ----
mean loss: 182.10
 ---- batch: 030 ----
mean loss: 176.37
 ---- batch: 040 ----
mean loss: 185.33
 ---- batch: 050 ----
mean loss: 182.04
 ---- batch: 060 ----
mean loss: 194.24
 ---- batch: 070 ----
mean loss: 189.02
 ---- batch: 080 ----
mean loss: 192.80
 ---- batch: 090 ----
mean loss: 186.46
 ---- batch: 100 ----
mean loss: 182.45
 ---- batch: 110 ----
mean loss: 191.32
train mean loss: 185.19
epoch train time: 0:00:01.917712
elapsed time: 0:03:02.908644
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 22:57:50.657878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.49
 ---- batch: 020 ----
mean loss: 186.14
 ---- batch: 030 ----
mean loss: 175.39
 ---- batch: 040 ----
mean loss: 191.06
 ---- batch: 050 ----
mean loss: 183.82
 ---- batch: 060 ----
mean loss: 191.41
 ---- batch: 070 ----
mean loss: 181.76
 ---- batch: 080 ----
mean loss: 181.54
 ---- batch: 090 ----
mean loss: 184.26
 ---- batch: 100 ----
mean loss: 176.54
 ---- batch: 110 ----
mean loss: 195.09
train mean loss: 183.80
epoch train time: 0:00:01.914478
elapsed time: 0:03:04.823713
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 22:57:52.572954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.88
 ---- batch: 020 ----
mean loss: 184.26
 ---- batch: 030 ----
mean loss: 181.20
 ---- batch: 040 ----
mean loss: 180.47
 ---- batch: 050 ----
mean loss: 182.71
 ---- batch: 060 ----
mean loss: 187.80
 ---- batch: 070 ----
mean loss: 185.12
 ---- batch: 080 ----
mean loss: 176.34
 ---- batch: 090 ----
mean loss: 183.90
 ---- batch: 100 ----
mean loss: 178.80
 ---- batch: 110 ----
mean loss: 176.68
train mean loss: 181.36
epoch train time: 0:00:01.896772
elapsed time: 0:03:06.721115
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 22:57:54.470314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.77
 ---- batch: 020 ----
mean loss: 178.35
 ---- batch: 030 ----
mean loss: 176.06
 ---- batch: 040 ----
mean loss: 181.61
 ---- batch: 050 ----
mean loss: 178.47
 ---- batch: 060 ----
mean loss: 175.90
 ---- batch: 070 ----
mean loss: 178.16
 ---- batch: 080 ----
mean loss: 194.08
 ---- batch: 090 ----
mean loss: 182.97
 ---- batch: 100 ----
mean loss: 169.56
 ---- batch: 110 ----
mean loss: 189.48
train mean loss: 179.79
epoch train time: 0:00:01.915025
elapsed time: 0:03:08.636690
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 22:57:56.385961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.27
 ---- batch: 020 ----
mean loss: 181.06
 ---- batch: 030 ----
mean loss: 178.49
 ---- batch: 040 ----
mean loss: 183.64
 ---- batch: 050 ----
mean loss: 174.21
 ---- batch: 060 ----
mean loss: 179.17
 ---- batch: 070 ----
mean loss: 183.31
 ---- batch: 080 ----
mean loss: 178.95
 ---- batch: 090 ----
mean loss: 183.65
 ---- batch: 100 ----
mean loss: 180.76
 ---- batch: 110 ----
mean loss: 176.67
train mean loss: 179.67
epoch train time: 0:00:01.891894
elapsed time: 0:03:10.529211
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 22:57:58.278405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.45
 ---- batch: 020 ----
mean loss: 179.50
 ---- batch: 030 ----
mean loss: 178.37
 ---- batch: 040 ----
mean loss: 169.24
 ---- batch: 050 ----
mean loss: 179.76
 ---- batch: 060 ----
mean loss: 174.89
 ---- batch: 070 ----
mean loss: 173.27
 ---- batch: 080 ----
mean loss: 174.67
 ---- batch: 090 ----
mean loss: 174.37
 ---- batch: 100 ----
mean loss: 181.71
 ---- batch: 110 ----
mean loss: 173.50
train mean loss: 176.31
epoch train time: 0:00:01.899282
elapsed time: 0:03:12.429079
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 22:58:00.178309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.61
 ---- batch: 020 ----
mean loss: 175.81
 ---- batch: 030 ----
mean loss: 173.81
 ---- batch: 040 ----
mean loss: 170.41
 ---- batch: 050 ----
mean loss: 170.80
 ---- batch: 060 ----
mean loss: 177.38
 ---- batch: 070 ----
mean loss: 185.70
 ---- batch: 080 ----
mean loss: 185.27
 ---- batch: 090 ----
mean loss: 180.33
 ---- batch: 100 ----
mean loss: 184.68
 ---- batch: 110 ----
mean loss: 172.33
train mean loss: 177.11
epoch train time: 0:00:01.899287
elapsed time: 0:03:14.328984
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 22:58:02.078249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.20
 ---- batch: 020 ----
mean loss: 178.32
 ---- batch: 030 ----
mean loss: 168.28
 ---- batch: 040 ----
mean loss: 177.24
 ---- batch: 050 ----
mean loss: 175.22
 ---- batch: 060 ----
mean loss: 177.14
 ---- batch: 070 ----
mean loss: 173.46
 ---- batch: 080 ----
mean loss: 179.65
 ---- batch: 090 ----
mean loss: 179.96
 ---- batch: 100 ----
mean loss: 178.71
 ---- batch: 110 ----
mean loss: 175.70
train mean loss: 175.38
epoch train time: 0:00:01.906417
elapsed time: 0:03:16.236034
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 22:58:03.985240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.45
 ---- batch: 020 ----
mean loss: 182.80
 ---- batch: 030 ----
mean loss: 168.14
 ---- batch: 040 ----
mean loss: 169.72
 ---- batch: 050 ----
mean loss: 178.59
 ---- batch: 060 ----
mean loss: 170.16
 ---- batch: 070 ----
mean loss: 170.79
 ---- batch: 080 ----
mean loss: 171.14
 ---- batch: 090 ----
mean loss: 177.97
 ---- batch: 100 ----
mean loss: 178.37
 ---- batch: 110 ----
mean loss: 183.94
train mean loss: 174.12
epoch train time: 0:00:01.899990
elapsed time: 0:03:18.136594
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 22:58:05.885832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.44
 ---- batch: 020 ----
mean loss: 175.08
 ---- batch: 030 ----
mean loss: 172.79
 ---- batch: 040 ----
mean loss: 164.82
 ---- batch: 050 ----
mean loss: 172.56
 ---- batch: 060 ----
mean loss: 169.61
 ---- batch: 070 ----
mean loss: 177.31
 ---- batch: 080 ----
mean loss: 181.55
 ---- batch: 090 ----
mean loss: 172.12
 ---- batch: 100 ----
mean loss: 168.09
 ---- batch: 110 ----
mean loss: 180.05
train mean loss: 173.17
epoch train time: 0:00:01.907405
elapsed time: 0:03:20.044610
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 22:58:07.793838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.71
 ---- batch: 020 ----
mean loss: 167.17
 ---- batch: 030 ----
mean loss: 175.98
 ---- batch: 040 ----
mean loss: 172.22
 ---- batch: 050 ----
mean loss: 170.14
 ---- batch: 060 ----
mean loss: 164.01
 ---- batch: 070 ----
mean loss: 177.65
 ---- batch: 080 ----
mean loss: 173.92
 ---- batch: 090 ----
mean loss: 175.15
 ---- batch: 100 ----
mean loss: 176.84
 ---- batch: 110 ----
mean loss: 169.98
train mean loss: 170.89
epoch train time: 0:00:01.919166
elapsed time: 0:03:21.964445
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 22:58:09.713755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.10
 ---- batch: 020 ----
mean loss: 166.41
 ---- batch: 030 ----
mean loss: 175.01
 ---- batch: 040 ----
mean loss: 172.63
 ---- batch: 050 ----
mean loss: 161.08
 ---- batch: 060 ----
mean loss: 167.99
 ---- batch: 070 ----
mean loss: 175.68
 ---- batch: 080 ----
mean loss: 170.31
 ---- batch: 090 ----
mean loss: 167.36
 ---- batch: 100 ----
mean loss: 168.73
 ---- batch: 110 ----
mean loss: 173.55
train mean loss: 169.88
epoch train time: 0:00:01.898630
elapsed time: 0:03:23.863764
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 22:58:11.612992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.88
 ---- batch: 020 ----
mean loss: 165.88
 ---- batch: 030 ----
mean loss: 165.71
 ---- batch: 040 ----
mean loss: 168.50
 ---- batch: 050 ----
mean loss: 165.86
 ---- batch: 060 ----
mean loss: 174.12
 ---- batch: 070 ----
mean loss: 180.55
 ---- batch: 080 ----
mean loss: 175.07
 ---- batch: 090 ----
mean loss: 165.31
 ---- batch: 100 ----
mean loss: 168.84
 ---- batch: 110 ----
mean loss: 176.71
train mean loss: 170.63
epoch train time: 0:00:01.910021
elapsed time: 0:03:25.774369
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 22:58:13.523556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.82
 ---- batch: 020 ----
mean loss: 172.52
 ---- batch: 030 ----
mean loss: 167.27
 ---- batch: 040 ----
mean loss: 161.33
 ---- batch: 050 ----
mean loss: 169.17
 ---- batch: 060 ----
mean loss: 170.59
 ---- batch: 070 ----
mean loss: 172.02
 ---- batch: 080 ----
mean loss: 166.38
 ---- batch: 090 ----
mean loss: 173.94
 ---- batch: 100 ----
mean loss: 167.82
 ---- batch: 110 ----
mean loss: 169.78
train mean loss: 168.57
epoch train time: 0:00:01.897246
elapsed time: 0:03:27.672210
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 22:58:15.421510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.71
 ---- batch: 020 ----
mean loss: 172.20
 ---- batch: 030 ----
mean loss: 160.47
 ---- batch: 040 ----
mean loss: 171.33
 ---- batch: 050 ----
mean loss: 160.31
 ---- batch: 060 ----
mean loss: 167.80
 ---- batch: 070 ----
mean loss: 173.73
 ---- batch: 080 ----
mean loss: 170.93
 ---- batch: 090 ----
mean loss: 164.05
 ---- batch: 100 ----
mean loss: 170.88
 ---- batch: 110 ----
mean loss: 173.35
train mean loss: 168.20
epoch train time: 0:00:01.902560
elapsed time: 0:03:29.575460
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 22:58:17.324628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.53
 ---- batch: 020 ----
mean loss: 168.52
 ---- batch: 030 ----
mean loss: 165.53
 ---- batch: 040 ----
mean loss: 170.48
 ---- batch: 050 ----
mean loss: 165.08
 ---- batch: 060 ----
mean loss: 159.08
 ---- batch: 070 ----
mean loss: 171.23
 ---- batch: 080 ----
mean loss: 155.51
 ---- batch: 090 ----
mean loss: 167.14
 ---- batch: 100 ----
mean loss: 168.75
 ---- batch: 110 ----
mean loss: 170.42
train mean loss: 166.40
epoch train time: 0:00:01.872821
elapsed time: 0:03:31.448865
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 22:58:19.198062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.32
 ---- batch: 020 ----
mean loss: 172.09
 ---- batch: 030 ----
mean loss: 162.69
 ---- batch: 040 ----
mean loss: 164.40
 ---- batch: 050 ----
mean loss: 167.58
 ---- batch: 060 ----
mean loss: 168.03
 ---- batch: 070 ----
mean loss: 177.57
 ---- batch: 080 ----
mean loss: 170.78
 ---- batch: 090 ----
mean loss: 161.08
 ---- batch: 100 ----
mean loss: 165.69
 ---- batch: 110 ----
mean loss: 162.95
train mean loss: 166.74
epoch train time: 0:00:01.898717
elapsed time: 0:03:33.348159
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 22:58:21.097364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.87
 ---- batch: 020 ----
mean loss: 166.30
 ---- batch: 030 ----
mean loss: 169.14
 ---- batch: 040 ----
mean loss: 160.55
 ---- batch: 050 ----
mean loss: 167.09
 ---- batch: 060 ----
mean loss: 164.64
 ---- batch: 070 ----
mean loss: 165.18
 ---- batch: 080 ----
mean loss: 165.50
 ---- batch: 090 ----
mean loss: 164.23
 ---- batch: 100 ----
mean loss: 161.30
 ---- batch: 110 ----
mean loss: 167.39
train mean loss: 164.69
epoch train time: 0:00:01.893731
elapsed time: 0:03:35.242448
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 22:58:22.991662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.78
 ---- batch: 020 ----
mean loss: 166.93
 ---- batch: 030 ----
mean loss: 161.48
 ---- batch: 040 ----
mean loss: 169.91
 ---- batch: 050 ----
mean loss: 173.23
 ---- batch: 060 ----
mean loss: 164.47
 ---- batch: 070 ----
mean loss: 159.81
 ---- batch: 080 ----
mean loss: 157.18
 ---- batch: 090 ----
mean loss: 166.47
 ---- batch: 100 ----
mean loss: 164.65
 ---- batch: 110 ----
mean loss: 162.72
train mean loss: 165.20
epoch train time: 0:00:01.927896
elapsed time: 0:03:37.170954
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 22:58:24.920261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.89
 ---- batch: 020 ----
mean loss: 155.25
 ---- batch: 030 ----
mean loss: 151.63
 ---- batch: 040 ----
mean loss: 165.08
 ---- batch: 050 ----
mean loss: 168.94
 ---- batch: 060 ----
mean loss: 164.53
 ---- batch: 070 ----
mean loss: 163.19
 ---- batch: 080 ----
mean loss: 162.91
 ---- batch: 090 ----
mean loss: 158.30
 ---- batch: 100 ----
mean loss: 168.54
 ---- batch: 110 ----
mean loss: 170.23
train mean loss: 163.05
epoch train time: 0:00:01.897127
elapsed time: 0:03:39.068762
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 22:58:26.817714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.42
 ---- batch: 020 ----
mean loss: 159.73
 ---- batch: 030 ----
mean loss: 157.90
 ---- batch: 040 ----
mean loss: 162.12
 ---- batch: 050 ----
mean loss: 169.59
 ---- batch: 060 ----
mean loss: 160.85
 ---- batch: 070 ----
mean loss: 159.25
 ---- batch: 080 ----
mean loss: 167.56
 ---- batch: 090 ----
mean loss: 166.57
 ---- batch: 100 ----
mean loss: 164.04
 ---- batch: 110 ----
mean loss: 164.01
train mean loss: 162.46
epoch train time: 0:00:01.918381
elapsed time: 0:03:40.987478
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 22:58:28.736662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.21
 ---- batch: 020 ----
mean loss: 159.25
 ---- batch: 030 ----
mean loss: 159.40
 ---- batch: 040 ----
mean loss: 156.44
 ---- batch: 050 ----
mean loss: 167.93
 ---- batch: 060 ----
mean loss: 166.32
 ---- batch: 070 ----
mean loss: 164.72
 ---- batch: 080 ----
mean loss: 170.88
 ---- batch: 090 ----
mean loss: 161.82
 ---- batch: 100 ----
mean loss: 163.36
 ---- batch: 110 ----
mean loss: 157.42
train mean loss: 161.90
epoch train time: 0:00:01.910782
elapsed time: 0:03:42.898877
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 22:58:30.648102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.71
 ---- batch: 020 ----
mean loss: 166.95
 ---- batch: 030 ----
mean loss: 153.31
 ---- batch: 040 ----
mean loss: 165.42
 ---- batch: 050 ----
mean loss: 159.79
 ---- batch: 060 ----
mean loss: 162.93
 ---- batch: 070 ----
mean loss: 155.77
 ---- batch: 080 ----
mean loss: 154.66
 ---- batch: 090 ----
mean loss: 158.50
 ---- batch: 100 ----
mean loss: 168.59
 ---- batch: 110 ----
mean loss: 167.58
train mean loss: 161.69
epoch train time: 0:00:01.899219
elapsed time: 0:03:44.798755
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 22:58:32.547968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.93
 ---- batch: 020 ----
mean loss: 160.32
 ---- batch: 030 ----
mean loss: 165.72
 ---- batch: 040 ----
mean loss: 154.09
 ---- batch: 050 ----
mean loss: 157.51
 ---- batch: 060 ----
mean loss: 165.18
 ---- batch: 070 ----
mean loss: 160.24
 ---- batch: 080 ----
mean loss: 159.07
 ---- batch: 090 ----
mean loss: 160.82
 ---- batch: 100 ----
mean loss: 165.65
 ---- batch: 110 ----
mean loss: 166.25
train mean loss: 160.42
epoch train time: 0:00:01.917096
elapsed time: 0:03:46.716423
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 22:58:34.465868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.47
 ---- batch: 020 ----
mean loss: 160.45
 ---- batch: 030 ----
mean loss: 160.12
 ---- batch: 040 ----
mean loss: 154.21
 ---- batch: 050 ----
mean loss: 164.45
 ---- batch: 060 ----
mean loss: 153.45
 ---- batch: 070 ----
mean loss: 158.07
 ---- batch: 080 ----
mean loss: 162.48
 ---- batch: 090 ----
mean loss: 159.45
 ---- batch: 100 ----
mean loss: 150.32
 ---- batch: 110 ----
mean loss: 166.69
train mean loss: 159.66
epoch train time: 0:00:01.919459
elapsed time: 0:03:48.636701
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 22:58:36.385907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.68
 ---- batch: 020 ----
mean loss: 158.91
 ---- batch: 030 ----
mean loss: 160.54
 ---- batch: 040 ----
mean loss: 161.05
 ---- batch: 050 ----
mean loss: 154.89
 ---- batch: 060 ----
mean loss: 159.81
 ---- batch: 070 ----
mean loss: 157.72
 ---- batch: 080 ----
mean loss: 160.38
 ---- batch: 090 ----
mean loss: 158.18
 ---- batch: 100 ----
mean loss: 160.22
 ---- batch: 110 ----
mean loss: 157.90
train mean loss: 159.53
epoch train time: 0:00:01.921243
elapsed time: 0:03:50.558534
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 22:58:38.307731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.59
 ---- batch: 020 ----
mean loss: 153.02
 ---- batch: 030 ----
mean loss: 156.22
 ---- batch: 040 ----
mean loss: 156.69
 ---- batch: 050 ----
mean loss: 156.92
 ---- batch: 060 ----
mean loss: 166.20
 ---- batch: 070 ----
mean loss: 155.34
 ---- batch: 080 ----
mean loss: 154.37
 ---- batch: 090 ----
mean loss: 156.16
 ---- batch: 100 ----
mean loss: 163.97
 ---- batch: 110 ----
mean loss: 162.37
train mean loss: 158.62
epoch train time: 0:00:01.928674
elapsed time: 0:03:52.487770
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 22:58:40.236997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.61
 ---- batch: 020 ----
mean loss: 154.04
 ---- batch: 030 ----
mean loss: 158.33
 ---- batch: 040 ----
mean loss: 154.67
 ---- batch: 050 ----
mean loss: 158.31
 ---- batch: 060 ----
mean loss: 155.82
 ---- batch: 070 ----
mean loss: 159.00
 ---- batch: 080 ----
mean loss: 153.27
 ---- batch: 090 ----
mean loss: 157.68
 ---- batch: 100 ----
mean loss: 166.48
 ---- batch: 110 ----
mean loss: 168.77
train mean loss: 157.61
epoch train time: 0:00:01.958144
elapsed time: 0:03:54.446492
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 22:58:42.195734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.75
 ---- batch: 020 ----
mean loss: 153.00
 ---- batch: 030 ----
mean loss: 155.88
 ---- batch: 040 ----
mean loss: 155.13
 ---- batch: 050 ----
mean loss: 152.70
 ---- batch: 060 ----
mean loss: 163.02
 ---- batch: 070 ----
mean loss: 158.86
 ---- batch: 080 ----
mean loss: 155.12
 ---- batch: 090 ----
mean loss: 157.28
 ---- batch: 100 ----
mean loss: 162.17
 ---- batch: 110 ----
mean loss: 156.29
train mean loss: 157.69
epoch train time: 0:00:01.946853
elapsed time: 0:03:56.394176
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 22:58:44.143163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.64
 ---- batch: 020 ----
mean loss: 164.00
 ---- batch: 030 ----
mean loss: 150.73
 ---- batch: 040 ----
mean loss: 153.30
 ---- batch: 050 ----
mean loss: 151.71
 ---- batch: 060 ----
mean loss: 154.14
 ---- batch: 070 ----
mean loss: 152.44
 ---- batch: 080 ----
mean loss: 160.81
 ---- batch: 090 ----
mean loss: 162.81
 ---- batch: 100 ----
mean loss: 155.67
 ---- batch: 110 ----
mean loss: 160.15
train mean loss: 156.47
epoch train time: 0:00:01.914838
elapsed time: 0:03:58.309399
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 22:58:46.058648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.46
 ---- batch: 020 ----
mean loss: 157.63
 ---- batch: 030 ----
mean loss: 153.45
 ---- batch: 040 ----
mean loss: 156.74
 ---- batch: 050 ----
mean loss: 158.74
 ---- batch: 060 ----
mean loss: 146.44
 ---- batch: 070 ----
mean loss: 164.67
 ---- batch: 080 ----
mean loss: 161.75
 ---- batch: 090 ----
mean loss: 159.36
 ---- batch: 100 ----
mean loss: 150.97
 ---- batch: 110 ----
mean loss: 158.51
train mean loss: 156.58
epoch train time: 0:00:01.927279
elapsed time: 0:04:00.237294
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 22:58:47.986494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.36
 ---- batch: 020 ----
mean loss: 157.81
 ---- batch: 030 ----
mean loss: 148.82
 ---- batch: 040 ----
mean loss: 150.75
 ---- batch: 050 ----
mean loss: 155.41
 ---- batch: 060 ----
mean loss: 155.63
 ---- batch: 070 ----
mean loss: 158.39
 ---- batch: 080 ----
mean loss: 156.96
 ---- batch: 090 ----
mean loss: 162.34
 ---- batch: 100 ----
mean loss: 160.87
 ---- batch: 110 ----
mean loss: 156.69
train mean loss: 155.91
epoch train time: 0:00:01.909208
elapsed time: 0:04:02.147085
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 22:58:49.896264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.52
 ---- batch: 020 ----
mean loss: 148.45
 ---- batch: 030 ----
mean loss: 159.80
 ---- batch: 040 ----
mean loss: 148.78
 ---- batch: 050 ----
mean loss: 161.47
 ---- batch: 060 ----
mean loss: 153.81
 ---- batch: 070 ----
mean loss: 164.16
 ---- batch: 080 ----
mean loss: 155.41
 ---- batch: 090 ----
mean loss: 151.30
 ---- batch: 100 ----
mean loss: 151.79
 ---- batch: 110 ----
mean loss: 158.88
train mean loss: 154.91
epoch train time: 0:00:01.897809
elapsed time: 0:04:04.045444
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 22:58:51.794707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.42
 ---- batch: 020 ----
mean loss: 149.63
 ---- batch: 030 ----
mean loss: 144.66
 ---- batch: 040 ----
mean loss: 143.65
 ---- batch: 050 ----
mean loss: 162.50
 ---- batch: 060 ----
mean loss: 153.93
 ---- batch: 070 ----
mean loss: 155.94
 ---- batch: 080 ----
mean loss: 158.74
 ---- batch: 090 ----
mean loss: 151.93
 ---- batch: 100 ----
mean loss: 155.39
 ---- batch: 110 ----
mean loss: 162.23
train mean loss: 154.18
epoch train time: 0:00:01.907871
elapsed time: 0:04:05.954091
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 22:58:53.703321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.48
 ---- batch: 020 ----
mean loss: 145.52
 ---- batch: 030 ----
mean loss: 149.58
 ---- batch: 040 ----
mean loss: 152.92
 ---- batch: 050 ----
mean loss: 153.84
 ---- batch: 060 ----
mean loss: 157.91
 ---- batch: 070 ----
mean loss: 155.22
 ---- batch: 080 ----
mean loss: 162.67
 ---- batch: 090 ----
mean loss: 160.81
 ---- batch: 100 ----
mean loss: 145.48
 ---- batch: 110 ----
mean loss: 155.72
train mean loss: 154.00
epoch train time: 0:00:01.922968
elapsed time: 0:04:07.877657
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 22:58:55.626932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.43
 ---- batch: 020 ----
mean loss: 140.96
 ---- batch: 030 ----
mean loss: 153.06
 ---- batch: 040 ----
mean loss: 142.76
 ---- batch: 050 ----
mean loss: 155.14
 ---- batch: 060 ----
mean loss: 155.61
 ---- batch: 070 ----
mean loss: 146.57
 ---- batch: 080 ----
mean loss: 161.45
 ---- batch: 090 ----
mean loss: 153.87
 ---- batch: 100 ----
mean loss: 159.90
 ---- batch: 110 ----
mean loss: 163.87
train mean loss: 153.75
epoch train time: 0:00:01.907714
elapsed time: 0:04:09.786021
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 22:58:57.535234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.80
 ---- batch: 020 ----
mean loss: 158.50
 ---- batch: 030 ----
mean loss: 148.40
 ---- batch: 040 ----
mean loss: 152.18
 ---- batch: 050 ----
mean loss: 158.21
 ---- batch: 060 ----
mean loss: 163.63
 ---- batch: 070 ----
mean loss: 147.83
 ---- batch: 080 ----
mean loss: 147.47
 ---- batch: 090 ----
mean loss: 151.42
 ---- batch: 100 ----
mean loss: 155.11
 ---- batch: 110 ----
mean loss: 151.04
train mean loss: 152.76
epoch train time: 0:00:01.903025
elapsed time: 0:04:11.689662
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 22:58:59.439036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.32
 ---- batch: 020 ----
mean loss: 159.40
 ---- batch: 030 ----
mean loss: 149.49
 ---- batch: 040 ----
mean loss: 149.37
 ---- batch: 050 ----
mean loss: 154.33
 ---- batch: 060 ----
mean loss: 149.27
 ---- batch: 070 ----
mean loss: 153.07
 ---- batch: 080 ----
mean loss: 158.98
 ---- batch: 090 ----
mean loss: 151.85
 ---- batch: 100 ----
mean loss: 144.21
 ---- batch: 110 ----
mean loss: 142.58
train mean loss: 152.01
epoch train time: 0:00:01.939085
elapsed time: 0:04:13.629501
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 22:59:01.378757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.53
 ---- batch: 020 ----
mean loss: 147.92
 ---- batch: 030 ----
mean loss: 145.93
 ---- batch: 040 ----
mean loss: 154.40
 ---- batch: 050 ----
mean loss: 155.70
 ---- batch: 060 ----
mean loss: 149.09
 ---- batch: 070 ----
mean loss: 155.74
 ---- batch: 080 ----
mean loss: 159.69
 ---- batch: 090 ----
mean loss: 152.32
 ---- batch: 100 ----
mean loss: 158.52
 ---- batch: 110 ----
mean loss: 145.76
train mean loss: 152.14
epoch train time: 0:00:01.915996
elapsed time: 0:04:15.546124
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 22:59:03.295340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.98
 ---- batch: 020 ----
mean loss: 147.43
 ---- batch: 030 ----
mean loss: 149.09
 ---- batch: 040 ----
mean loss: 145.94
 ---- batch: 050 ----
mean loss: 149.67
 ---- batch: 060 ----
mean loss: 151.62
 ---- batch: 070 ----
mean loss: 158.35
 ---- batch: 080 ----
mean loss: 154.58
 ---- batch: 090 ----
mean loss: 152.87
 ---- batch: 100 ----
mean loss: 156.23
 ---- batch: 110 ----
mean loss: 147.24
train mean loss: 151.33
epoch train time: 0:00:01.953999
elapsed time: 0:04:17.500744
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 22:59:05.250026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.46
 ---- batch: 020 ----
mean loss: 143.12
 ---- batch: 030 ----
mean loss: 147.89
 ---- batch: 040 ----
mean loss: 149.75
 ---- batch: 050 ----
mean loss: 147.94
 ---- batch: 060 ----
mean loss: 151.84
 ---- batch: 070 ----
mean loss: 155.61
 ---- batch: 080 ----
mean loss: 150.00
 ---- batch: 090 ----
mean loss: 152.96
 ---- batch: 100 ----
mean loss: 155.88
 ---- batch: 110 ----
mean loss: 155.22
train mean loss: 150.86
epoch train time: 0:00:01.908326
elapsed time: 0:04:19.409756
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 22:59:07.159003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.77
 ---- batch: 020 ----
mean loss: 149.61
 ---- batch: 030 ----
mean loss: 144.49
 ---- batch: 040 ----
mean loss: 148.81
 ---- batch: 050 ----
mean loss: 152.15
 ---- batch: 060 ----
mean loss: 154.08
 ---- batch: 070 ----
mean loss: 148.72
 ---- batch: 080 ----
mean loss: 150.49
 ---- batch: 090 ----
mean loss: 154.68
 ---- batch: 100 ----
mean loss: 149.81
 ---- batch: 110 ----
mean loss: 154.11
train mean loss: 149.93
epoch train time: 0:00:01.941812
elapsed time: 0:04:21.352220
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 22:59:09.101453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.11
 ---- batch: 020 ----
mean loss: 138.30
 ---- batch: 030 ----
mean loss: 155.28
 ---- batch: 040 ----
mean loss: 148.54
 ---- batch: 050 ----
mean loss: 144.25
 ---- batch: 060 ----
mean loss: 147.82
 ---- batch: 070 ----
mean loss: 156.33
 ---- batch: 080 ----
mean loss: 143.28
 ---- batch: 090 ----
mean loss: 157.37
 ---- batch: 100 ----
mean loss: 148.92
 ---- batch: 110 ----
mean loss: 149.42
train mean loss: 149.18
epoch train time: 0:00:01.951564
elapsed time: 0:04:23.304377
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 22:59:11.053566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.59
 ---- batch: 020 ----
mean loss: 143.10
 ---- batch: 030 ----
mean loss: 147.42
 ---- batch: 040 ----
mean loss: 148.35
 ---- batch: 050 ----
mean loss: 140.33
 ---- batch: 060 ----
mean loss: 149.65
 ---- batch: 070 ----
mean loss: 154.13
 ---- batch: 080 ----
mean loss: 150.54
 ---- batch: 090 ----
mean loss: 149.44
 ---- batch: 100 ----
mean loss: 153.38
 ---- batch: 110 ----
mean loss: 152.03
train mean loss: 149.09
epoch train time: 0:00:01.940568
elapsed time: 0:04:25.245489
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 22:59:12.994686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.66
 ---- batch: 020 ----
mean loss: 153.50
 ---- batch: 030 ----
mean loss: 141.15
 ---- batch: 040 ----
mean loss: 152.08
 ---- batch: 050 ----
mean loss: 153.63
 ---- batch: 060 ----
mean loss: 151.32
 ---- batch: 070 ----
mean loss: 156.89
 ---- batch: 080 ----
mean loss: 152.34
 ---- batch: 090 ----
mean loss: 143.30
 ---- batch: 100 ----
mean loss: 148.17
 ---- batch: 110 ----
mean loss: 141.38
train mean loss: 149.18
epoch train time: 0:00:02.031394
elapsed time: 0:04:27.277548
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 22:59:15.026777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.38
 ---- batch: 020 ----
mean loss: 141.17
 ---- batch: 030 ----
mean loss: 143.57
 ---- batch: 040 ----
mean loss: 149.11
 ---- batch: 050 ----
mean loss: 147.78
 ---- batch: 060 ----
mean loss: 151.26
 ---- batch: 070 ----
mean loss: 151.60
 ---- batch: 080 ----
mean loss: 150.96
 ---- batch: 090 ----
mean loss: 154.75
 ---- batch: 100 ----
mean loss: 151.25
 ---- batch: 110 ----
mean loss: 144.77
train mean loss: 148.38
epoch train time: 0:00:01.956505
elapsed time: 0:04:29.234673
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 22:59:16.983896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.77
 ---- batch: 020 ----
mean loss: 139.69
 ---- batch: 030 ----
mean loss: 146.30
 ---- batch: 040 ----
mean loss: 149.37
 ---- batch: 050 ----
mean loss: 152.28
 ---- batch: 060 ----
mean loss: 158.56
 ---- batch: 070 ----
mean loss: 155.17
 ---- batch: 080 ----
mean loss: 147.14
 ---- batch: 090 ----
mean loss: 148.83
 ---- batch: 100 ----
mean loss: 145.98
 ---- batch: 110 ----
mean loss: 143.02
train mean loss: 148.52
epoch train time: 0:00:01.946527
elapsed time: 0:04:31.181809
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 22:59:18.931105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.52
 ---- batch: 020 ----
mean loss: 139.47
 ---- batch: 030 ----
mean loss: 147.64
 ---- batch: 040 ----
mean loss: 149.42
 ---- batch: 050 ----
mean loss: 138.11
 ---- batch: 060 ----
mean loss: 150.14
 ---- batch: 070 ----
mean loss: 151.67
 ---- batch: 080 ----
mean loss: 153.49
 ---- batch: 090 ----
mean loss: 149.44
 ---- batch: 100 ----
mean loss: 147.15
 ---- batch: 110 ----
mean loss: 148.35
train mean loss: 147.24
epoch train time: 0:00:01.948637
elapsed time: 0:04:33.131130
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 22:59:20.880307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.63
 ---- batch: 020 ----
mean loss: 139.58
 ---- batch: 030 ----
mean loss: 148.75
 ---- batch: 040 ----
mean loss: 148.65
 ---- batch: 050 ----
mean loss: 148.57
 ---- batch: 060 ----
mean loss: 146.08
 ---- batch: 070 ----
mean loss: 146.75
 ---- batch: 080 ----
mean loss: 149.59
 ---- batch: 090 ----
mean loss: 146.94
 ---- batch: 100 ----
mean loss: 148.09
 ---- batch: 110 ----
mean loss: 145.00
train mean loss: 146.75
epoch train time: 0:00:01.958536
elapsed time: 0:04:35.090479
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 22:59:22.839460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.78
 ---- batch: 020 ----
mean loss: 145.77
 ---- batch: 030 ----
mean loss: 143.40
 ---- batch: 040 ----
mean loss: 129.57
 ---- batch: 050 ----
mean loss: 155.79
 ---- batch: 060 ----
mean loss: 147.34
 ---- batch: 070 ----
mean loss: 145.89
 ---- batch: 080 ----
mean loss: 148.72
 ---- batch: 090 ----
mean loss: 151.66
 ---- batch: 100 ----
mean loss: 145.04
 ---- batch: 110 ----
mean loss: 147.82
train mean loss: 146.30
epoch train time: 0:00:01.940616
elapsed time: 0:04:37.031519
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 22:59:24.780721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.22
 ---- batch: 020 ----
mean loss: 134.73
 ---- batch: 030 ----
mean loss: 146.17
 ---- batch: 040 ----
mean loss: 143.43
 ---- batch: 050 ----
mean loss: 149.44
 ---- batch: 060 ----
mean loss: 146.04
 ---- batch: 070 ----
mean loss: 152.05
 ---- batch: 080 ----
mean loss: 150.35
 ---- batch: 090 ----
mean loss: 141.35
 ---- batch: 100 ----
mean loss: 151.73
 ---- batch: 110 ----
mean loss: 139.60
train mean loss: 146.09
epoch train time: 0:00:01.956365
elapsed time: 0:04:38.988501
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 22:59:26.737802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.56
 ---- batch: 020 ----
mean loss: 151.89
 ---- batch: 030 ----
mean loss: 143.28
 ---- batch: 040 ----
mean loss: 144.45
 ---- batch: 050 ----
mean loss: 147.30
 ---- batch: 060 ----
mean loss: 142.71
 ---- batch: 070 ----
mean loss: 139.21
 ---- batch: 080 ----
mean loss: 147.70
 ---- batch: 090 ----
mean loss: 148.22
 ---- batch: 100 ----
mean loss: 145.27
 ---- batch: 110 ----
mean loss: 147.37
train mean loss: 145.67
epoch train time: 0:00:01.930506
elapsed time: 0:04:40.919760
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 22:59:28.668983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.70
 ---- batch: 020 ----
mean loss: 139.78
 ---- batch: 030 ----
mean loss: 143.01
 ---- batch: 040 ----
mean loss: 152.55
 ---- batch: 050 ----
mean loss: 137.91
 ---- batch: 060 ----
mean loss: 141.36
 ---- batch: 070 ----
mean loss: 149.06
 ---- batch: 080 ----
mean loss: 149.81
 ---- batch: 090 ----
mean loss: 145.90
 ---- batch: 100 ----
mean loss: 142.65
 ---- batch: 110 ----
mean loss: 144.27
train mean loss: 144.54
epoch train time: 0:00:01.981339
elapsed time: 0:04:42.901761
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 22:59:30.651000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.36
 ---- batch: 020 ----
mean loss: 141.10
 ---- batch: 030 ----
mean loss: 143.92
 ---- batch: 040 ----
mean loss: 144.21
 ---- batch: 050 ----
mean loss: 142.66
 ---- batch: 060 ----
mean loss: 144.25
 ---- batch: 070 ----
mean loss: 143.50
 ---- batch: 080 ----
mean loss: 139.81
 ---- batch: 090 ----
mean loss: 139.14
 ---- batch: 100 ----
mean loss: 150.47
 ---- batch: 110 ----
mean loss: 148.36
train mean loss: 144.26
epoch train time: 0:00:01.927675
elapsed time: 0:04:44.830124
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 22:59:32.579450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.66
 ---- batch: 020 ----
mean loss: 141.46
 ---- batch: 030 ----
mean loss: 143.14
 ---- batch: 040 ----
mean loss: 147.56
 ---- batch: 050 ----
mean loss: 141.86
 ---- batch: 060 ----
mean loss: 147.82
 ---- batch: 070 ----
mean loss: 153.35
 ---- batch: 080 ----
mean loss: 137.01
 ---- batch: 090 ----
mean loss: 139.89
 ---- batch: 100 ----
mean loss: 145.09
 ---- batch: 110 ----
mean loss: 142.63
train mean loss: 144.09
epoch train time: 0:00:01.957020
elapsed time: 0:04:46.787846
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 22:59:34.537024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.73
 ---- batch: 020 ----
mean loss: 140.89
 ---- batch: 030 ----
mean loss: 143.51
 ---- batch: 040 ----
mean loss: 136.14
 ---- batch: 050 ----
mean loss: 143.46
 ---- batch: 060 ----
mean loss: 134.83
 ---- batch: 070 ----
mean loss: 138.93
 ---- batch: 080 ----
mean loss: 141.48
 ---- batch: 090 ----
mean loss: 152.40
 ---- batch: 100 ----
mean loss: 141.37
 ---- batch: 110 ----
mean loss: 148.51
train mean loss: 143.37
epoch train time: 0:00:01.931861
elapsed time: 0:04:48.720247
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 22:59:36.469448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.91
 ---- batch: 020 ----
mean loss: 145.66
 ---- batch: 030 ----
mean loss: 149.77
 ---- batch: 040 ----
mean loss: 145.69
 ---- batch: 050 ----
mean loss: 144.17
 ---- batch: 060 ----
mean loss: 141.18
 ---- batch: 070 ----
mean loss: 142.43
 ---- batch: 080 ----
mean loss: 139.14
 ---- batch: 090 ----
mean loss: 136.82
 ---- batch: 100 ----
mean loss: 147.51
 ---- batch: 110 ----
mean loss: 140.53
train mean loss: 143.13
epoch train time: 0:00:01.910807
elapsed time: 0:04:50.631646
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 22:59:38.380821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.24
 ---- batch: 020 ----
mean loss: 145.45
 ---- batch: 030 ----
mean loss: 146.40
 ---- batch: 040 ----
mean loss: 147.11
 ---- batch: 050 ----
mean loss: 137.76
 ---- batch: 060 ----
mean loss: 140.18
 ---- batch: 070 ----
mean loss: 139.04
 ---- batch: 080 ----
mean loss: 138.27
 ---- batch: 090 ----
mean loss: 146.59
 ---- batch: 100 ----
mean loss: 143.19
 ---- batch: 110 ----
mean loss: 150.41
train mean loss: 143.15
epoch train time: 0:00:01.946352
elapsed time: 0:04:52.578527
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 22:59:40.327738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.16
 ---- batch: 020 ----
mean loss: 141.46
 ---- batch: 030 ----
mean loss: 139.48
 ---- batch: 040 ----
mean loss: 137.51
 ---- batch: 050 ----
mean loss: 135.22
 ---- batch: 060 ----
mean loss: 140.70
 ---- batch: 070 ----
mean loss: 145.39
 ---- batch: 080 ----
mean loss: 144.40
 ---- batch: 090 ----
mean loss: 148.23
 ---- batch: 100 ----
mean loss: 138.65
 ---- batch: 110 ----
mean loss: 140.60
train mean loss: 142.66
epoch train time: 0:00:01.933984
elapsed time: 0:04:54.513087
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 22:59:42.262285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.64
 ---- batch: 020 ----
mean loss: 139.63
 ---- batch: 030 ----
mean loss: 135.71
 ---- batch: 040 ----
mean loss: 145.70
 ---- batch: 050 ----
mean loss: 142.08
 ---- batch: 060 ----
mean loss: 143.04
 ---- batch: 070 ----
mean loss: 134.87
 ---- batch: 080 ----
mean loss: 140.63
 ---- batch: 090 ----
mean loss: 141.54
 ---- batch: 100 ----
mean loss: 152.80
 ---- batch: 110 ----
mean loss: 149.84
train mean loss: 141.77
epoch train time: 0:00:01.948720
elapsed time: 0:04:56.462416
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 22:59:44.211754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.75
 ---- batch: 020 ----
mean loss: 145.09
 ---- batch: 030 ----
mean loss: 136.90
 ---- batch: 040 ----
mean loss: 149.12
 ---- batch: 050 ----
mean loss: 134.24
 ---- batch: 060 ----
mean loss: 143.16
 ---- batch: 070 ----
mean loss: 139.61
 ---- batch: 080 ----
mean loss: 141.18
 ---- batch: 090 ----
mean loss: 136.69
 ---- batch: 100 ----
mean loss: 149.54
 ---- batch: 110 ----
mean loss: 137.54
train mean loss: 141.83
epoch train time: 0:00:01.943251
elapsed time: 0:04:58.406460
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 22:59:46.155674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.38
 ---- batch: 020 ----
mean loss: 142.12
 ---- batch: 030 ----
mean loss: 146.37
 ---- batch: 040 ----
mean loss: 145.17
 ---- batch: 050 ----
mean loss: 145.42
 ---- batch: 060 ----
mean loss: 146.23
 ---- batch: 070 ----
mean loss: 140.80
 ---- batch: 080 ----
mean loss: 136.25
 ---- batch: 090 ----
mean loss: 143.10
 ---- batch: 100 ----
mean loss: 133.75
 ---- batch: 110 ----
mean loss: 145.54
train mean loss: 141.63
epoch train time: 0:00:01.916381
elapsed time: 0:05:00.323435
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 22:59:48.072649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.67
 ---- batch: 020 ----
mean loss: 132.42
 ---- batch: 030 ----
mean loss: 144.40
 ---- batch: 040 ----
mean loss: 143.58
 ---- batch: 050 ----
mean loss: 140.77
 ---- batch: 060 ----
mean loss: 135.58
 ---- batch: 070 ----
mean loss: 139.18
 ---- batch: 080 ----
mean loss: 142.97
 ---- batch: 090 ----
mean loss: 154.75
 ---- batch: 100 ----
mean loss: 140.40
 ---- batch: 110 ----
mean loss: 138.72
train mean loss: 140.96
epoch train time: 0:00:01.966670
elapsed time: 0:05:02.290722
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 22:59:50.039924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.74
 ---- batch: 020 ----
mean loss: 144.00
 ---- batch: 030 ----
mean loss: 141.46
 ---- batch: 040 ----
mean loss: 138.10
 ---- batch: 050 ----
mean loss: 140.83
 ---- batch: 060 ----
mean loss: 139.92
 ---- batch: 070 ----
mean loss: 137.24
 ---- batch: 080 ----
mean loss: 140.74
 ---- batch: 090 ----
mean loss: 143.23
 ---- batch: 100 ----
mean loss: 138.60
 ---- batch: 110 ----
mean loss: 135.59
train mean loss: 140.54
epoch train time: 0:00:01.979334
elapsed time: 0:05:04.270658
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 22:59:52.019856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.72
 ---- batch: 020 ----
mean loss: 142.04
 ---- batch: 030 ----
mean loss: 140.64
 ---- batch: 040 ----
mean loss: 138.99
 ---- batch: 050 ----
mean loss: 133.30
 ---- batch: 060 ----
mean loss: 142.36
 ---- batch: 070 ----
mean loss: 137.88
 ---- batch: 080 ----
mean loss: 138.78
 ---- batch: 090 ----
mean loss: 136.41
 ---- batch: 100 ----
mean loss: 147.80
 ---- batch: 110 ----
mean loss: 149.80
train mean loss: 140.31
epoch train time: 0:00:01.924795
elapsed time: 0:05:06.196071
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 22:59:53.945305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.57
 ---- batch: 020 ----
mean loss: 133.20
 ---- batch: 030 ----
mean loss: 136.03
 ---- batch: 040 ----
mean loss: 142.39
 ---- batch: 050 ----
mean loss: 143.50
 ---- batch: 060 ----
mean loss: 136.50
 ---- batch: 070 ----
mean loss: 143.51
 ---- batch: 080 ----
mean loss: 147.39
 ---- batch: 090 ----
mean loss: 148.19
 ---- batch: 100 ----
mean loss: 142.21
 ---- batch: 110 ----
mean loss: 129.89
train mean loss: 139.91
epoch train time: 0:00:01.949085
elapsed time: 0:05:08.145808
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 22:59:55.895063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.85
 ---- batch: 020 ----
mean loss: 133.34
 ---- batch: 030 ----
mean loss: 141.25
 ---- batch: 040 ----
mean loss: 143.28
 ---- batch: 050 ----
mean loss: 141.00
 ---- batch: 060 ----
mean loss: 139.11
 ---- batch: 070 ----
mean loss: 135.11
 ---- batch: 080 ----
mean loss: 135.54
 ---- batch: 090 ----
mean loss: 141.16
 ---- batch: 100 ----
mean loss: 143.45
 ---- batch: 110 ----
mean loss: 139.92
train mean loss: 139.04
epoch train time: 0:00:01.903350
elapsed time: 0:05:10.049842
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 22:59:57.799056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.54
 ---- batch: 020 ----
mean loss: 141.12
 ---- batch: 030 ----
mean loss: 140.28
 ---- batch: 040 ----
mean loss: 139.07
 ---- batch: 050 ----
mean loss: 145.85
 ---- batch: 060 ----
mean loss: 139.13
 ---- batch: 070 ----
mean loss: 140.98
 ---- batch: 080 ----
mean loss: 138.90
 ---- batch: 090 ----
mean loss: 141.15
 ---- batch: 100 ----
mean loss: 138.99
 ---- batch: 110 ----
mean loss: 137.96
train mean loss: 139.28
epoch train time: 0:00:01.947268
elapsed time: 0:05:11.997715
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 22:59:59.746895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.26
 ---- batch: 020 ----
mean loss: 140.25
 ---- batch: 030 ----
mean loss: 132.08
 ---- batch: 040 ----
mean loss: 136.80
 ---- batch: 050 ----
mean loss: 135.74
 ---- batch: 060 ----
mean loss: 136.54
 ---- batch: 070 ----
mean loss: 144.67
 ---- batch: 080 ----
mean loss: 137.21
 ---- batch: 090 ----
mean loss: 138.08
 ---- batch: 100 ----
mean loss: 135.73
 ---- batch: 110 ----
mean loss: 139.62
train mean loss: 138.19
epoch train time: 0:00:01.947501
elapsed time: 0:05:13.945825
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 23:00:01.695076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.36
 ---- batch: 020 ----
mean loss: 139.09
 ---- batch: 030 ----
mean loss: 134.12
 ---- batch: 040 ----
mean loss: 135.06
 ---- batch: 050 ----
mean loss: 140.31
 ---- batch: 060 ----
mean loss: 140.00
 ---- batch: 070 ----
mean loss: 134.48
 ---- batch: 080 ----
mean loss: 138.47
 ---- batch: 090 ----
mean loss: 141.92
 ---- batch: 100 ----
mean loss: 144.75
 ---- batch: 110 ----
mean loss: 132.52
train mean loss: 138.08
epoch train time: 0:00:01.964898
elapsed time: 0:05:15.911392
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 23:00:03.660609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.36
 ---- batch: 020 ----
mean loss: 140.40
 ---- batch: 030 ----
mean loss: 140.46
 ---- batch: 040 ----
mean loss: 130.91
 ---- batch: 050 ----
mean loss: 135.62
 ---- batch: 060 ----
mean loss: 135.29
 ---- batch: 070 ----
mean loss: 139.91
 ---- batch: 080 ----
mean loss: 147.82
 ---- batch: 090 ----
mean loss: 142.31
 ---- batch: 100 ----
mean loss: 142.33
 ---- batch: 110 ----
mean loss: 134.13
train mean loss: 138.04
epoch train time: 0:00:01.892062
elapsed time: 0:05:17.804394
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 23:00:05.553269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.43
 ---- batch: 020 ----
mean loss: 132.25
 ---- batch: 030 ----
mean loss: 131.55
 ---- batch: 040 ----
mean loss: 125.46
 ---- batch: 050 ----
mean loss: 130.77
 ---- batch: 060 ----
mean loss: 137.18
 ---- batch: 070 ----
mean loss: 146.26
 ---- batch: 080 ----
mean loss: 141.03
 ---- batch: 090 ----
mean loss: 142.68
 ---- batch: 100 ----
mean loss: 144.95
 ---- batch: 110 ----
mean loss: 139.50
train mean loss: 137.60
epoch train time: 0:00:01.938191
elapsed time: 0:05:19.742895
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 23:00:07.492134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.38
 ---- batch: 020 ----
mean loss: 132.18
 ---- batch: 030 ----
mean loss: 141.22
 ---- batch: 040 ----
mean loss: 134.26
 ---- batch: 050 ----
mean loss: 139.40
 ---- batch: 060 ----
mean loss: 140.83
 ---- batch: 070 ----
mean loss: 132.41
 ---- batch: 080 ----
mean loss: 143.37
 ---- batch: 090 ----
mean loss: 143.72
 ---- batch: 100 ----
mean loss: 130.11
 ---- batch: 110 ----
mean loss: 132.92
train mean loss: 137.24
epoch train time: 0:00:01.994724
elapsed time: 0:05:21.738249
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 23:00:09.487186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.32
 ---- batch: 020 ----
mean loss: 128.88
 ---- batch: 030 ----
mean loss: 137.43
 ---- batch: 040 ----
mean loss: 135.09
 ---- batch: 050 ----
mean loss: 132.19
 ---- batch: 060 ----
mean loss: 140.23
 ---- batch: 070 ----
mean loss: 143.00
 ---- batch: 080 ----
mean loss: 140.25
 ---- batch: 090 ----
mean loss: 137.71
 ---- batch: 100 ----
mean loss: 140.01
 ---- batch: 110 ----
mean loss: 135.37
train mean loss: 137.26
epoch train time: 0:00:01.956386
elapsed time: 0:05:23.694966
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 23:00:11.444169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.61
 ---- batch: 020 ----
mean loss: 131.54
 ---- batch: 030 ----
mean loss: 132.36
 ---- batch: 040 ----
mean loss: 132.26
 ---- batch: 050 ----
mean loss: 133.93
 ---- batch: 060 ----
mean loss: 132.64
 ---- batch: 070 ----
mean loss: 136.60
 ---- batch: 080 ----
mean loss: 140.62
 ---- batch: 090 ----
mean loss: 143.84
 ---- batch: 100 ----
mean loss: 141.57
 ---- batch: 110 ----
mean loss: 134.05
train mean loss: 136.60
epoch train time: 0:00:01.991414
elapsed time: 0:05:25.686959
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 23:00:13.436156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.10
 ---- batch: 020 ----
mean loss: 126.58
 ---- batch: 030 ----
mean loss: 138.69
 ---- batch: 040 ----
mean loss: 131.91
 ---- batch: 050 ----
mean loss: 140.62
 ---- batch: 060 ----
mean loss: 137.01
 ---- batch: 070 ----
mean loss: 139.50
 ---- batch: 080 ----
mean loss: 132.23
 ---- batch: 090 ----
mean loss: 133.40
 ---- batch: 100 ----
mean loss: 137.07
 ---- batch: 110 ----
mean loss: 140.70
train mean loss: 136.15
epoch train time: 0:00:01.953207
elapsed time: 0:05:27.640744
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 23:00:15.390026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.51
 ---- batch: 020 ----
mean loss: 131.74
 ---- batch: 030 ----
mean loss: 132.48
 ---- batch: 040 ----
mean loss: 134.46
 ---- batch: 050 ----
mean loss: 136.44
 ---- batch: 060 ----
mean loss: 140.53
 ---- batch: 070 ----
mean loss: 137.55
 ---- batch: 080 ----
mean loss: 134.37
 ---- batch: 090 ----
mean loss: 139.42
 ---- batch: 100 ----
mean loss: 130.69
 ---- batch: 110 ----
mean loss: 139.72
train mean loss: 135.20
epoch train time: 0:00:01.931862
elapsed time: 0:05:29.573276
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 23:00:17.322548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.79
 ---- batch: 020 ----
mean loss: 133.10
 ---- batch: 030 ----
mean loss: 138.32
 ---- batch: 040 ----
mean loss: 136.09
 ---- batch: 050 ----
mean loss: 132.52
 ---- batch: 060 ----
mean loss: 132.04
 ---- batch: 070 ----
mean loss: 142.35
 ---- batch: 080 ----
mean loss: 133.90
 ---- batch: 090 ----
mean loss: 141.25
 ---- batch: 100 ----
mean loss: 134.20
 ---- batch: 110 ----
mean loss: 139.65
train mean loss: 136.27
epoch train time: 0:00:01.942762
elapsed time: 0:05:31.516689
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 23:00:19.265941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.29
 ---- batch: 020 ----
mean loss: 128.33
 ---- batch: 030 ----
mean loss: 140.85
 ---- batch: 040 ----
mean loss: 145.68
 ---- batch: 050 ----
mean loss: 134.85
 ---- batch: 060 ----
mean loss: 131.27
 ---- batch: 070 ----
mean loss: 136.70
 ---- batch: 080 ----
mean loss: 138.31
 ---- batch: 090 ----
mean loss: 127.36
 ---- batch: 100 ----
mean loss: 141.91
 ---- batch: 110 ----
mean loss: 137.51
train mean loss: 136.04
epoch train time: 0:00:01.960822
elapsed time: 0:05:33.478159
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 23:00:21.227343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.81
 ---- batch: 020 ----
mean loss: 130.10
 ---- batch: 030 ----
mean loss: 137.09
 ---- batch: 040 ----
mean loss: 133.07
 ---- batch: 050 ----
mean loss: 124.09
 ---- batch: 060 ----
mean loss: 141.29
 ---- batch: 070 ----
mean loss: 128.33
 ---- batch: 080 ----
mean loss: 135.92
 ---- batch: 090 ----
mean loss: 141.90
 ---- batch: 100 ----
mean loss: 134.38
 ---- batch: 110 ----
mean loss: 142.41
train mean loss: 134.80
epoch train time: 0:00:01.935643
elapsed time: 0:05:35.414446
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 23:00:23.163741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.77
 ---- batch: 020 ----
mean loss: 132.62
 ---- batch: 030 ----
mean loss: 131.18
 ---- batch: 040 ----
mean loss: 135.25
 ---- batch: 050 ----
mean loss: 132.51
 ---- batch: 060 ----
mean loss: 132.05
 ---- batch: 070 ----
mean loss: 138.31
 ---- batch: 080 ----
mean loss: 135.25
 ---- batch: 090 ----
mean loss: 126.40
 ---- batch: 100 ----
mean loss: 137.02
 ---- batch: 110 ----
mean loss: 139.16
train mean loss: 134.43
epoch train time: 0:00:01.959684
elapsed time: 0:05:37.374827
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 23:00:25.124023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.18
 ---- batch: 020 ----
mean loss: 129.68
 ---- batch: 030 ----
mean loss: 130.82
 ---- batch: 040 ----
mean loss: 137.78
 ---- batch: 050 ----
mean loss: 135.25
 ---- batch: 060 ----
mean loss: 127.43
 ---- batch: 070 ----
mean loss: 131.44
 ---- batch: 080 ----
mean loss: 135.25
 ---- batch: 090 ----
mean loss: 133.24
 ---- batch: 100 ----
mean loss: 144.23
 ---- batch: 110 ----
mean loss: 132.35
train mean loss: 134.17
epoch train time: 0:00:01.884255
elapsed time: 0:05:39.259687
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 23:00:27.008894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.28
 ---- batch: 020 ----
mean loss: 134.66
 ---- batch: 030 ----
mean loss: 124.65
 ---- batch: 040 ----
mean loss: 137.57
 ---- batch: 050 ----
mean loss: 145.64
 ---- batch: 060 ----
mean loss: 130.91
 ---- batch: 070 ----
mean loss: 126.77
 ---- batch: 080 ----
mean loss: 139.71
 ---- batch: 090 ----
mean loss: 140.91
 ---- batch: 100 ----
mean loss: 127.43
 ---- batch: 110 ----
mean loss: 130.40
train mean loss: 133.64
epoch train time: 0:00:01.876291
elapsed time: 0:05:41.136579
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 23:00:28.885807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.04
 ---- batch: 020 ----
mean loss: 134.69
 ---- batch: 030 ----
mean loss: 125.51
 ---- batch: 040 ----
mean loss: 133.16
 ---- batch: 050 ----
mean loss: 141.97
 ---- batch: 060 ----
mean loss: 135.27
 ---- batch: 070 ----
mean loss: 133.40
 ---- batch: 080 ----
mean loss: 130.13
 ---- batch: 090 ----
mean loss: 131.87
 ---- batch: 100 ----
mean loss: 139.15
 ---- batch: 110 ----
mean loss: 133.97
train mean loss: 133.38
epoch train time: 0:00:01.898784
elapsed time: 0:05:43.036063
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 23:00:30.785401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.86
 ---- batch: 020 ----
mean loss: 136.26
 ---- batch: 030 ----
mean loss: 135.81
 ---- batch: 040 ----
mean loss: 126.70
 ---- batch: 050 ----
mean loss: 141.44
 ---- batch: 060 ----
mean loss: 133.75
 ---- batch: 070 ----
mean loss: 129.84
 ---- batch: 080 ----
mean loss: 116.90
 ---- batch: 090 ----
mean loss: 127.27
 ---- batch: 100 ----
mean loss: 142.86
 ---- batch: 110 ----
mean loss: 140.59
train mean loss: 133.13
epoch train time: 0:00:02.013800
elapsed time: 0:05:45.050597
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 23:00:32.799784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.91
 ---- batch: 020 ----
mean loss: 133.85
 ---- batch: 030 ----
mean loss: 138.19
 ---- batch: 040 ----
mean loss: 131.44
 ---- batch: 050 ----
mean loss: 132.74
 ---- batch: 060 ----
mean loss: 133.13
 ---- batch: 070 ----
mean loss: 131.21
 ---- batch: 080 ----
mean loss: 129.76
 ---- batch: 090 ----
mean loss: 134.02
 ---- batch: 100 ----
mean loss: 123.87
 ---- batch: 110 ----
mean loss: 135.67
train mean loss: 132.48
epoch train time: 0:00:02.022856
elapsed time: 0:05:47.074079
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 23:00:34.823324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.82
 ---- batch: 020 ----
mean loss: 135.27
 ---- batch: 030 ----
mean loss: 131.69
 ---- batch: 040 ----
mean loss: 129.75
 ---- batch: 050 ----
mean loss: 134.33
 ---- batch: 060 ----
mean loss: 131.45
 ---- batch: 070 ----
mean loss: 138.25
 ---- batch: 080 ----
mean loss: 133.13
 ---- batch: 090 ----
mean loss: 124.97
 ---- batch: 100 ----
mean loss: 136.27
 ---- batch: 110 ----
mean loss: 135.05
train mean loss: 132.54
epoch train time: 0:00:02.055612
elapsed time: 0:05:49.130357
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 23:00:36.879565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.65
 ---- batch: 020 ----
mean loss: 125.60
 ---- batch: 030 ----
mean loss: 132.33
 ---- batch: 040 ----
mean loss: 128.03
 ---- batch: 050 ----
mean loss: 142.16
 ---- batch: 060 ----
mean loss: 128.41
 ---- batch: 070 ----
mean loss: 130.92
 ---- batch: 080 ----
mean loss: 137.09
 ---- batch: 090 ----
mean loss: 135.00
 ---- batch: 100 ----
mean loss: 133.00
 ---- batch: 110 ----
mean loss: 122.19
train mean loss: 132.10
epoch train time: 0:00:01.982500
elapsed time: 0:05:51.113497
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 23:00:38.862718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.59
 ---- batch: 020 ----
mean loss: 135.12
 ---- batch: 030 ----
mean loss: 132.75
 ---- batch: 040 ----
mean loss: 128.72
 ---- batch: 050 ----
mean loss: 125.54
 ---- batch: 060 ----
mean loss: 131.71
 ---- batch: 070 ----
mean loss: 123.87
 ---- batch: 080 ----
mean loss: 144.41
 ---- batch: 090 ----
mean loss: 135.67
 ---- batch: 100 ----
mean loss: 145.85
 ---- batch: 110 ----
mean loss: 127.09
train mean loss: 132.29
epoch train time: 0:00:02.001615
elapsed time: 0:05:53.115748
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 23:00:40.864967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.18
 ---- batch: 020 ----
mean loss: 127.94
 ---- batch: 030 ----
mean loss: 126.73
 ---- batch: 040 ----
mean loss: 125.99
 ---- batch: 050 ----
mean loss: 132.77
 ---- batch: 060 ----
mean loss: 127.91
 ---- batch: 070 ----
mean loss: 132.50
 ---- batch: 080 ----
mean loss: 127.55
 ---- batch: 090 ----
mean loss: 142.38
 ---- batch: 100 ----
mean loss: 137.37
 ---- batch: 110 ----
mean loss: 138.74
train mean loss: 131.73
epoch train time: 0:00:01.947282
elapsed time: 0:05:55.063662
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 23:00:42.812914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.40
 ---- batch: 020 ----
mean loss: 132.52
 ---- batch: 030 ----
mean loss: 126.87
 ---- batch: 040 ----
mean loss: 139.24
 ---- batch: 050 ----
mean loss: 137.03
 ---- batch: 060 ----
mean loss: 124.14
 ---- batch: 070 ----
mean loss: 119.89
 ---- batch: 080 ----
mean loss: 136.89
 ---- batch: 090 ----
mean loss: 133.22
 ---- batch: 100 ----
mean loss: 128.20
 ---- batch: 110 ----
mean loss: 131.76
train mean loss: 130.78
epoch train time: 0:00:01.960417
elapsed time: 0:05:57.024724
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 23:00:44.774002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.18
 ---- batch: 020 ----
mean loss: 130.60
 ---- batch: 030 ----
mean loss: 121.12
 ---- batch: 040 ----
mean loss: 135.64
 ---- batch: 050 ----
mean loss: 136.46
 ---- batch: 060 ----
mean loss: 134.31
 ---- batch: 070 ----
mean loss: 131.96
 ---- batch: 080 ----
mean loss: 136.46
 ---- batch: 090 ----
mean loss: 130.94
 ---- batch: 100 ----
mean loss: 134.61
 ---- batch: 110 ----
mean loss: 125.04
train mean loss: 131.22
epoch train time: 0:00:01.978315
elapsed time: 0:05:59.003762
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 23:00:46.753031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.58
 ---- batch: 020 ----
mean loss: 127.09
 ---- batch: 030 ----
mean loss: 129.07
 ---- batch: 040 ----
mean loss: 127.19
 ---- batch: 050 ----
mean loss: 134.71
 ---- batch: 060 ----
mean loss: 138.84
 ---- batch: 070 ----
mean loss: 129.12
 ---- batch: 080 ----
mean loss: 135.13
 ---- batch: 090 ----
mean loss: 137.46
 ---- batch: 100 ----
mean loss: 125.40
 ---- batch: 110 ----
mean loss: 125.87
train mean loss: 131.48
epoch train time: 0:00:01.965285
elapsed time: 0:06:00.969729
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 23:00:48.718946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.64
 ---- batch: 020 ----
mean loss: 138.38
 ---- batch: 030 ----
mean loss: 141.20
 ---- batch: 040 ----
mean loss: 124.36
 ---- batch: 050 ----
mean loss: 126.72
 ---- batch: 060 ----
mean loss: 125.37
 ---- batch: 070 ----
mean loss: 138.56
 ---- batch: 080 ----
mean loss: 126.52
 ---- batch: 090 ----
mean loss: 136.35
 ---- batch: 100 ----
mean loss: 134.50
 ---- batch: 110 ----
mean loss: 128.22
train mean loss: 130.90
epoch train time: 0:00:01.965160
elapsed time: 0:06:02.935529
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 23:00:50.684736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.96
 ---- batch: 020 ----
mean loss: 129.75
 ---- batch: 030 ----
mean loss: 131.99
 ---- batch: 040 ----
mean loss: 122.23
 ---- batch: 050 ----
mean loss: 128.94
 ---- batch: 060 ----
mean loss: 129.33
 ---- batch: 070 ----
mean loss: 127.83
 ---- batch: 080 ----
mean loss: 131.71
 ---- batch: 090 ----
mean loss: 132.52
 ---- batch: 100 ----
mean loss: 134.39
 ---- batch: 110 ----
mean loss: 137.08
train mean loss: 130.03
epoch train time: 0:00:01.922922
elapsed time: 0:06:04.859021
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 23:00:52.608190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.75
 ---- batch: 020 ----
mean loss: 126.41
 ---- batch: 030 ----
mean loss: 135.49
 ---- batch: 040 ----
mean loss: 131.13
 ---- batch: 050 ----
mean loss: 122.33
 ---- batch: 060 ----
mean loss: 125.72
 ---- batch: 070 ----
mean loss: 134.47
 ---- batch: 080 ----
mean loss: 126.45
 ---- batch: 090 ----
mean loss: 137.46
 ---- batch: 100 ----
mean loss: 122.87
 ---- batch: 110 ----
mean loss: 137.35
train mean loss: 130.13
epoch train time: 0:00:01.957460
elapsed time: 0:06:06.817287
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 23:00:54.566232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.33
 ---- batch: 020 ----
mean loss: 130.82
 ---- batch: 030 ----
mean loss: 126.29
 ---- batch: 040 ----
mean loss: 131.97
 ---- batch: 050 ----
mean loss: 131.91
 ---- batch: 060 ----
mean loss: 131.60
 ---- batch: 070 ----
mean loss: 131.67
 ---- batch: 080 ----
mean loss: 127.89
 ---- batch: 090 ----
mean loss: 136.87
 ---- batch: 100 ----
mean loss: 132.38
 ---- batch: 110 ----
mean loss: 132.97
train mean loss: 130.30
epoch train time: 0:00:01.956089
elapsed time: 0:06:08.773731
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 23:00:56.522905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.92
 ---- batch: 020 ----
mean loss: 122.93
 ---- batch: 030 ----
mean loss: 129.85
 ---- batch: 040 ----
mean loss: 129.22
 ---- batch: 050 ----
mean loss: 127.93
 ---- batch: 060 ----
mean loss: 134.53
 ---- batch: 070 ----
mean loss: 135.12
 ---- batch: 080 ----
mean loss: 129.68
 ---- batch: 090 ----
mean loss: 129.16
 ---- batch: 100 ----
mean loss: 129.09
 ---- batch: 110 ----
mean loss: 127.28
train mean loss: 129.76
epoch train time: 0:00:01.913493
elapsed time: 0:06:10.687964
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 23:00:58.437161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.96
 ---- batch: 020 ----
mean loss: 128.44
 ---- batch: 030 ----
mean loss: 121.20
 ---- batch: 040 ----
mean loss: 140.80
 ---- batch: 050 ----
mean loss: 128.76
 ---- batch: 060 ----
mean loss: 127.02
 ---- batch: 070 ----
mean loss: 125.08
 ---- batch: 080 ----
mean loss: 132.58
 ---- batch: 090 ----
mean loss: 121.28
 ---- batch: 100 ----
mean loss: 131.87
 ---- batch: 110 ----
mean loss: 129.84
train mean loss: 128.59
epoch train time: 0:00:01.906304
elapsed time: 0:06:12.594856
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 23:01:00.344069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.05
 ---- batch: 020 ----
mean loss: 132.62
 ---- batch: 030 ----
mean loss: 131.01
 ---- batch: 040 ----
mean loss: 123.77
 ---- batch: 050 ----
mean loss: 128.36
 ---- batch: 060 ----
mean loss: 122.86
 ---- batch: 070 ----
mean loss: 121.92
 ---- batch: 080 ----
mean loss: 130.37
 ---- batch: 090 ----
mean loss: 132.41
 ---- batch: 100 ----
mean loss: 130.59
 ---- batch: 110 ----
mean loss: 128.49
train mean loss: 128.58
epoch train time: 0:00:01.922287
elapsed time: 0:06:14.517754
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 23:01:02.267026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.29
 ---- batch: 020 ----
mean loss: 132.61
 ---- batch: 030 ----
mean loss: 130.53
 ---- batch: 040 ----
mean loss: 125.17
 ---- batch: 050 ----
mean loss: 127.92
 ---- batch: 060 ----
mean loss: 132.98
 ---- batch: 070 ----
mean loss: 123.07
 ---- batch: 080 ----
mean loss: 125.48
 ---- batch: 090 ----
mean loss: 131.44
 ---- batch: 100 ----
mean loss: 128.57
 ---- batch: 110 ----
mean loss: 133.45
train mean loss: 129.01
epoch train time: 0:00:01.988187
elapsed time: 0:06:16.506621
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 23:01:04.255804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.27
 ---- batch: 020 ----
mean loss: 125.04
 ---- batch: 030 ----
mean loss: 124.36
 ---- batch: 040 ----
mean loss: 133.21
 ---- batch: 050 ----
mean loss: 129.10
 ---- batch: 060 ----
mean loss: 124.82
 ---- batch: 070 ----
mean loss: 129.95
 ---- batch: 080 ----
mean loss: 130.60
 ---- batch: 090 ----
mean loss: 125.64
 ---- batch: 100 ----
mean loss: 128.67
 ---- batch: 110 ----
mean loss: 136.02
train mean loss: 128.12
epoch train time: 0:00:01.961584
elapsed time: 0:06:18.468759
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 23:01:06.217979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.31
 ---- batch: 020 ----
mean loss: 129.63
 ---- batch: 030 ----
mean loss: 118.85
 ---- batch: 040 ----
mean loss: 122.93
 ---- batch: 050 ----
mean loss: 136.01
 ---- batch: 060 ----
mean loss: 132.25
 ---- batch: 070 ----
mean loss: 132.48
 ---- batch: 080 ----
mean loss: 130.99
 ---- batch: 090 ----
mean loss: 126.47
 ---- batch: 100 ----
mean loss: 130.78
 ---- batch: 110 ----
mean loss: 129.09
train mean loss: 128.08
epoch train time: 0:00:02.001441
elapsed time: 0:06:20.470825
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 23:01:08.220070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.88
 ---- batch: 020 ----
mean loss: 127.58
 ---- batch: 030 ----
mean loss: 133.79
 ---- batch: 040 ----
mean loss: 119.57
 ---- batch: 050 ----
mean loss: 133.40
 ---- batch: 060 ----
mean loss: 120.47
 ---- batch: 070 ----
mean loss: 123.18
 ---- batch: 080 ----
mean loss: 129.67
 ---- batch: 090 ----
mean loss: 131.50
 ---- batch: 100 ----
mean loss: 133.67
 ---- batch: 110 ----
mean loss: 131.86
train mean loss: 128.14
epoch train time: 0:00:02.070044
elapsed time: 0:06:22.541540
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 23:01:10.290779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.96
 ---- batch: 020 ----
mean loss: 131.51
 ---- batch: 030 ----
mean loss: 124.52
 ---- batch: 040 ----
mean loss: 126.65
 ---- batch: 050 ----
mean loss: 128.45
 ---- batch: 060 ----
mean loss: 130.89
 ---- batch: 070 ----
mean loss: 125.13
 ---- batch: 080 ----
mean loss: 125.75
 ---- batch: 090 ----
mean loss: 129.92
 ---- batch: 100 ----
mean loss: 130.93
 ---- batch: 110 ----
mean loss: 123.68
train mean loss: 127.74
epoch train time: 0:00:01.999971
elapsed time: 0:06:24.542157
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 23:01:12.291370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.02
 ---- batch: 020 ----
mean loss: 126.12
 ---- batch: 030 ----
mean loss: 116.28
 ---- batch: 040 ----
mean loss: 125.11
 ---- batch: 050 ----
mean loss: 119.97
 ---- batch: 060 ----
mean loss: 121.18
 ---- batch: 070 ----
mean loss: 130.99
 ---- batch: 080 ----
mean loss: 133.13
 ---- batch: 090 ----
mean loss: 136.44
 ---- batch: 100 ----
mean loss: 130.01
 ---- batch: 110 ----
mean loss: 128.52
train mean loss: 126.76
epoch train time: 0:00:01.980222
elapsed time: 0:06:26.522984
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 23:01:14.272282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.60
 ---- batch: 020 ----
mean loss: 126.70
 ---- batch: 030 ----
mean loss: 124.76
 ---- batch: 040 ----
mean loss: 121.26
 ---- batch: 050 ----
mean loss: 131.47
 ---- batch: 060 ----
mean loss: 129.07
 ---- batch: 070 ----
mean loss: 126.84
 ---- batch: 080 ----
mean loss: 128.18
 ---- batch: 090 ----
mean loss: 119.50
 ---- batch: 100 ----
mean loss: 132.71
 ---- batch: 110 ----
mean loss: 135.52
train mean loss: 126.90
epoch train time: 0:00:01.967015
elapsed time: 0:06:28.490699
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 23:01:16.239876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.16
 ---- batch: 020 ----
mean loss: 120.54
 ---- batch: 030 ----
mean loss: 129.95
 ---- batch: 040 ----
mean loss: 127.63
 ---- batch: 050 ----
mean loss: 128.65
 ---- batch: 060 ----
mean loss: 129.27
 ---- batch: 070 ----
mean loss: 128.11
 ---- batch: 080 ----
mean loss: 121.74
 ---- batch: 090 ----
mean loss: 122.96
 ---- batch: 100 ----
mean loss: 128.54
 ---- batch: 110 ----
mean loss: 132.82
train mean loss: 126.73
epoch train time: 0:00:01.979814
elapsed time: 0:06:30.471100
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 23:01:18.220332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.75
 ---- batch: 020 ----
mean loss: 131.22
 ---- batch: 030 ----
mean loss: 131.04
 ---- batch: 040 ----
mean loss: 121.53
 ---- batch: 050 ----
mean loss: 124.58
 ---- batch: 060 ----
mean loss: 128.44
 ---- batch: 070 ----
mean loss: 125.58
 ---- batch: 080 ----
mean loss: 128.83
 ---- batch: 090 ----
mean loss: 123.42
 ---- batch: 100 ----
mean loss: 123.19
 ---- batch: 110 ----
mean loss: 128.99
train mean loss: 126.62
epoch train time: 0:00:01.997546
elapsed time: 0:06:32.469290
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 23:01:20.218568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.68
 ---- batch: 020 ----
mean loss: 120.82
 ---- batch: 030 ----
mean loss: 123.82
 ---- batch: 040 ----
mean loss: 129.01
 ---- batch: 050 ----
mean loss: 126.80
 ---- batch: 060 ----
mean loss: 128.77
 ---- batch: 070 ----
mean loss: 122.50
 ---- batch: 080 ----
mean loss: 120.46
 ---- batch: 090 ----
mean loss: 132.41
 ---- batch: 100 ----
mean loss: 128.22
 ---- batch: 110 ----
mean loss: 126.41
train mean loss: 125.45
epoch train time: 0:00:01.962720
elapsed time: 0:06:34.432723
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 23:01:22.182065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.74
 ---- batch: 020 ----
mean loss: 129.15
 ---- batch: 030 ----
mean loss: 128.99
 ---- batch: 040 ----
mean loss: 114.82
 ---- batch: 050 ----
mean loss: 125.65
 ---- batch: 060 ----
mean loss: 121.77
 ---- batch: 070 ----
mean loss: 122.67
 ---- batch: 080 ----
mean loss: 128.89
 ---- batch: 090 ----
mean loss: 131.62
 ---- batch: 100 ----
mean loss: 128.24
 ---- batch: 110 ----
mean loss: 123.78
train mean loss: 125.45
epoch train time: 0:00:01.991238
elapsed time: 0:06:36.424697
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 23:01:24.174014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.26
 ---- batch: 020 ----
mean loss: 123.08
 ---- batch: 030 ----
mean loss: 135.84
 ---- batch: 040 ----
mean loss: 125.77
 ---- batch: 050 ----
mean loss: 118.16
 ---- batch: 060 ----
mean loss: 125.15
 ---- batch: 070 ----
mean loss: 120.96
 ---- batch: 080 ----
mean loss: 128.01
 ---- batch: 090 ----
mean loss: 128.56
 ---- batch: 100 ----
mean loss: 121.95
 ---- batch: 110 ----
mean loss: 122.78
train mean loss: 125.34
epoch train time: 0:00:01.975556
elapsed time: 0:06:38.400982
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 23:01:26.150183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.63
 ---- batch: 020 ----
mean loss: 115.05
 ---- batch: 030 ----
mean loss: 124.52
 ---- batch: 040 ----
mean loss: 118.51
 ---- batch: 050 ----
mean loss: 123.07
 ---- batch: 060 ----
mean loss: 130.11
 ---- batch: 070 ----
mean loss: 132.61
 ---- batch: 080 ----
mean loss: 129.03
 ---- batch: 090 ----
mean loss: 121.56
 ---- batch: 100 ----
mean loss: 130.63
 ---- batch: 110 ----
mean loss: 130.37
train mean loss: 124.96
epoch train time: 0:00:01.938316
elapsed time: 0:06:40.339907
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 23:01:28.089107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.77
 ---- batch: 020 ----
mean loss: 116.38
 ---- batch: 030 ----
mean loss: 123.20
 ---- batch: 040 ----
mean loss: 126.51
 ---- batch: 050 ----
mean loss: 123.96
 ---- batch: 060 ----
mean loss: 126.68
 ---- batch: 070 ----
mean loss: 124.86
 ---- batch: 080 ----
mean loss: 125.18
 ---- batch: 090 ----
mean loss: 129.99
 ---- batch: 100 ----
mean loss: 129.40
 ---- batch: 110 ----
mean loss: 123.26
train mean loss: 124.75
epoch train time: 0:00:01.941122
elapsed time: 0:06:42.281627
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 23:01:30.030841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.36
 ---- batch: 020 ----
mean loss: 125.67
 ---- batch: 030 ----
mean loss: 129.93
 ---- batch: 040 ----
mean loss: 123.44
 ---- batch: 050 ----
mean loss: 124.79
 ---- batch: 060 ----
mean loss: 127.82
 ---- batch: 070 ----
mean loss: 116.41
 ---- batch: 080 ----
mean loss: 119.86
 ---- batch: 090 ----
mean loss: 123.08
 ---- batch: 100 ----
mean loss: 124.76
 ---- batch: 110 ----
mean loss: 129.33
train mean loss: 125.03
epoch train time: 0:00:01.987211
elapsed time: 0:06:44.269432
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 23:01:32.018614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.51
 ---- batch: 020 ----
mean loss: 123.12
 ---- batch: 030 ----
mean loss: 123.61
 ---- batch: 040 ----
mean loss: 123.88
 ---- batch: 050 ----
mean loss: 122.84
 ---- batch: 060 ----
mean loss: 119.38
 ---- batch: 070 ----
mean loss: 122.10
 ---- batch: 080 ----
mean loss: 120.76
 ---- batch: 090 ----
mean loss: 117.92
 ---- batch: 100 ----
mean loss: 134.86
 ---- batch: 110 ----
mean loss: 132.99
train mean loss: 124.08
epoch train time: 0:00:01.950727
elapsed time: 0:06:46.220740
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 23:01:33.969962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.52
 ---- batch: 020 ----
mean loss: 132.26
 ---- batch: 030 ----
mean loss: 130.10
 ---- batch: 040 ----
mean loss: 123.96
 ---- batch: 050 ----
mean loss: 119.98
 ---- batch: 060 ----
mean loss: 116.38
 ---- batch: 070 ----
mean loss: 130.70
 ---- batch: 080 ----
mean loss: 120.14
 ---- batch: 090 ----
mean loss: 121.03
 ---- batch: 100 ----
mean loss: 126.36
 ---- batch: 110 ----
mean loss: 121.03
train mean loss: 124.19
epoch train time: 0:00:01.976031
elapsed time: 0:06:48.197392
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 23:01:35.946615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.08
 ---- batch: 020 ----
mean loss: 124.40
 ---- batch: 030 ----
mean loss: 132.51
 ---- batch: 040 ----
mean loss: 130.59
 ---- batch: 050 ----
mean loss: 118.20
 ---- batch: 060 ----
mean loss: 120.91
 ---- batch: 070 ----
mean loss: 123.93
 ---- batch: 080 ----
mean loss: 123.23
 ---- batch: 090 ----
mean loss: 130.56
 ---- batch: 100 ----
mean loss: 123.35
 ---- batch: 110 ----
mean loss: 128.46
train mean loss: 125.11
epoch train time: 0:00:01.993783
elapsed time: 0:06:50.191786
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 23:01:37.940990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.76
 ---- batch: 020 ----
mean loss: 117.81
 ---- batch: 030 ----
mean loss: 119.83
 ---- batch: 040 ----
mean loss: 130.40
 ---- batch: 050 ----
mean loss: 115.28
 ---- batch: 060 ----
mean loss: 120.69
 ---- batch: 070 ----
mean loss: 127.04
 ---- batch: 080 ----
mean loss: 124.55
 ---- batch: 090 ----
mean loss: 125.68
 ---- batch: 100 ----
mean loss: 119.25
 ---- batch: 110 ----
mean loss: 132.57
train mean loss: 123.68
epoch train time: 0:00:01.967738
elapsed time: 0:06:52.160182
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 23:01:39.909400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.69
 ---- batch: 020 ----
mean loss: 121.28
 ---- batch: 030 ----
mean loss: 122.19
 ---- batch: 040 ----
mean loss: 119.12
 ---- batch: 050 ----
mean loss: 125.31
 ---- batch: 060 ----
mean loss: 114.54
 ---- batch: 070 ----
mean loss: 133.58
 ---- batch: 080 ----
mean loss: 127.57
 ---- batch: 090 ----
mean loss: 126.60
 ---- batch: 100 ----
mean loss: 126.89
 ---- batch: 110 ----
mean loss: 121.36
train mean loss: 123.33
epoch train time: 0:00:01.979491
elapsed time: 0:06:54.140321
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 23:01:41.889533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.86
 ---- batch: 020 ----
mean loss: 117.25
 ---- batch: 030 ----
mean loss: 123.12
 ---- batch: 040 ----
mean loss: 121.21
 ---- batch: 050 ----
mean loss: 121.16
 ---- batch: 060 ----
mean loss: 127.46
 ---- batch: 070 ----
mean loss: 118.68
 ---- batch: 080 ----
mean loss: 127.28
 ---- batch: 090 ----
mean loss: 116.62
 ---- batch: 100 ----
mean loss: 121.24
 ---- batch: 110 ----
mean loss: 127.85
train mean loss: 122.69
epoch train time: 0:00:01.969263
elapsed time: 0:06:56.110298
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 23:01:43.859642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.06
 ---- batch: 020 ----
mean loss: 115.29
 ---- batch: 030 ----
mean loss: 126.52
 ---- batch: 040 ----
mean loss: 119.12
 ---- batch: 050 ----
mean loss: 119.99
 ---- batch: 060 ----
mean loss: 128.15
 ---- batch: 070 ----
mean loss: 124.69
 ---- batch: 080 ----
mean loss: 123.41
 ---- batch: 090 ----
mean loss: 119.15
 ---- batch: 100 ----
mean loss: 121.38
 ---- batch: 110 ----
mean loss: 127.07
train mean loss: 122.56
epoch train time: 0:00:01.974976
elapsed time: 0:06:58.086045
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 23:01:45.835310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.40
 ---- batch: 020 ----
mean loss: 112.77
 ---- batch: 030 ----
mean loss: 120.47
 ---- batch: 040 ----
mean loss: 117.49
 ---- batch: 050 ----
mean loss: 123.32
 ---- batch: 060 ----
mean loss: 125.48
 ---- batch: 070 ----
mean loss: 123.92
 ---- batch: 080 ----
mean loss: 119.92
 ---- batch: 090 ----
mean loss: 125.88
 ---- batch: 100 ----
mean loss: 125.76
 ---- batch: 110 ----
mean loss: 131.49
train mean loss: 122.91
epoch train time: 0:00:02.011470
elapsed time: 0:07:00.098222
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 23:01:47.847431
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.42
 ---- batch: 020 ----
mean loss: 114.76
 ---- batch: 030 ----
mean loss: 125.56
 ---- batch: 040 ----
mean loss: 116.99
 ---- batch: 050 ----
mean loss: 115.92
 ---- batch: 060 ----
mean loss: 118.92
 ---- batch: 070 ----
mean loss: 110.28
 ---- batch: 080 ----
mean loss: 119.05
 ---- batch: 090 ----
mean loss: 121.57
 ---- batch: 100 ----
mean loss: 118.03
 ---- batch: 110 ----
mean loss: 115.88
train mean loss: 117.68
epoch train time: 0:00:01.988413
elapsed time: 0:07:02.087724
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 23:01:49.836603
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.53
 ---- batch: 020 ----
mean loss: 115.45
 ---- batch: 030 ----
mean loss: 112.85
 ---- batch: 040 ----
mean loss: 120.72
 ---- batch: 050 ----
mean loss: 119.57
 ---- batch: 060 ----
mean loss: 118.35
 ---- batch: 070 ----
mean loss: 110.22
 ---- batch: 080 ----
mean loss: 124.38
 ---- batch: 090 ----
mean loss: 118.19
 ---- batch: 100 ----
mean loss: 116.60
 ---- batch: 110 ----
mean loss: 111.77
train mean loss: 117.05
epoch train time: 0:00:02.003431
elapsed time: 0:07:04.091446
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 23:01:51.840649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.44
 ---- batch: 020 ----
mean loss: 111.42
 ---- batch: 030 ----
mean loss: 121.68
 ---- batch: 040 ----
mean loss: 114.90
 ---- batch: 050 ----
mean loss: 119.74
 ---- batch: 060 ----
mean loss: 119.51
 ---- batch: 070 ----
mean loss: 118.38
 ---- batch: 080 ----
mean loss: 119.31
 ---- batch: 090 ----
mean loss: 117.22
 ---- batch: 100 ----
mean loss: 116.22
 ---- batch: 110 ----
mean loss: 115.63
train mean loss: 116.82
epoch train time: 0:00:01.981863
elapsed time: 0:07:06.073951
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 23:01:53.822889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.42
 ---- batch: 020 ----
mean loss: 114.41
 ---- batch: 030 ----
mean loss: 112.22
 ---- batch: 040 ----
mean loss: 114.04
 ---- batch: 050 ----
mean loss: 120.34
 ---- batch: 060 ----
mean loss: 116.89
 ---- batch: 070 ----
mean loss: 123.36
 ---- batch: 080 ----
mean loss: 118.86
 ---- batch: 090 ----
mean loss: 121.11
 ---- batch: 100 ----
mean loss: 108.23
 ---- batch: 110 ----
mean loss: 113.37
train mean loss: 116.73
epoch train time: 0:00:01.988981
elapsed time: 0:07:08.063276
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 23:01:55.812491
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.02
 ---- batch: 020 ----
mean loss: 115.62
 ---- batch: 030 ----
mean loss: 117.72
 ---- batch: 040 ----
mean loss: 113.97
 ---- batch: 050 ----
mean loss: 117.23
 ---- batch: 060 ----
mean loss: 120.16
 ---- batch: 070 ----
mean loss: 117.43
 ---- batch: 080 ----
mean loss: 123.09
 ---- batch: 090 ----
mean loss: 105.52
 ---- batch: 100 ----
mean loss: 114.33
 ---- batch: 110 ----
mean loss: 119.30
train mean loss: 116.79
epoch train time: 0:00:01.973078
elapsed time: 0:07:10.036994
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 23:01:57.786190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.34
 ---- batch: 020 ----
mean loss: 111.08
 ---- batch: 030 ----
mean loss: 116.59
 ---- batch: 040 ----
mean loss: 121.60
 ---- batch: 050 ----
mean loss: 110.38
 ---- batch: 060 ----
mean loss: 117.16
 ---- batch: 070 ----
mean loss: 117.47
 ---- batch: 080 ----
mean loss: 119.44
 ---- batch: 090 ----
mean loss: 118.51
 ---- batch: 100 ----
mean loss: 108.79
 ---- batch: 110 ----
mean loss: 120.53
train mean loss: 116.67
epoch train time: 0:00:02.019773
elapsed time: 0:07:12.057478
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 23:01:59.806757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.89
 ---- batch: 020 ----
mean loss: 118.02
 ---- batch: 030 ----
mean loss: 117.84
 ---- batch: 040 ----
mean loss: 126.27
 ---- batch: 050 ----
mean loss: 114.59
 ---- batch: 060 ----
mean loss: 117.26
 ---- batch: 070 ----
mean loss: 111.21
 ---- batch: 080 ----
mean loss: 120.19
 ---- batch: 090 ----
mean loss: 115.60
 ---- batch: 100 ----
mean loss: 118.76
 ---- batch: 110 ----
mean loss: 113.50
train mean loss: 116.65
epoch train time: 0:00:01.969300
elapsed time: 0:07:14.027442
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 23:02:01.776675
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.22
 ---- batch: 020 ----
mean loss: 111.34
 ---- batch: 030 ----
mean loss: 113.48
 ---- batch: 040 ----
mean loss: 116.41
 ---- batch: 050 ----
mean loss: 117.14
 ---- batch: 060 ----
mean loss: 122.93
 ---- batch: 070 ----
mean loss: 120.29
 ---- batch: 080 ----
mean loss: 117.04
 ---- batch: 090 ----
mean loss: 118.08
 ---- batch: 100 ----
mean loss: 112.93
 ---- batch: 110 ----
mean loss: 120.00
train mean loss: 116.59
epoch train time: 0:00:01.975301
elapsed time: 0:07:16.003387
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 23:02:03.752631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.91
 ---- batch: 020 ----
mean loss: 109.71
 ---- batch: 030 ----
mean loss: 118.60
 ---- batch: 040 ----
mean loss: 112.28
 ---- batch: 050 ----
mean loss: 114.07
 ---- batch: 060 ----
mean loss: 121.90
 ---- batch: 070 ----
mean loss: 117.60
 ---- batch: 080 ----
mean loss: 110.64
 ---- batch: 090 ----
mean loss: 123.50
 ---- batch: 100 ----
mean loss: 114.51
 ---- batch: 110 ----
mean loss: 126.26
train mean loss: 116.51
epoch train time: 0:00:02.002199
elapsed time: 0:07:18.006228
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 23:02:05.755450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.69
 ---- batch: 020 ----
mean loss: 116.84
 ---- batch: 030 ----
mean loss: 114.93
 ---- batch: 040 ----
mean loss: 120.70
 ---- batch: 050 ----
mean loss: 120.62
 ---- batch: 060 ----
mean loss: 116.36
 ---- batch: 070 ----
mean loss: 113.28
 ---- batch: 080 ----
mean loss: 111.94
 ---- batch: 090 ----
mean loss: 113.59
 ---- batch: 100 ----
mean loss: 113.95
 ---- batch: 110 ----
mean loss: 113.97
train mean loss: 116.42
epoch train time: 0:00:01.990611
elapsed time: 0:07:19.997474
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 23:02:07.746749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.07
 ---- batch: 020 ----
mean loss: 118.04
 ---- batch: 030 ----
mean loss: 121.05
 ---- batch: 040 ----
mean loss: 113.26
 ---- batch: 050 ----
mean loss: 119.86
 ---- batch: 060 ----
mean loss: 115.96
 ---- batch: 070 ----
mean loss: 111.55
 ---- batch: 080 ----
mean loss: 113.42
 ---- batch: 090 ----
mean loss: 123.83
 ---- batch: 100 ----
mean loss: 111.72
 ---- batch: 110 ----
mean loss: 116.92
train mean loss: 116.52
epoch train time: 0:00:01.975946
elapsed time: 0:07:21.974106
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 23:02:09.723327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.99
 ---- batch: 020 ----
mean loss: 110.11
 ---- batch: 030 ----
mean loss: 117.69
 ---- batch: 040 ----
mean loss: 120.15
 ---- batch: 050 ----
mean loss: 119.21
 ---- batch: 060 ----
mean loss: 116.58
 ---- batch: 070 ----
mean loss: 121.02
 ---- batch: 080 ----
mean loss: 116.25
 ---- batch: 090 ----
mean loss: 114.91
 ---- batch: 100 ----
mean loss: 118.20
 ---- batch: 110 ----
mean loss: 113.26
train mean loss: 116.39
epoch train time: 0:00:01.951333
elapsed time: 0:07:23.926168
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 23:02:11.675419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.19
 ---- batch: 020 ----
mean loss: 113.62
 ---- batch: 030 ----
mean loss: 108.36
 ---- batch: 040 ----
mean loss: 126.73
 ---- batch: 050 ----
mean loss: 118.03
 ---- batch: 060 ----
mean loss: 119.83
 ---- batch: 070 ----
mean loss: 111.75
 ---- batch: 080 ----
mean loss: 110.09
 ---- batch: 090 ----
mean loss: 108.81
 ---- batch: 100 ----
mean loss: 121.43
 ---- batch: 110 ----
mean loss: 121.62
train mean loss: 116.35
epoch train time: 0:00:02.009811
elapsed time: 0:07:25.936660
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 23:02:13.685967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.32
 ---- batch: 020 ----
mean loss: 112.09
 ---- batch: 030 ----
mean loss: 112.82
 ---- batch: 040 ----
mean loss: 112.28
 ---- batch: 050 ----
mean loss: 117.86
 ---- batch: 060 ----
mean loss: 113.38
 ---- batch: 070 ----
mean loss: 123.04
 ---- batch: 080 ----
mean loss: 116.65
 ---- batch: 090 ----
mean loss: 117.06
 ---- batch: 100 ----
mean loss: 120.06
 ---- batch: 110 ----
mean loss: 121.05
train mean loss: 116.40
epoch train time: 0:00:01.965024
elapsed time: 0:07:27.902424
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 23:02:15.651656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.46
 ---- batch: 020 ----
mean loss: 116.58
 ---- batch: 030 ----
mean loss: 117.92
 ---- batch: 040 ----
mean loss: 123.04
 ---- batch: 050 ----
mean loss: 113.47
 ---- batch: 060 ----
mean loss: 113.47
 ---- batch: 070 ----
mean loss: 120.41
 ---- batch: 080 ----
mean loss: 112.54
 ---- batch: 090 ----
mean loss: 106.34
 ---- batch: 100 ----
mean loss: 117.75
 ---- batch: 110 ----
mean loss: 116.94
train mean loss: 116.33
epoch train time: 0:00:01.968611
elapsed time: 0:07:29.871655
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 23:02:17.620860
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.20
 ---- batch: 020 ----
mean loss: 115.39
 ---- batch: 030 ----
mean loss: 116.51
 ---- batch: 040 ----
mean loss: 114.29
 ---- batch: 050 ----
mean loss: 112.30
 ---- batch: 060 ----
mean loss: 116.82
 ---- batch: 070 ----
mean loss: 114.36
 ---- batch: 080 ----
mean loss: 118.10
 ---- batch: 090 ----
mean loss: 118.30
 ---- batch: 100 ----
mean loss: 117.42
 ---- batch: 110 ----
mean loss: 113.97
train mean loss: 116.23
epoch train time: 0:00:01.966557
elapsed time: 0:07:31.838863
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 23:02:19.588154
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.62
 ---- batch: 020 ----
mean loss: 114.39
 ---- batch: 030 ----
mean loss: 116.61
 ---- batch: 040 ----
mean loss: 113.99
 ---- batch: 050 ----
mean loss: 114.19
 ---- batch: 060 ----
mean loss: 116.25
 ---- batch: 070 ----
mean loss: 117.17
 ---- batch: 080 ----
mean loss: 118.33
 ---- batch: 090 ----
mean loss: 116.39
 ---- batch: 100 ----
mean loss: 122.14
 ---- batch: 110 ----
mean loss: 115.35
train mean loss: 116.15
epoch train time: 0:00:01.983120
elapsed time: 0:07:33.822681
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 23:02:21.571872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.40
 ---- batch: 020 ----
mean loss: 116.35
 ---- batch: 030 ----
mean loss: 115.78
 ---- batch: 040 ----
mean loss: 114.29
 ---- batch: 050 ----
mean loss: 114.79
 ---- batch: 060 ----
mean loss: 119.14
 ---- batch: 070 ----
mean loss: 113.77
 ---- batch: 080 ----
mean loss: 114.02
 ---- batch: 090 ----
mean loss: 116.92
 ---- batch: 100 ----
mean loss: 124.87
 ---- batch: 110 ----
mean loss: 114.60
train mean loss: 116.19
epoch train time: 0:00:01.969042
elapsed time: 0:07:35.792320
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 23:02:23.541528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.89
 ---- batch: 020 ----
mean loss: 114.45
 ---- batch: 030 ----
mean loss: 111.07
 ---- batch: 040 ----
mean loss: 111.86
 ---- batch: 050 ----
mean loss: 118.00
 ---- batch: 060 ----
mean loss: 107.52
 ---- batch: 070 ----
mean loss: 112.70
 ---- batch: 080 ----
mean loss: 121.10
 ---- batch: 090 ----
mean loss: 118.31
 ---- batch: 100 ----
mean loss: 119.32
 ---- batch: 110 ----
mean loss: 123.65
train mean loss: 116.22
epoch train time: 0:00:02.023681
elapsed time: 0:07:37.816672
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 23:02:25.566120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.34
 ---- batch: 020 ----
mean loss: 114.89
 ---- batch: 030 ----
mean loss: 121.02
 ---- batch: 040 ----
mean loss: 117.74
 ---- batch: 050 ----
mean loss: 121.86
 ---- batch: 060 ----
mean loss: 112.12
 ---- batch: 070 ----
mean loss: 112.49
 ---- batch: 080 ----
mean loss: 115.60
 ---- batch: 090 ----
mean loss: 114.87
 ---- batch: 100 ----
mean loss: 121.84
 ---- batch: 110 ----
mean loss: 118.52
train mean loss: 116.11
epoch train time: 0:00:01.981186
elapsed time: 0:07:39.798706
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 23:02:27.547906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.18
 ---- batch: 020 ----
mean loss: 111.39
 ---- batch: 030 ----
mean loss: 120.41
 ---- batch: 040 ----
mean loss: 120.34
 ---- batch: 050 ----
mean loss: 116.17
 ---- batch: 060 ----
mean loss: 113.55
 ---- batch: 070 ----
mean loss: 119.69
 ---- batch: 080 ----
mean loss: 122.22
 ---- batch: 090 ----
mean loss: 111.47
 ---- batch: 100 ----
mean loss: 113.09
 ---- batch: 110 ----
mean loss: 117.97
train mean loss: 116.04
epoch train time: 0:00:01.973988
elapsed time: 0:07:41.773416
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 23:02:29.522643
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.15
 ---- batch: 020 ----
mean loss: 119.13
 ---- batch: 030 ----
mean loss: 120.35
 ---- batch: 040 ----
mean loss: 111.61
 ---- batch: 050 ----
mean loss: 115.02
 ---- batch: 060 ----
mean loss: 117.73
 ---- batch: 070 ----
mean loss: 111.85
 ---- batch: 080 ----
mean loss: 110.54
 ---- batch: 090 ----
mean loss: 115.18
 ---- batch: 100 ----
mean loss: 116.59
 ---- batch: 110 ----
mean loss: 117.91
train mean loss: 116.10
epoch train time: 0:00:01.960385
elapsed time: 0:07:43.734432
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 23:02:31.483662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.16
 ---- batch: 020 ----
mean loss: 116.15
 ---- batch: 030 ----
mean loss: 111.46
 ---- batch: 040 ----
mean loss: 114.81
 ---- batch: 050 ----
mean loss: 120.39
 ---- batch: 060 ----
mean loss: 118.07
 ---- batch: 070 ----
mean loss: 116.58
 ---- batch: 080 ----
mean loss: 116.01
 ---- batch: 090 ----
mean loss: 122.41
 ---- batch: 100 ----
mean loss: 115.94
 ---- batch: 110 ----
mean loss: 110.99
train mean loss: 116.19
epoch train time: 0:00:01.979298
elapsed time: 0:07:45.714396
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 23:02:33.463592
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.15
 ---- batch: 020 ----
mean loss: 117.62
 ---- batch: 030 ----
mean loss: 113.66
 ---- batch: 040 ----
mean loss: 111.72
 ---- batch: 050 ----
mean loss: 115.87
 ---- batch: 060 ----
mean loss: 116.95
 ---- batch: 070 ----
mean loss: 116.10
 ---- batch: 080 ----
mean loss: 124.55
 ---- batch: 090 ----
mean loss: 116.89
 ---- batch: 100 ----
mean loss: 110.04
 ---- batch: 110 ----
mean loss: 119.04
train mean loss: 115.97
epoch train time: 0:00:01.997975
elapsed time: 0:07:47.713025
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 23:02:35.462255
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.23
 ---- batch: 020 ----
mean loss: 109.43
 ---- batch: 030 ----
mean loss: 118.20
 ---- batch: 040 ----
mean loss: 121.20
 ---- batch: 050 ----
mean loss: 109.32
 ---- batch: 060 ----
mean loss: 119.49
 ---- batch: 070 ----
mean loss: 113.05
 ---- batch: 080 ----
mean loss: 109.12
 ---- batch: 090 ----
mean loss: 119.44
 ---- batch: 100 ----
mean loss: 122.86
 ---- batch: 110 ----
mean loss: 116.57
train mean loss: 116.04
epoch train time: 0:00:01.970161
elapsed time: 0:07:49.683801
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 23:02:37.432991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.34
 ---- batch: 020 ----
mean loss: 122.05
 ---- batch: 030 ----
mean loss: 120.90
 ---- batch: 040 ----
mean loss: 113.64
 ---- batch: 050 ----
mean loss: 117.22
 ---- batch: 060 ----
mean loss: 109.51
 ---- batch: 070 ----
mean loss: 124.06
 ---- batch: 080 ----
mean loss: 110.72
 ---- batch: 090 ----
mean loss: 119.13
 ---- batch: 100 ----
mean loss: 114.91
 ---- batch: 110 ----
mean loss: 114.51
train mean loss: 115.90
epoch train time: 0:00:01.959222
elapsed time: 0:07:51.643635
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 23:02:39.392812
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.87
 ---- batch: 020 ----
mean loss: 112.53
 ---- batch: 030 ----
mean loss: 117.10
 ---- batch: 040 ----
mean loss: 109.96
 ---- batch: 050 ----
mean loss: 110.04
 ---- batch: 060 ----
mean loss: 123.09
 ---- batch: 070 ----
mean loss: 123.16
 ---- batch: 080 ----
mean loss: 122.81
 ---- batch: 090 ----
mean loss: 115.86
 ---- batch: 100 ----
mean loss: 115.54
 ---- batch: 110 ----
mean loss: 116.36
train mean loss: 115.88
epoch train time: 0:00:02.004115
elapsed time: 0:07:53.648324
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 23:02:41.397561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.17
 ---- batch: 020 ----
mean loss: 114.17
 ---- batch: 030 ----
mean loss: 111.51
 ---- batch: 040 ----
mean loss: 119.09
 ---- batch: 050 ----
mean loss: 116.55
 ---- batch: 060 ----
mean loss: 115.65
 ---- batch: 070 ----
mean loss: 118.39
 ---- batch: 080 ----
mean loss: 117.63
 ---- batch: 090 ----
mean loss: 117.06
 ---- batch: 100 ----
mean loss: 112.45
 ---- batch: 110 ----
mean loss: 115.20
train mean loss: 115.86
epoch train time: 0:00:01.994712
elapsed time: 0:07:55.643673
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 23:02:43.392877
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.69
 ---- batch: 020 ----
mean loss: 117.91
 ---- batch: 030 ----
mean loss: 126.94
 ---- batch: 040 ----
mean loss: 120.86
 ---- batch: 050 ----
mean loss: 113.86
 ---- batch: 060 ----
mean loss: 120.68
 ---- batch: 070 ----
mean loss: 113.00
 ---- batch: 080 ----
mean loss: 107.50
 ---- batch: 090 ----
mean loss: 118.97
 ---- batch: 100 ----
mean loss: 115.20
 ---- batch: 110 ----
mean loss: 114.73
train mean loss: 115.82
epoch train time: 0:00:01.984300
elapsed time: 0:07:57.628587
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 23:02:45.377825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.80
 ---- batch: 020 ----
mean loss: 112.61
 ---- batch: 030 ----
mean loss: 110.02
 ---- batch: 040 ----
mean loss: 125.41
 ---- batch: 050 ----
mean loss: 113.53
 ---- batch: 060 ----
mean loss: 118.04
 ---- batch: 070 ----
mean loss: 111.71
 ---- batch: 080 ----
mean loss: 117.84
 ---- batch: 090 ----
mean loss: 114.44
 ---- batch: 100 ----
mean loss: 113.99
 ---- batch: 110 ----
mean loss: 118.44
train mean loss: 115.85
epoch train time: 0:00:01.999120
elapsed time: 0:07:59.628360
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 23:02:47.377554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.22
 ---- batch: 020 ----
mean loss: 110.90
 ---- batch: 030 ----
mean loss: 109.43
 ---- batch: 040 ----
mean loss: 123.71
 ---- batch: 050 ----
mean loss: 116.62
 ---- batch: 060 ----
mean loss: 110.95
 ---- batch: 070 ----
mean loss: 114.97
 ---- batch: 080 ----
mean loss: 114.69
 ---- batch: 090 ----
mean loss: 118.31
 ---- batch: 100 ----
mean loss: 126.08
 ---- batch: 110 ----
mean loss: 116.96
train mean loss: 115.72
epoch train time: 0:00:01.967675
elapsed time: 0:08:01.596679
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 23:02:49.345890
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.10
 ---- batch: 020 ----
mean loss: 113.39
 ---- batch: 030 ----
mean loss: 115.82
 ---- batch: 040 ----
mean loss: 117.33
 ---- batch: 050 ----
mean loss: 107.34
 ---- batch: 060 ----
mean loss: 122.86
 ---- batch: 070 ----
mean loss: 119.60
 ---- batch: 080 ----
mean loss: 116.66
 ---- batch: 090 ----
mean loss: 117.69
 ---- batch: 100 ----
mean loss: 111.33
 ---- batch: 110 ----
mean loss: 114.04
train mean loss: 115.70
epoch train time: 0:00:01.996576
elapsed time: 0:08:03.593847
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 23:02:51.343030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.20
 ---- batch: 020 ----
mean loss: 107.74
 ---- batch: 030 ----
mean loss: 112.94
 ---- batch: 040 ----
mean loss: 118.18
 ---- batch: 050 ----
mean loss: 109.52
 ---- batch: 060 ----
mean loss: 119.80
 ---- batch: 070 ----
mean loss: 118.22
 ---- batch: 080 ----
mean loss: 119.48
 ---- batch: 090 ----
mean loss: 114.92
 ---- batch: 100 ----
mean loss: 121.02
 ---- batch: 110 ----
mean loss: 117.22
train mean loss: 115.72
epoch train time: 0:00:01.961422
elapsed time: 0:08:05.556234
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 23:02:53.305236
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.85
 ---- batch: 020 ----
mean loss: 112.33
 ---- batch: 030 ----
mean loss: 110.06
 ---- batch: 040 ----
mean loss: 116.89
 ---- batch: 050 ----
mean loss: 117.68
 ---- batch: 060 ----
mean loss: 120.23
 ---- batch: 070 ----
mean loss: 111.94
 ---- batch: 080 ----
mean loss: 121.14
 ---- batch: 090 ----
mean loss: 118.64
 ---- batch: 100 ----
mean loss: 115.65
 ---- batch: 110 ----
mean loss: 116.66
train mean loss: 115.70
epoch train time: 0:00:01.961865
elapsed time: 0:08:07.518512
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 23:02:55.267774
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.09
 ---- batch: 020 ----
mean loss: 110.65
 ---- batch: 030 ----
mean loss: 107.21
 ---- batch: 040 ----
mean loss: 124.47
 ---- batch: 050 ----
mean loss: 113.44
 ---- batch: 060 ----
mean loss: 119.75
 ---- batch: 070 ----
mean loss: 116.81
 ---- batch: 080 ----
mean loss: 111.01
 ---- batch: 090 ----
mean loss: 116.29
 ---- batch: 100 ----
mean loss: 119.49
 ---- batch: 110 ----
mean loss: 116.10
train mean loss: 115.68
epoch train time: 0:00:01.926108
elapsed time: 0:08:09.445295
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 23:02:57.194520
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.16
 ---- batch: 020 ----
mean loss: 116.24
 ---- batch: 030 ----
mean loss: 115.69
 ---- batch: 040 ----
mean loss: 120.02
 ---- batch: 050 ----
mean loss: 114.58
 ---- batch: 060 ----
mean loss: 126.79
 ---- batch: 070 ----
mean loss: 116.68
 ---- batch: 080 ----
mean loss: 114.74
 ---- batch: 090 ----
mean loss: 113.71
 ---- batch: 100 ----
mean loss: 115.43
 ---- batch: 110 ----
mean loss: 106.71
train mean loss: 115.64
epoch train time: 0:00:01.911720
elapsed time: 0:08:11.357634
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 23:02:59.106825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.67
 ---- batch: 020 ----
mean loss: 123.96
 ---- batch: 030 ----
mean loss: 115.36
 ---- batch: 040 ----
mean loss: 115.28
 ---- batch: 050 ----
mean loss: 113.11
 ---- batch: 060 ----
mean loss: 117.60
 ---- batch: 070 ----
mean loss: 107.30
 ---- batch: 080 ----
mean loss: 117.08
 ---- batch: 090 ----
mean loss: 117.89
 ---- batch: 100 ----
mean loss: 114.09
 ---- batch: 110 ----
mean loss: 112.00
train mean loss: 115.58
epoch train time: 0:00:01.958241
elapsed time: 0:08:13.316458
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 23:03:01.065677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.38
 ---- batch: 020 ----
mean loss: 119.58
 ---- batch: 030 ----
mean loss: 112.18
 ---- batch: 040 ----
mean loss: 118.21
 ---- batch: 050 ----
mean loss: 122.33
 ---- batch: 060 ----
mean loss: 116.67
 ---- batch: 070 ----
mean loss: 117.61
 ---- batch: 080 ----
mean loss: 109.25
 ---- batch: 090 ----
mean loss: 109.67
 ---- batch: 100 ----
mean loss: 114.82
 ---- batch: 110 ----
mean loss: 110.91
train mean loss: 115.44
epoch train time: 0:00:01.930458
elapsed time: 0:08:15.247549
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 23:03:02.996774
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.30
 ---- batch: 020 ----
mean loss: 114.95
 ---- batch: 030 ----
mean loss: 118.23
 ---- batch: 040 ----
mean loss: 115.83
 ---- batch: 050 ----
mean loss: 112.12
 ---- batch: 060 ----
mean loss: 125.54
 ---- batch: 070 ----
mean loss: 117.90
 ---- batch: 080 ----
mean loss: 122.16
 ---- batch: 090 ----
mean loss: 113.04
 ---- batch: 100 ----
mean loss: 111.96
 ---- batch: 110 ----
mean loss: 109.31
train mean loss: 115.59
epoch train time: 0:00:01.974466
elapsed time: 0:08:17.222643
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 23:03:04.971843
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.15
 ---- batch: 020 ----
mean loss: 113.13
 ---- batch: 030 ----
mean loss: 120.63
 ---- batch: 040 ----
mean loss: 115.14
 ---- batch: 050 ----
mean loss: 110.33
 ---- batch: 060 ----
mean loss: 112.15
 ---- batch: 070 ----
mean loss: 114.90
 ---- batch: 080 ----
mean loss: 112.36
 ---- batch: 090 ----
mean loss: 127.38
 ---- batch: 100 ----
mean loss: 110.70
 ---- batch: 110 ----
mean loss: 120.03
train mean loss: 115.49
epoch train time: 0:00:01.978967
elapsed time: 0:08:19.202232
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 23:03:06.951445
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.82
 ---- batch: 020 ----
mean loss: 108.81
 ---- batch: 030 ----
mean loss: 116.40
 ---- batch: 040 ----
mean loss: 113.52
 ---- batch: 050 ----
mean loss: 118.92
 ---- batch: 060 ----
mean loss: 120.03
 ---- batch: 070 ----
mean loss: 115.61
 ---- batch: 080 ----
mean loss: 114.15
 ---- batch: 090 ----
mean loss: 115.97
 ---- batch: 100 ----
mean loss: 120.34
 ---- batch: 110 ----
mean loss: 114.93
train mean loss: 115.35
epoch train time: 0:00:01.923596
elapsed time: 0:08:21.126441
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 23:03:08.875628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.43
 ---- batch: 020 ----
mean loss: 121.75
 ---- batch: 030 ----
mean loss: 120.66
 ---- batch: 040 ----
mean loss: 109.30
 ---- batch: 050 ----
mean loss: 113.15
 ---- batch: 060 ----
mean loss: 121.54
 ---- batch: 070 ----
mean loss: 119.87
 ---- batch: 080 ----
mean loss: 103.43
 ---- batch: 090 ----
mean loss: 107.10
 ---- batch: 100 ----
mean loss: 120.08
 ---- batch: 110 ----
mean loss: 118.33
train mean loss: 115.39
epoch train time: 0:00:01.953918
elapsed time: 0:08:23.080996
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 23:03:10.830305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.32
 ---- batch: 020 ----
mean loss: 122.92
 ---- batch: 030 ----
mean loss: 110.37
 ---- batch: 040 ----
mean loss: 121.59
 ---- batch: 050 ----
mean loss: 119.28
 ---- batch: 060 ----
mean loss: 112.35
 ---- batch: 070 ----
mean loss: 120.49
 ---- batch: 080 ----
mean loss: 106.30
 ---- batch: 090 ----
mean loss: 111.12
 ---- batch: 100 ----
mean loss: 119.07
 ---- batch: 110 ----
mean loss: 113.95
train mean loss: 115.41
epoch train time: 0:00:01.914825
elapsed time: 0:08:24.996523
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 23:03:12.745734
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.28
 ---- batch: 020 ----
mean loss: 111.25
 ---- batch: 030 ----
mean loss: 113.04
 ---- batch: 040 ----
mean loss: 114.95
 ---- batch: 050 ----
mean loss: 116.44
 ---- batch: 060 ----
mean loss: 116.50
 ---- batch: 070 ----
mean loss: 124.24
 ---- batch: 080 ----
mean loss: 116.88
 ---- batch: 090 ----
mean loss: 113.60
 ---- batch: 100 ----
mean loss: 119.12
 ---- batch: 110 ----
mean loss: 112.53
train mean loss: 115.29
epoch train time: 0:00:01.907202
elapsed time: 0:08:26.904335
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 23:03:14.653552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.85
 ---- batch: 020 ----
mean loss: 116.05
 ---- batch: 030 ----
mean loss: 115.33
 ---- batch: 040 ----
mean loss: 117.49
 ---- batch: 050 ----
mean loss: 110.54
 ---- batch: 060 ----
mean loss: 119.41
 ---- batch: 070 ----
mean loss: 116.35
 ---- batch: 080 ----
mean loss: 108.71
 ---- batch: 090 ----
mean loss: 116.15
 ---- batch: 100 ----
mean loss: 117.74
 ---- batch: 110 ----
mean loss: 117.46
train mean loss: 115.35
epoch train time: 0:00:01.893174
elapsed time: 0:08:28.798129
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 23:03:16.547313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.58
 ---- batch: 020 ----
mean loss: 118.71
 ---- batch: 030 ----
mean loss: 115.15
 ---- batch: 040 ----
mean loss: 112.43
 ---- batch: 050 ----
mean loss: 116.26
 ---- batch: 060 ----
mean loss: 113.36
 ---- batch: 070 ----
mean loss: 117.46
 ---- batch: 080 ----
mean loss: 115.26
 ---- batch: 090 ----
mean loss: 111.36
 ---- batch: 100 ----
mean loss: 117.93
 ---- batch: 110 ----
mean loss: 112.75
train mean loss: 115.31
epoch train time: 0:00:01.930525
elapsed time: 0:08:30.729230
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 23:03:18.478412
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.24
 ---- batch: 020 ----
mean loss: 116.21
 ---- batch: 030 ----
mean loss: 116.12
 ---- batch: 040 ----
mean loss: 112.34
 ---- batch: 050 ----
mean loss: 116.79
 ---- batch: 060 ----
mean loss: 112.95
 ---- batch: 070 ----
mean loss: 115.95
 ---- batch: 080 ----
mean loss: 108.32
 ---- batch: 090 ----
mean loss: 117.03
 ---- batch: 100 ----
mean loss: 122.18
 ---- batch: 110 ----
mean loss: 121.78
train mean loss: 115.14
epoch train time: 0:00:01.935983
elapsed time: 0:08:32.665797
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 23:03:20.415020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.81
 ---- batch: 020 ----
mean loss: 110.87
 ---- batch: 030 ----
mean loss: 109.97
 ---- batch: 040 ----
mean loss: 115.04
 ---- batch: 050 ----
mean loss: 111.16
 ---- batch: 060 ----
mean loss: 117.35
 ---- batch: 070 ----
mean loss: 117.00
 ---- batch: 080 ----
mean loss: 111.66
 ---- batch: 090 ----
mean loss: 115.57
 ---- batch: 100 ----
mean loss: 122.64
 ---- batch: 110 ----
mean loss: 119.54
train mean loss: 115.17
epoch train time: 0:00:01.929931
elapsed time: 0:08:34.596325
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 23:03:22.345518
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.50
 ---- batch: 020 ----
mean loss: 116.00
 ---- batch: 030 ----
mean loss: 112.14
 ---- batch: 040 ----
mean loss: 115.22
 ---- batch: 050 ----
mean loss: 111.96
 ---- batch: 060 ----
mean loss: 119.74
 ---- batch: 070 ----
mean loss: 113.31
 ---- batch: 080 ----
mean loss: 118.23
 ---- batch: 090 ----
mean loss: 116.83
 ---- batch: 100 ----
mean loss: 117.15
 ---- batch: 110 ----
mean loss: 114.82
train mean loss: 115.14
epoch train time: 0:00:01.946354
elapsed time: 0:08:36.550847
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_4/checkpoint.pth.tar
**** end time: 2019-09-26 23:03:24.299690 ****
