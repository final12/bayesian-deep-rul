Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_9', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18455
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:32:15.517746 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:32:15.520885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 5009.12
 ---- batch: 020 ----
mean loss: 4894.54
 ---- batch: 030 ----
mean loss: 4796.42
 ---- batch: 040 ----
mean loss: 4699.76
 ---- batch: 050 ----
mean loss: 4643.92
 ---- batch: 060 ----
mean loss: 4551.70
 ---- batch: 070 ----
mean loss: 4532.48
 ---- batch: 080 ----
mean loss: 4473.54
 ---- batch: 090 ----
mean loss: 4414.94
 ---- batch: 100 ----
mean loss: 4393.47
 ---- batch: 110 ----
mean loss: 4365.37
train mean loss: 4607.48
epoch train time: 0:00:32.426974
elapsed time: 0:00:32.432324
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:32:47.950119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4253.74
 ---- batch: 020 ----
mean loss: 4169.42
 ---- batch: 030 ----
mean loss: 4152.88
 ---- batch: 040 ----
mean loss: 4094.72
 ---- batch: 050 ----
mean loss: 4080.87
 ---- batch: 060 ----
mean loss: 3995.15
 ---- batch: 070 ----
mean loss: 3907.90
 ---- batch: 080 ----
mean loss: 3906.17
 ---- batch: 090 ----
mean loss: 3823.65
 ---- batch: 100 ----
mean loss: 3739.36
 ---- batch: 110 ----
mean loss: 3659.39
train mean loss: 3973.69
epoch train time: 0:00:00.551233
elapsed time: 0:00:32.983695
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:32:48.501489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3645.46
 ---- batch: 020 ----
mean loss: 3591.76
 ---- batch: 030 ----
mean loss: 3571.50
 ---- batch: 040 ----
mean loss: 3516.82
 ---- batch: 050 ----
mean loss: 3460.05
 ---- batch: 060 ----
mean loss: 3401.11
 ---- batch: 070 ----
mean loss: 3377.11
 ---- batch: 080 ----
mean loss: 3291.20
 ---- batch: 090 ----
mean loss: 3228.27
 ---- batch: 100 ----
mean loss: 3226.40
 ---- batch: 110 ----
mean loss: 3092.71
train mean loss: 3394.39
epoch train time: 0:00:00.554342
elapsed time: 0:00:33.538168
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:32:49.055962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3080.38
 ---- batch: 020 ----
mean loss: 2992.44
 ---- batch: 030 ----
mean loss: 3028.20
 ---- batch: 040 ----
mean loss: 2992.54
 ---- batch: 050 ----
mean loss: 2908.75
 ---- batch: 060 ----
mean loss: 2902.90
 ---- batch: 070 ----
mean loss: 2855.57
 ---- batch: 080 ----
mean loss: 2860.02
 ---- batch: 090 ----
mean loss: 2757.82
 ---- batch: 100 ----
mean loss: 2743.02
 ---- batch: 110 ----
mean loss: 2705.07
train mean loss: 2887.62
epoch train time: 0:00:00.549196
elapsed time: 0:00:34.087497
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:32:49.605293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2695.28
 ---- batch: 020 ----
mean loss: 2626.64
 ---- batch: 030 ----
mean loss: 2571.44
 ---- batch: 040 ----
mean loss: 2529.17
 ---- batch: 050 ----
mean loss: 2508.01
 ---- batch: 060 ----
mean loss: 2458.97
 ---- batch: 070 ----
mean loss: 2402.14
 ---- batch: 080 ----
mean loss: 2364.79
 ---- batch: 090 ----
mean loss: 2358.30
 ---- batch: 100 ----
mean loss: 2340.14
 ---- batch: 110 ----
mean loss: 2318.09
train mean loss: 2464.53
epoch train time: 0:00:00.557056
elapsed time: 0:00:34.644708
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:32:50.162504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2271.49
 ---- batch: 020 ----
mean loss: 2232.21
 ---- batch: 030 ----
mean loss: 2191.80
 ---- batch: 040 ----
mean loss: 2198.92
 ---- batch: 050 ----
mean loss: 2107.87
 ---- batch: 060 ----
mean loss: 2075.15
 ---- batch: 070 ----
mean loss: 2040.60
 ---- batch: 080 ----
mean loss: 2004.89
 ---- batch: 090 ----
mean loss: 1996.00
 ---- batch: 100 ----
mean loss: 1987.53
 ---- batch: 110 ----
mean loss: 1941.71
train mean loss: 2091.33
epoch train time: 0:00:00.564827
elapsed time: 0:00:35.209673
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:32:50.727469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1915.66
 ---- batch: 020 ----
mean loss: 1881.82
 ---- batch: 030 ----
mean loss: 1844.80
 ---- batch: 040 ----
mean loss: 1832.04
 ---- batch: 050 ----
mean loss: 1851.65
 ---- batch: 060 ----
mean loss: 1747.87
 ---- batch: 070 ----
mean loss: 1740.69
 ---- batch: 080 ----
mean loss: 1710.80
 ---- batch: 090 ----
mean loss: 1706.00
 ---- batch: 100 ----
mean loss: 1661.91
 ---- batch: 110 ----
mean loss: 1684.09
train mean loss: 1775.99
epoch train time: 0:00:00.586003
elapsed time: 0:00:35.795811
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:32:51.313644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1611.23
 ---- batch: 020 ----
mean loss: 1624.41
 ---- batch: 030 ----
mean loss: 1569.10
 ---- batch: 040 ----
mean loss: 1550.97
 ---- batch: 050 ----
mean loss: 1548.50
 ---- batch: 060 ----
mean loss: 1529.34
 ---- batch: 070 ----
mean loss: 1512.66
 ---- batch: 080 ----
mean loss: 1496.40
 ---- batch: 090 ----
mean loss: 1499.69
 ---- batch: 100 ----
mean loss: 1481.34
 ---- batch: 110 ----
mean loss: 1416.05
train mean loss: 1528.97
epoch train time: 0:00:00.578427
elapsed time: 0:00:36.374411
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:32:51.892219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1404.62
 ---- batch: 020 ----
mean loss: 1412.67
 ---- batch: 030 ----
mean loss: 1379.24
 ---- batch: 040 ----
mean loss: 1357.05
 ---- batch: 050 ----
mean loss: 1357.95
 ---- batch: 060 ----
mean loss: 1357.92
 ---- batch: 070 ----
mean loss: 1335.01
 ---- batch: 080 ----
mean loss: 1318.27
 ---- batch: 090 ----
mean loss: 1300.89
 ---- batch: 100 ----
mean loss: 1295.60
 ---- batch: 110 ----
mean loss: 1289.11
train mean loss: 1343.28
epoch train time: 0:00:00.587018
elapsed time: 0:00:36.961610
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:32:52.479446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1263.00
 ---- batch: 020 ----
mean loss: 1256.61
 ---- batch: 030 ----
mean loss: 1229.50
 ---- batch: 040 ----
mean loss: 1210.74
 ---- batch: 050 ----
mean loss: 1208.67
 ---- batch: 060 ----
mean loss: 1206.33
 ---- batch: 070 ----
mean loss: 1201.25
 ---- batch: 080 ----
mean loss: 1177.66
 ---- batch: 090 ----
mean loss: 1178.15
 ---- batch: 100 ----
mean loss: 1149.65
 ---- batch: 110 ----
mean loss: 1162.81
train mean loss: 1202.84
epoch train time: 0:00:00.583077
elapsed time: 0:00:37.544868
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:32:53.062666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1135.43
 ---- batch: 020 ----
mean loss: 1128.09
 ---- batch: 030 ----
mean loss: 1125.67
 ---- batch: 040 ----
mean loss: 1114.46
 ---- batch: 050 ----
mean loss: 1106.92
 ---- batch: 060 ----
mean loss: 1095.09
 ---- batch: 070 ----
mean loss: 1071.53
 ---- batch: 080 ----
mean loss: 1077.36
 ---- batch: 090 ----
mean loss: 1075.73
 ---- batch: 100 ----
mean loss: 1074.27
 ---- batch: 110 ----
mean loss: 1047.40
train mean loss: 1094.52
epoch train time: 0:00:00.576102
elapsed time: 0:00:38.121106
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:32:53.638898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1047.74
 ---- batch: 020 ----
mean loss: 1036.72
 ---- batch: 030 ----
mean loss: 1023.37
 ---- batch: 040 ----
mean loss: 1010.87
 ---- batch: 050 ----
mean loss: 997.87
 ---- batch: 060 ----
mean loss: 1006.49
 ---- batch: 070 ----
mean loss: 1007.19
 ---- batch: 080 ----
mean loss: 993.71
 ---- batch: 090 ----
mean loss: 990.63
 ---- batch: 100 ----
mean loss: 984.82
 ---- batch: 110 ----
mean loss: 972.45
train mean loss: 1006.23
epoch train time: 0:00:00.574631
elapsed time: 0:00:38.695911
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:32:54.213708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.60
 ---- batch: 020 ----
mean loss: 970.58
 ---- batch: 030 ----
mean loss: 961.44
 ---- batch: 040 ----
mean loss: 952.26
 ---- batch: 050 ----
mean loss: 941.25
 ---- batch: 060 ----
mean loss: 923.74
 ---- batch: 070 ----
mean loss: 948.98
 ---- batch: 080 ----
mean loss: 925.30
 ---- batch: 090 ----
mean loss: 927.85
 ---- batch: 100 ----
mean loss: 937.51
 ---- batch: 110 ----
mean loss: 913.00
train mean loss: 942.98
epoch train time: 0:00:00.576788
elapsed time: 0:00:39.272836
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:32:54.790628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.75
 ---- batch: 020 ----
mean loss: 907.46
 ---- batch: 030 ----
mean loss: 910.05
 ---- batch: 040 ----
mean loss: 903.31
 ---- batch: 050 ----
mean loss: 903.87
 ---- batch: 060 ----
mean loss: 912.30
 ---- batch: 070 ----
mean loss: 904.65
 ---- batch: 080 ----
mean loss: 902.07
 ---- batch: 090 ----
mean loss: 891.29
 ---- batch: 100 ----
mean loss: 897.90
 ---- batch: 110 ----
mean loss: 897.44
train mean loss: 903.81
epoch train time: 0:00:00.574537
elapsed time: 0:00:39.847505
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:32:55.365316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.86
 ---- batch: 020 ----
mean loss: 883.89
 ---- batch: 030 ----
mean loss: 899.23
 ---- batch: 040 ----
mean loss: 896.35
 ---- batch: 050 ----
mean loss: 879.62
 ---- batch: 060 ----
mean loss: 872.84
 ---- batch: 070 ----
mean loss: 870.83
 ---- batch: 080 ----
mean loss: 866.49
 ---- batch: 090 ----
mean loss: 874.82
 ---- batch: 100 ----
mean loss: 865.36
 ---- batch: 110 ----
mean loss: 882.99
train mean loss: 879.20
epoch train time: 0:00:00.570395
elapsed time: 0:00:40.418055
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:32:55.935848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.78
 ---- batch: 020 ----
mean loss: 872.16
 ---- batch: 030 ----
mean loss: 863.69
 ---- batch: 040 ----
mean loss: 852.95
 ---- batch: 050 ----
mean loss: 862.58
 ---- batch: 060 ----
mean loss: 866.55
 ---- batch: 070 ----
mean loss: 876.60
 ---- batch: 080 ----
mean loss: 853.75
 ---- batch: 090 ----
mean loss: 856.24
 ---- batch: 100 ----
mean loss: 867.87
 ---- batch: 110 ----
mean loss: 849.47
train mean loss: 864.20
epoch train time: 0:00:00.574994
elapsed time: 0:00:40.993183
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:32:56.510985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.07
 ---- batch: 020 ----
mean loss: 832.58
 ---- batch: 030 ----
mean loss: 856.62
 ---- batch: 040 ----
mean loss: 873.24
 ---- batch: 050 ----
mean loss: 872.02
 ---- batch: 060 ----
mean loss: 869.15
 ---- batch: 070 ----
mean loss: 862.65
 ---- batch: 080 ----
mean loss: 854.80
 ---- batch: 090 ----
mean loss: 841.61
 ---- batch: 100 ----
mean loss: 844.62
 ---- batch: 110 ----
mean loss: 846.80
train mean loss: 855.19
epoch train time: 0:00:00.582018
elapsed time: 0:00:41.575343
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:32:57.093135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.79
 ---- batch: 020 ----
mean loss: 855.08
 ---- batch: 030 ----
mean loss: 839.01
 ---- batch: 040 ----
mean loss: 850.99
 ---- batch: 050 ----
mean loss: 863.80
 ---- batch: 060 ----
mean loss: 831.35
 ---- batch: 070 ----
mean loss: 863.08
 ---- batch: 080 ----
mean loss: 845.90
 ---- batch: 090 ----
mean loss: 846.27
 ---- batch: 100 ----
mean loss: 863.58
 ---- batch: 110 ----
mean loss: 858.81
train mean loss: 849.83
epoch train time: 0:00:00.571307
elapsed time: 0:00:42.146783
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:32:57.664579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.28
 ---- batch: 020 ----
mean loss: 858.72
 ---- batch: 030 ----
mean loss: 845.90
 ---- batch: 040 ----
mean loss: 827.28
 ---- batch: 050 ----
mean loss: 838.16
 ---- batch: 060 ----
mean loss: 853.51
 ---- batch: 070 ----
mean loss: 843.52
 ---- batch: 080 ----
mean loss: 844.87
 ---- batch: 090 ----
mean loss: 854.01
 ---- batch: 100 ----
mean loss: 842.23
 ---- batch: 110 ----
mean loss: 864.22
train mean loss: 846.85
epoch train time: 0:00:00.576145
elapsed time: 0:00:42.723063
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:32:58.240856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.51
 ---- batch: 020 ----
mean loss: 854.07
 ---- batch: 030 ----
mean loss: 855.68
 ---- batch: 040 ----
mean loss: 840.47
 ---- batch: 050 ----
mean loss: 829.84
 ---- batch: 060 ----
mean loss: 852.41
 ---- batch: 070 ----
mean loss: 844.09
 ---- batch: 080 ----
mean loss: 849.34
 ---- batch: 090 ----
mean loss: 839.50
 ---- batch: 100 ----
mean loss: 847.38
 ---- batch: 110 ----
mean loss: 852.30
train mean loss: 845.36
epoch train time: 0:00:00.570804
elapsed time: 0:00:43.294014
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:32:58.811823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 809.39
 ---- batch: 020 ----
mean loss: 880.45
 ---- batch: 030 ----
mean loss: 843.88
 ---- batch: 040 ----
mean loss: 869.50
 ---- batch: 050 ----
mean loss: 848.15
 ---- batch: 060 ----
mean loss: 860.00
 ---- batch: 070 ----
mean loss: 840.88
 ---- batch: 080 ----
mean loss: 857.76
 ---- batch: 090 ----
mean loss: 831.31
 ---- batch: 100 ----
mean loss: 826.86
 ---- batch: 110 ----
mean loss: 828.47
train mean loss: 844.59
epoch train time: 0:00:00.571825
elapsed time: 0:00:43.865988
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:32:59.383801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.32
 ---- batch: 020 ----
mean loss: 856.58
 ---- batch: 030 ----
mean loss: 828.49
 ---- batch: 040 ----
mean loss: 858.93
 ---- batch: 050 ----
mean loss: 850.71
 ---- batch: 060 ----
mean loss: 841.80
 ---- batch: 070 ----
mean loss: 854.06
 ---- batch: 080 ----
mean loss: 837.90
 ---- batch: 090 ----
mean loss: 835.49
 ---- batch: 100 ----
mean loss: 832.00
 ---- batch: 110 ----
mean loss: 853.14
train mean loss: 844.19
epoch train time: 0:00:00.570495
elapsed time: 0:00:44.436647
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:32:59.954449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.62
 ---- batch: 020 ----
mean loss: 834.91
 ---- batch: 030 ----
mean loss: 819.31
 ---- batch: 040 ----
mean loss: 844.26
 ---- batch: 050 ----
mean loss: 871.32
 ---- batch: 060 ----
mean loss: 835.76
 ---- batch: 070 ----
mean loss: 864.70
 ---- batch: 080 ----
mean loss: 832.32
 ---- batch: 090 ----
mean loss: 846.11
 ---- batch: 100 ----
mean loss: 861.95
 ---- batch: 110 ----
mean loss: 844.29
train mean loss: 844.10
epoch train time: 0:00:00.565098
elapsed time: 0:00:45.001885
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:33:00.519679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.54
 ---- batch: 020 ----
mean loss: 841.69
 ---- batch: 030 ----
mean loss: 856.81
 ---- batch: 040 ----
mean loss: 844.17
 ---- batch: 050 ----
mean loss: 860.66
 ---- batch: 060 ----
mean loss: 834.29
 ---- batch: 070 ----
mean loss: 833.56
 ---- batch: 080 ----
mean loss: 858.24
 ---- batch: 090 ----
mean loss: 841.90
 ---- batch: 100 ----
mean loss: 851.77
 ---- batch: 110 ----
mean loss: 829.34
train mean loss: 844.02
epoch train time: 0:00:00.565319
elapsed time: 0:00:45.567341
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:33:01.085136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.91
 ---- batch: 020 ----
mean loss: 842.29
 ---- batch: 030 ----
mean loss: 827.39
 ---- batch: 040 ----
mean loss: 849.84
 ---- batch: 050 ----
mean loss: 846.94
 ---- batch: 060 ----
mean loss: 858.63
 ---- batch: 070 ----
mean loss: 818.95
 ---- batch: 080 ----
mean loss: 846.62
 ---- batch: 090 ----
mean loss: 854.55
 ---- batch: 100 ----
mean loss: 835.57
 ---- batch: 110 ----
mean loss: 854.66
train mean loss: 844.00
epoch train time: 0:00:00.559877
elapsed time: 0:00:46.127362
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:33:01.645159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.79
 ---- batch: 020 ----
mean loss: 845.52
 ---- batch: 030 ----
mean loss: 843.54
 ---- batch: 040 ----
mean loss: 840.50
 ---- batch: 050 ----
mean loss: 834.65
 ---- batch: 060 ----
mean loss: 852.34
 ---- batch: 070 ----
mean loss: 864.85
 ---- batch: 080 ----
mean loss: 830.97
 ---- batch: 090 ----
mean loss: 848.36
 ---- batch: 100 ----
mean loss: 838.76
 ---- batch: 110 ----
mean loss: 838.40
train mean loss: 843.97
epoch train time: 0:00:00.552058
elapsed time: 0:00:46.679589
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:33:02.197383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.06
 ---- batch: 020 ----
mean loss: 852.36
 ---- batch: 030 ----
mean loss: 858.96
 ---- batch: 040 ----
mean loss: 845.34
 ---- batch: 050 ----
mean loss: 839.66
 ---- batch: 060 ----
mean loss: 819.41
 ---- batch: 070 ----
mean loss: 828.51
 ---- batch: 080 ----
mean loss: 860.85
 ---- batch: 090 ----
mean loss: 863.12
 ---- batch: 100 ----
mean loss: 836.78
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.558904
elapsed time: 0:00:47.238661
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:33:02.756472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.66
 ---- batch: 030 ----
mean loss: 854.13
 ---- batch: 040 ----
mean loss: 865.56
 ---- batch: 050 ----
mean loss: 838.79
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 829.95
 ---- batch: 080 ----
mean loss: 849.24
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 835.78
 ---- batch: 110 ----
mean loss: 847.17
train mean loss: 843.84
epoch train time: 0:00:00.551226
elapsed time: 0:00:47.790035
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:33:03.307828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.62
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.25
 ---- batch: 040 ----
mean loss: 861.72
 ---- batch: 050 ----
mean loss: 858.48
 ---- batch: 060 ----
mean loss: 838.94
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.45
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.94
epoch train time: 0:00:00.550501
elapsed time: 0:00:48.340686
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:33:03.858478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.48
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.56
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.550939
elapsed time: 0:00:48.891754
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:33:04.409546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.22
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.43
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.61
 ---- batch: 090 ----
mean loss: 844.55
 ---- batch: 100 ----
mean loss: 855.06
 ---- batch: 110 ----
mean loss: 856.51
train mean loss: 844.02
epoch train time: 0:00:00.553976
elapsed time: 0:00:49.445859
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:33:04.963652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.89
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.12
 ---- batch: 050 ----
mean loss: 818.03
 ---- batch: 060 ----
mean loss: 833.12
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.87
 ---- batch: 090 ----
mean loss: 842.99
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.547911
elapsed time: 0:00:49.993902
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:33:05.511693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.32
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.55
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.01
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.00
train mean loss: 843.87
epoch train time: 0:00:00.555063
elapsed time: 0:00:50.549098
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:33:06.066893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.60
 ---- batch: 020 ----
mean loss: 848.83
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 824.87
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.42
 ---- batch: 080 ----
mean loss: 858.18
 ---- batch: 090 ----
mean loss: 855.87
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.83
train mean loss: 843.97
epoch train time: 0:00:00.559034
elapsed time: 0:00:51.108269
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:33:06.626087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.73
 ---- batch: 030 ----
mean loss: 838.89
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.73
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.551603
elapsed time: 0:00:51.660064
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:33:07.178914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.12
 ---- batch: 020 ----
mean loss: 861.63
 ---- batch: 030 ----
mean loss: 838.12
 ---- batch: 040 ----
mean loss: 845.01
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.19
 ---- batch: 090 ----
mean loss: 833.50
 ---- batch: 100 ----
mean loss: 834.01
 ---- batch: 110 ----
mean loss: 834.96
train mean loss: 844.04
epoch train time: 0:00:00.545541
elapsed time: 0:00:52.206809
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:33:07.724600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.51
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.74
 ---- batch: 060 ----
mean loss: 850.21
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.14
 ---- batch: 090 ----
mean loss: 864.26
 ---- batch: 100 ----
mean loss: 852.65
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.95
epoch train time: 0:00:00.562247
elapsed time: 0:00:52.769188
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:33:08.286981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.72
 ---- batch: 030 ----
mean loss: 830.98
 ---- batch: 040 ----
mean loss: 838.38
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.24
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.76
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.07
 ---- batch: 110 ----
mean loss: 855.24
train mean loss: 843.94
epoch train time: 0:00:00.561265
elapsed time: 0:00:53.330587
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:33:08.848381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.38
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.66
 ---- batch: 050 ----
mean loss: 817.83
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.04
 ---- batch: 080 ----
mean loss: 854.70
 ---- batch: 090 ----
mean loss: 824.21
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 844.56
train mean loss: 844.09
epoch train time: 0:00:00.552547
elapsed time: 0:00:53.883268
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:33:09.401061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.50
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.46
 ---- batch: 040 ----
mean loss: 845.66
 ---- batch: 050 ----
mean loss: 833.50
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.35
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.94
train mean loss: 844.01
epoch train time: 0:00:00.550517
elapsed time: 0:00:54.433918
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:33:09.951742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.23
 ---- batch: 030 ----
mean loss: 833.32
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.65
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.556602
elapsed time: 0:00:54.990684
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:33:10.508477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.49
 ---- batch: 020 ----
mean loss: 825.58
 ---- batch: 030 ----
mean loss: 839.14
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.23
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.50
 ---- batch: 090 ----
mean loss: 853.99
 ---- batch: 100 ----
mean loss: 844.02
 ---- batch: 110 ----
mean loss: 822.70
train mean loss: 843.96
epoch train time: 0:00:00.553692
elapsed time: 0:00:55.544507
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:33:11.062299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.69
 ---- batch: 030 ----
mean loss: 846.90
 ---- batch: 040 ----
mean loss: 835.58
 ---- batch: 050 ----
mean loss: 868.28
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.95
 ---- batch: 090 ----
mean loss: 841.46
 ---- batch: 100 ----
mean loss: 841.99
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.558522
elapsed time: 0:00:56.103167
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:33:11.620961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.79
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.94
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 809.14
 ---- batch: 080 ----
mean loss: 856.46
 ---- batch: 090 ----
mean loss: 845.25
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.550830
elapsed time: 0:00:56.654144
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:33:12.171961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.81
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.13
 ---- batch: 050 ----
mean loss: 833.71
 ---- batch: 060 ----
mean loss: 865.10
 ---- batch: 070 ----
mean loss: 852.62
 ---- batch: 080 ----
mean loss: 833.10
 ---- batch: 090 ----
mean loss: 822.29
 ---- batch: 100 ----
mean loss: 830.74
 ---- batch: 110 ----
mean loss: 843.89
train mean loss: 844.04
epoch train time: 0:00:00.553574
elapsed time: 0:00:57.207923
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:33:12.725729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.01
 ---- batch: 020 ----
mean loss: 869.89
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.31
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.34
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.571144
elapsed time: 0:00:57.779217
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:33:13.297011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.23
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.66
 ---- batch: 060 ----
mean loss: 825.05
 ---- batch: 070 ----
mean loss: 862.07
 ---- batch: 080 ----
mean loss: 821.71
 ---- batch: 090 ----
mean loss: 868.25
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 844.02
epoch train time: 0:00:00.553049
elapsed time: 0:00:58.332407
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:33:13.850222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.91
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.44
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.59
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.86
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.02
epoch train time: 0:00:00.564447
elapsed time: 0:00:58.897012
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:33:14.414806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.87
 ---- batch: 020 ----
mean loss: 843.81
 ---- batch: 030 ----
mean loss: 851.01
 ---- batch: 040 ----
mean loss: 856.66
 ---- batch: 050 ----
mean loss: 832.84
 ---- batch: 060 ----
mean loss: 851.55
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.21
 ---- batch: 090 ----
mean loss: 833.15
 ---- batch: 100 ----
mean loss: 851.12
 ---- batch: 110 ----
mean loss: 824.81
train mean loss: 844.02
epoch train time: 0:00:00.558578
elapsed time: 0:00:59.455722
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:33:14.973514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.35
 ---- batch: 030 ----
mean loss: 854.58
 ---- batch: 040 ----
mean loss: 833.74
 ---- batch: 050 ----
mean loss: 834.99
 ---- batch: 060 ----
mean loss: 847.39
 ---- batch: 070 ----
mean loss: 828.86
 ---- batch: 080 ----
mean loss: 845.62
 ---- batch: 090 ----
mean loss: 847.91
 ---- batch: 100 ----
mean loss: 839.44
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 844.00
epoch train time: 0:00:00.565444
elapsed time: 0:01:00.021299
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:33:15.539113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 846.50
 ---- batch: 030 ----
mean loss: 862.86
 ---- batch: 040 ----
mean loss: 852.98
 ---- batch: 050 ----
mean loss: 837.18
 ---- batch: 060 ----
mean loss: 840.36
 ---- batch: 070 ----
mean loss: 860.77
 ---- batch: 080 ----
mean loss: 855.69
 ---- batch: 090 ----
mean loss: 833.00
 ---- batch: 100 ----
mean loss: 817.31
 ---- batch: 110 ----
mean loss: 839.15
train mean loss: 843.95
epoch train time: 0:00:00.554322
elapsed time: 0:01:00.575773
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:33:16.093567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 842.83
 ---- batch: 030 ----
mean loss: 842.87
 ---- batch: 040 ----
mean loss: 838.17
 ---- batch: 050 ----
mean loss: 844.72
 ---- batch: 060 ----
mean loss: 863.76
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 808.23
 ---- batch: 090 ----
mean loss: 833.55
 ---- batch: 100 ----
mean loss: 859.01
 ---- batch: 110 ----
mean loss: 849.06
train mean loss: 844.05
epoch train time: 0:00:00.546287
elapsed time: 0:01:01.122210
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:33:16.640005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.97
 ---- batch: 020 ----
mean loss: 835.15
 ---- batch: 030 ----
mean loss: 837.94
 ---- batch: 040 ----
mean loss: 824.12
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 857.77
 ---- batch: 070 ----
mean loss: 822.83
 ---- batch: 080 ----
mean loss: 857.41
 ---- batch: 090 ----
mean loss: 835.62
 ---- batch: 100 ----
mean loss: 851.19
 ---- batch: 110 ----
mean loss: 863.65
train mean loss: 844.00
epoch train time: 0:00:00.557244
elapsed time: 0:01:01.679591
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:33:17.197403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.57
 ---- batch: 020 ----
mean loss: 858.19
 ---- batch: 030 ----
mean loss: 852.89
 ---- batch: 040 ----
mean loss: 839.76
 ---- batch: 050 ----
mean loss: 830.17
 ---- batch: 060 ----
mean loss: 841.44
 ---- batch: 070 ----
mean loss: 855.69
 ---- batch: 080 ----
mean loss: 849.65
 ---- batch: 090 ----
mean loss: 832.65
 ---- batch: 100 ----
mean loss: 836.73
 ---- batch: 110 ----
mean loss: 842.08
train mean loss: 843.93
epoch train time: 0:00:00.556618
elapsed time: 0:01:02.236374
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:33:17.754169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.38
 ---- batch: 020 ----
mean loss: 853.11
 ---- batch: 030 ----
mean loss: 847.68
 ---- batch: 040 ----
mean loss: 843.46
 ---- batch: 050 ----
mean loss: 859.03
 ---- batch: 060 ----
mean loss: 816.25
 ---- batch: 070 ----
mean loss: 860.16
 ---- batch: 080 ----
mean loss: 813.18
 ---- batch: 090 ----
mean loss: 834.16
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 857.17
train mean loss: 843.89
epoch train time: 0:00:00.557026
elapsed time: 0:01:02.793539
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:33:18.311336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.12
 ---- batch: 020 ----
mean loss: 837.07
 ---- batch: 030 ----
mean loss: 843.34
 ---- batch: 040 ----
mean loss: 852.86
 ---- batch: 050 ----
mean loss: 846.02
 ---- batch: 060 ----
mean loss: 836.78
 ---- batch: 070 ----
mean loss: 842.19
 ---- batch: 080 ----
mean loss: 823.29
 ---- batch: 090 ----
mean loss: 826.10
 ---- batch: 100 ----
mean loss: 802.58
 ---- batch: 110 ----
mean loss: 791.32
train mean loss: 830.88
epoch train time: 0:00:00.567173
elapsed time: 0:01:03.360869
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:33:18.878662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 775.49
 ---- batch: 020 ----
mean loss: 752.37
 ---- batch: 030 ----
mean loss: 732.61
 ---- batch: 040 ----
mean loss: 709.90
 ---- batch: 050 ----
mean loss: 699.41
 ---- batch: 060 ----
mean loss: 682.72
 ---- batch: 070 ----
mean loss: 647.67
 ---- batch: 080 ----
mean loss: 615.83
 ---- batch: 090 ----
mean loss: 571.00
 ---- batch: 100 ----
mean loss: 517.49
 ---- batch: 110 ----
mean loss: 457.41
train mean loss: 645.68
epoch train time: 0:00:00.555019
elapsed time: 0:01:03.916020
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:33:19.433812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.05
 ---- batch: 020 ----
mean loss: 406.33
 ---- batch: 030 ----
mean loss: 374.00
 ---- batch: 040 ----
mean loss: 367.97
 ---- batch: 050 ----
mean loss: 369.65
 ---- batch: 060 ----
mean loss: 356.19
 ---- batch: 070 ----
mean loss: 352.03
 ---- batch: 080 ----
mean loss: 338.76
 ---- batch: 090 ----
mean loss: 331.30
 ---- batch: 100 ----
mean loss: 324.11
 ---- batch: 110 ----
mean loss: 324.75
train mean loss: 359.65
epoch train time: 0:00:00.560860
elapsed time: 0:01:04.477014
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:33:19.994840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.00
 ---- batch: 020 ----
mean loss: 307.15
 ---- batch: 030 ----
mean loss: 302.57
 ---- batch: 040 ----
mean loss: 295.82
 ---- batch: 050 ----
mean loss: 286.22
 ---- batch: 060 ----
mean loss: 274.02
 ---- batch: 070 ----
mean loss: 277.11
 ---- batch: 080 ----
mean loss: 290.78
 ---- batch: 090 ----
mean loss: 272.67
 ---- batch: 100 ----
mean loss: 279.89
 ---- batch: 110 ----
mean loss: 265.85
train mean loss: 286.26
epoch train time: 0:00:00.547663
elapsed time: 0:01:05.024843
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:33:20.542633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.34
 ---- batch: 020 ----
mean loss: 259.40
 ---- batch: 030 ----
mean loss: 260.08
 ---- batch: 040 ----
mean loss: 261.65
 ---- batch: 050 ----
mean loss: 246.37
 ---- batch: 060 ----
mean loss: 253.11
 ---- batch: 070 ----
mean loss: 245.99
 ---- batch: 080 ----
mean loss: 255.67
 ---- batch: 090 ----
mean loss: 265.65
 ---- batch: 100 ----
mean loss: 261.77
 ---- batch: 110 ----
mean loss: 253.87
train mean loss: 256.93
epoch train time: 0:00:00.559047
elapsed time: 0:01:05.584021
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:33:21.101832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.32
 ---- batch: 020 ----
mean loss: 237.84
 ---- batch: 030 ----
mean loss: 251.12
 ---- batch: 040 ----
mean loss: 245.35
 ---- batch: 050 ----
mean loss: 247.69
 ---- batch: 060 ----
mean loss: 244.13
 ---- batch: 070 ----
mean loss: 239.23
 ---- batch: 080 ----
mean loss: 242.85
 ---- batch: 090 ----
mean loss: 243.06
 ---- batch: 100 ----
mean loss: 237.34
 ---- batch: 110 ----
mean loss: 235.69
train mean loss: 242.36
epoch train time: 0:00:00.551478
elapsed time: 0:01:06.135655
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:33:21.653477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.22
 ---- batch: 020 ----
mean loss: 238.91
 ---- batch: 030 ----
mean loss: 232.35
 ---- batch: 040 ----
mean loss: 222.02
 ---- batch: 050 ----
mean loss: 236.42
 ---- batch: 060 ----
mean loss: 231.11
 ---- batch: 070 ----
mean loss: 237.72
 ---- batch: 080 ----
mean loss: 224.78
 ---- batch: 090 ----
mean loss: 236.59
 ---- batch: 100 ----
mean loss: 220.33
 ---- batch: 110 ----
mean loss: 236.03
train mean loss: 231.04
epoch train time: 0:00:00.548006
elapsed time: 0:01:06.683818
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:33:22.201629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.53
 ---- batch: 020 ----
mean loss: 213.91
 ---- batch: 030 ----
mean loss: 220.80
 ---- batch: 040 ----
mean loss: 227.51
 ---- batch: 050 ----
mean loss: 215.62
 ---- batch: 060 ----
mean loss: 221.56
 ---- batch: 070 ----
mean loss: 207.92
 ---- batch: 080 ----
mean loss: 232.47
 ---- batch: 090 ----
mean loss: 223.66
 ---- batch: 100 ----
mean loss: 220.35
 ---- batch: 110 ----
mean loss: 220.85
train mean loss: 221.39
epoch train time: 0:00:00.548918
elapsed time: 0:01:07.232904
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:33:22.750698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.69
 ---- batch: 020 ----
mean loss: 210.50
 ---- batch: 030 ----
mean loss: 212.55
 ---- batch: 040 ----
mean loss: 218.19
 ---- batch: 050 ----
mean loss: 208.52
 ---- batch: 060 ----
mean loss: 211.42
 ---- batch: 070 ----
mean loss: 220.39
 ---- batch: 080 ----
mean loss: 212.14
 ---- batch: 090 ----
mean loss: 207.31
 ---- batch: 100 ----
mean loss: 212.21
 ---- batch: 110 ----
mean loss: 223.86
train mean loss: 213.67
epoch train time: 0:00:00.552379
elapsed time: 0:01:07.785414
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:33:23.303207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.38
 ---- batch: 020 ----
mean loss: 211.99
 ---- batch: 030 ----
mean loss: 206.13
 ---- batch: 040 ----
mean loss: 192.73
 ---- batch: 050 ----
mean loss: 207.38
 ---- batch: 060 ----
mean loss: 212.75
 ---- batch: 070 ----
mean loss: 213.00
 ---- batch: 080 ----
mean loss: 212.91
 ---- batch: 090 ----
mean loss: 218.24
 ---- batch: 100 ----
mean loss: 206.45
 ---- batch: 110 ----
mean loss: 201.40
train mean loss: 208.27
epoch train time: 0:00:00.549005
elapsed time: 0:01:08.334553
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:33:23.852347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.25
 ---- batch: 020 ----
mean loss: 196.72
 ---- batch: 030 ----
mean loss: 210.35
 ---- batch: 040 ----
mean loss: 209.25
 ---- batch: 050 ----
mean loss: 209.73
 ---- batch: 060 ----
mean loss: 206.75
 ---- batch: 070 ----
mean loss: 214.80
 ---- batch: 080 ----
mean loss: 204.47
 ---- batch: 090 ----
mean loss: 196.10
 ---- batch: 100 ----
mean loss: 198.26
 ---- batch: 110 ----
mean loss: 209.07
train mean loss: 205.82
epoch train time: 0:00:00.563342
elapsed time: 0:01:08.898025
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:33:24.415819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.92
 ---- batch: 020 ----
mean loss: 196.27
 ---- batch: 030 ----
mean loss: 197.39
 ---- batch: 040 ----
mean loss: 197.16
 ---- batch: 050 ----
mean loss: 189.88
 ---- batch: 060 ----
mean loss: 204.87
 ---- batch: 070 ----
mean loss: 198.64
 ---- batch: 080 ----
mean loss: 205.43
 ---- batch: 090 ----
mean loss: 197.22
 ---- batch: 100 ----
mean loss: 201.84
 ---- batch: 110 ----
mean loss: 198.32
train mean loss: 198.68
epoch train time: 0:00:00.549812
elapsed time: 0:01:09.447986
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:33:24.965800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.90
 ---- batch: 020 ----
mean loss: 191.95
 ---- batch: 030 ----
mean loss: 182.93
 ---- batch: 040 ----
mean loss: 199.17
 ---- batch: 050 ----
mean loss: 194.54
 ---- batch: 060 ----
mean loss: 189.54
 ---- batch: 070 ----
mean loss: 200.91
 ---- batch: 080 ----
mean loss: 197.36
 ---- batch: 090 ----
mean loss: 205.81
 ---- batch: 100 ----
mean loss: 199.47
 ---- batch: 110 ----
mean loss: 200.57
train mean loss: 196.35
epoch train time: 0:00:00.561194
elapsed time: 0:01:10.009330
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:33:25.527121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.07
 ---- batch: 020 ----
mean loss: 192.09
 ---- batch: 030 ----
mean loss: 184.35
 ---- batch: 040 ----
mean loss: 190.98
 ---- batch: 050 ----
mean loss: 197.39
 ---- batch: 060 ----
mean loss: 201.91
 ---- batch: 070 ----
mean loss: 199.27
 ---- batch: 080 ----
mean loss: 190.67
 ---- batch: 090 ----
mean loss: 203.12
 ---- batch: 100 ----
mean loss: 182.13
 ---- batch: 110 ----
mean loss: 198.80
train mean loss: 193.78
epoch train time: 0:00:00.548935
elapsed time: 0:01:10.558397
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:33:26.076191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.73
 ---- batch: 020 ----
mean loss: 193.52
 ---- batch: 030 ----
mean loss: 196.65
 ---- batch: 040 ----
mean loss: 183.56
 ---- batch: 050 ----
mean loss: 193.51
 ---- batch: 060 ----
mean loss: 195.52
 ---- batch: 070 ----
mean loss: 184.92
 ---- batch: 080 ----
mean loss: 187.07
 ---- batch: 090 ----
mean loss: 188.31
 ---- batch: 100 ----
mean loss: 184.77
 ---- batch: 110 ----
mean loss: 199.06
train mean loss: 191.19
epoch train time: 0:00:00.546639
elapsed time: 0:01:11.105175
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:33:26.622969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.84
 ---- batch: 020 ----
mean loss: 191.82
 ---- batch: 030 ----
mean loss: 198.19
 ---- batch: 040 ----
mean loss: 187.06
 ---- batch: 050 ----
mean loss: 185.79
 ---- batch: 060 ----
mean loss: 184.57
 ---- batch: 070 ----
mean loss: 195.16
 ---- batch: 080 ----
mean loss: 187.76
 ---- batch: 090 ----
mean loss: 194.70
 ---- batch: 100 ----
mean loss: 185.86
 ---- batch: 110 ----
mean loss: 190.27
train mean loss: 189.19
epoch train time: 0:00:00.548632
elapsed time: 0:01:11.653958
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:33:27.171753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.12
 ---- batch: 020 ----
mean loss: 179.54
 ---- batch: 030 ----
mean loss: 188.90
 ---- batch: 040 ----
mean loss: 187.35
 ---- batch: 050 ----
mean loss: 175.93
 ---- batch: 060 ----
mean loss: 189.18
 ---- batch: 070 ----
mean loss: 193.83
 ---- batch: 080 ----
mean loss: 199.01
 ---- batch: 090 ----
mean loss: 185.49
 ---- batch: 100 ----
mean loss: 187.38
 ---- batch: 110 ----
mean loss: 188.89
train mean loss: 187.39
epoch train time: 0:00:00.547224
elapsed time: 0:01:12.201322
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:33:27.719120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.34
 ---- batch: 020 ----
mean loss: 182.78
 ---- batch: 030 ----
mean loss: 186.41
 ---- batch: 040 ----
mean loss: 189.17
 ---- batch: 050 ----
mean loss: 190.67
 ---- batch: 060 ----
mean loss: 183.82
 ---- batch: 070 ----
mean loss: 188.59
 ---- batch: 080 ----
mean loss: 182.77
 ---- batch: 090 ----
mean loss: 180.68
 ---- batch: 100 ----
mean loss: 191.45
 ---- batch: 110 ----
mean loss: 182.84
train mean loss: 185.53
epoch train time: 0:00:00.563503
elapsed time: 0:01:12.764969
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:33:28.282781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.08
 ---- batch: 020 ----
mean loss: 185.80
 ---- batch: 030 ----
mean loss: 181.87
 ---- batch: 040 ----
mean loss: 180.46
 ---- batch: 050 ----
mean loss: 178.83
 ---- batch: 060 ----
mean loss: 192.55
 ---- batch: 070 ----
mean loss: 181.09
 ---- batch: 080 ----
mean loss: 188.34
 ---- batch: 090 ----
mean loss: 189.62
 ---- batch: 100 ----
mean loss: 188.35
 ---- batch: 110 ----
mean loss: 180.20
train mean loss: 184.67
epoch train time: 0:00:00.552018
elapsed time: 0:01:13.317176
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:33:28.834973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.53
 ---- batch: 020 ----
mean loss: 182.87
 ---- batch: 030 ----
mean loss: 179.83
 ---- batch: 040 ----
mean loss: 175.68
 ---- batch: 050 ----
mean loss: 184.69
 ---- batch: 060 ----
mean loss: 184.89
 ---- batch: 070 ----
mean loss: 184.45
 ---- batch: 080 ----
mean loss: 187.96
 ---- batch: 090 ----
mean loss: 182.48
 ---- batch: 100 ----
mean loss: 182.72
 ---- batch: 110 ----
mean loss: 183.87
train mean loss: 183.54
epoch train time: 0:00:00.557489
elapsed time: 0:01:13.874803
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:33:29.392598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.13
 ---- batch: 020 ----
mean loss: 177.95
 ---- batch: 030 ----
mean loss: 187.78
 ---- batch: 040 ----
mean loss: 183.98
 ---- batch: 050 ----
mean loss: 190.34
 ---- batch: 060 ----
mean loss: 175.42
 ---- batch: 070 ----
mean loss: 178.61
 ---- batch: 080 ----
mean loss: 181.87
 ---- batch: 090 ----
mean loss: 185.31
 ---- batch: 100 ----
mean loss: 179.36
 ---- batch: 110 ----
mean loss: 189.44
train mean loss: 182.55
epoch train time: 0:00:00.558246
elapsed time: 0:01:14.433183
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:33:29.950976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.45
 ---- batch: 020 ----
mean loss: 172.11
 ---- batch: 030 ----
mean loss: 184.14
 ---- batch: 040 ----
mean loss: 184.76
 ---- batch: 050 ----
mean loss: 186.72
 ---- batch: 060 ----
mean loss: 185.75
 ---- batch: 070 ----
mean loss: 183.37
 ---- batch: 080 ----
mean loss: 179.61
 ---- batch: 090 ----
mean loss: 181.94
 ---- batch: 100 ----
mean loss: 171.58
 ---- batch: 110 ----
mean loss: 184.16
train mean loss: 181.61
epoch train time: 0:00:00.557875
elapsed time: 0:01:14.991198
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:33:30.509009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.49
 ---- batch: 020 ----
mean loss: 183.44
 ---- batch: 030 ----
mean loss: 173.66
 ---- batch: 040 ----
mean loss: 178.13
 ---- batch: 050 ----
mean loss: 185.37
 ---- batch: 060 ----
mean loss: 180.59
 ---- batch: 070 ----
mean loss: 179.79
 ---- batch: 080 ----
mean loss: 178.25
 ---- batch: 090 ----
mean loss: 185.77
 ---- batch: 100 ----
mean loss: 184.89
 ---- batch: 110 ----
mean loss: 176.47
train mean loss: 181.04
epoch train time: 0:00:00.556659
elapsed time: 0:01:15.548042
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:33:31.065838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.97
 ---- batch: 020 ----
mean loss: 173.05
 ---- batch: 030 ----
mean loss: 174.07
 ---- batch: 040 ----
mean loss: 181.08
 ---- batch: 050 ----
mean loss: 180.91
 ---- batch: 060 ----
mean loss: 188.13
 ---- batch: 070 ----
mean loss: 176.41
 ---- batch: 080 ----
mean loss: 181.21
 ---- batch: 090 ----
mean loss: 179.60
 ---- batch: 100 ----
mean loss: 177.06
 ---- batch: 110 ----
mean loss: 181.01
train mean loss: 178.37
epoch train time: 0:00:00.556806
elapsed time: 0:01:16.104986
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:33:31.622781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.97
 ---- batch: 020 ----
mean loss: 179.21
 ---- batch: 030 ----
mean loss: 169.29
 ---- batch: 040 ----
mean loss: 183.70
 ---- batch: 050 ----
mean loss: 179.32
 ---- batch: 060 ----
mean loss: 183.05
 ---- batch: 070 ----
mean loss: 178.47
 ---- batch: 080 ----
mean loss: 174.24
 ---- batch: 090 ----
mean loss: 181.52
 ---- batch: 100 ----
mean loss: 170.46
 ---- batch: 110 ----
mean loss: 195.50
train mean loss: 178.48
epoch train time: 0:00:00.569651
elapsed time: 0:01:16.674772
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:33:32.192587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.42
 ---- batch: 020 ----
mean loss: 181.23
 ---- batch: 030 ----
mean loss: 170.21
 ---- batch: 040 ----
mean loss: 175.33
 ---- batch: 050 ----
mean loss: 179.30
 ---- batch: 060 ----
mean loss: 181.33
 ---- batch: 070 ----
mean loss: 184.68
 ---- batch: 080 ----
mean loss: 173.98
 ---- batch: 090 ----
mean loss: 180.08
 ---- batch: 100 ----
mean loss: 181.49
 ---- batch: 110 ----
mean loss: 175.48
train mean loss: 177.83
epoch train time: 0:00:00.560370
elapsed time: 0:01:17.235326
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:33:32.753136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.04
 ---- batch: 020 ----
mean loss: 173.09
 ---- batch: 030 ----
mean loss: 175.08
 ---- batch: 040 ----
mean loss: 177.15
 ---- batch: 050 ----
mean loss: 179.47
 ---- batch: 060 ----
mean loss: 171.95
 ---- batch: 070 ----
mean loss: 171.43
 ---- batch: 080 ----
mean loss: 190.42
 ---- batch: 090 ----
mean loss: 178.14
 ---- batch: 100 ----
mean loss: 165.87
 ---- batch: 110 ----
mean loss: 184.87
train mean loss: 176.40
epoch train time: 0:00:00.559622
elapsed time: 0:01:17.795095
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:33:33.312897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.33
 ---- batch: 020 ----
mean loss: 180.11
 ---- batch: 030 ----
mean loss: 179.61
 ---- batch: 040 ----
mean loss: 179.63
 ---- batch: 050 ----
mean loss: 166.39
 ---- batch: 060 ----
mean loss: 180.25
 ---- batch: 070 ----
mean loss: 187.34
 ---- batch: 080 ----
mean loss: 177.70
 ---- batch: 090 ----
mean loss: 178.69
 ---- batch: 100 ----
mean loss: 173.58
 ---- batch: 110 ----
mean loss: 176.81
train mean loss: 177.30
epoch train time: 0:00:00.563499
elapsed time: 0:01:18.358739
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:33:33.876531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.20
 ---- batch: 020 ----
mean loss: 175.09
 ---- batch: 030 ----
mean loss: 177.39
 ---- batch: 040 ----
mean loss: 169.41
 ---- batch: 050 ----
mean loss: 179.38
 ---- batch: 060 ----
mean loss: 173.80
 ---- batch: 070 ----
mean loss: 171.87
 ---- batch: 080 ----
mean loss: 173.49
 ---- batch: 090 ----
mean loss: 174.51
 ---- batch: 100 ----
mean loss: 173.37
 ---- batch: 110 ----
mean loss: 176.62
train mean loss: 174.91
epoch train time: 0:00:00.547230
elapsed time: 0:01:18.906098
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:33:34.423891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.19
 ---- batch: 020 ----
mean loss: 176.81
 ---- batch: 030 ----
mean loss: 172.55
 ---- batch: 040 ----
mean loss: 168.37
 ---- batch: 050 ----
mean loss: 168.28
 ---- batch: 060 ----
mean loss: 178.52
 ---- batch: 070 ----
mean loss: 183.06
 ---- batch: 080 ----
mean loss: 177.88
 ---- batch: 090 ----
mean loss: 172.80
 ---- batch: 100 ----
mean loss: 181.48
 ---- batch: 110 ----
mean loss: 171.79
train mean loss: 174.95
epoch train time: 0:00:00.547971
elapsed time: 0:01:19.454204
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:33:34.972012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.55
 ---- batch: 020 ----
mean loss: 179.73
 ---- batch: 030 ----
mean loss: 162.74
 ---- batch: 040 ----
mean loss: 173.68
 ---- batch: 050 ----
mean loss: 175.11
 ---- batch: 060 ----
mean loss: 172.55
 ---- batch: 070 ----
mean loss: 173.38
 ---- batch: 080 ----
mean loss: 180.15
 ---- batch: 090 ----
mean loss: 176.53
 ---- batch: 100 ----
mean loss: 176.04
 ---- batch: 110 ----
mean loss: 182.12
train mean loss: 174.34
epoch train time: 0:00:00.546418
elapsed time: 0:01:20.000767
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:33:35.518592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.75
 ---- batch: 020 ----
mean loss: 180.74
 ---- batch: 030 ----
mean loss: 164.42
 ---- batch: 040 ----
mean loss: 167.86
 ---- batch: 050 ----
mean loss: 179.10
 ---- batch: 060 ----
mean loss: 172.85
 ---- batch: 070 ----
mean loss: 171.51
 ---- batch: 080 ----
mean loss: 171.17
 ---- batch: 090 ----
mean loss: 177.10
 ---- batch: 100 ----
mean loss: 178.56
 ---- batch: 110 ----
mean loss: 183.14
train mean loss: 173.52
epoch train time: 0:00:00.565226
elapsed time: 0:01:20.566160
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:33:36.083964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.98
 ---- batch: 020 ----
mean loss: 174.47
 ---- batch: 030 ----
mean loss: 171.22
 ---- batch: 040 ----
mean loss: 162.38
 ---- batch: 050 ----
mean loss: 170.05
 ---- batch: 060 ----
mean loss: 172.56
 ---- batch: 070 ----
mean loss: 175.66
 ---- batch: 080 ----
mean loss: 183.36
 ---- batch: 090 ----
mean loss: 172.46
 ---- batch: 100 ----
mean loss: 165.62
 ---- batch: 110 ----
mean loss: 179.11
train mean loss: 172.46
epoch train time: 0:00:00.548956
elapsed time: 0:01:21.115264
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:33:36.633059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.71
 ---- batch: 020 ----
mean loss: 166.39
 ---- batch: 030 ----
mean loss: 174.92
 ---- batch: 040 ----
mean loss: 169.71
 ---- batch: 050 ----
mean loss: 172.09
 ---- batch: 060 ----
mean loss: 166.59
 ---- batch: 070 ----
mean loss: 181.13
 ---- batch: 080 ----
mean loss: 173.03
 ---- batch: 090 ----
mean loss: 172.78
 ---- batch: 100 ----
mean loss: 178.66
 ---- batch: 110 ----
mean loss: 170.87
train mean loss: 171.48
epoch train time: 0:00:00.564646
elapsed time: 0:01:21.680050
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:33:37.197860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.92
 ---- batch: 020 ----
mean loss: 169.95
 ---- batch: 030 ----
mean loss: 168.85
 ---- batch: 040 ----
mean loss: 175.03
 ---- batch: 050 ----
mean loss: 162.00
 ---- batch: 060 ----
mean loss: 169.32
 ---- batch: 070 ----
mean loss: 171.94
 ---- batch: 080 ----
mean loss: 176.67
 ---- batch: 090 ----
mean loss: 170.35
 ---- batch: 100 ----
mean loss: 173.03
 ---- batch: 110 ----
mean loss: 172.12
train mean loss: 170.83
epoch train time: 0:00:00.559860
elapsed time: 0:01:22.240062
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:33:37.757858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.92
 ---- batch: 020 ----
mean loss: 164.18
 ---- batch: 030 ----
mean loss: 162.79
 ---- batch: 040 ----
mean loss: 168.13
 ---- batch: 050 ----
mean loss: 164.26
 ---- batch: 060 ----
mean loss: 175.70
 ---- batch: 070 ----
mean loss: 179.58
 ---- batch: 080 ----
mean loss: 174.08
 ---- batch: 090 ----
mean loss: 166.45
 ---- batch: 100 ----
mean loss: 173.92
 ---- batch: 110 ----
mean loss: 172.59
train mean loss: 170.39
epoch train time: 0:00:00.550638
elapsed time: 0:01:22.790834
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:33:38.308627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.81
 ---- batch: 020 ----
mean loss: 170.94
 ---- batch: 030 ----
mean loss: 171.03
 ---- batch: 040 ----
mean loss: 164.01
 ---- batch: 050 ----
mean loss: 168.52
 ---- batch: 060 ----
mean loss: 173.89
 ---- batch: 070 ----
mean loss: 169.65
 ---- batch: 080 ----
mean loss: 168.53
 ---- batch: 090 ----
mean loss: 172.29
 ---- batch: 100 ----
mean loss: 171.98
 ---- batch: 110 ----
mean loss: 172.82
train mean loss: 169.78
epoch train time: 0:00:00.554332
elapsed time: 0:01:23.345296
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:33:38.863088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.69
 ---- batch: 020 ----
mean loss: 174.23
 ---- batch: 030 ----
mean loss: 162.09
 ---- batch: 040 ----
mean loss: 173.05
 ---- batch: 050 ----
mean loss: 160.58
 ---- batch: 060 ----
mean loss: 171.09
 ---- batch: 070 ----
mean loss: 175.28
 ---- batch: 080 ----
mean loss: 178.87
 ---- batch: 090 ----
mean loss: 166.68
 ---- batch: 100 ----
mean loss: 174.01
 ---- batch: 110 ----
mean loss: 172.45
train mean loss: 170.36
epoch train time: 0:00:00.553135
elapsed time: 0:01:23.898577
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:33:39.416420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.70
 ---- batch: 020 ----
mean loss: 170.99
 ---- batch: 030 ----
mean loss: 167.64
 ---- batch: 040 ----
mean loss: 174.01
 ---- batch: 050 ----
mean loss: 169.35
 ---- batch: 060 ----
mean loss: 158.94
 ---- batch: 070 ----
mean loss: 172.33
 ---- batch: 080 ----
mean loss: 160.29
 ---- batch: 090 ----
mean loss: 173.74
 ---- batch: 100 ----
mean loss: 169.55
 ---- batch: 110 ----
mean loss: 171.36
train mean loss: 168.63
epoch train time: 0:00:00.563061
elapsed time: 0:01:24.461821
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:33:39.979615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.74
 ---- batch: 020 ----
mean loss: 174.35
 ---- batch: 030 ----
mean loss: 170.54
 ---- batch: 040 ----
mean loss: 164.19
 ---- batch: 050 ----
mean loss: 166.24
 ---- batch: 060 ----
mean loss: 171.36
 ---- batch: 070 ----
mean loss: 172.68
 ---- batch: 080 ----
mean loss: 172.34
 ---- batch: 090 ----
mean loss: 163.37
 ---- batch: 100 ----
mean loss: 166.62
 ---- batch: 110 ----
mean loss: 166.34
train mean loss: 168.25
epoch train time: 0:00:00.550657
elapsed time: 0:01:25.012610
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:33:40.530409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.88
 ---- batch: 020 ----
mean loss: 167.40
 ---- batch: 030 ----
mean loss: 175.11
 ---- batch: 040 ----
mean loss: 163.31
 ---- batch: 050 ----
mean loss: 169.47
 ---- batch: 060 ----
mean loss: 167.29
 ---- batch: 070 ----
mean loss: 164.89
 ---- batch: 080 ----
mean loss: 169.24
 ---- batch: 090 ----
mean loss: 169.02
 ---- batch: 100 ----
mean loss: 166.68
 ---- batch: 110 ----
mean loss: 167.85
train mean loss: 168.24
epoch train time: 0:00:00.544438
elapsed time: 0:01:25.557195
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:33:41.075008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.84
 ---- batch: 020 ----
mean loss: 171.87
 ---- batch: 030 ----
mean loss: 165.44
 ---- batch: 040 ----
mean loss: 176.66
 ---- batch: 050 ----
mean loss: 175.49
 ---- batch: 060 ----
mean loss: 165.92
 ---- batch: 070 ----
mean loss: 164.46
 ---- batch: 080 ----
mean loss: 163.07
 ---- batch: 090 ----
mean loss: 171.24
 ---- batch: 100 ----
mean loss: 168.09
 ---- batch: 110 ----
mean loss: 166.71
train mean loss: 169.05
epoch train time: 0:00:00.546211
elapsed time: 0:01:26.103586
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:33:41.621379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.38
 ---- batch: 020 ----
mean loss: 157.56
 ---- batch: 030 ----
mean loss: 154.72
 ---- batch: 040 ----
mean loss: 169.49
 ---- batch: 050 ----
mean loss: 175.20
 ---- batch: 060 ----
mean loss: 174.20
 ---- batch: 070 ----
mean loss: 168.80
 ---- batch: 080 ----
mean loss: 166.58
 ---- batch: 090 ----
mean loss: 164.30
 ---- batch: 100 ----
mean loss: 171.04
 ---- batch: 110 ----
mean loss: 173.12
train mean loss: 167.42
epoch train time: 0:00:00.551092
elapsed time: 0:01:26.654810
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:33:42.172603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.50
 ---- batch: 020 ----
mean loss: 165.93
 ---- batch: 030 ----
mean loss: 168.83
 ---- batch: 040 ----
mean loss: 166.98
 ---- batch: 050 ----
mean loss: 175.78
 ---- batch: 060 ----
mean loss: 160.91
 ---- batch: 070 ----
mean loss: 158.55
 ---- batch: 080 ----
mean loss: 170.08
 ---- batch: 090 ----
mean loss: 175.93
 ---- batch: 100 ----
mean loss: 168.90
 ---- batch: 110 ----
mean loss: 165.82
train mean loss: 167.13
epoch train time: 0:00:00.549438
elapsed time: 0:01:27.204381
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:33:42.722224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.42
 ---- batch: 020 ----
mean loss: 165.95
 ---- batch: 030 ----
mean loss: 164.50
 ---- batch: 040 ----
mean loss: 164.42
 ---- batch: 050 ----
mean loss: 165.77
 ---- batch: 060 ----
mean loss: 170.79
 ---- batch: 070 ----
mean loss: 167.53
 ---- batch: 080 ----
mean loss: 175.01
 ---- batch: 090 ----
mean loss: 163.26
 ---- batch: 100 ----
mean loss: 166.46
 ---- batch: 110 ----
mean loss: 164.10
train mean loss: 166.01
epoch train time: 0:00:00.551255
elapsed time: 0:01:27.755815
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:33:43.273627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.33
 ---- batch: 020 ----
mean loss: 168.49
 ---- batch: 030 ----
mean loss: 153.95
 ---- batch: 040 ----
mean loss: 167.19
 ---- batch: 050 ----
mean loss: 161.13
 ---- batch: 060 ----
mean loss: 166.80
 ---- batch: 070 ----
mean loss: 158.72
 ---- batch: 080 ----
mean loss: 159.38
 ---- batch: 090 ----
mean loss: 167.50
 ---- batch: 100 ----
mean loss: 173.16
 ---- batch: 110 ----
mean loss: 174.15
train mean loss: 165.58
epoch train time: 0:00:00.555716
elapsed time: 0:01:28.311693
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:33:43.829491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.40
 ---- batch: 020 ----
mean loss: 163.12
 ---- batch: 030 ----
mean loss: 166.10
 ---- batch: 040 ----
mean loss: 158.95
 ---- batch: 050 ----
mean loss: 165.33
 ---- batch: 060 ----
mean loss: 173.34
 ---- batch: 070 ----
mean loss: 163.85
 ---- batch: 080 ----
mean loss: 162.60
 ---- batch: 090 ----
mean loss: 163.45
 ---- batch: 100 ----
mean loss: 171.65
 ---- batch: 110 ----
mean loss: 166.30
train mean loss: 164.57
epoch train time: 0:00:00.566572
elapsed time: 0:01:28.878421
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:33:44.396215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.19
 ---- batch: 020 ----
mean loss: 162.60
 ---- batch: 030 ----
mean loss: 163.27
 ---- batch: 040 ----
mean loss: 163.63
 ---- batch: 050 ----
mean loss: 168.87
 ---- batch: 060 ----
mean loss: 156.73
 ---- batch: 070 ----
mean loss: 164.00
 ---- batch: 080 ----
mean loss: 166.66
 ---- batch: 090 ----
mean loss: 163.78
 ---- batch: 100 ----
mean loss: 153.72
 ---- batch: 110 ----
mean loss: 169.51
train mean loss: 163.99
epoch train time: 0:00:00.565213
elapsed time: 0:01:29.443775
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:33:44.961569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.87
 ---- batch: 020 ----
mean loss: 164.21
 ---- batch: 030 ----
mean loss: 164.09
 ---- batch: 040 ----
mean loss: 164.93
 ---- batch: 050 ----
mean loss: 159.73
 ---- batch: 060 ----
mean loss: 168.82
 ---- batch: 070 ----
mean loss: 161.75
 ---- batch: 080 ----
mean loss: 173.65
 ---- batch: 090 ----
mean loss: 161.43
 ---- batch: 100 ----
mean loss: 166.80
 ---- batch: 110 ----
mean loss: 164.85
train mean loss: 165.26
epoch train time: 0:00:00.548984
elapsed time: 0:01:29.992908
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:33:45.510715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.05
 ---- batch: 020 ----
mean loss: 159.05
 ---- batch: 030 ----
mean loss: 161.22
 ---- batch: 040 ----
mean loss: 163.42
 ---- batch: 050 ----
mean loss: 156.47
 ---- batch: 060 ----
mean loss: 169.50
 ---- batch: 070 ----
mean loss: 159.95
 ---- batch: 080 ----
mean loss: 160.07
 ---- batch: 090 ----
mean loss: 161.26
 ---- batch: 100 ----
mean loss: 167.90
 ---- batch: 110 ----
mean loss: 168.96
train mean loss: 163.55
epoch train time: 0:00:00.555766
elapsed time: 0:01:30.548824
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:33:46.066620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.81
 ---- batch: 020 ----
mean loss: 162.20
 ---- batch: 030 ----
mean loss: 166.58
 ---- batch: 040 ----
mean loss: 160.60
 ---- batch: 050 ----
mean loss: 162.44
 ---- batch: 060 ----
mean loss: 162.37
 ---- batch: 070 ----
mean loss: 158.69
 ---- batch: 080 ----
mean loss: 159.31
 ---- batch: 090 ----
mean loss: 163.33
 ---- batch: 100 ----
mean loss: 171.16
 ---- batch: 110 ----
mean loss: 173.56
train mean loss: 163.03
epoch train time: 0:00:00.554788
elapsed time: 0:01:31.103748
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:33:46.621564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.17
 ---- batch: 020 ----
mean loss: 160.01
 ---- batch: 030 ----
mean loss: 162.90
 ---- batch: 040 ----
mean loss: 159.33
 ---- batch: 050 ----
mean loss: 154.98
 ---- batch: 060 ----
mean loss: 166.06
 ---- batch: 070 ----
mean loss: 164.16
 ---- batch: 080 ----
mean loss: 160.83
 ---- batch: 090 ----
mean loss: 165.09
 ---- batch: 100 ----
mean loss: 171.19
 ---- batch: 110 ----
mean loss: 162.06
train mean loss: 163.16
epoch train time: 0:00:00.551511
elapsed time: 0:01:31.655427
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:33:47.173213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.32
 ---- batch: 020 ----
mean loss: 171.55
 ---- batch: 030 ----
mean loss: 156.92
 ---- batch: 040 ----
mean loss: 159.33
 ---- batch: 050 ----
mean loss: 163.52
 ---- batch: 060 ----
mean loss: 161.95
 ---- batch: 070 ----
mean loss: 160.86
 ---- batch: 080 ----
mean loss: 165.83
 ---- batch: 090 ----
mean loss: 169.05
 ---- batch: 100 ----
mean loss: 157.29
 ---- batch: 110 ----
mean loss: 164.40
train mean loss: 162.51
epoch train time: 0:00:00.553054
elapsed time: 0:01:32.208609
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:33:47.726404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.81
 ---- batch: 020 ----
mean loss: 167.77
 ---- batch: 030 ----
mean loss: 160.01
 ---- batch: 040 ----
mean loss: 156.05
 ---- batch: 050 ----
mean loss: 171.11
 ---- batch: 060 ----
mean loss: 155.03
 ---- batch: 070 ----
mean loss: 169.49
 ---- batch: 080 ----
mean loss: 166.42
 ---- batch: 090 ----
mean loss: 165.45
 ---- batch: 100 ----
mean loss: 154.53
 ---- batch: 110 ----
mean loss: 163.69
train mean loss: 162.25
epoch train time: 0:00:00.561868
elapsed time: 0:01:32.770614
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:33:48.288417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.82
 ---- batch: 020 ----
mean loss: 161.14
 ---- batch: 030 ----
mean loss: 151.70
 ---- batch: 040 ----
mean loss: 157.97
 ---- batch: 050 ----
mean loss: 167.72
 ---- batch: 060 ----
mean loss: 158.81
 ---- batch: 070 ----
mean loss: 169.80
 ---- batch: 080 ----
mean loss: 164.62
 ---- batch: 090 ----
mean loss: 167.04
 ---- batch: 100 ----
mean loss: 167.55
 ---- batch: 110 ----
mean loss: 159.97
train mean loss: 162.03
epoch train time: 0:00:00.556717
elapsed time: 0:01:33.327473
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:33:48.845268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.57
 ---- batch: 020 ----
mean loss: 156.17
 ---- batch: 030 ----
mean loss: 162.12
 ---- batch: 040 ----
mean loss: 156.84
 ---- batch: 050 ----
mean loss: 160.92
 ---- batch: 060 ----
mean loss: 159.00
 ---- batch: 070 ----
mean loss: 167.77
 ---- batch: 080 ----
mean loss: 163.23
 ---- batch: 090 ----
mean loss: 156.66
 ---- batch: 100 ----
mean loss: 162.24
 ---- batch: 110 ----
mean loss: 167.51
train mean loss: 160.93
epoch train time: 0:00:00.547900
elapsed time: 0:01:33.875543
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:33:49.393338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.11
 ---- batch: 020 ----
mean loss: 157.84
 ---- batch: 030 ----
mean loss: 157.78
 ---- batch: 040 ----
mean loss: 152.54
 ---- batch: 050 ----
mean loss: 165.00
 ---- batch: 060 ----
mean loss: 161.96
 ---- batch: 070 ----
mean loss: 159.77
 ---- batch: 080 ----
mean loss: 163.75
 ---- batch: 090 ----
mean loss: 158.43
 ---- batch: 100 ----
mean loss: 157.30
 ---- batch: 110 ----
mean loss: 165.34
train mean loss: 160.19
epoch train time: 0:00:00.548938
elapsed time: 0:01:34.424612
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:33:49.942412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.39
 ---- batch: 020 ----
mean loss: 153.83
 ---- batch: 030 ----
mean loss: 160.48
 ---- batch: 040 ----
mean loss: 157.84
 ---- batch: 050 ----
mean loss: 152.49
 ---- batch: 060 ----
mean loss: 160.44
 ---- batch: 070 ----
mean loss: 160.66
 ---- batch: 080 ----
mean loss: 171.63
 ---- batch: 090 ----
mean loss: 169.73
 ---- batch: 100 ----
mean loss: 156.71
 ---- batch: 110 ----
mean loss: 164.38
train mean loss: 160.97
epoch train time: 0:00:00.548532
elapsed time: 0:01:34.973286
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:33:50.491099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.86
 ---- batch: 020 ----
mean loss: 151.62
 ---- batch: 030 ----
mean loss: 160.03
 ---- batch: 040 ----
mean loss: 150.84
 ---- batch: 050 ----
mean loss: 161.15
 ---- batch: 060 ----
mean loss: 160.42
 ---- batch: 070 ----
mean loss: 151.12
 ---- batch: 080 ----
mean loss: 165.88
 ---- batch: 090 ----
mean loss: 165.86
 ---- batch: 100 ----
mean loss: 163.68
 ---- batch: 110 ----
mean loss: 166.21
train mean loss: 160.07
epoch train time: 0:00:00.550928
elapsed time: 0:01:35.524371
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:33:51.042167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.96
 ---- batch: 020 ----
mean loss: 170.77
 ---- batch: 030 ----
mean loss: 156.25
 ---- batch: 040 ----
mean loss: 158.67
 ---- batch: 050 ----
mean loss: 162.75
 ---- batch: 060 ----
mean loss: 170.73
 ---- batch: 070 ----
mean loss: 158.07
 ---- batch: 080 ----
mean loss: 151.39
 ---- batch: 090 ----
mean loss: 156.74
 ---- batch: 100 ----
mean loss: 161.45
 ---- batch: 110 ----
mean loss: 158.73
train mean loss: 160.24
epoch train time: 0:00:00.550546
elapsed time: 0:01:36.075052
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:33:51.592846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.47
 ---- batch: 020 ----
mean loss: 163.66
 ---- batch: 030 ----
mean loss: 153.89
 ---- batch: 040 ----
mean loss: 151.39
 ---- batch: 050 ----
mean loss: 157.38
 ---- batch: 060 ----
mean loss: 159.67
 ---- batch: 070 ----
mean loss: 161.85
 ---- batch: 080 ----
mean loss: 166.59
 ---- batch: 090 ----
mean loss: 159.02
 ---- batch: 100 ----
mean loss: 152.50
 ---- batch: 110 ----
mean loss: 152.78
train mean loss: 158.43
epoch train time: 0:00:00.560502
elapsed time: 0:01:36.635693
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:33:52.153485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.80
 ---- batch: 020 ----
mean loss: 159.06
 ---- batch: 030 ----
mean loss: 155.45
 ---- batch: 040 ----
mean loss: 157.75
 ---- batch: 050 ----
mean loss: 160.15
 ---- batch: 060 ----
mean loss: 166.63
 ---- batch: 070 ----
mean loss: 166.94
 ---- batch: 080 ----
mean loss: 166.64
 ---- batch: 090 ----
mean loss: 156.69
 ---- batch: 100 ----
mean loss: 161.65
 ---- batch: 110 ----
mean loss: 156.87
train mean loss: 160.20
epoch train time: 0:00:00.557946
elapsed time: 0:01:37.193816
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:33:52.711632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.30
 ---- batch: 020 ----
mean loss: 160.66
 ---- batch: 030 ----
mean loss: 157.72
 ---- batch: 040 ----
mean loss: 152.04
 ---- batch: 050 ----
mean loss: 155.40
 ---- batch: 060 ----
mean loss: 158.87
 ---- batch: 070 ----
mean loss: 161.25
 ---- batch: 080 ----
mean loss: 159.95
 ---- batch: 090 ----
mean loss: 162.68
 ---- batch: 100 ----
mean loss: 161.74
 ---- batch: 110 ----
mean loss: 153.80
train mean loss: 159.09
epoch train time: 0:00:00.560557
elapsed time: 0:01:37.754613
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:33:53.272408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.03
 ---- batch: 020 ----
mean loss: 153.91
 ---- batch: 030 ----
mean loss: 154.87
 ---- batch: 040 ----
mean loss: 156.99
 ---- batch: 050 ----
mean loss: 161.52
 ---- batch: 060 ----
mean loss: 155.76
 ---- batch: 070 ----
mean loss: 156.98
 ---- batch: 080 ----
mean loss: 164.51
 ---- batch: 090 ----
mean loss: 161.39
 ---- batch: 100 ----
mean loss: 158.55
 ---- batch: 110 ----
mean loss: 162.55
train mean loss: 158.25
epoch train time: 0:00:00.560361
elapsed time: 0:01:38.315108
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:33:53.832932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.09
 ---- batch: 020 ----
mean loss: 152.32
 ---- batch: 030 ----
mean loss: 153.86
 ---- batch: 040 ----
mean loss: 155.01
 ---- batch: 050 ----
mean loss: 161.63
 ---- batch: 060 ----
mean loss: 159.73
 ---- batch: 070 ----
mean loss: 158.73
 ---- batch: 080 ----
mean loss: 154.28
 ---- batch: 090 ----
mean loss: 164.53
 ---- batch: 100 ----
mean loss: 157.02
 ---- batch: 110 ----
mean loss: 162.53
train mean loss: 157.91
epoch train time: 0:00:00.565752
elapsed time: 0:01:38.881058
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:33:54.398868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.20
 ---- batch: 020 ----
mean loss: 144.02
 ---- batch: 030 ----
mean loss: 160.47
 ---- batch: 040 ----
mean loss: 158.71
 ---- batch: 050 ----
mean loss: 155.49
 ---- batch: 060 ----
mean loss: 155.27
 ---- batch: 070 ----
mean loss: 164.16
 ---- batch: 080 ----
mean loss: 152.65
 ---- batch: 090 ----
mean loss: 167.58
 ---- batch: 100 ----
mean loss: 158.40
 ---- batch: 110 ----
mean loss: 155.31
train mean loss: 157.26
epoch train time: 0:00:00.550928
elapsed time: 0:01:39.432140
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:33:54.949942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.04
 ---- batch: 020 ----
mean loss: 150.22
 ---- batch: 030 ----
mean loss: 154.55
 ---- batch: 040 ----
mean loss: 156.13
 ---- batch: 050 ----
mean loss: 147.91
 ---- batch: 060 ----
mean loss: 153.15
 ---- batch: 070 ----
mean loss: 159.83
 ---- batch: 080 ----
mean loss: 159.00
 ---- batch: 090 ----
mean loss: 159.66
 ---- batch: 100 ----
mean loss: 164.76
 ---- batch: 110 ----
mean loss: 162.41
train mean loss: 156.44
epoch train time: 0:00:00.548812
elapsed time: 0:01:39.981095
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:33:55.498889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.61
 ---- batch: 020 ----
mean loss: 165.27
 ---- batch: 030 ----
mean loss: 150.14
 ---- batch: 040 ----
mean loss: 152.62
 ---- batch: 050 ----
mean loss: 165.19
 ---- batch: 060 ----
mean loss: 161.95
 ---- batch: 070 ----
mean loss: 165.66
 ---- batch: 080 ----
mean loss: 161.13
 ---- batch: 090 ----
mean loss: 151.52
 ---- batch: 100 ----
mean loss: 161.90
 ---- batch: 110 ----
mean loss: 150.87
train mean loss: 158.07
epoch train time: 0:00:00.559589
elapsed time: 0:01:40.540854
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:33:56.058664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.98
 ---- batch: 020 ----
mean loss: 152.05
 ---- batch: 030 ----
mean loss: 154.24
 ---- batch: 040 ----
mean loss: 156.97
 ---- batch: 050 ----
mean loss: 155.62
 ---- batch: 060 ----
mean loss: 155.69
 ---- batch: 070 ----
mean loss: 156.54
 ---- batch: 080 ----
mean loss: 155.88
 ---- batch: 090 ----
mean loss: 164.05
 ---- batch: 100 ----
mean loss: 163.51
 ---- batch: 110 ----
mean loss: 149.21
train mean loss: 155.99
epoch train time: 0:00:00.556286
elapsed time: 0:01:41.097291
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:33:56.615102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.49
 ---- batch: 020 ----
mean loss: 146.84
 ---- batch: 030 ----
mean loss: 156.30
 ---- batch: 040 ----
mean loss: 159.84
 ---- batch: 050 ----
mean loss: 164.19
 ---- batch: 060 ----
mean loss: 162.75
 ---- batch: 070 ----
mean loss: 163.95
 ---- batch: 080 ----
mean loss: 151.16
 ---- batch: 090 ----
mean loss: 155.80
 ---- batch: 100 ----
mean loss: 152.93
 ---- batch: 110 ----
mean loss: 151.01
train mean loss: 155.88
epoch train time: 0:00:00.552040
elapsed time: 0:01:41.649480
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:33:57.167273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.78
 ---- batch: 020 ----
mean loss: 145.69
 ---- batch: 030 ----
mean loss: 157.52
 ---- batch: 040 ----
mean loss: 159.42
 ---- batch: 050 ----
mean loss: 149.11
 ---- batch: 060 ----
mean loss: 161.47
 ---- batch: 070 ----
mean loss: 162.30
 ---- batch: 080 ----
mean loss: 161.34
 ---- batch: 090 ----
mean loss: 156.56
 ---- batch: 100 ----
mean loss: 152.11
 ---- batch: 110 ----
mean loss: 158.20
train mean loss: 156.40
epoch train time: 0:00:00.548538
elapsed time: 0:01:42.198155
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:33:57.715971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.42
 ---- batch: 020 ----
mean loss: 146.54
 ---- batch: 030 ----
mean loss: 156.78
 ---- batch: 040 ----
mean loss: 161.30
 ---- batch: 050 ----
mean loss: 156.42
 ---- batch: 060 ----
mean loss: 155.29
 ---- batch: 070 ----
mean loss: 155.82
 ---- batch: 080 ----
mean loss: 160.32
 ---- batch: 090 ----
mean loss: 157.44
 ---- batch: 100 ----
mean loss: 156.63
 ---- batch: 110 ----
mean loss: 153.43
train mean loss: 155.51
epoch train time: 0:00:00.559013
elapsed time: 0:01:42.757336
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:33:58.275137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.84
 ---- batch: 020 ----
mean loss: 152.49
 ---- batch: 030 ----
mean loss: 155.13
 ---- batch: 040 ----
mean loss: 140.91
 ---- batch: 050 ----
mean loss: 165.49
 ---- batch: 060 ----
mean loss: 158.27
 ---- batch: 070 ----
mean loss: 155.84
 ---- batch: 080 ----
mean loss: 155.24
 ---- batch: 090 ----
mean loss: 157.93
 ---- batch: 100 ----
mean loss: 153.73
 ---- batch: 110 ----
mean loss: 157.23
train mean loss: 155.49
epoch train time: 0:00:00.550221
elapsed time: 0:01:43.307704
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:33:58.825497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.43
 ---- batch: 020 ----
mean loss: 143.47
 ---- batch: 030 ----
mean loss: 155.01
 ---- batch: 040 ----
mean loss: 154.43
 ---- batch: 050 ----
mean loss: 156.71
 ---- batch: 060 ----
mean loss: 156.10
 ---- batch: 070 ----
mean loss: 159.08
 ---- batch: 080 ----
mean loss: 162.94
 ---- batch: 090 ----
mean loss: 152.82
 ---- batch: 100 ----
mean loss: 165.99
 ---- batch: 110 ----
mean loss: 147.24
train mean loss: 155.54
epoch train time: 0:00:00.552562
elapsed time: 0:01:43.860401
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:33:59.378195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.44
 ---- batch: 020 ----
mean loss: 158.86
 ---- batch: 030 ----
mean loss: 152.30
 ---- batch: 040 ----
mean loss: 155.22
 ---- batch: 050 ----
mean loss: 155.03
 ---- batch: 060 ----
mean loss: 156.18
 ---- batch: 070 ----
mean loss: 146.41
 ---- batch: 080 ----
mean loss: 155.74
 ---- batch: 090 ----
mean loss: 155.55
 ---- batch: 100 ----
mean loss: 154.74
 ---- batch: 110 ----
mean loss: 159.51
train mean loss: 154.59
epoch train time: 0:00:00.550558
elapsed time: 0:01:44.411108
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:33:59.928900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.46
 ---- batch: 020 ----
mean loss: 150.78
 ---- batch: 030 ----
mean loss: 153.28
 ---- batch: 040 ----
mean loss: 157.10
 ---- batch: 050 ----
mean loss: 150.75
 ---- batch: 060 ----
mean loss: 150.86
 ---- batch: 070 ----
mean loss: 160.86
 ---- batch: 080 ----
mean loss: 156.11
 ---- batch: 090 ----
mean loss: 159.05
 ---- batch: 100 ----
mean loss: 152.05
 ---- batch: 110 ----
mean loss: 155.63
train mean loss: 154.34
epoch train time: 0:00:00.547557
elapsed time: 0:01:44.958811
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:34:00.476605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.14
 ---- batch: 020 ----
mean loss: 151.92
 ---- batch: 030 ----
mean loss: 157.00
 ---- batch: 040 ----
mean loss: 156.20
 ---- batch: 050 ----
mean loss: 148.62
 ---- batch: 060 ----
mean loss: 151.16
 ---- batch: 070 ----
mean loss: 155.38
 ---- batch: 080 ----
mean loss: 150.63
 ---- batch: 090 ----
mean loss: 149.23
 ---- batch: 100 ----
mean loss: 156.01
 ---- batch: 110 ----
mean loss: 158.88
train mean loss: 153.85
epoch train time: 0:00:00.560664
elapsed time: 0:01:45.519625
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:34:01.037446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.48
 ---- batch: 020 ----
mean loss: 150.80
 ---- batch: 030 ----
mean loss: 150.98
 ---- batch: 040 ----
mean loss: 154.95
 ---- batch: 050 ----
mean loss: 154.00
 ---- batch: 060 ----
mean loss: 156.97
 ---- batch: 070 ----
mean loss: 157.99
 ---- batch: 080 ----
mean loss: 147.38
 ---- batch: 090 ----
mean loss: 146.73
 ---- batch: 100 ----
mean loss: 155.73
 ---- batch: 110 ----
mean loss: 153.34
train mean loss: 152.79
epoch train time: 0:00:00.547224
elapsed time: 0:01:46.067020
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:34:01.584812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.40
 ---- batch: 020 ----
mean loss: 148.40
 ---- batch: 030 ----
mean loss: 149.74
 ---- batch: 040 ----
mean loss: 148.04
 ---- batch: 050 ----
mean loss: 151.74
 ---- batch: 060 ----
mean loss: 146.04
 ---- batch: 070 ----
mean loss: 157.54
 ---- batch: 080 ----
mean loss: 152.59
 ---- batch: 090 ----
mean loss: 168.89
 ---- batch: 100 ----
mean loss: 145.85
 ---- batch: 110 ----
mean loss: 160.88
train mean loss: 153.91
epoch train time: 0:00:00.561660
elapsed time: 0:01:46.628809
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:34:02.146598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.31
 ---- batch: 020 ----
mean loss: 153.93
 ---- batch: 030 ----
mean loss: 155.90
 ---- batch: 040 ----
mean loss: 154.17
 ---- batch: 050 ----
mean loss: 153.81
 ---- batch: 060 ----
mean loss: 153.72
 ---- batch: 070 ----
mean loss: 155.14
 ---- batch: 080 ----
mean loss: 150.71
 ---- batch: 090 ----
mean loss: 149.60
 ---- batch: 100 ----
mean loss: 157.87
 ---- batch: 110 ----
mean loss: 153.65
train mean loss: 153.65
epoch train time: 0:00:00.548863
elapsed time: 0:01:47.177803
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:34:02.695600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.36
 ---- batch: 020 ----
mean loss: 154.76
 ---- batch: 030 ----
mean loss: 154.77
 ---- batch: 040 ----
mean loss: 159.39
 ---- batch: 050 ----
mean loss: 149.33
 ---- batch: 060 ----
mean loss: 155.41
 ---- batch: 070 ----
mean loss: 151.77
 ---- batch: 080 ----
mean loss: 155.05
 ---- batch: 090 ----
mean loss: 153.99
 ---- batch: 100 ----
mean loss: 146.36
 ---- batch: 110 ----
mean loss: 156.63
train mean loss: 153.43
epoch train time: 0:00:00.546442
elapsed time: 0:01:47.724396
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:34:03.242189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.79
 ---- batch: 020 ----
mean loss: 151.44
 ---- batch: 030 ----
mean loss: 154.52
 ---- batch: 040 ----
mean loss: 151.20
 ---- batch: 050 ----
mean loss: 150.25
 ---- batch: 060 ----
mean loss: 151.76
 ---- batch: 070 ----
mean loss: 156.95
 ---- batch: 080 ----
mean loss: 153.14
 ---- batch: 090 ----
mean loss: 155.65
 ---- batch: 100 ----
mean loss: 148.09
 ---- batch: 110 ----
mean loss: 147.57
train mean loss: 153.03
epoch train time: 0:00:00.562587
elapsed time: 0:01:48.287115
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:34:03.804918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.42
 ---- batch: 020 ----
mean loss: 151.84
 ---- batch: 030 ----
mean loss: 149.10
 ---- batch: 040 ----
mean loss: 152.92
 ---- batch: 050 ----
mean loss: 155.05
 ---- batch: 060 ----
mean loss: 155.58
 ---- batch: 070 ----
mean loss: 148.29
 ---- batch: 080 ----
mean loss: 150.51
 ---- batch: 090 ----
mean loss: 143.03
 ---- batch: 100 ----
mean loss: 160.75
 ---- batch: 110 ----
mean loss: 163.84
train mean loss: 151.80
epoch train time: 0:00:00.550277
elapsed time: 0:01:48.837553
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:34:04.355379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.16
 ---- batch: 020 ----
mean loss: 157.05
 ---- batch: 030 ----
mean loss: 142.33
 ---- batch: 040 ----
mean loss: 158.73
 ---- batch: 050 ----
mean loss: 146.76
 ---- batch: 060 ----
mean loss: 155.16
 ---- batch: 070 ----
mean loss: 146.75
 ---- batch: 080 ----
mean loss: 153.71
 ---- batch: 090 ----
mean loss: 147.55
 ---- batch: 100 ----
mean loss: 157.88
 ---- batch: 110 ----
mean loss: 148.12
train mean loss: 152.04
epoch train time: 0:00:00.555562
elapsed time: 0:01:49.393280
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:34:04.911094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.03
 ---- batch: 020 ----
mean loss: 153.03
 ---- batch: 030 ----
mean loss: 153.53
 ---- batch: 040 ----
mean loss: 160.05
 ---- batch: 050 ----
mean loss: 155.88
 ---- batch: 060 ----
mean loss: 153.19
 ---- batch: 070 ----
mean loss: 150.41
 ---- batch: 080 ----
mean loss: 147.10
 ---- batch: 090 ----
mean loss: 150.51
 ---- batch: 100 ----
mean loss: 142.30
 ---- batch: 110 ----
mean loss: 156.50
train mean loss: 151.68
epoch train time: 0:00:00.546823
elapsed time: 0:01:49.940273
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:34:05.458069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.31
 ---- batch: 020 ----
mean loss: 150.18
 ---- batch: 030 ----
mean loss: 151.39
 ---- batch: 040 ----
mean loss: 155.04
 ---- batch: 050 ----
mean loss: 151.32
 ---- batch: 060 ----
mean loss: 142.08
 ---- batch: 070 ----
mean loss: 151.63
 ---- batch: 080 ----
mean loss: 158.17
 ---- batch: 090 ----
mean loss: 161.83
 ---- batch: 100 ----
mean loss: 152.31
 ---- batch: 110 ----
mean loss: 146.23
train mean loss: 151.12
epoch train time: 0:00:00.555825
elapsed time: 0:01:50.496234
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:34:06.014026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.21
 ---- batch: 020 ----
mean loss: 151.66
 ---- batch: 030 ----
mean loss: 149.95
 ---- batch: 040 ----
mean loss: 144.78
 ---- batch: 050 ----
mean loss: 152.50
 ---- batch: 060 ----
mean loss: 152.99
 ---- batch: 070 ----
mean loss: 146.77
 ---- batch: 080 ----
mean loss: 154.27
 ---- batch: 090 ----
mean loss: 153.37
 ---- batch: 100 ----
mean loss: 150.61
 ---- batch: 110 ----
mean loss: 145.66
train mean loss: 150.96
epoch train time: 0:00:00.551077
elapsed time: 0:01:51.047443
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:34:06.565236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.78
 ---- batch: 020 ----
mean loss: 151.93
 ---- batch: 030 ----
mean loss: 145.10
 ---- batch: 040 ----
mean loss: 148.85
 ---- batch: 050 ----
mean loss: 145.66
 ---- batch: 060 ----
mean loss: 149.68
 ---- batch: 070 ----
mean loss: 149.33
 ---- batch: 080 ----
mean loss: 154.08
 ---- batch: 090 ----
mean loss: 147.18
 ---- batch: 100 ----
mean loss: 158.65
 ---- batch: 110 ----
mean loss: 158.71
train mean loss: 150.57
epoch train time: 0:00:00.550140
elapsed time: 0:01:51.597717
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:34:07.115507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.43
 ---- batch: 020 ----
mean loss: 148.51
 ---- batch: 030 ----
mean loss: 144.54
 ---- batch: 040 ----
mean loss: 149.85
 ---- batch: 050 ----
mean loss: 151.98
 ---- batch: 060 ----
mean loss: 143.88
 ---- batch: 070 ----
mean loss: 158.57
 ---- batch: 080 ----
mean loss: 158.51
 ---- batch: 090 ----
mean loss: 159.98
 ---- batch: 100 ----
mean loss: 155.19
 ---- batch: 110 ----
mean loss: 143.57
train mean loss: 151.33
epoch train time: 0:00:00.553099
elapsed time: 0:01:52.150977
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:34:07.668773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.09
 ---- batch: 020 ----
mean loss: 145.84
 ---- batch: 030 ----
mean loss: 154.82
 ---- batch: 040 ----
mean loss: 154.42
 ---- batch: 050 ----
mean loss: 147.24
 ---- batch: 060 ----
mean loss: 149.85
 ---- batch: 070 ----
mean loss: 144.22
 ---- batch: 080 ----
mean loss: 151.25
 ---- batch: 090 ----
mean loss: 149.45
 ---- batch: 100 ----
mean loss: 151.24
 ---- batch: 110 ----
mean loss: 155.94
train mean loss: 150.12
epoch train time: 0:00:00.556653
elapsed time: 0:01:52.707771
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:34:08.225564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.55
 ---- batch: 020 ----
mean loss: 155.85
 ---- batch: 030 ----
mean loss: 152.58
 ---- batch: 040 ----
mean loss: 146.37
 ---- batch: 050 ----
mean loss: 158.78
 ---- batch: 060 ----
mean loss: 151.46
 ---- batch: 070 ----
mean loss: 147.26
 ---- batch: 080 ----
mean loss: 149.28
 ---- batch: 090 ----
mean loss: 153.91
 ---- batch: 100 ----
mean loss: 150.69
 ---- batch: 110 ----
mean loss: 148.76
train mean loss: 150.18
epoch train time: 0:00:00.555968
elapsed time: 0:01:53.263897
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:34:08.781717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.31
 ---- batch: 020 ----
mean loss: 151.29
 ---- batch: 030 ----
mean loss: 142.63
 ---- batch: 040 ----
mean loss: 148.17
 ---- batch: 050 ----
mean loss: 145.40
 ---- batch: 060 ----
mean loss: 150.62
 ---- batch: 070 ----
mean loss: 154.03
 ---- batch: 080 ----
mean loss: 145.20
 ---- batch: 090 ----
mean loss: 149.65
 ---- batch: 100 ----
mean loss: 147.18
 ---- batch: 110 ----
mean loss: 151.58
train mean loss: 149.01
epoch train time: 0:00:00.557796
elapsed time: 0:01:53.821852
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:34:09.339646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.79
 ---- batch: 020 ----
mean loss: 148.94
 ---- batch: 030 ----
mean loss: 146.29
 ---- batch: 040 ----
mean loss: 147.86
 ---- batch: 050 ----
mean loss: 151.83
 ---- batch: 060 ----
mean loss: 147.48
 ---- batch: 070 ----
mean loss: 147.88
 ---- batch: 080 ----
mean loss: 149.67
 ---- batch: 090 ----
mean loss: 152.68
 ---- batch: 100 ----
mean loss: 159.65
 ---- batch: 110 ----
mean loss: 144.83
train mean loss: 149.38
epoch train time: 0:00:00.574069
elapsed time: 0:01:54.396068
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:34:09.913862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.91
 ---- batch: 020 ----
mean loss: 153.47
 ---- batch: 030 ----
mean loss: 150.82
 ---- batch: 040 ----
mean loss: 146.91
 ---- batch: 050 ----
mean loss: 142.41
 ---- batch: 060 ----
mean loss: 143.90
 ---- batch: 070 ----
mean loss: 151.66
 ---- batch: 080 ----
mean loss: 158.90
 ---- batch: 090 ----
mean loss: 151.82
 ---- batch: 100 ----
mean loss: 152.02
 ---- batch: 110 ----
mean loss: 149.53
train mean loss: 149.19
epoch train time: 0:00:00.550739
elapsed time: 0:01:54.946982
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:34:10.464764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.56
 ---- batch: 020 ----
mean loss: 147.84
 ---- batch: 030 ----
mean loss: 144.57
 ---- batch: 040 ----
mean loss: 137.99
 ---- batch: 050 ----
mean loss: 142.72
 ---- batch: 060 ----
mean loss: 152.86
 ---- batch: 070 ----
mean loss: 156.87
 ---- batch: 080 ----
mean loss: 153.50
 ---- batch: 090 ----
mean loss: 153.30
 ---- batch: 100 ----
mean loss: 151.84
 ---- batch: 110 ----
mean loss: 146.95
train mean loss: 149.36
epoch train time: 0:00:00.553228
elapsed time: 0:01:55.500333
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:34:11.018127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.93
 ---- batch: 020 ----
mean loss: 141.74
 ---- batch: 030 ----
mean loss: 148.71
 ---- batch: 040 ----
mean loss: 146.50
 ---- batch: 050 ----
mean loss: 150.11
 ---- batch: 060 ----
mean loss: 155.27
 ---- batch: 070 ----
mean loss: 149.61
 ---- batch: 080 ----
mean loss: 155.87
 ---- batch: 090 ----
mean loss: 153.03
 ---- batch: 100 ----
mean loss: 146.33
 ---- batch: 110 ----
mean loss: 142.50
train mean loss: 148.53
epoch train time: 0:00:00.560456
elapsed time: 0:01:56.060941
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:34:11.578735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.07
 ---- batch: 020 ----
mean loss: 142.08
 ---- batch: 030 ----
mean loss: 152.62
 ---- batch: 040 ----
mean loss: 147.91
 ---- batch: 050 ----
mean loss: 145.03
 ---- batch: 060 ----
mean loss: 156.22
 ---- batch: 070 ----
mean loss: 150.43
 ---- batch: 080 ----
mean loss: 148.64
 ---- batch: 090 ----
mean loss: 152.42
 ---- batch: 100 ----
mean loss: 152.70
 ---- batch: 110 ----
mean loss: 144.47
train mean loss: 149.40
epoch train time: 0:00:00.556070
elapsed time: 0:01:56.617149
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:34:12.134963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.23
 ---- batch: 020 ----
mean loss: 143.92
 ---- batch: 030 ----
mean loss: 148.38
 ---- batch: 040 ----
mean loss: 147.53
 ---- batch: 050 ----
mean loss: 147.72
 ---- batch: 060 ----
mean loss: 149.98
 ---- batch: 070 ----
mean loss: 147.10
 ---- batch: 080 ----
mean loss: 154.92
 ---- batch: 090 ----
mean loss: 147.69
 ---- batch: 100 ----
mean loss: 150.16
 ---- batch: 110 ----
mean loss: 145.90
train mean loss: 148.90
epoch train time: 0:00:00.569924
elapsed time: 0:01:57.187242
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:34:12.705036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.53
 ---- batch: 020 ----
mean loss: 136.82
 ---- batch: 030 ----
mean loss: 148.13
 ---- batch: 040 ----
mean loss: 148.21
 ---- batch: 050 ----
mean loss: 151.70
 ---- batch: 060 ----
mean loss: 153.40
 ---- batch: 070 ----
mean loss: 154.34
 ---- batch: 080 ----
mean loss: 147.55
 ---- batch: 090 ----
mean loss: 141.60
 ---- batch: 100 ----
mean loss: 149.36
 ---- batch: 110 ----
mean loss: 149.74
train mean loss: 148.33
epoch train time: 0:00:00.555622
elapsed time: 0:01:57.743017
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:34:13.260811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.18
 ---- batch: 020 ----
mean loss: 142.00
 ---- batch: 030 ----
mean loss: 147.45
 ---- batch: 040 ----
mean loss: 147.12
 ---- batch: 050 ----
mean loss: 147.61
 ---- batch: 060 ----
mean loss: 149.10
 ---- batch: 070 ----
mean loss: 147.24
 ---- batch: 080 ----
mean loss: 144.39
 ---- batch: 090 ----
mean loss: 154.29
 ---- batch: 100 ----
mean loss: 144.71
 ---- batch: 110 ----
mean loss: 157.01
train mean loss: 147.61
epoch train time: 0:00:00.552418
elapsed time: 0:01:58.295569
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:34:13.813363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.91
 ---- batch: 020 ----
mean loss: 141.53
 ---- batch: 030 ----
mean loss: 148.81
 ---- batch: 040 ----
mean loss: 143.06
 ---- batch: 050 ----
mean loss: 144.72
 ---- batch: 060 ----
mean loss: 143.30
 ---- batch: 070 ----
mean loss: 155.12
 ---- batch: 080 ----
mean loss: 148.04
 ---- batch: 090 ----
mean loss: 151.68
 ---- batch: 100 ----
mean loss: 151.57
 ---- batch: 110 ----
mean loss: 155.87
train mean loss: 148.27
epoch train time: 0:00:00.548721
elapsed time: 0:01:58.844424
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:34:14.362218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.74
 ---- batch: 020 ----
mean loss: 144.48
 ---- batch: 030 ----
mean loss: 154.04
 ---- batch: 040 ----
mean loss: 157.51
 ---- batch: 050 ----
mean loss: 147.49
 ---- batch: 060 ----
mean loss: 147.38
 ---- batch: 070 ----
mean loss: 149.34
 ---- batch: 080 ----
mean loss: 145.18
 ---- batch: 090 ----
mean loss: 143.17
 ---- batch: 100 ----
mean loss: 150.64
 ---- batch: 110 ----
mean loss: 145.18
train mean loss: 148.37
epoch train time: 0:00:00.549120
elapsed time: 0:01:59.393700
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:34:14.911506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.58
 ---- batch: 020 ----
mean loss: 141.50
 ---- batch: 030 ----
mean loss: 155.40
 ---- batch: 040 ----
mean loss: 144.71
 ---- batch: 050 ----
mean loss: 137.86
 ---- batch: 060 ----
mean loss: 149.23
 ---- batch: 070 ----
mean loss: 141.34
 ---- batch: 080 ----
mean loss: 148.17
 ---- batch: 090 ----
mean loss: 154.95
 ---- batch: 100 ----
mean loss: 147.65
 ---- batch: 110 ----
mean loss: 156.23
train mean loss: 146.95
epoch train time: 0:00:00.548785
elapsed time: 0:01:59.942628
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:34:15.460420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.43
 ---- batch: 020 ----
mean loss: 143.14
 ---- batch: 030 ----
mean loss: 148.18
 ---- batch: 040 ----
mean loss: 143.71
 ---- batch: 050 ----
mean loss: 147.95
 ---- batch: 060 ----
mean loss: 144.17
 ---- batch: 070 ----
mean loss: 153.21
 ---- batch: 080 ----
mean loss: 145.55
 ---- batch: 090 ----
mean loss: 137.16
 ---- batch: 100 ----
mean loss: 149.11
 ---- batch: 110 ----
mean loss: 152.15
train mean loss: 146.62
epoch train time: 0:00:00.551541
elapsed time: 0:02:00.494299
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:34:16.012108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.87
 ---- batch: 020 ----
mean loss: 148.59
 ---- batch: 030 ----
mean loss: 146.31
 ---- batch: 040 ----
mean loss: 153.82
 ---- batch: 050 ----
mean loss: 146.55
 ---- batch: 060 ----
mean loss: 142.39
 ---- batch: 070 ----
mean loss: 147.74
 ---- batch: 080 ----
mean loss: 149.05
 ---- batch: 090 ----
mean loss: 142.83
 ---- batch: 100 ----
mean loss: 155.46
 ---- batch: 110 ----
mean loss: 142.31
train mean loss: 147.85
epoch train time: 0:00:00.551069
elapsed time: 0:02:01.045516
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:34:16.563307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.10
 ---- batch: 020 ----
mean loss: 143.68
 ---- batch: 030 ----
mean loss: 142.08
 ---- batch: 040 ----
mean loss: 151.07
 ---- batch: 050 ----
mean loss: 156.98
 ---- batch: 060 ----
mean loss: 139.31
 ---- batch: 070 ----
mean loss: 139.01
 ---- batch: 080 ----
mean loss: 156.42
 ---- batch: 090 ----
mean loss: 152.80
 ---- batch: 100 ----
mean loss: 139.64
 ---- batch: 110 ----
mean loss: 142.85
train mean loss: 146.23
epoch train time: 0:00:00.551743
elapsed time: 0:02:01.597402
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:34:17.115212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.11
 ---- batch: 020 ----
mean loss: 142.77
 ---- batch: 030 ----
mean loss: 141.08
 ---- batch: 040 ----
mean loss: 149.25
 ---- batch: 050 ----
mean loss: 153.93
 ---- batch: 060 ----
mean loss: 152.93
 ---- batch: 070 ----
mean loss: 144.10
 ---- batch: 080 ----
mean loss: 143.16
 ---- batch: 090 ----
mean loss: 144.06
 ---- batch: 100 ----
mean loss: 149.74
 ---- batch: 110 ----
mean loss: 150.29
train mean loss: 146.39
epoch train time: 0:00:00.546975
elapsed time: 0:02:02.144525
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:34:17.662319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.81
 ---- batch: 020 ----
mean loss: 150.32
 ---- batch: 030 ----
mean loss: 144.71
 ---- batch: 040 ----
mean loss: 138.07
 ---- batch: 050 ----
mean loss: 154.35
 ---- batch: 060 ----
mean loss: 146.57
 ---- batch: 070 ----
mean loss: 143.13
 ---- batch: 080 ----
mean loss: 130.73
 ---- batch: 090 ----
mean loss: 145.26
 ---- batch: 100 ----
mean loss: 153.99
 ---- batch: 110 ----
mean loss: 151.17
train mean loss: 145.78
epoch train time: 0:00:00.568770
elapsed time: 0:02:02.713427
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:34:18.231234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.61
 ---- batch: 020 ----
mean loss: 145.05
 ---- batch: 030 ----
mean loss: 150.22
 ---- batch: 040 ----
mean loss: 151.09
 ---- batch: 050 ----
mean loss: 150.16
 ---- batch: 060 ----
mean loss: 144.55
 ---- batch: 070 ----
mean loss: 142.09
 ---- batch: 080 ----
mean loss: 146.36
 ---- batch: 090 ----
mean loss: 142.38
 ---- batch: 100 ----
mean loss: 139.14
 ---- batch: 110 ----
mean loss: 145.01
train mean loss: 145.94
epoch train time: 0:00:00.556403
elapsed time: 0:02:03.269991
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:34:18.787783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.12
 ---- batch: 020 ----
mean loss: 149.15
 ---- batch: 030 ----
mean loss: 145.61
 ---- batch: 040 ----
mean loss: 144.82
 ---- batch: 050 ----
mean loss: 146.33
 ---- batch: 060 ----
mean loss: 142.95
 ---- batch: 070 ----
mean loss: 154.77
 ---- batch: 080 ----
mean loss: 144.66
 ---- batch: 090 ----
mean loss: 133.20
 ---- batch: 100 ----
mean loss: 149.40
 ---- batch: 110 ----
mean loss: 146.05
train mean loss: 145.29
epoch train time: 0:00:00.551875
elapsed time: 0:02:03.821995
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:34:19.339805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.82
 ---- batch: 020 ----
mean loss: 141.64
 ---- batch: 030 ----
mean loss: 149.04
 ---- batch: 040 ----
mean loss: 141.42
 ---- batch: 050 ----
mean loss: 158.20
 ---- batch: 060 ----
mean loss: 146.49
 ---- batch: 070 ----
mean loss: 144.32
 ---- batch: 080 ----
mean loss: 144.55
 ---- batch: 090 ----
mean loss: 155.93
 ---- batch: 100 ----
mean loss: 140.63
 ---- batch: 110 ----
mean loss: 135.04
train mean loss: 145.75
epoch train time: 0:00:00.552772
elapsed time: 0:02:04.374915
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:34:19.892707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.94
 ---- batch: 020 ----
mean loss: 147.27
 ---- batch: 030 ----
mean loss: 140.32
 ---- batch: 040 ----
mean loss: 144.96
 ---- batch: 050 ----
mean loss: 140.06
 ---- batch: 060 ----
mean loss: 145.98
 ---- batch: 070 ----
mean loss: 135.82
 ---- batch: 080 ----
mean loss: 154.44
 ---- batch: 090 ----
mean loss: 147.78
 ---- batch: 100 ----
mean loss: 159.69
 ---- batch: 110 ----
mean loss: 138.63
train mean loss: 144.80
epoch train time: 0:00:00.547510
elapsed time: 0:02:04.922552
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:34:20.440344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.18
 ---- batch: 020 ----
mean loss: 144.15
 ---- batch: 030 ----
mean loss: 139.53
 ---- batch: 040 ----
mean loss: 141.17
 ---- batch: 050 ----
mean loss: 143.93
 ---- batch: 060 ----
mean loss: 143.47
 ---- batch: 070 ----
mean loss: 142.84
 ---- batch: 080 ----
mean loss: 140.63
 ---- batch: 090 ----
mean loss: 149.42
 ---- batch: 100 ----
mean loss: 151.41
 ---- batch: 110 ----
mean loss: 149.26
train mean loss: 144.28
epoch train time: 0:00:00.545888
elapsed time: 0:02:05.468606
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:34:20.986402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.39
 ---- batch: 020 ----
mean loss: 146.32
 ---- batch: 030 ----
mean loss: 137.41
 ---- batch: 040 ----
mean loss: 154.13
 ---- batch: 050 ----
mean loss: 149.96
 ---- batch: 060 ----
mean loss: 139.77
 ---- batch: 070 ----
mean loss: 137.64
 ---- batch: 080 ----
mean loss: 151.38
 ---- batch: 090 ----
mean loss: 145.29
 ---- batch: 100 ----
mean loss: 145.71
 ---- batch: 110 ----
mean loss: 143.50
train mean loss: 144.44
epoch train time: 0:00:00.552896
elapsed time: 0:02:06.021647
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:34:21.539442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.40
 ---- batch: 020 ----
mean loss: 142.52
 ---- batch: 030 ----
mean loss: 134.92
 ---- batch: 040 ----
mean loss: 151.00
 ---- batch: 050 ----
mean loss: 150.73
 ---- batch: 060 ----
mean loss: 148.58
 ---- batch: 070 ----
mean loss: 144.87
 ---- batch: 080 ----
mean loss: 148.34
 ---- batch: 090 ----
mean loss: 145.23
 ---- batch: 100 ----
mean loss: 151.31
 ---- batch: 110 ----
mean loss: 137.83
train mean loss: 144.64
epoch train time: 0:00:00.559038
elapsed time: 0:02:06.580821
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:34:22.098615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.85
 ---- batch: 020 ----
mean loss: 143.30
 ---- batch: 030 ----
mean loss: 142.62
 ---- batch: 040 ----
mean loss: 142.51
 ---- batch: 050 ----
mean loss: 145.79
 ---- batch: 060 ----
mean loss: 148.09
 ---- batch: 070 ----
mean loss: 143.16
 ---- batch: 080 ----
mean loss: 149.11
 ---- batch: 090 ----
mean loss: 147.56
 ---- batch: 100 ----
mean loss: 140.03
 ---- batch: 110 ----
mean loss: 140.56
train mean loss: 144.48
epoch train time: 0:00:00.554582
elapsed time: 0:02:07.135536
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:34:22.653331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.28
 ---- batch: 020 ----
mean loss: 151.48
 ---- batch: 030 ----
mean loss: 154.53
 ---- batch: 040 ----
mean loss: 140.17
 ---- batch: 050 ----
mean loss: 140.11
 ---- batch: 060 ----
mean loss: 139.27
 ---- batch: 070 ----
mean loss: 150.26
 ---- batch: 080 ----
mean loss: 140.56
 ---- batch: 090 ----
mean loss: 142.92
 ---- batch: 100 ----
mean loss: 147.30
 ---- batch: 110 ----
mean loss: 143.70
train mean loss: 144.28
epoch train time: 0:00:00.555744
elapsed time: 0:02:07.691429
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:34:23.209249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.12
 ---- batch: 020 ----
mean loss: 142.44
 ---- batch: 030 ----
mean loss: 146.98
 ---- batch: 040 ----
mean loss: 140.59
 ---- batch: 050 ----
mean loss: 144.32
 ---- batch: 060 ----
mean loss: 146.70
 ---- batch: 070 ----
mean loss: 136.32
 ---- batch: 080 ----
mean loss: 145.84
 ---- batch: 090 ----
mean loss: 148.25
 ---- batch: 100 ----
mean loss: 147.39
 ---- batch: 110 ----
mean loss: 150.62
train mean loss: 144.24
epoch train time: 0:00:00.557557
elapsed time: 0:02:08.249184
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:34:23.767012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.11
 ---- batch: 020 ----
mean loss: 138.72
 ---- batch: 030 ----
mean loss: 143.74
 ---- batch: 040 ----
mean loss: 144.03
 ---- batch: 050 ----
mean loss: 138.17
 ---- batch: 060 ----
mean loss: 142.35
 ---- batch: 070 ----
mean loss: 149.58
 ---- batch: 080 ----
mean loss: 136.98
 ---- batch: 090 ----
mean loss: 148.31
 ---- batch: 100 ----
mean loss: 138.70
 ---- batch: 110 ----
mean loss: 152.02
train mean loss: 143.49
epoch train time: 0:00:00.548189
elapsed time: 0:02:08.797553
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:34:24.315338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.52
 ---- batch: 020 ----
mean loss: 143.46
 ---- batch: 030 ----
mean loss: 141.45
 ---- batch: 040 ----
mean loss: 145.38
 ---- batch: 050 ----
mean loss: 148.38
 ---- batch: 060 ----
mean loss: 141.70
 ---- batch: 070 ----
mean loss: 146.24
 ---- batch: 080 ----
mean loss: 139.36
 ---- batch: 090 ----
mean loss: 146.76
 ---- batch: 100 ----
mean loss: 148.44
 ---- batch: 110 ----
mean loss: 144.90
train mean loss: 143.72
epoch train time: 0:00:00.546957
elapsed time: 0:02:09.344649
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:34:24.862442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.46
 ---- batch: 020 ----
mean loss: 139.86
 ---- batch: 030 ----
mean loss: 139.33
 ---- batch: 040 ----
mean loss: 140.36
 ---- batch: 050 ----
mean loss: 138.62
 ---- batch: 060 ----
mean loss: 144.58
 ---- batch: 070 ----
mean loss: 150.14
 ---- batch: 080 ----
mean loss: 145.53
 ---- batch: 090 ----
mean loss: 141.76
 ---- batch: 100 ----
mean loss: 147.77
 ---- batch: 110 ----
mean loss: 142.76
train mean loss: 143.59
epoch train time: 0:00:00.548623
elapsed time: 0:02:09.893408
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:34:25.411202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.88
 ---- batch: 020 ----
mean loss: 144.46
 ---- batch: 030 ----
mean loss: 136.74
 ---- batch: 040 ----
mean loss: 150.69
 ---- batch: 050 ----
mean loss: 150.04
 ---- batch: 060 ----
mean loss: 142.27
 ---- batch: 070 ----
mean loss: 143.11
 ---- batch: 080 ----
mean loss: 146.34
 ---- batch: 090 ----
mean loss: 138.50
 ---- batch: 100 ----
mean loss: 142.49
 ---- batch: 110 ----
mean loss: 142.26
train mean loss: 143.67
epoch train time: 0:00:00.555732
elapsed time: 0:02:10.449271
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:34:25.967063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.37
 ---- batch: 020 ----
mean loss: 150.31
 ---- batch: 030 ----
mean loss: 147.34
 ---- batch: 040 ----
mean loss: 136.87
 ---- batch: 050 ----
mean loss: 141.48
 ---- batch: 060 ----
mean loss: 142.89
 ---- batch: 070 ----
mean loss: 135.14
 ---- batch: 080 ----
mean loss: 142.83
 ---- batch: 090 ----
mean loss: 143.57
 ---- batch: 100 ----
mean loss: 147.17
 ---- batch: 110 ----
mean loss: 144.89
train mean loss: 143.76
epoch train time: 0:00:00.548803
elapsed time: 0:02:10.998212
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:34:26.516023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.55
 ---- batch: 020 ----
mean loss: 148.41
 ---- batch: 030 ----
mean loss: 146.49
 ---- batch: 040 ----
mean loss: 136.29
 ---- batch: 050 ----
mean loss: 140.07
 ---- batch: 060 ----
mean loss: 144.44
 ---- batch: 070 ----
mean loss: 139.25
 ---- batch: 080 ----
mean loss: 135.31
 ---- batch: 090 ----
mean loss: 148.64
 ---- batch: 100 ----
mean loss: 137.57
 ---- batch: 110 ----
mean loss: 148.42
train mean loss: 142.67
epoch train time: 0:00:00.552734
elapsed time: 0:02:11.551128
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:34:27.068938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.15
 ---- batch: 020 ----
mean loss: 142.23
 ---- batch: 030 ----
mean loss: 135.85
 ---- batch: 040 ----
mean loss: 147.73
 ---- batch: 050 ----
mean loss: 144.36
 ---- batch: 060 ----
mean loss: 139.31
 ---- batch: 070 ----
mean loss: 145.49
 ---- batch: 080 ----
mean loss: 145.92
 ---- batch: 090 ----
mean loss: 141.39
 ---- batch: 100 ----
mean loss: 144.95
 ---- batch: 110 ----
mean loss: 151.95
train mean loss: 143.26
epoch train time: 0:00:00.559022
elapsed time: 0:02:12.110296
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:34:27.628092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.71
 ---- batch: 020 ----
mean loss: 147.40
 ---- batch: 030 ----
mean loss: 137.49
 ---- batch: 040 ----
mean loss: 137.32
 ---- batch: 050 ----
mean loss: 154.12
 ---- batch: 060 ----
mean loss: 142.79
 ---- batch: 070 ----
mean loss: 143.57
 ---- batch: 080 ----
mean loss: 145.99
 ---- batch: 090 ----
mean loss: 137.03
 ---- batch: 100 ----
mean loss: 146.55
 ---- batch: 110 ----
mean loss: 142.09
train mean loss: 142.57
epoch train time: 0:00:00.551303
elapsed time: 0:02:12.661781
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:34:28.179574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.91
 ---- batch: 020 ----
mean loss: 142.38
 ---- batch: 030 ----
mean loss: 150.31
 ---- batch: 040 ----
mean loss: 128.51
 ---- batch: 050 ----
mean loss: 143.78
 ---- batch: 060 ----
mean loss: 137.35
 ---- batch: 070 ----
mean loss: 138.67
 ---- batch: 080 ----
mean loss: 143.16
 ---- batch: 090 ----
mean loss: 147.44
 ---- batch: 100 ----
mean loss: 147.61
 ---- batch: 110 ----
mean loss: 150.12
train mean loss: 142.41
epoch train time: 0:00:00.563766
elapsed time: 0:02:13.225680
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:34:28.743473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.98
 ---- batch: 020 ----
mean loss: 146.56
 ---- batch: 030 ----
mean loss: 144.45
 ---- batch: 040 ----
mean loss: 146.42
 ---- batch: 050 ----
mean loss: 146.01
 ---- batch: 060 ----
mean loss: 147.49
 ---- batch: 070 ----
mean loss: 138.84
 ---- batch: 080 ----
mean loss: 135.76
 ---- batch: 090 ----
mean loss: 141.13
 ---- batch: 100 ----
mean loss: 148.24
 ---- batch: 110 ----
mean loss: 146.50
train mean loss: 144.36
epoch train time: 0:00:00.566964
elapsed time: 0:02:13.792777
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:34:29.310573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.38
 ---- batch: 020 ----
mean loss: 143.88
 ---- batch: 030 ----
mean loss: 132.72
 ---- batch: 040 ----
mean loss: 140.51
 ---- batch: 050 ----
mean loss: 130.99
 ---- batch: 060 ----
mean loss: 137.38
 ---- batch: 070 ----
mean loss: 144.99
 ---- batch: 080 ----
mean loss: 148.59
 ---- batch: 090 ----
mean loss: 155.54
 ---- batch: 100 ----
mean loss: 141.33
 ---- batch: 110 ----
mean loss: 139.41
train mean loss: 141.61
epoch train time: 0:00:00.558817
elapsed time: 0:02:14.351735
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:34:29.869558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.60
 ---- batch: 020 ----
mean loss: 141.90
 ---- batch: 030 ----
mean loss: 143.12
 ---- batch: 040 ----
mean loss: 137.87
 ---- batch: 050 ----
mean loss: 144.62
 ---- batch: 060 ----
mean loss: 141.53
 ---- batch: 070 ----
mean loss: 142.17
 ---- batch: 080 ----
mean loss: 143.49
 ---- batch: 090 ----
mean loss: 132.15
 ---- batch: 100 ----
mean loss: 145.56
 ---- batch: 110 ----
mean loss: 152.17
train mean loss: 141.90
epoch train time: 0:00:00.560939
elapsed time: 0:02:14.912841
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:34:30.430654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.33
 ---- batch: 020 ----
mean loss: 133.40
 ---- batch: 030 ----
mean loss: 149.38
 ---- batch: 040 ----
mean loss: 137.45
 ---- batch: 050 ----
mean loss: 144.87
 ---- batch: 060 ----
mean loss: 144.71
 ---- batch: 070 ----
mean loss: 143.16
 ---- batch: 080 ----
mean loss: 136.18
 ---- batch: 090 ----
mean loss: 140.19
 ---- batch: 100 ----
mean loss: 144.21
 ---- batch: 110 ----
mean loss: 147.51
train mean loss: 141.79
epoch train time: 0:00:00.558255
elapsed time: 0:02:15.471257
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:34:30.989052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.31
 ---- batch: 020 ----
mean loss: 148.64
 ---- batch: 030 ----
mean loss: 152.13
 ---- batch: 040 ----
mean loss: 141.68
 ---- batch: 050 ----
mean loss: 139.38
 ---- batch: 060 ----
mean loss: 141.38
 ---- batch: 070 ----
mean loss: 140.99
 ---- batch: 080 ----
mean loss: 138.32
 ---- batch: 090 ----
mean loss: 138.29
 ---- batch: 100 ----
mean loss: 135.43
 ---- batch: 110 ----
mean loss: 140.61
train mean loss: 141.48
epoch train time: 0:00:00.560317
elapsed time: 0:02:16.031710
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:34:31.549505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.12
 ---- batch: 020 ----
mean loss: 140.25
 ---- batch: 030 ----
mean loss: 138.58
 ---- batch: 040 ----
mean loss: 143.74
 ---- batch: 050 ----
mean loss: 142.71
 ---- batch: 060 ----
mean loss: 150.24
 ---- batch: 070 ----
mean loss: 132.16
 ---- batch: 080 ----
mean loss: 137.95
 ---- batch: 090 ----
mean loss: 148.08
 ---- batch: 100 ----
mean loss: 147.48
 ---- batch: 110 ----
mean loss: 143.27
train mean loss: 141.51
epoch train time: 0:00:00.552802
elapsed time: 0:02:16.584649
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:34:32.102449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.06
 ---- batch: 020 ----
mean loss: 141.94
 ---- batch: 030 ----
mean loss: 148.44
 ---- batch: 040 ----
mean loss: 136.18
 ---- batch: 050 ----
mean loss: 142.55
 ---- batch: 060 ----
mean loss: 141.51
 ---- batch: 070 ----
mean loss: 141.82
 ---- batch: 080 ----
mean loss: 138.48
 ---- batch: 090 ----
mean loss: 146.17
 ---- batch: 100 ----
mean loss: 143.80
 ---- batch: 110 ----
mean loss: 137.43
train mean loss: 141.58
epoch train time: 0:00:00.550607
elapsed time: 0:02:17.135393
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:34:32.653187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.06
 ---- batch: 020 ----
mean loss: 140.00
 ---- batch: 030 ----
mean loss: 153.97
 ---- batch: 040 ----
mean loss: 136.96
 ---- batch: 050 ----
mean loss: 138.33
 ---- batch: 060 ----
mean loss: 139.27
 ---- batch: 070 ----
mean loss: 134.73
 ---- batch: 080 ----
mean loss: 147.26
 ---- batch: 090 ----
mean loss: 139.82
 ---- batch: 100 ----
mean loss: 137.50
 ---- batch: 110 ----
mean loss: 143.50
train mean loss: 141.07
epoch train time: 0:00:00.548947
elapsed time: 0:02:17.684475
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:34:33.202270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.31
 ---- batch: 020 ----
mean loss: 140.22
 ---- batch: 030 ----
mean loss: 139.11
 ---- batch: 040 ----
mean loss: 136.99
 ---- batch: 050 ----
mean loss: 137.17
 ---- batch: 060 ----
mean loss: 149.09
 ---- batch: 070 ----
mean loss: 147.91
 ---- batch: 080 ----
mean loss: 139.58
 ---- batch: 090 ----
mean loss: 144.07
 ---- batch: 100 ----
mean loss: 145.47
 ---- batch: 110 ----
mean loss: 148.27
train mean loss: 141.97
epoch train time: 0:00:00.548880
elapsed time: 0:02:18.233490
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:34:33.751284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.45
 ---- batch: 020 ----
mean loss: 131.74
 ---- batch: 030 ----
mean loss: 139.26
 ---- batch: 040 ----
mean loss: 139.63
 ---- batch: 050 ----
mean loss: 142.01
 ---- batch: 060 ----
mean loss: 144.26
 ---- batch: 070 ----
mean loss: 139.38
 ---- batch: 080 ----
mean loss: 135.02
 ---- batch: 090 ----
mean loss: 142.90
 ---- batch: 100 ----
mean loss: 143.36
 ---- batch: 110 ----
mean loss: 140.83
train mean loss: 140.10
epoch train time: 0:00:00.562028
elapsed time: 0:02:18.795658
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:34:34.313471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.15
 ---- batch: 020 ----
mean loss: 142.98
 ---- batch: 030 ----
mean loss: 142.59
 ---- batch: 040 ----
mean loss: 141.41
 ---- batch: 050 ----
mean loss: 144.18
 ---- batch: 060 ----
mean loss: 146.87
 ---- batch: 070 ----
mean loss: 135.01
 ---- batch: 080 ----
mean loss: 136.67
 ---- batch: 090 ----
mean loss: 140.10
 ---- batch: 100 ----
mean loss: 132.97
 ---- batch: 110 ----
mean loss: 142.24
train mean loss: 140.81
epoch train time: 0:00:00.570572
elapsed time: 0:02:19.366384
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:34:34.884178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.82
 ---- batch: 020 ----
mean loss: 142.17
 ---- batch: 030 ----
mean loss: 136.52
 ---- batch: 040 ----
mean loss: 145.71
 ---- batch: 050 ----
mean loss: 140.66
 ---- batch: 060 ----
mean loss: 133.74
 ---- batch: 070 ----
mean loss: 134.77
 ---- batch: 080 ----
mean loss: 135.90
 ---- batch: 090 ----
mean loss: 137.85
 ---- batch: 100 ----
mean loss: 146.88
 ---- batch: 110 ----
mean loss: 148.11
train mean loss: 139.86
epoch train time: 0:00:00.549227
elapsed time: 0:02:19.915758
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:34:35.433565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.41
 ---- batch: 020 ----
mean loss: 147.73
 ---- batch: 030 ----
mean loss: 140.07
 ---- batch: 040 ----
mean loss: 138.11
 ---- batch: 050 ----
mean loss: 135.35
 ---- batch: 060 ----
mean loss: 131.90
 ---- batch: 070 ----
mean loss: 149.33
 ---- batch: 080 ----
mean loss: 137.26
 ---- batch: 090 ----
mean loss: 141.87
 ---- batch: 100 ----
mean loss: 147.82
 ---- batch: 110 ----
mean loss: 142.38
train mean loss: 140.87
epoch train time: 0:00:00.562542
elapsed time: 0:02:20.478479
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:34:35.996304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.50
 ---- batch: 020 ----
mean loss: 149.17
 ---- batch: 030 ----
mean loss: 148.99
 ---- batch: 040 ----
mean loss: 143.55
 ---- batch: 050 ----
mean loss: 133.48
 ---- batch: 060 ----
mean loss: 137.29
 ---- batch: 070 ----
mean loss: 141.96
 ---- batch: 080 ----
mean loss: 137.31
 ---- batch: 090 ----
mean loss: 140.51
 ---- batch: 100 ----
mean loss: 138.69
 ---- batch: 110 ----
mean loss: 140.67
train mean loss: 140.71
epoch train time: 0:00:00.553136
elapsed time: 0:02:21.031780
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:34:36.549576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.72
 ---- batch: 020 ----
mean loss: 134.90
 ---- batch: 030 ----
mean loss: 134.76
 ---- batch: 040 ----
mean loss: 148.83
 ---- batch: 050 ----
mean loss: 134.43
 ---- batch: 060 ----
mean loss: 133.54
 ---- batch: 070 ----
mean loss: 142.24
 ---- batch: 080 ----
mean loss: 144.12
 ---- batch: 090 ----
mean loss: 141.24
 ---- batch: 100 ----
mean loss: 136.03
 ---- batch: 110 ----
mean loss: 145.34
train mean loss: 139.72
epoch train time: 0:00:00.554442
elapsed time: 0:02:21.586359
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:34:37.104153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.36
 ---- batch: 020 ----
mean loss: 134.20
 ---- batch: 030 ----
mean loss: 137.29
 ---- batch: 040 ----
mean loss: 134.17
 ---- batch: 050 ----
mean loss: 140.56
 ---- batch: 060 ----
mean loss: 133.15
 ---- batch: 070 ----
mean loss: 154.05
 ---- batch: 080 ----
mean loss: 143.24
 ---- batch: 090 ----
mean loss: 143.15
 ---- batch: 100 ----
mean loss: 141.76
 ---- batch: 110 ----
mean loss: 138.18
train mean loss: 139.38
epoch train time: 0:00:00.572084
elapsed time: 0:02:22.158588
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:34:37.676422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.94
 ---- batch: 020 ----
mean loss: 133.34
 ---- batch: 030 ----
mean loss: 138.02
 ---- batch: 040 ----
mean loss: 137.08
 ---- batch: 050 ----
mean loss: 135.74
 ---- batch: 060 ----
mean loss: 145.08
 ---- batch: 070 ----
mean loss: 133.76
 ---- batch: 080 ----
mean loss: 143.11
 ---- batch: 090 ----
mean loss: 139.15
 ---- batch: 100 ----
mean loss: 134.47
 ---- batch: 110 ----
mean loss: 147.68
train mean loss: 139.22
epoch train time: 0:00:00.571165
elapsed time: 0:02:22.729929
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:34:38.247724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.71
 ---- batch: 020 ----
mean loss: 134.27
 ---- batch: 030 ----
mean loss: 143.91
 ---- batch: 040 ----
mean loss: 129.30
 ---- batch: 050 ----
mean loss: 140.84
 ---- batch: 060 ----
mean loss: 148.17
 ---- batch: 070 ----
mean loss: 143.81
 ---- batch: 080 ----
mean loss: 142.86
 ---- batch: 090 ----
mean loss: 139.06
 ---- batch: 100 ----
mean loss: 136.78
 ---- batch: 110 ----
mean loss: 143.72
train mean loss: 140.03
epoch train time: 0:00:00.578025
elapsed time: 0:02:23.308101
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:34:38.825918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.70
 ---- batch: 020 ----
mean loss: 132.51
 ---- batch: 030 ----
mean loss: 133.74
 ---- batch: 040 ----
mean loss: 135.98
 ---- batch: 050 ----
mean loss: 140.52
 ---- batch: 060 ----
mean loss: 141.64
 ---- batch: 070 ----
mean loss: 140.46
 ---- batch: 080 ----
mean loss: 135.90
 ---- batch: 090 ----
mean loss: 142.39
 ---- batch: 100 ----
mean loss: 142.34
 ---- batch: 110 ----
mean loss: 150.36
train mean loss: 139.53
epoch train time: 0:00:00.548010
elapsed time: 0:02:23.856265
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:34:39.374057
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.11
 ---- batch: 020 ----
mean loss: 132.54
 ---- batch: 030 ----
mean loss: 140.35
 ---- batch: 040 ----
mean loss: 131.87
 ---- batch: 050 ----
mean loss: 128.71
 ---- batch: 060 ----
mean loss: 133.60
 ---- batch: 070 ----
mean loss: 126.64
 ---- batch: 080 ----
mean loss: 138.98
 ---- batch: 090 ----
mean loss: 134.20
 ---- batch: 100 ----
mean loss: 135.09
 ---- batch: 110 ----
mean loss: 131.14
train mean loss: 133.58
epoch train time: 0:00:00.568166
elapsed time: 0:02:24.424578
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:34:39.942362
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 140.90
 ---- batch: 020 ----
mean loss: 129.56
 ---- batch: 030 ----
mean loss: 127.50
 ---- batch: 040 ----
mean loss: 131.55
 ---- batch: 050 ----
mean loss: 130.79
 ---- batch: 060 ----
mean loss: 133.49
 ---- batch: 070 ----
mean loss: 125.79
 ---- batch: 080 ----
mean loss: 140.78
 ---- batch: 090 ----
mean loss: 133.38
 ---- batch: 100 ----
mean loss: 133.58
 ---- batch: 110 ----
mean loss: 130.44
train mean loss: 132.68
epoch train time: 0:00:00.551938
elapsed time: 0:02:24.976645
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:34:40.494439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.56
 ---- batch: 020 ----
mean loss: 130.83
 ---- batch: 030 ----
mean loss: 135.29
 ---- batch: 040 ----
mean loss: 132.69
 ---- batch: 050 ----
mean loss: 137.34
 ---- batch: 060 ----
mean loss: 133.04
 ---- batch: 070 ----
mean loss: 126.73
 ---- batch: 080 ----
mean loss: 137.52
 ---- batch: 090 ----
mean loss: 130.12
 ---- batch: 100 ----
mean loss: 131.48
 ---- batch: 110 ----
mean loss: 134.22
train mean loss: 132.45
epoch train time: 0:00:00.560761
elapsed time: 0:02:25.537545
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:34:41.055342
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.64
 ---- batch: 020 ----
mean loss: 130.17
 ---- batch: 030 ----
mean loss: 129.85
 ---- batch: 040 ----
mean loss: 127.56
 ---- batch: 050 ----
mean loss: 138.15
 ---- batch: 060 ----
mean loss: 134.88
 ---- batch: 070 ----
mean loss: 136.13
 ---- batch: 080 ----
mean loss: 130.64
 ---- batch: 090 ----
mean loss: 138.62
 ---- batch: 100 ----
mean loss: 128.73
 ---- batch: 110 ----
mean loss: 126.54
train mean loss: 132.32
epoch train time: 0:00:00.548225
elapsed time: 0:02:26.085907
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:34:41.603717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.83
 ---- batch: 020 ----
mean loss: 131.87
 ---- batch: 030 ----
mean loss: 128.44
 ---- batch: 040 ----
mean loss: 134.19
 ---- batch: 050 ----
mean loss: 130.25
 ---- batch: 060 ----
mean loss: 131.84
 ---- batch: 070 ----
mean loss: 135.17
 ---- batch: 080 ----
mean loss: 140.81
 ---- batch: 090 ----
mean loss: 125.40
 ---- batch: 100 ----
mean loss: 126.74
 ---- batch: 110 ----
mean loss: 137.68
train mean loss: 132.37
epoch train time: 0:00:00.561639
elapsed time: 0:02:26.647701
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:34:42.165494
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.08
 ---- batch: 020 ----
mean loss: 126.68
 ---- batch: 030 ----
mean loss: 131.58
 ---- batch: 040 ----
mean loss: 135.79
 ---- batch: 050 ----
mean loss: 123.77
 ---- batch: 060 ----
mean loss: 139.90
 ---- batch: 070 ----
mean loss: 136.30
 ---- batch: 080 ----
mean loss: 138.48
 ---- batch: 090 ----
mean loss: 131.26
 ---- batch: 100 ----
mean loss: 123.08
 ---- batch: 110 ----
mean loss: 136.05
train mean loss: 132.23
epoch train time: 0:00:00.569053
elapsed time: 0:02:27.216892
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:34:42.734704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.71
 ---- batch: 020 ----
mean loss: 133.17
 ---- batch: 030 ----
mean loss: 129.25
 ---- batch: 040 ----
mean loss: 139.03
 ---- batch: 050 ----
mean loss: 130.91
 ---- batch: 060 ----
mean loss: 131.79
 ---- batch: 070 ----
mean loss: 128.29
 ---- batch: 080 ----
mean loss: 137.25
 ---- batch: 090 ----
mean loss: 133.51
 ---- batch: 100 ----
mean loss: 136.02
 ---- batch: 110 ----
mean loss: 130.63
train mean loss: 132.14
epoch train time: 0:00:00.556146
elapsed time: 0:02:27.773191
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:34:43.290983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.32
 ---- batch: 020 ----
mean loss: 132.38
 ---- batch: 030 ----
mean loss: 130.14
 ---- batch: 040 ----
mean loss: 129.06
 ---- batch: 050 ----
mean loss: 133.03
 ---- batch: 060 ----
mean loss: 137.26
 ---- batch: 070 ----
mean loss: 136.49
 ---- batch: 080 ----
mean loss: 133.45
 ---- batch: 090 ----
mean loss: 134.52
 ---- batch: 100 ----
mean loss: 128.48
 ---- batch: 110 ----
mean loss: 134.05
train mean loss: 132.16
epoch train time: 0:00:00.565422
elapsed time: 0:02:28.338758
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:34:43.856580
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.45
 ---- batch: 020 ----
mean loss: 121.41
 ---- batch: 030 ----
mean loss: 138.49
 ---- batch: 040 ----
mean loss: 128.46
 ---- batch: 050 ----
mean loss: 127.23
 ---- batch: 060 ----
mean loss: 134.83
 ---- batch: 070 ----
mean loss: 130.59
 ---- batch: 080 ----
mean loss: 131.18
 ---- batch: 090 ----
mean loss: 135.17
 ---- batch: 100 ----
mean loss: 134.39
 ---- batch: 110 ----
mean loss: 142.02
train mean loss: 132.03
epoch train time: 0:00:00.553110
elapsed time: 0:02:28.892029
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:34:44.409821
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.65
 ---- batch: 020 ----
mean loss: 130.77
 ---- batch: 030 ----
mean loss: 134.32
 ---- batch: 040 ----
mean loss: 135.61
 ---- batch: 050 ----
mean loss: 135.30
 ---- batch: 060 ----
mean loss: 132.14
 ---- batch: 070 ----
mean loss: 124.05
 ---- batch: 080 ----
mean loss: 128.80
 ---- batch: 090 ----
mean loss: 131.24
 ---- batch: 100 ----
mean loss: 126.39
 ---- batch: 110 ----
mean loss: 132.10
train mean loss: 132.04
epoch train time: 0:00:00.555220
elapsed time: 0:02:29.447399
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:34:44.965211
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.87
 ---- batch: 020 ----
mean loss: 134.41
 ---- batch: 030 ----
mean loss: 138.56
 ---- batch: 040 ----
mean loss: 134.98
 ---- batch: 050 ----
mean loss: 136.06
 ---- batch: 060 ----
mean loss: 129.16
 ---- batch: 070 ----
mean loss: 125.40
 ---- batch: 080 ----
mean loss: 127.47
 ---- batch: 090 ----
mean loss: 135.62
 ---- batch: 100 ----
mean loss: 121.20
 ---- batch: 110 ----
mean loss: 132.56
train mean loss: 131.99
epoch train time: 0:00:00.549882
elapsed time: 0:02:29.997450
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:34:45.515245
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.19
 ---- batch: 020 ----
mean loss: 125.39
 ---- batch: 030 ----
mean loss: 131.56
 ---- batch: 040 ----
mean loss: 135.17
 ---- batch: 050 ----
mean loss: 133.53
 ---- batch: 060 ----
mean loss: 132.61
 ---- batch: 070 ----
mean loss: 135.36
 ---- batch: 080 ----
mean loss: 133.70
 ---- batch: 090 ----
mean loss: 130.62
 ---- batch: 100 ----
mean loss: 133.84
 ---- batch: 110 ----
mean loss: 131.09
train mean loss: 131.95
epoch train time: 0:00:00.557398
elapsed time: 0:02:30.555000
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:34:46.072793
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.79
 ---- batch: 020 ----
mean loss: 131.73
 ---- batch: 030 ----
mean loss: 124.37
 ---- batch: 040 ----
mean loss: 142.09
 ---- batch: 050 ----
mean loss: 132.40
 ---- batch: 060 ----
mean loss: 134.93
 ---- batch: 070 ----
mean loss: 126.69
 ---- batch: 080 ----
mean loss: 130.61
 ---- batch: 090 ----
mean loss: 121.66
 ---- batch: 100 ----
mean loss: 135.46
 ---- batch: 110 ----
mean loss: 137.09
train mean loss: 131.99
epoch train time: 0:00:00.556575
elapsed time: 0:02:31.111708
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:34:46.629503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.37
 ---- batch: 020 ----
mean loss: 130.02
 ---- batch: 030 ----
mean loss: 128.65
 ---- batch: 040 ----
mean loss: 127.88
 ---- batch: 050 ----
mean loss: 134.60
 ---- batch: 060 ----
mean loss: 127.34
 ---- batch: 070 ----
mean loss: 136.10
 ---- batch: 080 ----
mean loss: 134.22
 ---- batch: 090 ----
mean loss: 129.29
 ---- batch: 100 ----
mean loss: 136.57
 ---- batch: 110 ----
mean loss: 136.79
train mean loss: 131.94
epoch train time: 0:00:00.557341
elapsed time: 0:02:31.669186
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:34:47.186980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.26
 ---- batch: 020 ----
mean loss: 128.23
 ---- batch: 030 ----
mean loss: 134.72
 ---- batch: 040 ----
mean loss: 138.80
 ---- batch: 050 ----
mean loss: 125.59
 ---- batch: 060 ----
mean loss: 128.17
 ---- batch: 070 ----
mean loss: 136.04
 ---- batch: 080 ----
mean loss: 129.68
 ---- batch: 090 ----
mean loss: 124.94
 ---- batch: 100 ----
mean loss: 132.43
 ---- batch: 110 ----
mean loss: 133.13
train mean loss: 132.05
epoch train time: 0:00:00.556162
elapsed time: 0:02:32.225525
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:34:47.743352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 139.92
 ---- batch: 020 ----
mean loss: 131.75
 ---- batch: 030 ----
mean loss: 132.33
 ---- batch: 040 ----
mean loss: 130.87
 ---- batch: 050 ----
mean loss: 131.88
 ---- batch: 060 ----
mean loss: 130.56
 ---- batch: 070 ----
mean loss: 128.94
 ---- batch: 080 ----
mean loss: 135.44
 ---- batch: 090 ----
mean loss: 133.10
 ---- batch: 100 ----
mean loss: 130.70
 ---- batch: 110 ----
mean loss: 128.40
train mean loss: 131.85
epoch train time: 0:00:00.559954
elapsed time: 0:02:32.785648
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:34:48.303443
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.64
 ---- batch: 020 ----
mean loss: 129.82
 ---- batch: 030 ----
mean loss: 133.66
 ---- batch: 040 ----
mean loss: 131.71
 ---- batch: 050 ----
mean loss: 130.06
 ---- batch: 060 ----
mean loss: 130.94
 ---- batch: 070 ----
mean loss: 130.63
 ---- batch: 080 ----
mean loss: 133.12
 ---- batch: 090 ----
mean loss: 132.32
 ---- batch: 100 ----
mean loss: 139.57
 ---- batch: 110 ----
mean loss: 130.34
train mean loss: 131.83
epoch train time: 0:00:00.572144
elapsed time: 0:02:33.357946
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:34:48.875761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.01
 ---- batch: 020 ----
mean loss: 133.43
 ---- batch: 030 ----
mean loss: 135.39
 ---- batch: 040 ----
mean loss: 134.51
 ---- batch: 050 ----
mean loss: 129.46
 ---- batch: 060 ----
mean loss: 131.30
 ---- batch: 070 ----
mean loss: 131.12
 ---- batch: 080 ----
mean loss: 124.91
 ---- batch: 090 ----
mean loss: 131.50
 ---- batch: 100 ----
mean loss: 138.55
 ---- batch: 110 ----
mean loss: 126.95
train mean loss: 131.89
epoch train time: 0:00:00.568347
elapsed time: 0:02:33.926449
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:34:49.444244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.11
 ---- batch: 020 ----
mean loss: 129.02
 ---- batch: 030 ----
mean loss: 128.28
 ---- batch: 040 ----
mean loss: 132.12
 ---- batch: 050 ----
mean loss: 132.42
 ---- batch: 060 ----
mean loss: 122.35
 ---- batch: 070 ----
mean loss: 131.13
 ---- batch: 080 ----
mean loss: 138.35
 ---- batch: 090 ----
mean loss: 134.78
 ---- batch: 100 ----
mean loss: 131.13
 ---- batch: 110 ----
mean loss: 135.81
train mean loss: 131.95
epoch train time: 0:00:00.569170
elapsed time: 0:02:34.495751
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:34:50.013544
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.50
 ---- batch: 020 ----
mean loss: 131.15
 ---- batch: 030 ----
mean loss: 139.70
 ---- batch: 040 ----
mean loss: 131.50
 ---- batch: 050 ----
mean loss: 137.63
 ---- batch: 060 ----
mean loss: 129.71
 ---- batch: 070 ----
mean loss: 126.28
 ---- batch: 080 ----
mean loss: 131.08
 ---- batch: 090 ----
mean loss: 130.05
 ---- batch: 100 ----
mean loss: 136.87
 ---- batch: 110 ----
mean loss: 132.71
train mean loss: 131.79
epoch train time: 0:00:00.552680
elapsed time: 0:02:35.048599
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:34:50.566393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.64
 ---- batch: 020 ----
mean loss: 127.03
 ---- batch: 030 ----
mean loss: 136.25
 ---- batch: 040 ----
mean loss: 134.67
 ---- batch: 050 ----
mean loss: 132.42
 ---- batch: 060 ----
mean loss: 127.58
 ---- batch: 070 ----
mean loss: 134.89
 ---- batch: 080 ----
mean loss: 139.87
 ---- batch: 090 ----
mean loss: 127.84
 ---- batch: 100 ----
mean loss: 128.35
 ---- batch: 110 ----
mean loss: 132.61
train mean loss: 131.73
epoch train time: 0:00:00.570181
elapsed time: 0:02:35.618920
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:34:51.136732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.34
 ---- batch: 020 ----
mean loss: 132.67
 ---- batch: 030 ----
mean loss: 136.16
 ---- batch: 040 ----
mean loss: 128.30
 ---- batch: 050 ----
mean loss: 130.55
 ---- batch: 060 ----
mean loss: 131.03
 ---- batch: 070 ----
mean loss: 131.39
 ---- batch: 080 ----
mean loss: 128.87
 ---- batch: 090 ----
mean loss: 129.44
 ---- batch: 100 ----
mean loss: 133.71
 ---- batch: 110 ----
mean loss: 127.98
train mean loss: 131.74
epoch train time: 0:00:00.554625
elapsed time: 0:02:36.173703
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:34:51.691497
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.00
 ---- batch: 020 ----
mean loss: 131.90
 ---- batch: 030 ----
mean loss: 126.98
 ---- batch: 040 ----
mean loss: 132.59
 ---- batch: 050 ----
mean loss: 136.34
 ---- batch: 060 ----
mean loss: 132.30
 ---- batch: 070 ----
mean loss: 129.65
 ---- batch: 080 ----
mean loss: 131.85
 ---- batch: 090 ----
mean loss: 134.58
 ---- batch: 100 ----
mean loss: 128.53
 ---- batch: 110 ----
mean loss: 129.76
train mean loss: 131.78
epoch train time: 0:00:00.563801
elapsed time: 0:02:36.737656
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:34:52.255523
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.51
 ---- batch: 020 ----
mean loss: 132.39
 ---- batch: 030 ----
mean loss: 130.65
 ---- batch: 040 ----
mean loss: 127.01
 ---- batch: 050 ----
mean loss: 133.30
 ---- batch: 060 ----
mean loss: 130.85
 ---- batch: 070 ----
mean loss: 137.30
 ---- batch: 080 ----
mean loss: 140.56
 ---- batch: 090 ----
mean loss: 131.05
 ---- batch: 100 ----
mean loss: 121.11
 ---- batch: 110 ----
mean loss: 136.31
train mean loss: 131.74
epoch train time: 0:00:00.569803
elapsed time: 0:02:37.307681
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:34:52.825479
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.20
 ---- batch: 020 ----
mean loss: 126.33
 ---- batch: 030 ----
mean loss: 134.00
 ---- batch: 040 ----
mean loss: 135.41
 ---- batch: 050 ----
mean loss: 123.80
 ---- batch: 060 ----
mean loss: 138.33
 ---- batch: 070 ----
mean loss: 129.20
 ---- batch: 080 ----
mean loss: 125.23
 ---- batch: 090 ----
mean loss: 133.84
 ---- batch: 100 ----
mean loss: 137.15
 ---- batch: 110 ----
mean loss: 130.74
train mean loss: 131.79
epoch train time: 0:00:00.555060
elapsed time: 0:02:37.862878
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:34:53.380673
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.36
 ---- batch: 020 ----
mean loss: 139.74
 ---- batch: 030 ----
mean loss: 135.16
 ---- batch: 040 ----
mean loss: 129.50
 ---- batch: 050 ----
mean loss: 127.60
 ---- batch: 060 ----
mean loss: 122.42
 ---- batch: 070 ----
mean loss: 144.44
 ---- batch: 080 ----
mean loss: 123.96
 ---- batch: 090 ----
mean loss: 137.23
 ---- batch: 100 ----
mean loss: 133.51
 ---- batch: 110 ----
mean loss: 130.28
train mean loss: 131.48
epoch train time: 0:00:00.555149
elapsed time: 0:02:38.418162
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:34:53.935975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.92
 ---- batch: 020 ----
mean loss: 131.51
 ---- batch: 030 ----
mean loss: 131.23
 ---- batch: 040 ----
mean loss: 125.32
 ---- batch: 050 ----
mean loss: 129.15
 ---- batch: 060 ----
mean loss: 135.77
 ---- batch: 070 ----
mean loss: 136.75
 ---- batch: 080 ----
mean loss: 136.60
 ---- batch: 090 ----
mean loss: 131.04
 ---- batch: 100 ----
mean loss: 135.39
 ---- batch: 110 ----
mean loss: 131.54
train mean loss: 131.61
epoch train time: 0:00:00.554699
elapsed time: 0:02:38.973016
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:34:54.490811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.90
 ---- batch: 020 ----
mean loss: 128.72
 ---- batch: 030 ----
mean loss: 131.12
 ---- batch: 040 ----
mean loss: 136.41
 ---- batch: 050 ----
mean loss: 131.30
 ---- batch: 060 ----
mean loss: 129.34
 ---- batch: 070 ----
mean loss: 138.18
 ---- batch: 080 ----
mean loss: 132.58
 ---- batch: 090 ----
mean loss: 129.70
 ---- batch: 100 ----
mean loss: 127.35
 ---- batch: 110 ----
mean loss: 131.47
train mean loss: 131.54
epoch train time: 0:00:00.553791
elapsed time: 0:02:39.526943
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:34:55.044737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.27
 ---- batch: 020 ----
mean loss: 134.74
 ---- batch: 030 ----
mean loss: 138.61
 ---- batch: 040 ----
mean loss: 134.54
 ---- batch: 050 ----
mean loss: 130.33
 ---- batch: 060 ----
mean loss: 139.17
 ---- batch: 070 ----
mean loss: 125.44
 ---- batch: 080 ----
mean loss: 122.41
 ---- batch: 090 ----
mean loss: 133.39
 ---- batch: 100 ----
mean loss: 135.83
 ---- batch: 110 ----
mean loss: 129.14
train mean loss: 131.51
epoch train time: 0:00:00.554700
elapsed time: 0:02:40.081792
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:34:55.599601
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.26
 ---- batch: 020 ----
mean loss: 130.35
 ---- batch: 030 ----
mean loss: 129.20
 ---- batch: 040 ----
mean loss: 144.86
 ---- batch: 050 ----
mean loss: 123.22
 ---- batch: 060 ----
mean loss: 132.30
 ---- batch: 070 ----
mean loss: 128.26
 ---- batch: 080 ----
mean loss: 130.91
 ---- batch: 090 ----
mean loss: 128.64
 ---- batch: 100 ----
mean loss: 130.76
 ---- batch: 110 ----
mean loss: 134.50
train mean loss: 131.70
epoch train time: 0:00:00.552045
elapsed time: 0:02:40.634001
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:34:56.151829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.37
 ---- batch: 020 ----
mean loss: 126.08
 ---- batch: 030 ----
mean loss: 128.39
 ---- batch: 040 ----
mean loss: 136.90
 ---- batch: 050 ----
mean loss: 133.72
 ---- batch: 060 ----
mean loss: 121.80
 ---- batch: 070 ----
mean loss: 130.02
 ---- batch: 080 ----
mean loss: 135.17
 ---- batch: 090 ----
mean loss: 129.70
 ---- batch: 100 ----
mean loss: 143.51
 ---- batch: 110 ----
mean loss: 138.61
train mean loss: 131.54
epoch train time: 0:00:00.555745
elapsed time: 0:02:41.189912
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:34:56.707706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.60
 ---- batch: 020 ----
mean loss: 128.90
 ---- batch: 030 ----
mean loss: 133.82
 ---- batch: 040 ----
mean loss: 133.13
 ---- batch: 050 ----
mean loss: 121.99
 ---- batch: 060 ----
mean loss: 140.86
 ---- batch: 070 ----
mean loss: 132.96
 ---- batch: 080 ----
mean loss: 135.66
 ---- batch: 090 ----
mean loss: 131.40
 ---- batch: 100 ----
mean loss: 124.95
 ---- batch: 110 ----
mean loss: 129.59
train mean loss: 131.53
epoch train time: 0:00:00.551867
elapsed time: 0:02:41.741912
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:34:57.259705
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.45
 ---- batch: 020 ----
mean loss: 124.00
 ---- batch: 030 ----
mean loss: 128.88
 ---- batch: 040 ----
mean loss: 136.75
 ---- batch: 050 ----
mean loss: 131.28
 ---- batch: 060 ----
mean loss: 133.82
 ---- batch: 070 ----
mean loss: 133.03
 ---- batch: 080 ----
mean loss: 134.96
 ---- batch: 090 ----
mean loss: 128.74
 ---- batch: 100 ----
mean loss: 135.34
 ---- batch: 110 ----
mean loss: 132.29
train mean loss: 131.51
epoch train time: 0:00:00.556668
elapsed time: 0:02:42.298727
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:34:57.816515
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.34
 ---- batch: 020 ----
mean loss: 128.97
 ---- batch: 030 ----
mean loss: 125.69
 ---- batch: 040 ----
mean loss: 134.06
 ---- batch: 050 ----
mean loss: 130.61
 ---- batch: 060 ----
mean loss: 135.38
 ---- batch: 070 ----
mean loss: 129.93
 ---- batch: 080 ----
mean loss: 133.44
 ---- batch: 090 ----
mean loss: 133.66
 ---- batch: 100 ----
mean loss: 134.63
 ---- batch: 110 ----
mean loss: 130.42
train mean loss: 131.43
epoch train time: 0:00:00.562743
elapsed time: 0:02:42.861598
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:34:58.379393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.76
 ---- batch: 020 ----
mean loss: 125.97
 ---- batch: 030 ----
mean loss: 130.45
 ---- batch: 040 ----
mean loss: 137.67
 ---- batch: 050 ----
mean loss: 126.01
 ---- batch: 060 ----
mean loss: 134.83
 ---- batch: 070 ----
mean loss: 135.04
 ---- batch: 080 ----
mean loss: 128.23
 ---- batch: 090 ----
mean loss: 132.40
 ---- batch: 100 ----
mean loss: 133.21
 ---- batch: 110 ----
mean loss: 127.57
train mean loss: 131.49
epoch train time: 0:00:00.561489
elapsed time: 0:02:43.423220
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:34:58.941013
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.06
 ---- batch: 020 ----
mean loss: 129.25
 ---- batch: 030 ----
mean loss: 132.69
 ---- batch: 040 ----
mean loss: 134.02
 ---- batch: 050 ----
mean loss: 130.26
 ---- batch: 060 ----
mean loss: 139.86
 ---- batch: 070 ----
mean loss: 134.45
 ---- batch: 080 ----
mean loss: 129.03
 ---- batch: 090 ----
mean loss: 129.20
 ---- batch: 100 ----
mean loss: 130.30
 ---- batch: 110 ----
mean loss: 127.53
train mean loss: 131.39
epoch train time: 0:00:00.550648
elapsed time: 0:02:43.974001
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:34:59.491795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.65
 ---- batch: 020 ----
mean loss: 141.97
 ---- batch: 030 ----
mean loss: 128.78
 ---- batch: 040 ----
mean loss: 128.64
 ---- batch: 050 ----
mean loss: 127.47
 ---- batch: 060 ----
mean loss: 132.06
 ---- batch: 070 ----
mean loss: 125.45
 ---- batch: 080 ----
mean loss: 129.80
 ---- batch: 090 ----
mean loss: 134.68
 ---- batch: 100 ----
mean loss: 129.84
 ---- batch: 110 ----
mean loss: 126.18
train mean loss: 131.37
epoch train time: 0:00:00.551469
elapsed time: 0:02:44.525604
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:35:00.043418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.32
 ---- batch: 020 ----
mean loss: 138.40
 ---- batch: 030 ----
mean loss: 129.63
 ---- batch: 040 ----
mean loss: 132.02
 ---- batch: 050 ----
mean loss: 135.15
 ---- batch: 060 ----
mean loss: 131.48
 ---- batch: 070 ----
mean loss: 139.38
 ---- batch: 080 ----
mean loss: 123.55
 ---- batch: 090 ----
mean loss: 127.07
 ---- batch: 100 ----
mean loss: 133.05
 ---- batch: 110 ----
mean loss: 124.23
train mean loss: 131.35
epoch train time: 0:00:00.547323
elapsed time: 0:02:45.073080
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:35:00.590873
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.78
 ---- batch: 020 ----
mean loss: 134.45
 ---- batch: 030 ----
mean loss: 133.67
 ---- batch: 040 ----
mean loss: 133.22
 ---- batch: 050 ----
mean loss: 131.86
 ---- batch: 060 ----
mean loss: 141.99
 ---- batch: 070 ----
mean loss: 135.52
 ---- batch: 080 ----
mean loss: 135.18
 ---- batch: 090 ----
mean loss: 126.42
 ---- batch: 100 ----
mean loss: 128.37
 ---- batch: 110 ----
mean loss: 121.67
train mean loss: 131.42
epoch train time: 0:00:00.548869
elapsed time: 0:02:45.622094
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:35:01.139912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.07
 ---- batch: 020 ----
mean loss: 126.60
 ---- batch: 030 ----
mean loss: 132.41
 ---- batch: 040 ----
mean loss: 126.57
 ---- batch: 050 ----
mean loss: 128.48
 ---- batch: 060 ----
mean loss: 131.09
 ---- batch: 070 ----
mean loss: 131.52
 ---- batch: 080 ----
mean loss: 131.27
 ---- batch: 090 ----
mean loss: 146.07
 ---- batch: 100 ----
mean loss: 126.41
 ---- batch: 110 ----
mean loss: 136.18
train mean loss: 131.36
epoch train time: 0:00:00.557661
elapsed time: 0:02:46.179951
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:35:01.697749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.18
 ---- batch: 020 ----
mean loss: 125.27
 ---- batch: 030 ----
mean loss: 130.60
 ---- batch: 040 ----
mean loss: 128.03
 ---- batch: 050 ----
mean loss: 137.01
 ---- batch: 060 ----
mean loss: 135.32
 ---- batch: 070 ----
mean loss: 135.36
 ---- batch: 080 ----
mean loss: 129.58
 ---- batch: 090 ----
mean loss: 133.86
 ---- batch: 100 ----
mean loss: 135.22
 ---- batch: 110 ----
mean loss: 126.96
train mean loss: 131.23
epoch train time: 0:00:00.566968
elapsed time: 0:02:46.747123
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:35:02.264918
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.05
 ---- batch: 020 ----
mean loss: 140.11
 ---- batch: 030 ----
mean loss: 134.64
 ---- batch: 040 ----
mean loss: 124.65
 ---- batch: 050 ----
mean loss: 128.87
 ---- batch: 060 ----
mean loss: 134.71
 ---- batch: 070 ----
mean loss: 135.14
 ---- batch: 080 ----
mean loss: 117.17
 ---- batch: 090 ----
mean loss: 126.91
 ---- batch: 100 ----
mean loss: 133.77
 ---- batch: 110 ----
mean loss: 137.70
train mean loss: 131.25
epoch train time: 0:00:00.565216
elapsed time: 0:02:47.312474
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:35:02.830268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.68
 ---- batch: 020 ----
mean loss: 137.73
 ---- batch: 030 ----
mean loss: 128.28
 ---- batch: 040 ----
mean loss: 139.02
 ---- batch: 050 ----
mean loss: 137.16
 ---- batch: 060 ----
mean loss: 126.71
 ---- batch: 070 ----
mean loss: 134.68
 ---- batch: 080 ----
mean loss: 122.80
 ---- batch: 090 ----
mean loss: 127.82
 ---- batch: 100 ----
mean loss: 134.85
 ---- batch: 110 ----
mean loss: 131.39
train mean loss: 131.29
epoch train time: 0:00:00.550447
elapsed time: 0:02:47.863053
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:35:03.380865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.41
 ---- batch: 020 ----
mean loss: 125.46
 ---- batch: 030 ----
mean loss: 132.01
 ---- batch: 040 ----
mean loss: 132.63
 ---- batch: 050 ----
mean loss: 130.43
 ---- batch: 060 ----
mean loss: 127.77
 ---- batch: 070 ----
mean loss: 138.99
 ---- batch: 080 ----
mean loss: 134.99
 ---- batch: 090 ----
mean loss: 127.32
 ---- batch: 100 ----
mean loss: 136.56
 ---- batch: 110 ----
mean loss: 132.19
train mean loss: 131.29
epoch train time: 0:00:00.557902
elapsed time: 0:02:48.421106
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:35:03.938899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.91
 ---- batch: 020 ----
mean loss: 128.18
 ---- batch: 030 ----
mean loss: 133.50
 ---- batch: 040 ----
mean loss: 130.70
 ---- batch: 050 ----
mean loss: 124.34
 ---- batch: 060 ----
mean loss: 134.81
 ---- batch: 070 ----
mean loss: 131.19
 ---- batch: 080 ----
mean loss: 125.83
 ---- batch: 090 ----
mean loss: 133.82
 ---- batch: 100 ----
mean loss: 136.40
 ---- batch: 110 ----
mean loss: 135.91
train mean loss: 131.27
epoch train time: 0:00:00.563311
elapsed time: 0:02:48.984554
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:35:04.502348
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.26
 ---- batch: 020 ----
mean loss: 132.08
 ---- batch: 030 ----
mean loss: 126.49
 ---- batch: 040 ----
mean loss: 126.26
 ---- batch: 050 ----
mean loss: 134.12
 ---- batch: 060 ----
mean loss: 135.47
 ---- batch: 070 ----
mean loss: 135.40
 ---- batch: 080 ----
mean loss: 131.27
 ---- batch: 090 ----
mean loss: 132.49
 ---- batch: 100 ----
mean loss: 130.63
 ---- batch: 110 ----
mean loss: 125.22
train mean loss: 131.23
epoch train time: 0:00:00.554516
elapsed time: 0:02:49.539202
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:35:05.057010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.00
 ---- batch: 020 ----
mean loss: 128.95
 ---- batch: 030 ----
mean loss: 131.13
 ---- batch: 040 ----
mean loss: 128.45
 ---- batch: 050 ----
mean loss: 131.41
 ---- batch: 060 ----
mean loss: 136.41
 ---- batch: 070 ----
mean loss: 132.12
 ---- batch: 080 ----
mean loss: 126.88
 ---- batch: 090 ----
mean loss: 131.27
 ---- batch: 100 ----
mean loss: 137.82
 ---- batch: 110 ----
mean loss: 136.40
train mean loss: 131.03
epoch train time: 0:00:00.557460
elapsed time: 0:02:50.096814
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:35:05.614609
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.09
 ---- batch: 020 ----
mean loss: 130.00
 ---- batch: 030 ----
mean loss: 131.71
 ---- batch: 040 ----
mean loss: 131.88
 ---- batch: 050 ----
mean loss: 127.26
 ---- batch: 060 ----
mean loss: 131.16
 ---- batch: 070 ----
mean loss: 126.25
 ---- batch: 080 ----
mean loss: 129.83
 ---- batch: 090 ----
mean loss: 134.46
 ---- batch: 100 ----
mean loss: 137.00
 ---- batch: 110 ----
mean loss: 135.21
train mean loss: 131.09
epoch train time: 0:00:00.573585
elapsed time: 0:02:50.670535
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:35:06.188333
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.84
 ---- batch: 020 ----
mean loss: 128.61
 ---- batch: 030 ----
mean loss: 127.19
 ---- batch: 040 ----
mean loss: 131.24
 ---- batch: 050 ----
mean loss: 129.60
 ---- batch: 060 ----
mean loss: 135.81
 ---- batch: 070 ----
mean loss: 127.73
 ---- batch: 080 ----
mean loss: 133.84
 ---- batch: 090 ----
mean loss: 131.24
 ---- batch: 100 ----
mean loss: 134.99
 ---- batch: 110 ----
mean loss: 133.66
train mean loss: 131.10
epoch train time: 0:00:00.572567
elapsed time: 0:02:51.246504
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_9/checkpoint.pth.tar
**** end time: 2019-09-27 02:35:06.764265 ****
