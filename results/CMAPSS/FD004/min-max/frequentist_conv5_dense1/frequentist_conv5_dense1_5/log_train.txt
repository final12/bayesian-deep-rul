Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 16200
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-27 00:36:41.101332 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 16, 24]             100
              Tanh-2           [-1, 10, 16, 24]               0
            Conv2d-3           [-1, 10, 15, 24]           1,000
              Tanh-4           [-1, 10, 15, 24]               0
            Conv2d-5           [-1, 10, 16, 24]           1,000
              Tanh-6           [-1, 10, 16, 24]               0
            Conv2d-7           [-1, 10, 15, 24]           1,000
              Tanh-8           [-1, 10, 15, 24]               0
            Conv2d-9            [-1, 1, 15, 24]              30
             Tanh-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
          Dropout-12                  [-1, 360]               0
           Linear-13                  [-1, 100]          36,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 39,230
Trainable params: 39,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 00:36:41.111036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4825.95
 ---- batch: 020 ----
mean loss: 3279.26
 ---- batch: 030 ----
mean loss: 1611.61
 ---- batch: 040 ----
mean loss: 1356.63
 ---- batch: 050 ----
mean loss: 1221.46
 ---- batch: 060 ----
mean loss: 1131.05
 ---- batch: 070 ----
mean loss: 1080.84
 ---- batch: 080 ----
mean loss: 1049.03
 ---- batch: 090 ----
mean loss: 987.58
 ---- batch: 100 ----
mean loss: 959.23
 ---- batch: 110 ----
mean loss: 918.30
train mean loss: 1655.07
epoch train time: 0:00:33.839259
elapsed time: 0:00:33.851636
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 00:37:14.953018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.87
 ---- batch: 020 ----
mean loss: 877.42
 ---- batch: 030 ----
mean loss: 836.77
 ---- batch: 040 ----
mean loss: 837.05
 ---- batch: 050 ----
mean loss: 807.61
 ---- batch: 060 ----
mean loss: 779.43
 ---- batch: 070 ----
mean loss: 793.80
 ---- batch: 080 ----
mean loss: 782.13
 ---- batch: 090 ----
mean loss: 780.68
 ---- batch: 100 ----
mean loss: 768.13
 ---- batch: 110 ----
mean loss: 785.79
train mean loss: 810.99
epoch train time: 0:00:02.250518
elapsed time: 0:00:36.102370
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 00:37:17.203774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 719.01
 ---- batch: 020 ----
mean loss: 735.64
 ---- batch: 030 ----
mean loss: 718.52
 ---- batch: 040 ----
mean loss: 711.56
 ---- batch: 050 ----
mean loss: 703.27
 ---- batch: 060 ----
mean loss: 693.97
 ---- batch: 070 ----
mean loss: 728.65
 ---- batch: 080 ----
mean loss: 692.62
 ---- batch: 090 ----
mean loss: 687.93
 ---- batch: 100 ----
mean loss: 689.32
 ---- batch: 110 ----
mean loss: 683.43
train mean loss: 704.42
epoch train time: 0:00:02.172391
elapsed time: 0:00:38.275009
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 00:37:19.376406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.21
 ---- batch: 020 ----
mean loss: 685.37
 ---- batch: 030 ----
mean loss: 671.04
 ---- batch: 040 ----
mean loss: 658.51
 ---- batch: 050 ----
mean loss: 651.86
 ---- batch: 060 ----
mean loss: 656.93
 ---- batch: 070 ----
mean loss: 655.72
 ---- batch: 080 ----
mean loss: 641.30
 ---- batch: 090 ----
mean loss: 637.38
 ---- batch: 100 ----
mean loss: 649.20
 ---- batch: 110 ----
mean loss: 636.93
train mean loss: 657.41
epoch train time: 0:00:02.172578
elapsed time: 0:00:40.447764
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 00:37:21.549156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 631.64
 ---- batch: 020 ----
mean loss: 638.79
 ---- batch: 030 ----
mean loss: 644.31
 ---- batch: 040 ----
mean loss: 636.39
 ---- batch: 050 ----
mean loss: 623.96
 ---- batch: 060 ----
mean loss: 618.71
 ---- batch: 070 ----
mean loss: 621.68
 ---- batch: 080 ----
mean loss: 635.34
 ---- batch: 090 ----
mean loss: 633.36
 ---- batch: 100 ----
mean loss: 634.49
 ---- batch: 110 ----
mean loss: 624.50
train mean loss: 630.44
epoch train time: 0:00:02.160525
elapsed time: 0:00:42.608464
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 00:37:23.709852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 612.97
 ---- batch: 020 ----
mean loss: 611.69
 ---- batch: 030 ----
mean loss: 593.92
 ---- batch: 040 ----
mean loss: 596.82
 ---- batch: 050 ----
mean loss: 612.09
 ---- batch: 060 ----
mean loss: 604.64
 ---- batch: 070 ----
mean loss: 602.72
 ---- batch: 080 ----
mean loss: 624.23
 ---- batch: 090 ----
mean loss: 588.58
 ---- batch: 100 ----
mean loss: 597.04
 ---- batch: 110 ----
mean loss: 603.61
train mean loss: 603.43
epoch train time: 0:00:02.160324
elapsed time: 0:00:44.768951
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 00:37:25.870357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 588.11
 ---- batch: 020 ----
mean loss: 591.12
 ---- batch: 030 ----
mean loss: 577.34
 ---- batch: 040 ----
mean loss: 599.33
 ---- batch: 050 ----
mean loss: 589.31
 ---- batch: 060 ----
mean loss: 591.05
 ---- batch: 070 ----
mean loss: 587.89
 ---- batch: 080 ----
mean loss: 595.09
 ---- batch: 090 ----
mean loss: 588.35
 ---- batch: 100 ----
mean loss: 594.13
 ---- batch: 110 ----
mean loss: 586.35
train mean loss: 590.17
epoch train time: 0:00:02.168397
elapsed time: 0:00:46.937532
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 00:37:28.038922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 577.70
 ---- batch: 020 ----
mean loss: 594.67
 ---- batch: 030 ----
mean loss: 600.30
 ---- batch: 040 ----
mean loss: 574.00
 ---- batch: 050 ----
mean loss: 571.45
 ---- batch: 060 ----
mean loss: 580.49
 ---- batch: 070 ----
mean loss: 581.67
 ---- batch: 080 ----
mean loss: 567.60
 ---- batch: 090 ----
mean loss: 575.91
 ---- batch: 100 ----
mean loss: 580.71
 ---- batch: 110 ----
mean loss: 579.25
train mean loss: 579.61
epoch train time: 0:00:02.156953
elapsed time: 0:00:49.094686
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 00:37:30.196074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.37
 ---- batch: 020 ----
mean loss: 574.61
 ---- batch: 030 ----
mean loss: 574.33
 ---- batch: 040 ----
mean loss: 575.61
 ---- batch: 050 ----
mean loss: 583.81
 ---- batch: 060 ----
mean loss: 584.37
 ---- batch: 070 ----
mean loss: 556.83
 ---- batch: 080 ----
mean loss: 547.51
 ---- batch: 090 ----
mean loss: 580.82
 ---- batch: 100 ----
mean loss: 582.69
 ---- batch: 110 ----
mean loss: 572.45
train mean loss: 573.97
epoch train time: 0:00:02.164032
elapsed time: 0:00:51.258880
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 00:37:32.360269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 566.24
 ---- batch: 020 ----
mean loss: 560.78
 ---- batch: 030 ----
mean loss: 540.75
 ---- batch: 040 ----
mean loss: 581.90
 ---- batch: 050 ----
mean loss: 544.01
 ---- batch: 060 ----
mean loss: 568.77
 ---- batch: 070 ----
mean loss: 554.36
 ---- batch: 080 ----
mean loss: 578.40
 ---- batch: 090 ----
mean loss: 537.08
 ---- batch: 100 ----
mean loss: 542.25
 ---- batch: 110 ----
mean loss: 568.65
train mean loss: 558.01
epoch train time: 0:00:02.166277
elapsed time: 0:00:53.425323
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 00:37:34.526761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.05
 ---- batch: 020 ----
mean loss: 538.57
 ---- batch: 030 ----
mean loss: 543.01
 ---- batch: 040 ----
mean loss: 544.89
 ---- batch: 050 ----
mean loss: 534.91
 ---- batch: 060 ----
mean loss: 554.06
 ---- batch: 070 ----
mean loss: 561.01
 ---- batch: 080 ----
mean loss: 537.39
 ---- batch: 090 ----
mean loss: 545.93
 ---- batch: 100 ----
mean loss: 537.36
 ---- batch: 110 ----
mean loss: 542.88
train mean loss: 544.27
epoch train time: 0:00:02.159985
elapsed time: 0:00:55.585536
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 00:37:36.686932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.76
 ---- batch: 020 ----
mean loss: 552.64
 ---- batch: 030 ----
mean loss: 562.80
 ---- batch: 040 ----
mean loss: 538.20
 ---- batch: 050 ----
mean loss: 533.42
 ---- batch: 060 ----
mean loss: 510.31
 ---- batch: 070 ----
mean loss: 533.77
 ---- batch: 080 ----
mean loss: 555.79
 ---- batch: 090 ----
mean loss: 552.41
 ---- batch: 100 ----
mean loss: 546.38
 ---- batch: 110 ----
mean loss: 524.07
train mean loss: 542.29
epoch train time: 0:00:02.165047
elapsed time: 0:00:57.750747
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 00:37:38.852132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 547.75
 ---- batch: 020 ----
mean loss: 560.81
 ---- batch: 030 ----
mean loss: 527.08
 ---- batch: 040 ----
mean loss: 527.50
 ---- batch: 050 ----
mean loss: 536.52
 ---- batch: 060 ----
mean loss: 555.40
 ---- batch: 070 ----
mean loss: 527.89
 ---- batch: 080 ----
mean loss: 539.92
 ---- batch: 090 ----
mean loss: 524.26
 ---- batch: 100 ----
mean loss: 549.34
 ---- batch: 110 ----
mean loss: 516.00
train mean loss: 536.77
epoch train time: 0:00:02.161351
elapsed time: 0:00:59.912305
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 00:37:41.013713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.61
 ---- batch: 020 ----
mean loss: 526.07
 ---- batch: 030 ----
mean loss: 514.95
 ---- batch: 040 ----
mean loss: 533.03
 ---- batch: 050 ----
mean loss: 521.45
 ---- batch: 060 ----
mean loss: 534.34
 ---- batch: 070 ----
mean loss: 525.84
 ---- batch: 080 ----
mean loss: 527.05
 ---- batch: 090 ----
mean loss: 523.98
 ---- batch: 100 ----
mean loss: 539.20
 ---- batch: 110 ----
mean loss: 539.69
train mean loss: 528.65
epoch train time: 0:00:02.157961
elapsed time: 0:01:02.070446
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 00:37:43.171849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.45
 ---- batch: 020 ----
mean loss: 533.48
 ---- batch: 030 ----
mean loss: 532.55
 ---- batch: 040 ----
mean loss: 532.63
 ---- batch: 050 ----
mean loss: 534.59
 ---- batch: 060 ----
mean loss: 521.04
 ---- batch: 070 ----
mean loss: 525.62
 ---- batch: 080 ----
mean loss: 521.53
 ---- batch: 090 ----
mean loss: 508.95
 ---- batch: 100 ----
mean loss: 501.94
 ---- batch: 110 ----
mean loss: 537.97
train mean loss: 524.94
epoch train time: 0:00:02.162932
elapsed time: 0:01:04.233582
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 00:37:45.334968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.59
 ---- batch: 020 ----
mean loss: 529.95
 ---- batch: 030 ----
mean loss: 508.09
 ---- batch: 040 ----
mean loss: 531.54
 ---- batch: 050 ----
mean loss: 546.41
 ---- batch: 060 ----
mean loss: 522.02
 ---- batch: 070 ----
mean loss: 514.67
 ---- batch: 080 ----
mean loss: 516.27
 ---- batch: 090 ----
mean loss: 522.11
 ---- batch: 100 ----
mean loss: 531.05
 ---- batch: 110 ----
mean loss: 519.25
train mean loss: 525.66
epoch train time: 0:00:02.163422
elapsed time: 0:01:06.397172
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 00:37:47.498566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.02
 ---- batch: 020 ----
mean loss: 513.55
 ---- batch: 030 ----
mean loss: 516.45
 ---- batch: 040 ----
mean loss: 513.61
 ---- batch: 050 ----
mean loss: 548.22
 ---- batch: 060 ----
mean loss: 518.37
 ---- batch: 070 ----
mean loss: 525.95
 ---- batch: 080 ----
mean loss: 522.62
 ---- batch: 090 ----
mean loss: 514.73
 ---- batch: 100 ----
mean loss: 516.56
 ---- batch: 110 ----
mean loss: 538.88
train mean loss: 522.75
epoch train time: 0:00:02.159109
elapsed time: 0:01:08.556452
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 00:37:49.657840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.60
 ---- batch: 020 ----
mean loss: 534.25
 ---- batch: 030 ----
mean loss: 518.53
 ---- batch: 040 ----
mean loss: 510.12
 ---- batch: 050 ----
mean loss: 530.61
 ---- batch: 060 ----
mean loss: 511.61
 ---- batch: 070 ----
mean loss: 531.76
 ---- batch: 080 ----
mean loss: 514.62
 ---- batch: 090 ----
mean loss: 514.24
 ---- batch: 100 ----
mean loss: 524.77
 ---- batch: 110 ----
mean loss: 534.22
train mean loss: 521.43
epoch train time: 0:00:02.157834
elapsed time: 0:01:10.714464
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 00:37:51.815856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.00
 ---- batch: 020 ----
mean loss: 532.31
 ---- batch: 030 ----
mean loss: 519.64
 ---- batch: 040 ----
mean loss: 507.54
 ---- batch: 050 ----
mean loss: 522.33
 ---- batch: 060 ----
mean loss: 532.43
 ---- batch: 070 ----
mean loss: 520.90
 ---- batch: 080 ----
mean loss: 521.15
 ---- batch: 090 ----
mean loss: 502.23
 ---- batch: 100 ----
mean loss: 527.63
 ---- batch: 110 ----
mean loss: 525.16
train mean loss: 521.00
epoch train time: 0:00:02.151882
elapsed time: 0:01:12.866531
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 00:37:53.967962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.51
 ---- batch: 020 ----
mean loss: 509.16
 ---- batch: 030 ----
mean loss: 524.40
 ---- batch: 040 ----
mean loss: 523.61
 ---- batch: 050 ----
mean loss: 508.96
 ---- batch: 060 ----
mean loss: 515.13
 ---- batch: 070 ----
mean loss: 519.15
 ---- batch: 080 ----
mean loss: 546.67
 ---- batch: 090 ----
mean loss: 531.57
 ---- batch: 100 ----
mean loss: 519.33
 ---- batch: 110 ----
mean loss: 522.08
train mean loss: 521.23
epoch train time: 0:00:02.162970
elapsed time: 0:01:15.029734
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 00:37:56.131121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.01
 ---- batch: 020 ----
mean loss: 522.83
 ---- batch: 030 ----
mean loss: 526.58
 ---- batch: 040 ----
mean loss: 534.62
 ---- batch: 050 ----
mean loss: 525.02
 ---- batch: 060 ----
mean loss: 503.85
 ---- batch: 070 ----
mean loss: 531.28
 ---- batch: 080 ----
mean loss: 522.53
 ---- batch: 090 ----
mean loss: 505.04
 ---- batch: 100 ----
mean loss: 512.06
 ---- batch: 110 ----
mean loss: 507.38
train mean loss: 518.41
epoch train time: 0:00:02.170421
elapsed time: 0:01:17.200396
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 00:37:58.301788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.06
 ---- batch: 020 ----
mean loss: 506.74
 ---- batch: 030 ----
mean loss: 512.36
 ---- batch: 040 ----
mean loss: 530.00
 ---- batch: 050 ----
mean loss: 528.40
 ---- batch: 060 ----
mean loss: 514.68
 ---- batch: 070 ----
mean loss: 520.67
 ---- batch: 080 ----
mean loss: 509.74
 ---- batch: 090 ----
mean loss: 512.44
 ---- batch: 100 ----
mean loss: 529.05
 ---- batch: 110 ----
mean loss: 506.50
train mean loss: 516.28
epoch train time: 0:00:02.172724
elapsed time: 0:01:19.373288
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 00:38:00.474674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.72
 ---- batch: 020 ----
mean loss: 505.51
 ---- batch: 030 ----
mean loss: 508.19
 ---- batch: 040 ----
mean loss: 517.31
 ---- batch: 050 ----
mean loss: 510.61
 ---- batch: 060 ----
mean loss: 514.39
 ---- batch: 070 ----
mean loss: 530.21
 ---- batch: 080 ----
mean loss: 529.15
 ---- batch: 090 ----
mean loss: 513.00
 ---- batch: 100 ----
mean loss: 540.17
 ---- batch: 110 ----
mean loss: 512.78
train mean loss: 518.32
epoch train time: 0:00:02.166817
elapsed time: 0:01:21.540268
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 00:38:02.641675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.35
 ---- batch: 020 ----
mean loss: 526.82
 ---- batch: 030 ----
mean loss: 533.48
 ---- batch: 040 ----
mean loss: 515.19
 ---- batch: 050 ----
mean loss: 540.71
 ---- batch: 060 ----
mean loss: 524.90
 ---- batch: 070 ----
mean loss: 508.13
 ---- batch: 080 ----
mean loss: 524.60
 ---- batch: 090 ----
mean loss: 509.21
 ---- batch: 100 ----
mean loss: 530.10
 ---- batch: 110 ----
mean loss: 500.70
train mean loss: 520.28
epoch train time: 0:00:02.160995
elapsed time: 0:01:23.701485
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 00:38:04.802898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.71
 ---- batch: 020 ----
mean loss: 510.61
 ---- batch: 030 ----
mean loss: 511.84
 ---- batch: 040 ----
mean loss: 515.53
 ---- batch: 050 ----
mean loss: 519.84
 ---- batch: 060 ----
mean loss: 521.18
 ---- batch: 070 ----
mean loss: 522.71
 ---- batch: 080 ----
mean loss: 524.10
 ---- batch: 090 ----
mean loss: 500.31
 ---- batch: 100 ----
mean loss: 513.70
 ---- batch: 110 ----
mean loss: 531.95
train mean loss: 516.73
epoch train time: 0:00:02.174019
elapsed time: 0:01:25.875725
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 00:38:06.977115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.76
 ---- batch: 020 ----
mean loss: 534.01
 ---- batch: 030 ----
mean loss: 493.77
 ---- batch: 040 ----
mean loss: 516.12
 ---- batch: 050 ----
mean loss: 523.95
 ---- batch: 060 ----
mean loss: 519.16
 ---- batch: 070 ----
mean loss: 523.93
 ---- batch: 080 ----
mean loss: 526.09
 ---- batch: 090 ----
mean loss: 494.96
 ---- batch: 100 ----
mean loss: 512.52
 ---- batch: 110 ----
mean loss: 515.17
train mean loss: 514.68
epoch train time: 0:00:02.164242
elapsed time: 0:01:28.040152
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 00:38:09.141562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 517.52
 ---- batch: 020 ----
mean loss: 523.02
 ---- batch: 030 ----
mean loss: 513.90
 ---- batch: 040 ----
mean loss: 509.71
 ---- batch: 050 ----
mean loss: 512.10
 ---- batch: 060 ----
mean loss: 507.82
 ---- batch: 070 ----
mean loss: 502.78
 ---- batch: 080 ----
mean loss: 508.23
 ---- batch: 090 ----
mean loss: 503.93
 ---- batch: 100 ----
mean loss: 508.48
 ---- batch: 110 ----
mean loss: 517.94
train mean loss: 511.06
epoch train time: 0:00:02.170939
elapsed time: 0:01:30.211291
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 00:38:11.312679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 521.18
 ---- batch: 020 ----
mean loss: 511.40
 ---- batch: 030 ----
mean loss: 538.81
 ---- batch: 040 ----
mean loss: 518.24
 ---- batch: 050 ----
mean loss: 525.16
 ---- batch: 060 ----
mean loss: 509.08
 ---- batch: 070 ----
mean loss: 522.21
 ---- batch: 080 ----
mean loss: 510.62
 ---- batch: 090 ----
mean loss: 514.95
 ---- batch: 100 ----
mean loss: 502.35
 ---- batch: 110 ----
mean loss: 512.31
train mean loss: 517.08
epoch train time: 0:00:02.173222
elapsed time: 0:01:32.384712
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 00:38:13.486106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.20
 ---- batch: 020 ----
mean loss: 519.70
 ---- batch: 030 ----
mean loss: 513.51
 ---- batch: 040 ----
mean loss: 518.71
 ---- batch: 050 ----
mean loss: 531.59
 ---- batch: 060 ----
mean loss: 500.82
 ---- batch: 070 ----
mean loss: 523.32
 ---- batch: 080 ----
mean loss: 503.16
 ---- batch: 090 ----
mean loss: 500.74
 ---- batch: 100 ----
mean loss: 509.46
 ---- batch: 110 ----
mean loss: 509.90
train mean loss: 510.31
epoch train time: 0:00:02.168241
elapsed time: 0:01:34.553174
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 00:38:15.654565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.20
 ---- batch: 020 ----
mean loss: 513.25
 ---- batch: 030 ----
mean loss: 503.60
 ---- batch: 040 ----
mean loss: 502.27
 ---- batch: 050 ----
mean loss: 521.42
 ---- batch: 060 ----
mean loss: 487.17
 ---- batch: 070 ----
mean loss: 515.52
 ---- batch: 080 ----
mean loss: 513.19
 ---- batch: 090 ----
mean loss: 515.09
 ---- batch: 100 ----
mean loss: 530.27
 ---- batch: 110 ----
mean loss: 525.07
train mean loss: 511.69
epoch train time: 0:00:02.160437
elapsed time: 0:01:36.713829
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 00:38:17.815249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.20
 ---- batch: 020 ----
mean loss: 516.06
 ---- batch: 030 ----
mean loss: 517.21
 ---- batch: 040 ----
mean loss: 501.60
 ---- batch: 050 ----
mean loss: 494.87
 ---- batch: 060 ----
mean loss: 511.36
 ---- batch: 070 ----
mean loss: 478.16
 ---- batch: 080 ----
mean loss: 518.01
 ---- batch: 090 ----
mean loss: 500.54
 ---- batch: 100 ----
mean loss: 519.84
 ---- batch: 110 ----
mean loss: 516.66
train mean loss: 506.96
epoch train time: 0:00:02.154759
elapsed time: 0:01:38.868785
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 00:38:19.970170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.22
 ---- batch: 020 ----
mean loss: 528.73
 ---- batch: 030 ----
mean loss: 514.91
 ---- batch: 040 ----
mean loss: 512.61
 ---- batch: 050 ----
mean loss: 500.49
 ---- batch: 060 ----
mean loss: 513.92
 ---- batch: 070 ----
mean loss: 504.54
 ---- batch: 080 ----
mean loss: 498.11
 ---- batch: 090 ----
mean loss: 510.61
 ---- batch: 100 ----
mean loss: 499.40
 ---- batch: 110 ----
mean loss: 505.37
train mean loss: 510.11
epoch train time: 0:00:02.156010
elapsed time: 0:01:41.025039
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 00:38:22.126455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.95
 ---- batch: 020 ----
mean loss: 505.19
 ---- batch: 030 ----
mean loss: 517.80
 ---- batch: 040 ----
mean loss: 503.03
 ---- batch: 050 ----
mean loss: 512.70
 ---- batch: 060 ----
mean loss: 525.13
 ---- batch: 070 ----
mean loss: 492.88
 ---- batch: 080 ----
mean loss: 524.38
 ---- batch: 090 ----
mean loss: 513.75
 ---- batch: 100 ----
mean loss: 499.06
 ---- batch: 110 ----
mean loss: 505.97
train mean loss: 509.87
epoch train time: 0:00:02.166516
elapsed time: 0:01:43.191749
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 00:38:24.293165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.15
 ---- batch: 020 ----
mean loss: 517.54
 ---- batch: 030 ----
mean loss: 510.43
 ---- batch: 040 ----
mean loss: 498.50
 ---- batch: 050 ----
mean loss: 489.48
 ---- batch: 060 ----
mean loss: 496.10
 ---- batch: 070 ----
mean loss: 522.66
 ---- batch: 080 ----
mean loss: 505.94
 ---- batch: 090 ----
mean loss: 512.07
 ---- batch: 100 ----
mean loss: 501.52
 ---- batch: 110 ----
mean loss: 509.98
train mean loss: 506.59
epoch train time: 0:00:02.157443
elapsed time: 0:01:45.349400
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 00:38:26.450785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.93
 ---- batch: 020 ----
mean loss: 505.45
 ---- batch: 030 ----
mean loss: 519.51
 ---- batch: 040 ----
mean loss: 530.79
 ---- batch: 050 ----
mean loss: 524.39
 ---- batch: 060 ----
mean loss: 501.32
 ---- batch: 070 ----
mean loss: 509.48
 ---- batch: 080 ----
mean loss: 516.20
 ---- batch: 090 ----
mean loss: 510.88
 ---- batch: 100 ----
mean loss: 521.59
 ---- batch: 110 ----
mean loss: 512.60
train mean loss: 513.55
epoch train time: 0:00:02.147651
elapsed time: 0:01:47.497216
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 00:38:28.598609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.83
 ---- batch: 020 ----
mean loss: 511.58
 ---- batch: 030 ----
mean loss: 520.83
 ---- batch: 040 ----
mean loss: 499.69
 ---- batch: 050 ----
mean loss: 504.10
 ---- batch: 060 ----
mean loss: 528.95
 ---- batch: 070 ----
mean loss: 516.50
 ---- batch: 080 ----
mean loss: 510.19
 ---- batch: 090 ----
mean loss: 501.66
 ---- batch: 100 ----
mean loss: 519.02
 ---- batch: 110 ----
mean loss: 502.18
train mean loss: 512.65
epoch train time: 0:00:02.142554
elapsed time: 0:01:49.639961
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 00:38:30.741351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 517.69
 ---- batch: 020 ----
mean loss: 504.31
 ---- batch: 030 ----
mean loss: 512.71
 ---- batch: 040 ----
mean loss: 513.49
 ---- batch: 050 ----
mean loss: 502.62
 ---- batch: 060 ----
mean loss: 511.02
 ---- batch: 070 ----
mean loss: 499.81
 ---- batch: 080 ----
mean loss: 511.52
 ---- batch: 090 ----
mean loss: 508.41
 ---- batch: 100 ----
mean loss: 510.49
 ---- batch: 110 ----
mean loss: 488.32
train mean loss: 507.46
epoch train time: 0:00:02.141267
elapsed time: 0:01:51.781399
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 00:38:32.882788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.34
 ---- batch: 020 ----
mean loss: 516.14
 ---- batch: 030 ----
mean loss: 517.85
 ---- batch: 040 ----
mean loss: 505.76
 ---- batch: 050 ----
mean loss: 526.83
 ---- batch: 060 ----
mean loss: 508.80
 ---- batch: 070 ----
mean loss: 505.29
 ---- batch: 080 ----
mean loss: 504.20
 ---- batch: 090 ----
mean loss: 490.73
 ---- batch: 100 ----
mean loss: 510.84
 ---- batch: 110 ----
mean loss: 514.74
train mean loss: 511.48
epoch train time: 0:00:02.139042
elapsed time: 0:01:53.920632
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 00:38:35.022024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.32
 ---- batch: 020 ----
mean loss: 514.62
 ---- batch: 030 ----
mean loss: 510.71
 ---- batch: 040 ----
mean loss: 519.92
 ---- batch: 050 ----
mean loss: 496.51
 ---- batch: 060 ----
mean loss: 510.73
 ---- batch: 070 ----
mean loss: 500.33
 ---- batch: 080 ----
mean loss: 517.78
 ---- batch: 090 ----
mean loss: 504.66
 ---- batch: 100 ----
mean loss: 501.51
 ---- batch: 110 ----
mean loss: 499.68
train mean loss: 507.17
epoch train time: 0:00:02.150968
elapsed time: 0:01:56.071790
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 00:38:37.173197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.21
 ---- batch: 020 ----
mean loss: 513.95
 ---- batch: 030 ----
mean loss: 507.65
 ---- batch: 040 ----
mean loss: 502.33
 ---- batch: 050 ----
mean loss: 499.90
 ---- batch: 060 ----
mean loss: 512.49
 ---- batch: 070 ----
mean loss: 527.02
 ---- batch: 080 ----
mean loss: 517.30
 ---- batch: 090 ----
mean loss: 512.53
 ---- batch: 100 ----
mean loss: 526.66
 ---- batch: 110 ----
mean loss: 505.00
train mean loss: 509.56
epoch train time: 0:00:02.148497
elapsed time: 0:01:58.220513
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 00:38:39.321901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.97
 ---- batch: 020 ----
mean loss: 504.67
 ---- batch: 030 ----
mean loss: 509.32
 ---- batch: 040 ----
mean loss: 513.66
 ---- batch: 050 ----
mean loss: 501.24
 ---- batch: 060 ----
mean loss: 502.57
 ---- batch: 070 ----
mean loss: 506.99
 ---- batch: 080 ----
mean loss: 512.68
 ---- batch: 090 ----
mean loss: 501.90
 ---- batch: 100 ----
mean loss: 510.44
 ---- batch: 110 ----
mean loss: 497.41
train mean loss: 505.60
epoch train time: 0:00:02.142342
elapsed time: 0:02:00.363040
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 00:38:41.464424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.68
 ---- batch: 020 ----
mean loss: 491.60
 ---- batch: 030 ----
mean loss: 505.84
 ---- batch: 040 ----
mean loss: 504.99
 ---- batch: 050 ----
mean loss: 504.27
 ---- batch: 060 ----
mean loss: 510.85
 ---- batch: 070 ----
mean loss: 519.71
 ---- batch: 080 ----
mean loss: 524.76
 ---- batch: 090 ----
mean loss: 509.79
 ---- batch: 100 ----
mean loss: 495.93
 ---- batch: 110 ----
mean loss: 488.73
train mean loss: 506.13
epoch train time: 0:00:02.138633
elapsed time: 0:02:02.501864
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 00:38:43.603249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.81
 ---- batch: 020 ----
mean loss: 515.32
 ---- batch: 030 ----
mean loss: 510.04
 ---- batch: 040 ----
mean loss: 514.79
 ---- batch: 050 ----
mean loss: 514.08
 ---- batch: 060 ----
mean loss: 505.34
 ---- batch: 070 ----
mean loss: 510.69
 ---- batch: 080 ----
mean loss: 511.47
 ---- batch: 090 ----
mean loss: 507.00
 ---- batch: 100 ----
mean loss: 482.70
 ---- batch: 110 ----
mean loss: 510.36
train mean loss: 507.09
epoch train time: 0:00:02.139921
elapsed time: 0:02:04.641944
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 00:38:45.743347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.02
 ---- batch: 020 ----
mean loss: 514.86
 ---- batch: 030 ----
mean loss: 516.24
 ---- batch: 040 ----
mean loss: 508.11
 ---- batch: 050 ----
mean loss: 509.74
 ---- batch: 060 ----
mean loss: 499.25
 ---- batch: 070 ----
mean loss: 496.33
 ---- batch: 080 ----
mean loss: 507.47
 ---- batch: 090 ----
mean loss: 500.53
 ---- batch: 100 ----
mean loss: 495.56
 ---- batch: 110 ----
mean loss: 497.42
train mean loss: 505.57
epoch train time: 0:00:02.140186
elapsed time: 0:02:06.782300
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 00:38:47.883690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.09
 ---- batch: 020 ----
mean loss: 502.51
 ---- batch: 030 ----
mean loss: 515.13
 ---- batch: 040 ----
mean loss: 514.15
 ---- batch: 050 ----
mean loss: 505.03
 ---- batch: 060 ----
mean loss: 520.46
 ---- batch: 070 ----
mean loss: 511.51
 ---- batch: 080 ----
mean loss: 511.87
 ---- batch: 090 ----
mean loss: 505.16
 ---- batch: 100 ----
mean loss: 497.59
 ---- batch: 110 ----
mean loss: 498.20
train mean loss: 507.59
epoch train time: 0:00:02.140231
elapsed time: 0:02:08.922700
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 00:38:50.024091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.91
 ---- batch: 020 ----
mean loss: 512.51
 ---- batch: 030 ----
mean loss: 507.75
 ---- batch: 040 ----
mean loss: 513.57
 ---- batch: 050 ----
mean loss: 487.61
 ---- batch: 060 ----
mean loss: 505.03
 ---- batch: 070 ----
mean loss: 493.16
 ---- batch: 080 ----
mean loss: 505.93
 ---- batch: 090 ----
mean loss: 512.63
 ---- batch: 100 ----
mean loss: 499.72
 ---- batch: 110 ----
mean loss: 504.41
train mean loss: 503.37
epoch train time: 0:00:02.145222
elapsed time: 0:02:11.068107
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 00:38:52.169527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.41
 ---- batch: 020 ----
mean loss: 519.26
 ---- batch: 030 ----
mean loss: 506.96
 ---- batch: 040 ----
mean loss: 506.82
 ---- batch: 050 ----
mean loss: 509.51
 ---- batch: 060 ----
mean loss: 499.93
 ---- batch: 070 ----
mean loss: 514.57
 ---- batch: 080 ----
mean loss: 492.40
 ---- batch: 090 ----
mean loss: 509.86
 ---- batch: 100 ----
mean loss: 492.40
 ---- batch: 110 ----
mean loss: 521.66
train mean loss: 507.50
epoch train time: 0:00:02.145334
elapsed time: 0:02:13.213658
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 00:38:54.315048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.73
 ---- batch: 020 ----
mean loss: 512.14
 ---- batch: 030 ----
mean loss: 493.73
 ---- batch: 040 ----
mean loss: 508.96
 ---- batch: 050 ----
mean loss: 519.37
 ---- batch: 060 ----
mean loss: 503.67
 ---- batch: 070 ----
mean loss: 514.90
 ---- batch: 080 ----
mean loss: 498.27
 ---- batch: 090 ----
mean loss: 507.22
 ---- batch: 100 ----
mean loss: 495.31
 ---- batch: 110 ----
mean loss: 497.95
train mean loss: 504.68
epoch train time: 0:00:02.160827
elapsed time: 0:02:15.374704
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 00:38:56.476095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.31
 ---- batch: 020 ----
mean loss: 517.87
 ---- batch: 030 ----
mean loss: 517.95
 ---- batch: 040 ----
mean loss: 526.05
 ---- batch: 050 ----
mean loss: 521.73
 ---- batch: 060 ----
mean loss: 517.12
 ---- batch: 070 ----
mean loss: 508.61
 ---- batch: 080 ----
mean loss: 539.08
 ---- batch: 090 ----
mean loss: 502.47
 ---- batch: 100 ----
mean loss: 493.34
 ---- batch: 110 ----
mean loss: 498.98
train mean loss: 512.61
epoch train time: 0:00:02.164059
elapsed time: 0:02:17.538940
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 00:38:58.640330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.62
 ---- batch: 020 ----
mean loss: 507.32
 ---- batch: 030 ----
mean loss: 504.55
 ---- batch: 040 ----
mean loss: 519.02
 ---- batch: 050 ----
mean loss: 506.38
 ---- batch: 060 ----
mean loss: 496.39
 ---- batch: 070 ----
mean loss: 497.18
 ---- batch: 080 ----
mean loss: 503.72
 ---- batch: 090 ----
mean loss: 515.76
 ---- batch: 100 ----
mean loss: 509.86
 ---- batch: 110 ----
mean loss: 499.30
train mean loss: 507.18
epoch train time: 0:00:02.167163
elapsed time: 0:02:19.706282
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 00:39:00.807671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.98
 ---- batch: 020 ----
mean loss: 496.88
 ---- batch: 030 ----
mean loss: 503.56
 ---- batch: 040 ----
mean loss: 497.09
 ---- batch: 050 ----
mean loss: 498.96
 ---- batch: 060 ----
mean loss: 491.98
 ---- batch: 070 ----
mean loss: 521.29
 ---- batch: 080 ----
mean loss: 500.12
 ---- batch: 090 ----
mean loss: 506.06
 ---- batch: 100 ----
mean loss: 496.62
 ---- batch: 110 ----
mean loss: 507.97
train mean loss: 502.78
epoch train time: 0:00:02.167280
elapsed time: 0:02:21.873762
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 00:39:02.975229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.64
 ---- batch: 020 ----
mean loss: 513.28
 ---- batch: 030 ----
mean loss: 515.13
 ---- batch: 040 ----
mean loss: 486.68
 ---- batch: 050 ----
mean loss: 489.79
 ---- batch: 060 ----
mean loss: 516.31
 ---- batch: 070 ----
mean loss: 499.63
 ---- batch: 080 ----
mean loss: 488.48
 ---- batch: 090 ----
mean loss: 497.10
 ---- batch: 100 ----
mean loss: 504.24
 ---- batch: 110 ----
mean loss: 480.82
train mean loss: 499.32
epoch train time: 0:00:02.171688
elapsed time: 0:02:24.045707
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 00:39:05.147115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.38
 ---- batch: 020 ----
mean loss: 504.13
 ---- batch: 030 ----
mean loss: 505.17
 ---- batch: 040 ----
mean loss: 502.69
 ---- batch: 050 ----
mean loss: 499.28
 ---- batch: 060 ----
mean loss: 499.16
 ---- batch: 070 ----
mean loss: 493.99
 ---- batch: 080 ----
mean loss: 503.22
 ---- batch: 090 ----
mean loss: 497.06
 ---- batch: 100 ----
mean loss: 501.55
 ---- batch: 110 ----
mean loss: 491.91
train mean loss: 501.38
epoch train time: 0:00:02.154405
elapsed time: 0:02:26.200380
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 00:39:07.301774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.10
 ---- batch: 020 ----
mean loss: 516.55
 ---- batch: 030 ----
mean loss: 516.10
 ---- batch: 040 ----
mean loss: 507.75
 ---- batch: 050 ----
mean loss: 492.96
 ---- batch: 060 ----
mean loss: 512.36
 ---- batch: 070 ----
mean loss: 497.80
 ---- batch: 080 ----
mean loss: 505.74
 ---- batch: 090 ----
mean loss: 506.12
 ---- batch: 100 ----
mean loss: 502.70
 ---- batch: 110 ----
mean loss: 502.75
train mean loss: 506.30
epoch train time: 0:00:02.163205
elapsed time: 0:02:28.363823
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 00:39:09.465215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.68
 ---- batch: 020 ----
mean loss: 507.25
 ---- batch: 030 ----
mean loss: 510.52
 ---- batch: 040 ----
mean loss: 500.35
 ---- batch: 050 ----
mean loss: 508.90
 ---- batch: 060 ----
mean loss: 493.50
 ---- batch: 070 ----
mean loss: 491.38
 ---- batch: 080 ----
mean loss: 485.57
 ---- batch: 090 ----
mean loss: 498.51
 ---- batch: 100 ----
mean loss: 501.98
 ---- batch: 110 ----
mean loss: 503.59
train mean loss: 500.41
epoch train time: 0:00:02.160327
elapsed time: 0:02:30.524376
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 00:39:11.625768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.95
 ---- batch: 020 ----
mean loss: 499.15
 ---- batch: 030 ----
mean loss: 516.00
 ---- batch: 040 ----
mean loss: 502.93
 ---- batch: 050 ----
mean loss: 493.49
 ---- batch: 060 ----
mean loss: 496.02
 ---- batch: 070 ----
mean loss: 494.47
 ---- batch: 080 ----
mean loss: 504.19
 ---- batch: 090 ----
mean loss: 498.48
 ---- batch: 100 ----
mean loss: 516.04
 ---- batch: 110 ----
mean loss: 513.26
train mean loss: 503.76
epoch train time: 0:00:02.160327
elapsed time: 0:02:32.684885
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 00:39:13.786300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.76
 ---- batch: 020 ----
mean loss: 499.78
 ---- batch: 030 ----
mean loss: 507.38
 ---- batch: 040 ----
mean loss: 499.37
 ---- batch: 050 ----
mean loss: 498.72
 ---- batch: 060 ----
mean loss: 500.03
 ---- batch: 070 ----
mean loss: 491.60
 ---- batch: 080 ----
mean loss: 490.97
 ---- batch: 090 ----
mean loss: 501.05
 ---- batch: 100 ----
mean loss: 501.04
 ---- batch: 110 ----
mean loss: 502.68
train mean loss: 499.79
epoch train time: 0:00:02.163641
elapsed time: 0:02:34.848753
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 00:39:15.950140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.46
 ---- batch: 020 ----
mean loss: 503.48
 ---- batch: 030 ----
mean loss: 485.93
 ---- batch: 040 ----
mean loss: 484.56
 ---- batch: 050 ----
mean loss: 507.75
 ---- batch: 060 ----
mean loss: 493.46
 ---- batch: 070 ----
mean loss: 494.56
 ---- batch: 080 ----
mean loss: 491.17
 ---- batch: 090 ----
mean loss: 520.98
 ---- batch: 100 ----
mean loss: 502.07
 ---- batch: 110 ----
mean loss: 509.17
train mean loss: 500.01
epoch train time: 0:00:02.153076
elapsed time: 0:02:37.002004
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 00:39:18.103395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.69
 ---- batch: 020 ----
mean loss: 504.61
 ---- batch: 030 ----
mean loss: 508.51
 ---- batch: 040 ----
mean loss: 506.41
 ---- batch: 050 ----
mean loss: 495.00
 ---- batch: 060 ----
mean loss: 479.15
 ---- batch: 070 ----
mean loss: 515.53
 ---- batch: 080 ----
mean loss: 503.96
 ---- batch: 090 ----
mean loss: 496.28
 ---- batch: 100 ----
mean loss: 510.08
 ---- batch: 110 ----
mean loss: 511.21
train mean loss: 501.55
epoch train time: 0:00:02.159697
elapsed time: 0:02:39.161894
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 00:39:20.263305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.18
 ---- batch: 020 ----
mean loss: 515.86
 ---- batch: 030 ----
mean loss: 514.95
 ---- batch: 040 ----
mean loss: 500.69
 ---- batch: 050 ----
mean loss: 492.96
 ---- batch: 060 ----
mean loss: 517.61
 ---- batch: 070 ----
mean loss: 493.05
 ---- batch: 080 ----
mean loss: 501.06
 ---- batch: 090 ----
mean loss: 510.67
 ---- batch: 100 ----
mean loss: 505.58
 ---- batch: 110 ----
mean loss: 501.80
train mean loss: 506.02
epoch train time: 0:00:02.159575
elapsed time: 0:02:41.321674
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 00:39:22.423066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.15
 ---- batch: 020 ----
mean loss: 500.73
 ---- batch: 030 ----
mean loss: 498.71
 ---- batch: 040 ----
mean loss: 496.14
 ---- batch: 050 ----
mean loss: 507.94
 ---- batch: 060 ----
mean loss: 489.81
 ---- batch: 070 ----
mean loss: 497.59
 ---- batch: 080 ----
mean loss: 501.35
 ---- batch: 090 ----
mean loss: 505.70
 ---- batch: 100 ----
mean loss: 502.83
 ---- batch: 110 ----
mean loss: 516.57
train mean loss: 502.65
epoch train time: 0:00:02.152837
elapsed time: 0:02:43.474707
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 00:39:24.576097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.26
 ---- batch: 020 ----
mean loss: 486.28
 ---- batch: 030 ----
mean loss: 502.33
 ---- batch: 040 ----
mean loss: 492.22
 ---- batch: 050 ----
mean loss: 504.75
 ---- batch: 060 ----
mean loss: 504.85
 ---- batch: 070 ----
mean loss: 504.56
 ---- batch: 080 ----
mean loss: 501.68
 ---- batch: 090 ----
mean loss: 512.60
 ---- batch: 100 ----
mean loss: 491.53
 ---- batch: 110 ----
mean loss: 500.14
train mean loss: 500.35
epoch train time: 0:00:02.155142
elapsed time: 0:02:45.630072
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 00:39:26.731463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.84
 ---- batch: 020 ----
mean loss: 493.56
 ---- batch: 030 ----
mean loss: 504.54
 ---- batch: 040 ----
mean loss: 505.09
 ---- batch: 050 ----
mean loss: 504.63
 ---- batch: 060 ----
mean loss: 485.72
 ---- batch: 070 ----
mean loss: 480.27
 ---- batch: 080 ----
mean loss: 504.26
 ---- batch: 090 ----
mean loss: 501.54
 ---- batch: 100 ----
mean loss: 495.88
 ---- batch: 110 ----
mean loss: 509.31
train mean loss: 498.52
epoch train time: 0:00:02.161747
elapsed time: 0:02:47.791994
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 00:39:28.893386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.23
 ---- batch: 020 ----
mean loss: 497.21
 ---- batch: 030 ----
mean loss: 492.33
 ---- batch: 040 ----
mean loss: 496.36
 ---- batch: 050 ----
mean loss: 488.79
 ---- batch: 060 ----
mean loss: 483.93
 ---- batch: 070 ----
mean loss: 507.12
 ---- batch: 080 ----
mean loss: 494.70
 ---- batch: 090 ----
mean loss: 487.49
 ---- batch: 100 ----
mean loss: 520.60
 ---- batch: 110 ----
mean loss: 504.48
train mean loss: 498.08
epoch train time: 0:00:02.158621
elapsed time: 0:02:49.950812
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 00:39:31.052216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.24
 ---- batch: 020 ----
mean loss: 513.88
 ---- batch: 030 ----
mean loss: 494.85
 ---- batch: 040 ----
mean loss: 506.60
 ---- batch: 050 ----
mean loss: 499.96
 ---- batch: 060 ----
mean loss: 491.05
 ---- batch: 070 ----
mean loss: 496.46
 ---- batch: 080 ----
mean loss: 500.16
 ---- batch: 090 ----
mean loss: 505.37
 ---- batch: 100 ----
mean loss: 496.43
 ---- batch: 110 ----
mean loss: 480.55
train mean loss: 500.01
epoch train time: 0:00:02.160314
elapsed time: 0:02:52.111329
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 00:39:33.212719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.58
 ---- batch: 020 ----
mean loss: 482.99
 ---- batch: 030 ----
mean loss: 489.08
 ---- batch: 040 ----
mean loss: 510.13
 ---- batch: 050 ----
mean loss: 504.15
 ---- batch: 060 ----
mean loss: 497.81
 ---- batch: 070 ----
mean loss: 503.71
 ---- batch: 080 ----
mean loss: 495.82
 ---- batch: 090 ----
mean loss: 494.43
 ---- batch: 100 ----
mean loss: 484.00
 ---- batch: 110 ----
mean loss: 504.53
train mean loss: 498.52
epoch train time: 0:00:02.157445
elapsed time: 0:02:54.268944
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 00:39:35.370363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.07
 ---- batch: 020 ----
mean loss: 505.05
 ---- batch: 030 ----
mean loss: 498.20
 ---- batch: 040 ----
mean loss: 503.11
 ---- batch: 050 ----
mean loss: 503.54
 ---- batch: 060 ----
mean loss: 490.74
 ---- batch: 070 ----
mean loss: 496.04
 ---- batch: 080 ----
mean loss: 490.81
 ---- batch: 090 ----
mean loss: 499.48
 ---- batch: 100 ----
mean loss: 482.10
 ---- batch: 110 ----
mean loss: 492.17
train mean loss: 496.23
epoch train time: 0:00:02.161885
elapsed time: 0:02:56.431043
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 00:39:37.532432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.30
 ---- batch: 020 ----
mean loss: 476.39
 ---- batch: 030 ----
mean loss: 484.24
 ---- batch: 040 ----
mean loss: 497.83
 ---- batch: 050 ----
mean loss: 507.05
 ---- batch: 060 ----
mean loss: 499.32
 ---- batch: 070 ----
mean loss: 500.33
 ---- batch: 080 ----
mean loss: 512.22
 ---- batch: 090 ----
mean loss: 502.26
 ---- batch: 100 ----
mean loss: 493.67
 ---- batch: 110 ----
mean loss: 511.60
train mean loss: 498.03
epoch train time: 0:00:02.155837
elapsed time: 0:02:58.587088
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 00:39:39.688483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.98
 ---- batch: 020 ----
mean loss: 505.16
 ---- batch: 030 ----
mean loss: 503.35
 ---- batch: 040 ----
mean loss: 492.01
 ---- batch: 050 ----
mean loss: 503.37
 ---- batch: 060 ----
mean loss: 519.31
 ---- batch: 070 ----
mean loss: 509.41
 ---- batch: 080 ----
mean loss: 496.06
 ---- batch: 090 ----
mean loss: 503.29
 ---- batch: 100 ----
mean loss: 494.56
 ---- batch: 110 ----
mean loss: 507.39
train mean loss: 501.62
epoch train time: 0:00:02.160718
elapsed time: 0:03:00.747985
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 00:39:41.849372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.04
 ---- batch: 020 ----
mean loss: 473.60
 ---- batch: 030 ----
mean loss: 499.53
 ---- batch: 040 ----
mean loss: 494.05
 ---- batch: 050 ----
mean loss: 497.29
 ---- batch: 060 ----
mean loss: 508.94
 ---- batch: 070 ----
mean loss: 484.77
 ---- batch: 080 ----
mean loss: 493.81
 ---- batch: 090 ----
mean loss: 497.53
 ---- batch: 100 ----
mean loss: 491.16
 ---- batch: 110 ----
mean loss: 499.51
train mean loss: 495.52
epoch train time: 0:00:02.163252
elapsed time: 0:03:02.911399
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 00:39:44.012786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.70
 ---- batch: 020 ----
mean loss: 497.23
 ---- batch: 030 ----
mean loss: 500.90
 ---- batch: 040 ----
mean loss: 504.19
 ---- batch: 050 ----
mean loss: 491.56
 ---- batch: 060 ----
mean loss: 485.33
 ---- batch: 070 ----
mean loss: 516.02
 ---- batch: 080 ----
mean loss: 477.13
 ---- batch: 090 ----
mean loss: 500.19
 ---- batch: 100 ----
mean loss: 491.12
 ---- batch: 110 ----
mean loss: 489.43
train mean loss: 496.25
epoch train time: 0:00:02.160079
elapsed time: 0:03:05.071653
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 00:39:46.173046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.52
 ---- batch: 020 ----
mean loss: 493.52
 ---- batch: 030 ----
mean loss: 496.25
 ---- batch: 040 ----
mean loss: 485.37
 ---- batch: 050 ----
mean loss: 472.70
 ---- batch: 060 ----
mean loss: 491.50
 ---- batch: 070 ----
mean loss: 501.72
 ---- batch: 080 ----
mean loss: 508.51
 ---- batch: 090 ----
mean loss: 509.40
 ---- batch: 100 ----
mean loss: 509.36
 ---- batch: 110 ----
mean loss: 499.55
train mean loss: 496.45
epoch train time: 0:00:02.164303
elapsed time: 0:03:07.236129
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 00:39:48.337537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.21
 ---- batch: 020 ----
mean loss: 491.37
 ---- batch: 030 ----
mean loss: 498.34
 ---- batch: 040 ----
mean loss: 501.21
 ---- batch: 050 ----
mean loss: 503.41
 ---- batch: 060 ----
mean loss: 488.82
 ---- batch: 070 ----
mean loss: 488.09
 ---- batch: 080 ----
mean loss: 517.37
 ---- batch: 090 ----
mean loss: 485.62
 ---- batch: 100 ----
mean loss: 496.95
 ---- batch: 110 ----
mean loss: 495.29
train mean loss: 494.48
epoch train time: 0:00:02.160683
elapsed time: 0:03:09.397019
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 00:39:50.498414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.13
 ---- batch: 020 ----
mean loss: 495.45
 ---- batch: 030 ----
mean loss: 480.31
 ---- batch: 040 ----
mean loss: 473.23
 ---- batch: 050 ----
mean loss: 493.78
 ---- batch: 060 ----
mean loss: 494.69
 ---- batch: 070 ----
mean loss: 497.94
 ---- batch: 080 ----
mean loss: 509.19
 ---- batch: 090 ----
mean loss: 506.44
 ---- batch: 100 ----
mean loss: 486.00
 ---- batch: 110 ----
mean loss: 492.65
train mean loss: 493.80
epoch train time: 0:00:02.158338
elapsed time: 0:03:11.555542
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 00:39:52.656932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.71
 ---- batch: 020 ----
mean loss: 497.51
 ---- batch: 030 ----
mean loss: 481.54
 ---- batch: 040 ----
mean loss: 479.67
 ---- batch: 050 ----
mean loss: 476.29
 ---- batch: 060 ----
mean loss: 500.58
 ---- batch: 070 ----
mean loss: 505.84
 ---- batch: 080 ----
mean loss: 509.87
 ---- batch: 090 ----
mean loss: 493.29
 ---- batch: 100 ----
mean loss: 490.26
 ---- batch: 110 ----
mean loss: 493.28
train mean loss: 493.24
epoch train time: 0:00:02.159590
elapsed time: 0:03:13.715299
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 00:39:54.816689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.97
 ---- batch: 020 ----
mean loss: 491.78
 ---- batch: 030 ----
mean loss: 498.30
 ---- batch: 040 ----
mean loss: 499.73
 ---- batch: 050 ----
mean loss: 501.58
 ---- batch: 060 ----
mean loss: 504.61
 ---- batch: 070 ----
mean loss: 495.21
 ---- batch: 080 ----
mean loss: 496.02
 ---- batch: 090 ----
mean loss: 497.81
 ---- batch: 100 ----
mean loss: 505.05
 ---- batch: 110 ----
mean loss: 489.84
train mean loss: 497.38
epoch train time: 0:00:02.165030
elapsed time: 0:03:15.880508
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 00:39:56.981911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.42
 ---- batch: 020 ----
mean loss: 489.81
 ---- batch: 030 ----
mean loss: 486.96
 ---- batch: 040 ----
mean loss: 498.93
 ---- batch: 050 ----
mean loss: 508.11
 ---- batch: 060 ----
mean loss: 493.26
 ---- batch: 070 ----
mean loss: 485.51
 ---- batch: 080 ----
mean loss: 479.97
 ---- batch: 090 ----
mean loss: 498.33
 ---- batch: 100 ----
mean loss: 483.35
 ---- batch: 110 ----
mean loss: 490.97
train mean loss: 491.26
epoch train time: 0:00:02.159241
elapsed time: 0:03:18.039925
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 00:39:59.141311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.13
 ---- batch: 020 ----
mean loss: 502.28
 ---- batch: 030 ----
mean loss: 486.02
 ---- batch: 040 ----
mean loss: 503.99
 ---- batch: 050 ----
mean loss: 504.67
 ---- batch: 060 ----
mean loss: 496.75
 ---- batch: 070 ----
mean loss: 483.80
 ---- batch: 080 ----
mean loss: 488.71
 ---- batch: 090 ----
mean loss: 508.09
 ---- batch: 100 ----
mean loss: 484.28
 ---- batch: 110 ----
mean loss: 490.46
train mean loss: 495.37
epoch train time: 0:00:02.168864
elapsed time: 0:03:20.208983
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 00:40:01.310376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.43
 ---- batch: 020 ----
mean loss: 495.35
 ---- batch: 030 ----
mean loss: 475.68
 ---- batch: 040 ----
mean loss: 475.37
 ---- batch: 050 ----
mean loss: 497.59
 ---- batch: 060 ----
mean loss: 493.61
 ---- batch: 070 ----
mean loss: 505.42
 ---- batch: 080 ----
mean loss: 497.46
 ---- batch: 090 ----
mean loss: 483.43
 ---- batch: 100 ----
mean loss: 508.32
 ---- batch: 110 ----
mean loss: 496.78
train mean loss: 492.71
epoch train time: 0:00:02.156316
elapsed time: 0:03:22.365469
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 00:40:03.466873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.53
 ---- batch: 020 ----
mean loss: 484.89
 ---- batch: 030 ----
mean loss: 496.95
 ---- batch: 040 ----
mean loss: 493.30
 ---- batch: 050 ----
mean loss: 493.01
 ---- batch: 060 ----
mean loss: 482.71
 ---- batch: 070 ----
mean loss: 480.53
 ---- batch: 080 ----
mean loss: 497.03
 ---- batch: 090 ----
mean loss: 508.01
 ---- batch: 100 ----
mean loss: 471.27
 ---- batch: 110 ----
mean loss: 497.52
train mean loss: 490.81
epoch train time: 0:00:02.154146
elapsed time: 0:03:24.519787
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 00:40:05.621176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.22
 ---- batch: 020 ----
mean loss: 487.24
 ---- batch: 030 ----
mean loss: 488.27
 ---- batch: 040 ----
mean loss: 481.64
 ---- batch: 050 ----
mean loss: 488.27
 ---- batch: 060 ----
mean loss: 489.42
 ---- batch: 070 ----
mean loss: 494.76
 ---- batch: 080 ----
mean loss: 497.91
 ---- batch: 090 ----
mean loss: 496.74
 ---- batch: 100 ----
mean loss: 504.08
 ---- batch: 110 ----
mean loss: 477.24
train mean loss: 490.56
epoch train time: 0:00:02.146803
elapsed time: 0:03:26.666749
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 00:40:07.768134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.89
 ---- batch: 020 ----
mean loss: 482.85
 ---- batch: 030 ----
mean loss: 494.79
 ---- batch: 040 ----
mean loss: 495.12
 ---- batch: 050 ----
mean loss: 495.50
 ---- batch: 060 ----
mean loss: 489.62
 ---- batch: 070 ----
mean loss: 488.83
 ---- batch: 080 ----
mean loss: 492.79
 ---- batch: 090 ----
mean loss: 489.69
 ---- batch: 100 ----
mean loss: 478.46
 ---- batch: 110 ----
mean loss: 498.11
train mean loss: 490.81
epoch train time: 0:00:02.135172
elapsed time: 0:03:28.802089
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 00:40:09.903481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.22
 ---- batch: 020 ----
mean loss: 485.20
 ---- batch: 030 ----
mean loss: 506.56
 ---- batch: 040 ----
mean loss: 480.88
 ---- batch: 050 ----
mean loss: 475.99
 ---- batch: 060 ----
mean loss: 491.89
 ---- batch: 070 ----
mean loss: 504.32
 ---- batch: 080 ----
mean loss: 493.48
 ---- batch: 090 ----
mean loss: 484.28
 ---- batch: 100 ----
mean loss: 504.29
 ---- batch: 110 ----
mean loss: 483.41
train mean loss: 488.96
epoch train time: 0:00:02.147993
elapsed time: 0:03:30.950246
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 00:40:12.051631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.21
 ---- batch: 020 ----
mean loss: 497.90
 ---- batch: 030 ----
mean loss: 481.92
 ---- batch: 040 ----
mean loss: 497.57
 ---- batch: 050 ----
mean loss: 489.75
 ---- batch: 060 ----
mean loss: 486.53
 ---- batch: 070 ----
mean loss: 508.63
 ---- batch: 080 ----
mean loss: 486.29
 ---- batch: 090 ----
mean loss: 484.08
 ---- batch: 100 ----
mean loss: 486.97
 ---- batch: 110 ----
mean loss: 491.97
train mean loss: 491.16
epoch train time: 0:00:02.139911
elapsed time: 0:03:33.090314
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 00:40:14.191702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.78
 ---- batch: 020 ----
mean loss: 492.55
 ---- batch: 030 ----
mean loss: 504.24
 ---- batch: 040 ----
mean loss: 490.74
 ---- batch: 050 ----
mean loss: 483.59
 ---- batch: 060 ----
mean loss: 490.26
 ---- batch: 070 ----
mean loss: 487.06
 ---- batch: 080 ----
mean loss: 506.97
 ---- batch: 090 ----
mean loss: 496.96
 ---- batch: 100 ----
mean loss: 494.00
 ---- batch: 110 ----
mean loss: 482.31
train mean loss: 492.13
epoch train time: 0:00:02.144720
elapsed time: 0:03:35.235197
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 00:40:16.336604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.29
 ---- batch: 020 ----
mean loss: 502.73
 ---- batch: 030 ----
mean loss: 499.82
 ---- batch: 040 ----
mean loss: 497.13
 ---- batch: 050 ----
mean loss: 490.26
 ---- batch: 060 ----
mean loss: 482.72
 ---- batch: 070 ----
mean loss: 481.83
 ---- batch: 080 ----
mean loss: 487.40
 ---- batch: 090 ----
mean loss: 499.98
 ---- batch: 100 ----
mean loss: 475.09
 ---- batch: 110 ----
mean loss: 490.08
train mean loss: 487.11
epoch train time: 0:00:02.143655
elapsed time: 0:03:37.379028
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 00:40:18.480414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.19
 ---- batch: 020 ----
mean loss: 505.74
 ---- batch: 030 ----
mean loss: 503.83
 ---- batch: 040 ----
mean loss: 492.05
 ---- batch: 050 ----
mean loss: 499.04
 ---- batch: 060 ----
mean loss: 493.93
 ---- batch: 070 ----
mean loss: 484.69
 ---- batch: 080 ----
mean loss: 477.34
 ---- batch: 090 ----
mean loss: 503.50
 ---- batch: 100 ----
mean loss: 488.46
 ---- batch: 110 ----
mean loss: 489.66
train mean loss: 489.74
epoch train time: 0:00:02.149591
elapsed time: 0:03:39.528819
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 00:40:20.630210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.07
 ---- batch: 020 ----
mean loss: 499.31
 ---- batch: 030 ----
mean loss: 497.57
 ---- batch: 040 ----
mean loss: 481.31
 ---- batch: 050 ----
mean loss: 493.47
 ---- batch: 060 ----
mean loss: 502.30
 ---- batch: 070 ----
mean loss: 485.43
 ---- batch: 080 ----
mean loss: 505.39
 ---- batch: 090 ----
mean loss: 484.65
 ---- batch: 100 ----
mean loss: 478.20
 ---- batch: 110 ----
mean loss: 495.57
train mean loss: 491.39
epoch train time: 0:00:02.145513
elapsed time: 0:03:41.674523
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 00:40:22.775912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.96
 ---- batch: 020 ----
mean loss: 490.26
 ---- batch: 030 ----
mean loss: 489.16
 ---- batch: 040 ----
mean loss: 488.37
 ---- batch: 050 ----
mean loss: 509.09
 ---- batch: 060 ----
mean loss: 487.42
 ---- batch: 070 ----
mean loss: 486.76
 ---- batch: 080 ----
mean loss: 500.91
 ---- batch: 090 ----
mean loss: 494.55
 ---- batch: 100 ----
mean loss: 492.49
 ---- batch: 110 ----
mean loss: 491.92
train mean loss: 492.14
epoch train time: 0:00:02.146178
elapsed time: 0:03:43.820866
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 00:40:24.922276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.64
 ---- batch: 020 ----
mean loss: 481.83
 ---- batch: 030 ----
mean loss: 469.33
 ---- batch: 040 ----
mean loss: 492.82
 ---- batch: 050 ----
mean loss: 484.50
 ---- batch: 060 ----
mean loss: 478.23
 ---- batch: 070 ----
mean loss: 483.97
 ---- batch: 080 ----
mean loss: 493.80
 ---- batch: 090 ----
mean loss: 480.88
 ---- batch: 100 ----
mean loss: 487.02
 ---- batch: 110 ----
mean loss: 477.08
train mean loss: 484.79
epoch train time: 0:00:02.153191
elapsed time: 0:03:45.974254
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 00:40:27.075638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.43
 ---- batch: 020 ----
mean loss: 477.09
 ---- batch: 030 ----
mean loss: 463.90
 ---- batch: 040 ----
mean loss: 496.97
 ---- batch: 050 ----
mean loss: 495.50
 ---- batch: 060 ----
mean loss: 486.61
 ---- batch: 070 ----
mean loss: 493.24
 ---- batch: 080 ----
mean loss: 480.51
 ---- batch: 090 ----
mean loss: 506.75
 ---- batch: 100 ----
mean loss: 497.64
 ---- batch: 110 ----
mean loss: 492.64
train mean loss: 488.77
epoch train time: 0:00:02.147395
elapsed time: 0:03:48.121808
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 00:40:29.223194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.07
 ---- batch: 020 ----
mean loss: 495.54
 ---- batch: 030 ----
mean loss: 485.46
 ---- batch: 040 ----
mean loss: 480.74
 ---- batch: 050 ----
mean loss: 493.20
 ---- batch: 060 ----
mean loss: 491.11
 ---- batch: 070 ----
mean loss: 483.89
 ---- batch: 080 ----
mean loss: 474.60
 ---- batch: 090 ----
mean loss: 478.84
 ---- batch: 100 ----
mean loss: 470.55
 ---- batch: 110 ----
mean loss: 484.37
train mean loss: 484.40
epoch train time: 0:00:02.140238
elapsed time: 0:03:50.262192
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 00:40:31.363608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.02
 ---- batch: 020 ----
mean loss: 493.62
 ---- batch: 030 ----
mean loss: 489.59
 ---- batch: 040 ----
mean loss: 485.80
 ---- batch: 050 ----
mean loss: 484.45
 ---- batch: 060 ----
mean loss: 479.85
 ---- batch: 070 ----
mean loss: 497.97
 ---- batch: 080 ----
mean loss: 476.92
 ---- batch: 090 ----
mean loss: 476.50
 ---- batch: 100 ----
mean loss: 474.43
 ---- batch: 110 ----
mean loss: 497.53
train mean loss: 488.60
epoch train time: 0:00:02.147349
elapsed time: 0:03:52.409789
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 00:40:33.511208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.40
 ---- batch: 020 ----
mean loss: 513.60
 ---- batch: 030 ----
mean loss: 502.38
 ---- batch: 040 ----
mean loss: 475.81
 ---- batch: 050 ----
mean loss: 474.08
 ---- batch: 060 ----
mean loss: 478.50
 ---- batch: 070 ----
mean loss: 481.16
 ---- batch: 080 ----
mean loss: 480.41
 ---- batch: 090 ----
mean loss: 483.80
 ---- batch: 100 ----
mean loss: 483.24
 ---- batch: 110 ----
mean loss: 477.93
train mean loss: 482.86
epoch train time: 0:00:02.145350
elapsed time: 0:03:54.555341
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 00:40:35.656753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.15
 ---- batch: 020 ----
mean loss: 476.90
 ---- batch: 030 ----
mean loss: 477.78
 ---- batch: 040 ----
mean loss: 480.26
 ---- batch: 050 ----
mean loss: 506.11
 ---- batch: 060 ----
mean loss: 495.74
 ---- batch: 070 ----
mean loss: 487.63
 ---- batch: 080 ----
mean loss: 468.01
 ---- batch: 090 ----
mean loss: 486.20
 ---- batch: 100 ----
mean loss: 480.18
 ---- batch: 110 ----
mean loss: 476.18
train mean loss: 483.82
epoch train time: 0:00:02.140938
elapsed time: 0:03:56.696466
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 00:40:37.797851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.12
 ---- batch: 020 ----
mean loss: 474.02
 ---- batch: 030 ----
mean loss: 468.33
 ---- batch: 040 ----
mean loss: 470.01
 ---- batch: 050 ----
mean loss: 491.25
 ---- batch: 060 ----
mean loss: 485.91
 ---- batch: 070 ----
mean loss: 502.12
 ---- batch: 080 ----
mean loss: 503.02
 ---- batch: 090 ----
mean loss: 502.50
 ---- batch: 100 ----
mean loss: 474.38
 ---- batch: 110 ----
mean loss: 481.50
train mean loss: 484.23
epoch train time: 0:00:02.149348
elapsed time: 0:03:58.845986
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 00:40:39.947426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.64
 ---- batch: 020 ----
mean loss: 485.86
 ---- batch: 030 ----
mean loss: 483.97
 ---- batch: 040 ----
mean loss: 478.43
 ---- batch: 050 ----
mean loss: 488.29
 ---- batch: 060 ----
mean loss: 468.28
 ---- batch: 070 ----
mean loss: 478.56
 ---- batch: 080 ----
mean loss: 461.13
 ---- batch: 090 ----
mean loss: 487.03
 ---- batch: 100 ----
mean loss: 488.56
 ---- batch: 110 ----
mean loss: 476.51
train mean loss: 479.92
epoch train time: 0:00:02.144886
elapsed time: 0:04:00.991079
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 00:40:42.092464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.75
 ---- batch: 020 ----
mean loss: 461.67
 ---- batch: 030 ----
mean loss: 457.70
 ---- batch: 040 ----
mean loss: 479.13
 ---- batch: 050 ----
mean loss: 485.66
 ---- batch: 060 ----
mean loss: 486.03
 ---- batch: 070 ----
mean loss: 482.46
 ---- batch: 080 ----
mean loss: 482.61
 ---- batch: 090 ----
mean loss: 483.21
 ---- batch: 100 ----
mean loss: 471.96
 ---- batch: 110 ----
mean loss: 488.22
train mean loss: 478.97
epoch train time: 0:00:02.135454
elapsed time: 0:04:03.126791
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 00:40:44.228193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.53
 ---- batch: 020 ----
mean loss: 468.50
 ---- batch: 030 ----
mean loss: 492.26
 ---- batch: 040 ----
mean loss: 485.05
 ---- batch: 050 ----
mean loss: 485.55
 ---- batch: 060 ----
mean loss: 480.65
 ---- batch: 070 ----
mean loss: 484.42
 ---- batch: 080 ----
mean loss: 485.61
 ---- batch: 090 ----
mean loss: 475.84
 ---- batch: 100 ----
mean loss: 485.13
 ---- batch: 110 ----
mean loss: 483.12
train mean loss: 482.24
epoch train time: 0:00:02.141437
elapsed time: 0:04:05.268401
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 00:40:46.369790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.86
 ---- batch: 020 ----
mean loss: 476.91
 ---- batch: 030 ----
mean loss: 468.45
 ---- batch: 040 ----
mean loss: 478.57
 ---- batch: 050 ----
mean loss: 489.17
 ---- batch: 060 ----
mean loss: 494.83
 ---- batch: 070 ----
mean loss: 478.41
 ---- batch: 080 ----
mean loss: 487.81
 ---- batch: 090 ----
mean loss: 473.94
 ---- batch: 100 ----
mean loss: 480.35
 ---- batch: 110 ----
mean loss: 477.43
train mean loss: 479.23
epoch train time: 0:00:02.140795
elapsed time: 0:04:07.409351
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 00:40:48.510749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.27
 ---- batch: 020 ----
mean loss: 475.71
 ---- batch: 030 ----
mean loss: 473.08
 ---- batch: 040 ----
mean loss: 489.45
 ---- batch: 050 ----
mean loss: 460.73
 ---- batch: 060 ----
mean loss: 477.40
 ---- batch: 070 ----
mean loss: 474.15
 ---- batch: 080 ----
mean loss: 484.20
 ---- batch: 090 ----
mean loss: 485.71
 ---- batch: 100 ----
mean loss: 483.30
 ---- batch: 110 ----
mean loss: 482.94
train mean loss: 478.69
epoch train time: 0:00:02.144688
elapsed time: 0:04:09.554231
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 00:40:50.655616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.93
 ---- batch: 020 ----
mean loss: 490.21
 ---- batch: 030 ----
mean loss: 494.97
 ---- batch: 040 ----
mean loss: 484.73
 ---- batch: 050 ----
mean loss: 485.86
 ---- batch: 060 ----
mean loss: 473.61
 ---- batch: 070 ----
mean loss: 484.56
 ---- batch: 080 ----
mean loss: 474.81
 ---- batch: 090 ----
mean loss: 470.93
 ---- batch: 100 ----
mean loss: 503.65
 ---- batch: 110 ----
mean loss: 478.83
train mean loss: 483.91
epoch train time: 0:00:02.146509
elapsed time: 0:04:11.700892
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 00:40:52.802276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.36
 ---- batch: 020 ----
mean loss: 491.89
 ---- batch: 030 ----
mean loss: 479.72
 ---- batch: 040 ----
mean loss: 470.53
 ---- batch: 050 ----
mean loss: 473.85
 ---- batch: 060 ----
mean loss: 466.40
 ---- batch: 070 ----
mean loss: 479.78
 ---- batch: 080 ----
mean loss: 479.13
 ---- batch: 090 ----
mean loss: 491.10
 ---- batch: 100 ----
mean loss: 486.97
 ---- batch: 110 ----
mean loss: 457.52
train mean loss: 478.57
epoch train time: 0:00:02.141510
elapsed time: 0:04:13.842566
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 00:40:54.943951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.17
 ---- batch: 020 ----
mean loss: 471.83
 ---- batch: 030 ----
mean loss: 482.00
 ---- batch: 040 ----
mean loss: 470.50
 ---- batch: 050 ----
mean loss: 478.71
 ---- batch: 060 ----
mean loss: 476.05
 ---- batch: 070 ----
mean loss: 476.73
 ---- batch: 080 ----
mean loss: 491.80
 ---- batch: 090 ----
mean loss: 492.58
 ---- batch: 100 ----
mean loss: 471.57
 ---- batch: 110 ----
mean loss: 469.54
train mean loss: 478.66
epoch train time: 0:00:02.134357
elapsed time: 0:04:15.977081
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 00:40:57.078470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.16
 ---- batch: 020 ----
mean loss: 484.45
 ---- batch: 030 ----
mean loss: 465.54
 ---- batch: 040 ----
mean loss: 478.20
 ---- batch: 050 ----
mean loss: 488.98
 ---- batch: 060 ----
mean loss: 491.20
 ---- batch: 070 ----
mean loss: 480.70
 ---- batch: 080 ----
mean loss: 487.39
 ---- batch: 090 ----
mean loss: 485.48
 ---- batch: 100 ----
mean loss: 483.81
 ---- batch: 110 ----
mean loss: 483.95
train mean loss: 484.90
epoch train time: 0:00:02.138692
elapsed time: 0:04:18.115946
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 00:40:59.217351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.86
 ---- batch: 020 ----
mean loss: 481.99
 ---- batch: 030 ----
mean loss: 497.74
 ---- batch: 040 ----
mean loss: 473.28
 ---- batch: 050 ----
mean loss: 480.07
 ---- batch: 060 ----
mean loss: 471.18
 ---- batch: 070 ----
mean loss: 486.43
 ---- batch: 080 ----
mean loss: 483.91
 ---- batch: 090 ----
mean loss: 474.84
 ---- batch: 100 ----
mean loss: 480.84
 ---- batch: 110 ----
mean loss: 486.66
train mean loss: 481.31
epoch train time: 0:00:02.140136
elapsed time: 0:04:20.256269
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 00:41:01.357680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.03
 ---- batch: 020 ----
mean loss: 489.48
 ---- batch: 030 ----
mean loss: 475.84
 ---- batch: 040 ----
mean loss: 472.73
 ---- batch: 050 ----
mean loss: 476.58
 ---- batch: 060 ----
mean loss: 483.25
 ---- batch: 070 ----
mean loss: 486.90
 ---- batch: 080 ----
mean loss: 482.06
 ---- batch: 090 ----
mean loss: 465.52
 ---- batch: 100 ----
mean loss: 479.77
 ---- batch: 110 ----
mean loss: 475.33
train mean loss: 478.73
epoch train time: 0:00:02.144049
elapsed time: 0:04:22.400519
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 00:41:03.501895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.92
 ---- batch: 020 ----
mean loss: 478.28
 ---- batch: 030 ----
mean loss: 465.14
 ---- batch: 040 ----
mean loss: 489.70
 ---- batch: 050 ----
mean loss: 485.42
 ---- batch: 060 ----
mean loss: 477.60
 ---- batch: 070 ----
mean loss: 468.33
 ---- batch: 080 ----
mean loss: 484.04
 ---- batch: 090 ----
mean loss: 490.52
 ---- batch: 100 ----
mean loss: 456.52
 ---- batch: 110 ----
mean loss: 472.06
train mean loss: 477.81
epoch train time: 0:00:02.131052
elapsed time: 0:04:24.531712
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 00:41:05.633097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.01
 ---- batch: 020 ----
mean loss: 483.19
 ---- batch: 030 ----
mean loss: 494.54
 ---- batch: 040 ----
mean loss: 473.20
 ---- batch: 050 ----
mean loss: 485.16
 ---- batch: 060 ----
mean loss: 483.67
 ---- batch: 070 ----
mean loss: 478.52
 ---- batch: 080 ----
mean loss: 485.52
 ---- batch: 090 ----
mean loss: 482.54
 ---- batch: 100 ----
mean loss: 459.22
 ---- batch: 110 ----
mean loss: 460.16
train mean loss: 477.83
epoch train time: 0:00:02.136053
elapsed time: 0:04:26.667963
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 00:41:07.769360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.99
 ---- batch: 020 ----
mean loss: 464.58
 ---- batch: 030 ----
mean loss: 466.85
 ---- batch: 040 ----
mean loss: 463.94
 ---- batch: 050 ----
mean loss: 476.33
 ---- batch: 060 ----
mean loss: 472.39
 ---- batch: 070 ----
mean loss: 481.04
 ---- batch: 080 ----
mean loss: 469.23
 ---- batch: 090 ----
mean loss: 470.10
 ---- batch: 100 ----
mean loss: 471.95
 ---- batch: 110 ----
mean loss: 468.85
train mean loss: 471.34
epoch train time: 0:00:02.129565
elapsed time: 0:04:28.797690
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 00:41:09.899073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.15
 ---- batch: 020 ----
mean loss: 477.08
 ---- batch: 030 ----
mean loss: 477.78
 ---- batch: 040 ----
mean loss: 470.47
 ---- batch: 050 ----
mean loss: 477.16
 ---- batch: 060 ----
mean loss: 471.25
 ---- batch: 070 ----
mean loss: 478.70
 ---- batch: 080 ----
mean loss: 490.08
 ---- batch: 090 ----
mean loss: 470.07
 ---- batch: 100 ----
mean loss: 484.93
 ---- batch: 110 ----
mean loss: 493.51
train mean loss: 479.19
epoch train time: 0:00:02.133759
elapsed time: 0:04:30.931613
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 00:41:12.033018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.41
 ---- batch: 020 ----
mean loss: 473.62
 ---- batch: 030 ----
mean loss: 465.15
 ---- batch: 040 ----
mean loss: 470.74
 ---- batch: 050 ----
mean loss: 474.61
 ---- batch: 060 ----
mean loss: 484.40
 ---- batch: 070 ----
mean loss: 464.17
 ---- batch: 080 ----
mean loss: 474.93
 ---- batch: 090 ----
mean loss: 480.00
 ---- batch: 100 ----
mean loss: 474.03
 ---- batch: 110 ----
mean loss: 493.95
train mean loss: 475.69
epoch train time: 0:00:02.151546
elapsed time: 0:04:33.083354
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 00:41:14.184773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.91
 ---- batch: 020 ----
mean loss: 490.85
 ---- batch: 030 ----
mean loss: 488.21
 ---- batch: 040 ----
mean loss: 473.48
 ---- batch: 050 ----
mean loss: 450.27
 ---- batch: 060 ----
mean loss: 471.34
 ---- batch: 070 ----
mean loss: 468.19
 ---- batch: 080 ----
mean loss: 489.60
 ---- batch: 090 ----
mean loss: 496.26
 ---- batch: 100 ----
mean loss: 484.96
 ---- batch: 110 ----
mean loss: 483.85
train mean loss: 479.05
epoch train time: 0:00:02.154558
elapsed time: 0:04:35.238109
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 00:41:16.339500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.38
 ---- batch: 020 ----
mean loss: 472.18
 ---- batch: 030 ----
mean loss: 485.19
 ---- batch: 040 ----
mean loss: 478.17
 ---- batch: 050 ----
mean loss: 470.81
 ---- batch: 060 ----
mean loss: 478.67
 ---- batch: 070 ----
mean loss: 480.32
 ---- batch: 080 ----
mean loss: 467.40
 ---- batch: 090 ----
mean loss: 475.17
 ---- batch: 100 ----
mean loss: 455.25
 ---- batch: 110 ----
mean loss: 482.05
train mean loss: 474.43
epoch train time: 0:00:02.161239
elapsed time: 0:04:37.399523
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 00:41:18.500905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.22
 ---- batch: 020 ----
mean loss: 485.90
 ---- batch: 030 ----
mean loss: 462.82
 ---- batch: 040 ----
mean loss: 460.80
 ---- batch: 050 ----
mean loss: 471.82
 ---- batch: 060 ----
mean loss: 476.11
 ---- batch: 070 ----
mean loss: 472.93
 ---- batch: 080 ----
mean loss: 474.63
 ---- batch: 090 ----
mean loss: 466.29
 ---- batch: 100 ----
mean loss: 464.84
 ---- batch: 110 ----
mean loss: 472.17
train mean loss: 470.88
epoch train time: 0:00:02.153803
elapsed time: 0:04:39.553522
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 00:41:20.654917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.79
 ---- batch: 020 ----
mean loss: 488.01
 ---- batch: 030 ----
mean loss: 485.80
 ---- batch: 040 ----
mean loss: 468.67
 ---- batch: 050 ----
mean loss: 489.02
 ---- batch: 060 ----
mean loss: 481.30
 ---- batch: 070 ----
mean loss: 464.31
 ---- batch: 080 ----
mean loss: 476.39
 ---- batch: 090 ----
mean loss: 477.33
 ---- batch: 100 ----
mean loss: 456.56
 ---- batch: 110 ----
mean loss: 463.69
train mean loss: 474.64
epoch train time: 0:00:02.166323
elapsed time: 0:04:41.720047
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 00:41:22.821438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.54
 ---- batch: 020 ----
mean loss: 468.10
 ---- batch: 030 ----
mean loss: 469.36
 ---- batch: 040 ----
mean loss: 472.29
 ---- batch: 050 ----
mean loss: 462.93
 ---- batch: 060 ----
mean loss: 488.91
 ---- batch: 070 ----
mean loss: 475.42
 ---- batch: 080 ----
mean loss: 459.03
 ---- batch: 090 ----
mean loss: 463.03
 ---- batch: 100 ----
mean loss: 488.50
 ---- batch: 110 ----
mean loss: 474.50
train mean loss: 472.23
epoch train time: 0:00:02.163923
elapsed time: 0:04:43.884182
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 00:41:24.985573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.94
 ---- batch: 020 ----
mean loss: 475.48
 ---- batch: 030 ----
mean loss: 478.54
 ---- batch: 040 ----
mean loss: 471.54
 ---- batch: 050 ----
mean loss: 474.57
 ---- batch: 060 ----
mean loss: 469.70
 ---- batch: 070 ----
mean loss: 477.69
 ---- batch: 080 ----
mean loss: 480.47
 ---- batch: 090 ----
mean loss: 462.12
 ---- batch: 100 ----
mean loss: 466.86
 ---- batch: 110 ----
mean loss: 464.83
train mean loss: 472.82
epoch train time: 0:00:02.155268
elapsed time: 0:04:46.039663
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 00:41:27.141078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.60
 ---- batch: 020 ----
mean loss: 472.43
 ---- batch: 030 ----
mean loss: 467.89
 ---- batch: 040 ----
mean loss: 475.82
 ---- batch: 050 ----
mean loss: 474.19
 ---- batch: 060 ----
mean loss: 462.52
 ---- batch: 070 ----
mean loss: 467.89
 ---- batch: 080 ----
mean loss: 484.87
 ---- batch: 090 ----
mean loss: 483.48
 ---- batch: 100 ----
mean loss: 487.28
 ---- batch: 110 ----
mean loss: 475.64
train mean loss: 474.07
epoch train time: 0:00:02.159521
elapsed time: 0:04:48.199374
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 00:41:29.300764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.31
 ---- batch: 020 ----
mean loss: 465.45
 ---- batch: 030 ----
mean loss: 459.75
 ---- batch: 040 ----
mean loss: 458.60
 ---- batch: 050 ----
mean loss: 485.58
 ---- batch: 060 ----
mean loss: 475.07
 ---- batch: 070 ----
mean loss: 495.85
 ---- batch: 080 ----
mean loss: 479.96
 ---- batch: 090 ----
mean loss: 469.94
 ---- batch: 100 ----
mean loss: 441.50
 ---- batch: 110 ----
mean loss: 479.13
train mean loss: 470.73
epoch train time: 0:00:02.162414
elapsed time: 0:04:50.361952
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 00:41:31.463342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.29
 ---- batch: 020 ----
mean loss: 444.89
 ---- batch: 030 ----
mean loss: 457.04
 ---- batch: 040 ----
mean loss: 471.77
 ---- batch: 050 ----
mean loss: 468.42
 ---- batch: 060 ----
mean loss: 461.09
 ---- batch: 070 ----
mean loss: 451.32
 ---- batch: 080 ----
mean loss: 462.25
 ---- batch: 090 ----
mean loss: 472.97
 ---- batch: 100 ----
mean loss: 479.63
 ---- batch: 110 ----
mean loss: 475.01
train mean loss: 462.82
epoch train time: 0:00:02.154627
elapsed time: 0:04:52.516742
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 00:41:33.618127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.03
 ---- batch: 020 ----
mean loss: 467.48
 ---- batch: 030 ----
mean loss: 479.39
 ---- batch: 040 ----
mean loss: 487.17
 ---- batch: 050 ----
mean loss: 454.90
 ---- batch: 060 ----
mean loss: 483.21
 ---- batch: 070 ----
mean loss: 474.66
 ---- batch: 080 ----
mean loss: 468.91
 ---- batch: 090 ----
mean loss: 474.25
 ---- batch: 100 ----
mean loss: 463.07
 ---- batch: 110 ----
mean loss: 457.97
train mean loss: 469.73
epoch train time: 0:00:02.162607
elapsed time: 0:04:54.679570
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 00:41:35.780969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.61
 ---- batch: 020 ----
mean loss: 473.55
 ---- batch: 030 ----
mean loss: 459.58
 ---- batch: 040 ----
mean loss: 459.11
 ---- batch: 050 ----
mean loss: 466.74
 ---- batch: 060 ----
mean loss: 463.33
 ---- batch: 070 ----
mean loss: 468.42
 ---- batch: 080 ----
mean loss: 482.30
 ---- batch: 090 ----
mean loss: 456.66
 ---- batch: 100 ----
mean loss: 478.92
 ---- batch: 110 ----
mean loss: 456.88
train mean loss: 466.69
epoch train time: 0:00:02.161710
elapsed time: 0:04:56.841467
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 00:41:37.942858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.28
 ---- batch: 020 ----
mean loss: 451.81
 ---- batch: 030 ----
mean loss: 444.75
 ---- batch: 040 ----
mean loss: 468.73
 ---- batch: 050 ----
mean loss: 469.78
 ---- batch: 060 ----
mean loss: 452.45
 ---- batch: 070 ----
mean loss: 462.59
 ---- batch: 080 ----
mean loss: 463.82
 ---- batch: 090 ----
mean loss: 476.76
 ---- batch: 100 ----
mean loss: 471.96
 ---- batch: 110 ----
mean loss: 464.55
train mean loss: 463.75
epoch train time: 0:00:02.157774
elapsed time: 0:04:58.999408
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 00:41:40.100797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.40
 ---- batch: 020 ----
mean loss: 444.36
 ---- batch: 030 ----
mean loss: 479.06
 ---- batch: 040 ----
mean loss: 476.84
 ---- batch: 050 ----
mean loss: 463.72
 ---- batch: 060 ----
mean loss: 441.53
 ---- batch: 070 ----
mean loss: 466.95
 ---- batch: 080 ----
mean loss: 469.28
 ---- batch: 090 ----
mean loss: 470.50
 ---- batch: 100 ----
mean loss: 458.90
 ---- batch: 110 ----
mean loss: 477.72
train mean loss: 463.95
epoch train time: 0:00:02.161769
elapsed time: 0:05:01.161342
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 00:41:42.262766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.85
 ---- batch: 020 ----
mean loss: 452.56
 ---- batch: 030 ----
mean loss: 466.58
 ---- batch: 040 ----
mean loss: 487.79
 ---- batch: 050 ----
mean loss: 468.96
 ---- batch: 060 ----
mean loss: 449.88
 ---- batch: 070 ----
mean loss: 459.18
 ---- batch: 080 ----
mean loss: 477.11
 ---- batch: 090 ----
mean loss: 458.12
 ---- batch: 100 ----
mean loss: 472.23
 ---- batch: 110 ----
mean loss: 467.80
train mean loss: 465.13
epoch train time: 0:00:02.163001
elapsed time: 0:05:03.324598
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 00:41:44.426005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.07
 ---- batch: 020 ----
mean loss: 462.85
 ---- batch: 030 ----
mean loss: 456.11
 ---- batch: 040 ----
mean loss: 455.11
 ---- batch: 050 ----
mean loss: 469.83
 ---- batch: 060 ----
mean loss: 460.20
 ---- batch: 070 ----
mean loss: 463.41
 ---- batch: 080 ----
mean loss: 475.09
 ---- batch: 090 ----
mean loss: 468.89
 ---- batch: 100 ----
mean loss: 474.62
 ---- batch: 110 ----
mean loss: 476.36
train mean loss: 466.61
epoch train time: 0:00:02.161562
elapsed time: 0:05:05.486373
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 00:41:46.587751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.94
 ---- batch: 020 ----
mean loss: 466.56
 ---- batch: 030 ----
mean loss: 460.86
 ---- batch: 040 ----
mean loss: 465.75
 ---- batch: 050 ----
mean loss: 470.61
 ---- batch: 060 ----
mean loss: 464.64
 ---- batch: 070 ----
mean loss: 465.48
 ---- batch: 080 ----
mean loss: 463.48
 ---- batch: 090 ----
mean loss: 460.43
 ---- batch: 100 ----
mean loss: 461.66
 ---- batch: 110 ----
mean loss: 458.23
train mean loss: 462.94
epoch train time: 0:00:02.159763
elapsed time: 0:05:07.646349
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 00:41:48.747747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.29
 ---- batch: 020 ----
mean loss: 461.24
 ---- batch: 030 ----
mean loss: 465.01
 ---- batch: 040 ----
mean loss: 457.36
 ---- batch: 050 ----
mean loss: 467.86
 ---- batch: 060 ----
mean loss: 458.16
 ---- batch: 070 ----
mean loss: 455.76
 ---- batch: 080 ----
mean loss: 458.57
 ---- batch: 090 ----
mean loss: 456.86
 ---- batch: 100 ----
mean loss: 483.21
 ---- batch: 110 ----
mean loss: 473.23
train mean loss: 463.51
epoch train time: 0:00:02.157729
elapsed time: 0:05:09.804309
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 00:41:50.905701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.47
 ---- batch: 020 ----
mean loss: 477.58
 ---- batch: 030 ----
mean loss: 458.03
 ---- batch: 040 ----
mean loss: 466.93
 ---- batch: 050 ----
mean loss: 478.53
 ---- batch: 060 ----
mean loss: 457.49
 ---- batch: 070 ----
mean loss: 443.37
 ---- batch: 080 ----
mean loss: 454.04
 ---- batch: 090 ----
mean loss: 455.55
 ---- batch: 100 ----
mean loss: 470.96
 ---- batch: 110 ----
mean loss: 463.34
train mean loss: 462.30
epoch train time: 0:00:02.160059
elapsed time: 0:05:11.964546
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 00:41:53.065935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.48
 ---- batch: 020 ----
mean loss: 466.62
 ---- batch: 030 ----
mean loss: 459.07
 ---- batch: 040 ----
mean loss: 465.81
 ---- batch: 050 ----
mean loss: 457.97
 ---- batch: 060 ----
mean loss: 471.16
 ---- batch: 070 ----
mean loss: 472.03
 ---- batch: 080 ----
mean loss: 463.92
 ---- batch: 090 ----
mean loss: 482.65
 ---- batch: 100 ----
mean loss: 462.02
 ---- batch: 110 ----
mean loss: 456.15
train mean loss: 462.90
epoch train time: 0:00:02.164359
elapsed time: 0:05:14.129087
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 00:41:55.230498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.30
 ---- batch: 020 ----
mean loss: 450.25
 ---- batch: 030 ----
mean loss: 466.57
 ---- batch: 040 ----
mean loss: 449.36
 ---- batch: 050 ----
mean loss: 460.16
 ---- batch: 060 ----
mean loss: 461.55
 ---- batch: 070 ----
mean loss: 454.41
 ---- batch: 080 ----
mean loss: 456.79
 ---- batch: 090 ----
mean loss: 471.82
 ---- batch: 100 ----
mean loss: 459.82
 ---- batch: 110 ----
mean loss: 454.01
train mean loss: 458.51
epoch train time: 0:00:02.157172
elapsed time: 0:05:16.286454
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 00:41:57.387862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.67
 ---- batch: 020 ----
mean loss: 464.86
 ---- batch: 030 ----
mean loss: 465.89
 ---- batch: 040 ----
mean loss: 433.86
 ---- batch: 050 ----
mean loss: 455.12
 ---- batch: 060 ----
mean loss: 459.90
 ---- batch: 070 ----
mean loss: 471.43
 ---- batch: 080 ----
mean loss: 452.25
 ---- batch: 090 ----
mean loss: 455.94
 ---- batch: 100 ----
mean loss: 453.32
 ---- batch: 110 ----
mean loss: 460.68
train mean loss: 458.18
epoch train time: 0:00:02.165499
elapsed time: 0:05:18.452159
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 00:41:59.553548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.96
 ---- batch: 020 ----
mean loss: 452.28
 ---- batch: 030 ----
mean loss: 452.52
 ---- batch: 040 ----
mean loss: 455.61
 ---- batch: 050 ----
mean loss: 440.45
 ---- batch: 060 ----
mean loss: 448.84
 ---- batch: 070 ----
mean loss: 454.14
 ---- batch: 080 ----
mean loss: 449.10
 ---- batch: 090 ----
mean loss: 451.79
 ---- batch: 100 ----
mean loss: 461.49
 ---- batch: 110 ----
mean loss: 460.21
train mean loss: 452.92
epoch train time: 0:00:02.168102
elapsed time: 0:05:20.620484
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 00:42:01.721877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.03
 ---- batch: 020 ----
mean loss: 454.06
 ---- batch: 030 ----
mean loss: 461.49
 ---- batch: 040 ----
mean loss: 447.12
 ---- batch: 050 ----
mean loss: 467.74
 ---- batch: 060 ----
mean loss: 468.61
 ---- batch: 070 ----
mean loss: 460.86
 ---- batch: 080 ----
mean loss: 467.30
 ---- batch: 090 ----
mean loss: 449.57
 ---- batch: 100 ----
mean loss: 452.60
 ---- batch: 110 ----
mean loss: 441.55
train mean loss: 456.11
epoch train time: 0:00:02.165377
elapsed time: 0:05:22.786041
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 00:42:03.887464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.22
 ---- batch: 020 ----
mean loss: 457.21
 ---- batch: 030 ----
mean loss: 450.29
 ---- batch: 040 ----
mean loss: 454.64
 ---- batch: 050 ----
mean loss: 451.21
 ---- batch: 060 ----
mean loss: 434.97
 ---- batch: 070 ----
mean loss: 429.29
 ---- batch: 080 ----
mean loss: 447.59
 ---- batch: 090 ----
mean loss: 456.53
 ---- batch: 100 ----
mean loss: 444.70
 ---- batch: 110 ----
mean loss: 446.33
train mean loss: 446.56
epoch train time: 0:00:02.166691
elapsed time: 0:05:24.952946
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 00:42:06.054336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.63
 ---- batch: 020 ----
mean loss: 445.45
 ---- batch: 030 ----
mean loss: 442.89
 ---- batch: 040 ----
mean loss: 443.81
 ---- batch: 050 ----
mean loss: 441.70
 ---- batch: 060 ----
mean loss: 448.79
 ---- batch: 070 ----
mean loss: 448.55
 ---- batch: 080 ----
mean loss: 452.36
 ---- batch: 090 ----
mean loss: 450.62
 ---- batch: 100 ----
mean loss: 433.49
 ---- batch: 110 ----
mean loss: 444.55
train mean loss: 445.64
epoch train time: 0:00:02.157646
elapsed time: 0:05:27.110797
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 00:42:08.212192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.92
 ---- batch: 020 ----
mean loss: 450.32
 ---- batch: 030 ----
mean loss: 436.78
 ---- batch: 040 ----
mean loss: 431.50
 ---- batch: 050 ----
mean loss: 431.01
 ---- batch: 060 ----
mean loss: 441.19
 ---- batch: 070 ----
mean loss: 425.85
 ---- batch: 080 ----
mean loss: 446.95
 ---- batch: 090 ----
mean loss: 415.34
 ---- batch: 100 ----
mean loss: 439.54
 ---- batch: 110 ----
mean loss: 445.89
train mean loss: 437.65
epoch train time: 0:00:02.171897
elapsed time: 0:05:29.282868
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 00:42:10.384278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.32
 ---- batch: 020 ----
mean loss: 450.31
 ---- batch: 030 ----
mean loss: 440.64
 ---- batch: 040 ----
mean loss: 431.27
 ---- batch: 050 ----
mean loss: 442.88
 ---- batch: 060 ----
mean loss: 425.65
 ---- batch: 070 ----
mean loss: 422.87
 ---- batch: 080 ----
mean loss: 442.84
 ---- batch: 090 ----
mean loss: 444.13
 ---- batch: 100 ----
mean loss: 439.75
 ---- batch: 110 ----
mean loss: 437.08
train mean loss: 438.17
epoch train time: 0:00:02.160733
elapsed time: 0:05:31.443872
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 00:42:12.545412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.55
 ---- batch: 020 ----
mean loss: 444.82
 ---- batch: 030 ----
mean loss: 429.30
 ---- batch: 040 ----
mean loss: 419.67
 ---- batch: 050 ----
mean loss: 442.67
 ---- batch: 060 ----
mean loss: 420.79
 ---- batch: 070 ----
mean loss: 424.87
 ---- batch: 080 ----
mean loss: 438.76
 ---- batch: 090 ----
mean loss: 433.21
 ---- batch: 100 ----
mean loss: 414.67
 ---- batch: 110 ----
mean loss: 439.03
train mean loss: 430.13
epoch train time: 0:00:02.159145
elapsed time: 0:05:33.603366
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 00:42:14.704750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.23
 ---- batch: 020 ----
mean loss: 439.61
 ---- batch: 030 ----
mean loss: 440.34
 ---- batch: 040 ----
mean loss: 425.67
 ---- batch: 050 ----
mean loss: 438.67
 ---- batch: 060 ----
mean loss: 422.05
 ---- batch: 070 ----
mean loss: 435.59
 ---- batch: 080 ----
mean loss: 439.82
 ---- batch: 090 ----
mean loss: 431.41
 ---- batch: 100 ----
mean loss: 431.10
 ---- batch: 110 ----
mean loss: 415.14
train mean loss: 429.45
epoch train time: 0:00:02.164074
elapsed time: 0:05:35.767604
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 00:42:16.868992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.74
 ---- batch: 020 ----
mean loss: 425.18
 ---- batch: 030 ----
mean loss: 421.59
 ---- batch: 040 ----
mean loss: 431.60
 ---- batch: 050 ----
mean loss: 420.67
 ---- batch: 060 ----
mean loss: 413.98
 ---- batch: 070 ----
mean loss: 425.39
 ---- batch: 080 ----
mean loss: 419.40
 ---- batch: 090 ----
mean loss: 423.22
 ---- batch: 100 ----
mean loss: 414.34
 ---- batch: 110 ----
mean loss: 419.15
train mean loss: 421.82
epoch train time: 0:00:02.159209
elapsed time: 0:05:37.926983
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 00:42:19.028371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.84
 ---- batch: 020 ----
mean loss: 427.13
 ---- batch: 030 ----
mean loss: 420.22
 ---- batch: 040 ----
mean loss: 416.99
 ---- batch: 050 ----
mean loss: 410.52
 ---- batch: 060 ----
mean loss: 432.57
 ---- batch: 070 ----
mean loss: 418.49
 ---- batch: 080 ----
mean loss: 426.65
 ---- batch: 090 ----
mean loss: 409.14
 ---- batch: 100 ----
mean loss: 421.83
 ---- batch: 110 ----
mean loss: 429.67
train mean loss: 421.80
epoch train time: 0:00:02.164744
elapsed time: 0:05:40.091911
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 00:42:21.193300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.14
 ---- batch: 020 ----
mean loss: 414.04
 ---- batch: 030 ----
mean loss: 416.07
 ---- batch: 040 ----
mean loss: 417.58
 ---- batch: 050 ----
mean loss: 412.73
 ---- batch: 060 ----
mean loss: 408.80
 ---- batch: 070 ----
mean loss: 413.02
 ---- batch: 080 ----
mean loss: 426.22
 ---- batch: 090 ----
mean loss: 411.70
 ---- batch: 100 ----
mean loss: 410.86
 ---- batch: 110 ----
mean loss: 414.21
train mean loss: 415.16
epoch train time: 0:00:02.160573
elapsed time: 0:05:42.252687
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 00:42:23.354111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.08
 ---- batch: 020 ----
mean loss: 410.96
 ---- batch: 030 ----
mean loss: 412.30
 ---- batch: 040 ----
mean loss: 410.07
 ---- batch: 050 ----
mean loss: 408.19
 ---- batch: 060 ----
mean loss: 406.10
 ---- batch: 070 ----
mean loss: 411.16
 ---- batch: 080 ----
mean loss: 415.81
 ---- batch: 090 ----
mean loss: 412.37
 ---- batch: 100 ----
mean loss: 409.71
 ---- batch: 110 ----
mean loss: 402.26
train mean loss: 408.83
epoch train time: 0:00:02.164315
elapsed time: 0:05:44.417210
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 00:42:25.518600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.81
 ---- batch: 020 ----
mean loss: 404.13
 ---- batch: 030 ----
mean loss: 411.64
 ---- batch: 040 ----
mean loss: 404.63
 ---- batch: 050 ----
mean loss: 416.47
 ---- batch: 060 ----
mean loss: 392.65
 ---- batch: 070 ----
mean loss: 392.20
 ---- batch: 080 ----
mean loss: 399.62
 ---- batch: 090 ----
mean loss: 394.50
 ---- batch: 100 ----
mean loss: 396.80
 ---- batch: 110 ----
mean loss: 398.29
train mean loss: 401.06
epoch train time: 0:00:02.160408
elapsed time: 0:05:46.577805
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 00:42:27.679196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.32
 ---- batch: 020 ----
mean loss: 408.21
 ---- batch: 030 ----
mean loss: 404.90
 ---- batch: 040 ----
mean loss: 395.03
 ---- batch: 050 ----
mean loss: 391.02
 ---- batch: 060 ----
mean loss: 386.96
 ---- batch: 070 ----
mean loss: 407.11
 ---- batch: 080 ----
mean loss: 404.56
 ---- batch: 090 ----
mean loss: 409.82
 ---- batch: 100 ----
mean loss: 402.87
 ---- batch: 110 ----
mean loss: 396.71
train mean loss: 403.64
epoch train time: 0:00:02.162098
elapsed time: 0:05:48.740083
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 00:42:29.841473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.28
 ---- batch: 020 ----
mean loss: 423.22
 ---- batch: 030 ----
mean loss: 405.58
 ---- batch: 040 ----
mean loss: 399.45
 ---- batch: 050 ----
mean loss: 403.33
 ---- batch: 060 ----
mean loss: 387.93
 ---- batch: 070 ----
mean loss: 397.37
 ---- batch: 080 ----
mean loss: 403.41
 ---- batch: 090 ----
mean loss: 407.94
 ---- batch: 100 ----
mean loss: 396.98
 ---- batch: 110 ----
mean loss: 398.87
train mean loss: 402.73
epoch train time: 0:00:02.165438
elapsed time: 0:05:50.905746
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 00:42:32.007137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.85
 ---- batch: 020 ----
mean loss: 403.11
 ---- batch: 030 ----
mean loss: 416.29
 ---- batch: 040 ----
mean loss: 393.02
 ---- batch: 050 ----
mean loss: 403.24
 ---- batch: 060 ----
mean loss: 398.82
 ---- batch: 070 ----
mean loss: 407.41
 ---- batch: 080 ----
mean loss: 414.40
 ---- batch: 090 ----
mean loss: 406.61
 ---- batch: 100 ----
mean loss: 398.71
 ---- batch: 110 ----
mean loss: 387.75
train mean loss: 402.47
epoch train time: 0:00:02.166352
elapsed time: 0:05:53.072332
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 00:42:34.173716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.91
 ---- batch: 020 ----
mean loss: 410.46
 ---- batch: 030 ----
mean loss: 380.57
 ---- batch: 040 ----
mean loss: 376.95
 ---- batch: 050 ----
mean loss: 392.73
 ---- batch: 060 ----
mean loss: 396.12
 ---- batch: 070 ----
mean loss: 400.95
 ---- batch: 080 ----
mean loss: 401.71
 ---- batch: 090 ----
mean loss: 395.35
 ---- batch: 100 ----
mean loss: 402.49
 ---- batch: 110 ----
mean loss: 396.54
train mean loss: 395.44
epoch train time: 0:00:02.166548
elapsed time: 0:05:55.239055
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 00:42:36.340455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.29
 ---- batch: 020 ----
mean loss: 397.00
 ---- batch: 030 ----
mean loss: 392.58
 ---- batch: 040 ----
mean loss: 395.00
 ---- batch: 050 ----
mean loss: 387.97
 ---- batch: 060 ----
mean loss: 403.32
 ---- batch: 070 ----
mean loss: 386.27
 ---- batch: 080 ----
mean loss: 396.21
 ---- batch: 090 ----
mean loss: 379.94
 ---- batch: 100 ----
mean loss: 388.23
 ---- batch: 110 ----
mean loss: 396.13
train mean loss: 392.39
epoch train time: 0:00:02.165954
elapsed time: 0:05:57.405200
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 00:42:38.506642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.33
 ---- batch: 020 ----
mean loss: 393.35
 ---- batch: 030 ----
mean loss: 404.55
 ---- batch: 040 ----
mean loss: 392.45
 ---- batch: 050 ----
mean loss: 390.25
 ---- batch: 060 ----
mean loss: 394.19
 ---- batch: 070 ----
mean loss: 392.66
 ---- batch: 080 ----
mean loss: 390.18
 ---- batch: 090 ----
mean loss: 394.35
 ---- batch: 100 ----
mean loss: 399.36
 ---- batch: 110 ----
mean loss: 378.80
train mean loss: 391.71
epoch train time: 0:00:02.161721
elapsed time: 0:05:59.567153
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 00:42:40.668540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.25
 ---- batch: 020 ----
mean loss: 390.79
 ---- batch: 030 ----
mean loss: 398.65
 ---- batch: 040 ----
mean loss: 392.10
 ---- batch: 050 ----
mean loss: 407.23
 ---- batch: 060 ----
mean loss: 382.27
 ---- batch: 070 ----
mean loss: 389.91
 ---- batch: 080 ----
mean loss: 388.24
 ---- batch: 090 ----
mean loss: 382.87
 ---- batch: 100 ----
mean loss: 375.60
 ---- batch: 110 ----
mean loss: 371.00
train mean loss: 389.34
epoch train time: 0:00:02.169974
elapsed time: 0:06:01.737301
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 00:42:42.838690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.21
 ---- batch: 020 ----
mean loss: 369.15
 ---- batch: 030 ----
mean loss: 382.67
 ---- batch: 040 ----
mean loss: 380.55
 ---- batch: 050 ----
mean loss: 396.08
 ---- batch: 060 ----
mean loss: 397.79
 ---- batch: 070 ----
mean loss: 385.43
 ---- batch: 080 ----
mean loss: 378.66
 ---- batch: 090 ----
mean loss: 391.79
 ---- batch: 100 ----
mean loss: 383.41
 ---- batch: 110 ----
mean loss: 379.44
train mean loss: 383.89
epoch train time: 0:00:02.164479
elapsed time: 0:06:03.901964
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 00:42:45.003352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.40
 ---- batch: 020 ----
mean loss: 377.88
 ---- batch: 030 ----
mean loss: 389.37
 ---- batch: 040 ----
mean loss: 372.51
 ---- batch: 050 ----
mean loss: 399.26
 ---- batch: 060 ----
mean loss: 379.22
 ---- batch: 070 ----
mean loss: 387.45
 ---- batch: 080 ----
mean loss: 378.98
 ---- batch: 090 ----
mean loss: 375.08
 ---- batch: 100 ----
mean loss: 372.41
 ---- batch: 110 ----
mean loss: 384.28
train mean loss: 381.02
epoch train time: 0:00:02.162527
elapsed time: 0:06:06.064689
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 00:42:47.166078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.69
 ---- batch: 020 ----
mean loss: 378.52
 ---- batch: 030 ----
mean loss: 377.91
 ---- batch: 040 ----
mean loss: 383.96
 ---- batch: 050 ----
mean loss: 379.76
 ---- batch: 060 ----
mean loss: 372.39
 ---- batch: 070 ----
mean loss: 391.42
 ---- batch: 080 ----
mean loss: 393.08
 ---- batch: 090 ----
mean loss: 385.10
 ---- batch: 100 ----
mean loss: 372.69
 ---- batch: 110 ----
mean loss: 394.16
train mean loss: 382.50
epoch train time: 0:00:02.167816
elapsed time: 0:06:08.232687
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 00:42:49.334072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.57
 ---- batch: 020 ----
mean loss: 379.96
 ---- batch: 030 ----
mean loss: 393.48
 ---- batch: 040 ----
mean loss: 389.65
 ---- batch: 050 ----
mean loss: 386.16
 ---- batch: 060 ----
mean loss: 374.46
 ---- batch: 070 ----
mean loss: 389.26
 ---- batch: 080 ----
mean loss: 391.83
 ---- batch: 090 ----
mean loss: 380.74
 ---- batch: 100 ----
mean loss: 377.06
 ---- batch: 110 ----
mean loss: 383.87
train mean loss: 385.03
epoch train time: 0:00:02.164929
elapsed time: 0:06:10.397802
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 00:42:51.499192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.60
 ---- batch: 020 ----
mean loss: 374.99
 ---- batch: 030 ----
mean loss: 383.03
 ---- batch: 040 ----
mean loss: 373.15
 ---- batch: 050 ----
mean loss: 369.79
 ---- batch: 060 ----
mean loss: 381.19
 ---- batch: 070 ----
mean loss: 377.73
 ---- batch: 080 ----
mean loss: 378.59
 ---- batch: 090 ----
mean loss: 379.19
 ---- batch: 100 ----
mean loss: 374.92
 ---- batch: 110 ----
mean loss: 380.57
train mean loss: 377.25
epoch train time: 0:00:02.170113
elapsed time: 0:06:12.568096
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 00:42:53.669509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.09
 ---- batch: 020 ----
mean loss: 370.46
 ---- batch: 030 ----
mean loss: 378.13
 ---- batch: 040 ----
mean loss: 379.62
 ---- batch: 050 ----
mean loss: 372.13
 ---- batch: 060 ----
mean loss: 381.56
 ---- batch: 070 ----
mean loss: 373.25
 ---- batch: 080 ----
mean loss: 376.32
 ---- batch: 090 ----
mean loss: 375.09
 ---- batch: 100 ----
mean loss: 384.75
 ---- batch: 110 ----
mean loss: 381.20
train mean loss: 378.71
epoch train time: 0:00:02.158984
elapsed time: 0:06:14.727341
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 00:42:55.828745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.56
 ---- batch: 020 ----
mean loss: 391.36
 ---- batch: 030 ----
mean loss: 371.46
 ---- batch: 040 ----
mean loss: 377.77
 ---- batch: 050 ----
mean loss: 369.25
 ---- batch: 060 ----
mean loss: 380.02
 ---- batch: 070 ----
mean loss: 369.23
 ---- batch: 080 ----
mean loss: 378.62
 ---- batch: 090 ----
mean loss: 373.45
 ---- batch: 100 ----
mean loss: 395.74
 ---- batch: 110 ----
mean loss: 381.69
train mean loss: 379.38
epoch train time: 0:00:02.162630
elapsed time: 0:06:16.890162
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 00:42:57.991552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.92
 ---- batch: 020 ----
mean loss: 370.64
 ---- batch: 030 ----
mean loss: 374.77
 ---- batch: 040 ----
mean loss: 372.45
 ---- batch: 050 ----
mean loss: 385.53
 ---- batch: 060 ----
mean loss: 370.83
 ---- batch: 070 ----
mean loss: 359.94
 ---- batch: 080 ----
mean loss: 386.86
 ---- batch: 090 ----
mean loss: 386.01
 ---- batch: 100 ----
mean loss: 363.37
 ---- batch: 110 ----
mean loss: 359.93
train mean loss: 372.47
epoch train time: 0:00:02.164031
elapsed time: 0:06:19.054394
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 00:43:00.155789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.32
 ---- batch: 020 ----
mean loss: 375.19
 ---- batch: 030 ----
mean loss: 375.94
 ---- batch: 040 ----
mean loss: 371.83
 ---- batch: 050 ----
mean loss: 373.99
 ---- batch: 060 ----
mean loss: 377.77
 ---- batch: 070 ----
mean loss: 372.07
 ---- batch: 080 ----
mean loss: 365.53
 ---- batch: 090 ----
mean loss: 384.81
 ---- batch: 100 ----
mean loss: 381.49
 ---- batch: 110 ----
mean loss: 364.36
train mean loss: 372.87
epoch train time: 0:00:02.170684
elapsed time: 0:06:21.225265
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 00:43:02.326653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.62
 ---- batch: 020 ----
mean loss: 369.93
 ---- batch: 030 ----
mean loss: 376.69
 ---- batch: 040 ----
mean loss: 356.58
 ---- batch: 050 ----
mean loss: 380.69
 ---- batch: 060 ----
mean loss: 365.66
 ---- batch: 070 ----
mean loss: 363.83
 ---- batch: 080 ----
mean loss: 353.92
 ---- batch: 090 ----
mean loss: 368.78
 ---- batch: 100 ----
mean loss: 373.15
 ---- batch: 110 ----
mean loss: 370.11
train mean loss: 368.29
epoch train time: 0:00:02.176146
elapsed time: 0:06:23.401627
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 00:43:04.503031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.10
 ---- batch: 020 ----
mean loss: 373.55
 ---- batch: 030 ----
mean loss: 369.26
 ---- batch: 040 ----
mean loss: 373.85
 ---- batch: 050 ----
mean loss: 374.15
 ---- batch: 060 ----
mean loss: 360.21
 ---- batch: 070 ----
mean loss: 373.10
 ---- batch: 080 ----
mean loss: 378.46
 ---- batch: 090 ----
mean loss: 364.73
 ---- batch: 100 ----
mean loss: 371.52
 ---- batch: 110 ----
mean loss: 368.16
train mean loss: 370.09
epoch train time: 0:00:02.163986
elapsed time: 0:06:25.565863
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 00:43:06.667277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.57
 ---- batch: 020 ----
mean loss: 369.21
 ---- batch: 030 ----
mean loss: 369.07
 ---- batch: 040 ----
mean loss: 371.86
 ---- batch: 050 ----
mean loss: 371.15
 ---- batch: 060 ----
mean loss: 375.94
 ---- batch: 070 ----
mean loss: 379.11
 ---- batch: 080 ----
mean loss: 349.12
 ---- batch: 090 ----
mean loss: 356.21
 ---- batch: 100 ----
mean loss: 373.70
 ---- batch: 110 ----
mean loss: 374.28
train mean loss: 367.73
epoch train time: 0:00:02.161367
elapsed time: 0:06:27.727428
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 00:43:08.828819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.23
 ---- batch: 020 ----
mean loss: 355.81
 ---- batch: 030 ----
mean loss: 372.45
 ---- batch: 040 ----
mean loss: 358.60
 ---- batch: 050 ----
mean loss: 381.77
 ---- batch: 060 ----
mean loss: 370.35
 ---- batch: 070 ----
mean loss: 373.78
 ---- batch: 080 ----
mean loss: 362.86
 ---- batch: 090 ----
mean loss: 364.33
 ---- batch: 100 ----
mean loss: 355.85
 ---- batch: 110 ----
mean loss: 373.16
train mean loss: 366.76
epoch train time: 0:00:02.165169
elapsed time: 0:06:29.892784
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 00:43:10.994170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.06
 ---- batch: 020 ----
mean loss: 365.51
 ---- batch: 030 ----
mean loss: 367.17
 ---- batch: 040 ----
mean loss: 363.50
 ---- batch: 050 ----
mean loss: 344.60
 ---- batch: 060 ----
mean loss: 375.69
 ---- batch: 070 ----
mean loss: 349.43
 ---- batch: 080 ----
mean loss: 377.64
 ---- batch: 090 ----
mean loss: 364.54
 ---- batch: 100 ----
mean loss: 370.77
 ---- batch: 110 ----
mean loss: 367.78
train mean loss: 366.12
epoch train time: 0:00:02.170871
elapsed time: 0:06:32.063832
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 00:43:13.165220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.05
 ---- batch: 020 ----
mean loss: 363.94
 ---- batch: 030 ----
mean loss: 356.40
 ---- batch: 040 ----
mean loss: 361.97
 ---- batch: 050 ----
mean loss: 361.30
 ---- batch: 060 ----
mean loss: 364.45
 ---- batch: 070 ----
mean loss: 377.18
 ---- batch: 080 ----
mean loss: 365.05
 ---- batch: 090 ----
mean loss: 367.10
 ---- batch: 100 ----
mean loss: 366.11
 ---- batch: 110 ----
mean loss: 362.98
train mean loss: 364.88
epoch train time: 0:00:02.168012
elapsed time: 0:06:34.232031
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 00:43:15.333431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.64
 ---- batch: 020 ----
mean loss: 367.15
 ---- batch: 030 ----
mean loss: 354.17
 ---- batch: 040 ----
mean loss: 379.93
 ---- batch: 050 ----
mean loss: 378.23
 ---- batch: 060 ----
mean loss: 357.23
 ---- batch: 070 ----
mean loss: 350.33
 ---- batch: 080 ----
mean loss: 362.62
 ---- batch: 090 ----
mean loss: 351.24
 ---- batch: 100 ----
mean loss: 359.22
 ---- batch: 110 ----
mean loss: 361.64
train mean loss: 362.12
epoch train time: 0:00:02.163254
elapsed time: 0:06:36.395471
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 00:43:17.496860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.52
 ---- batch: 020 ----
mean loss: 357.86
 ---- batch: 030 ----
mean loss: 348.32
 ---- batch: 040 ----
mean loss: 372.01
 ---- batch: 050 ----
mean loss: 356.71
 ---- batch: 060 ----
mean loss: 364.43
 ---- batch: 070 ----
mean loss: 368.12
 ---- batch: 080 ----
mean loss: 362.99
 ---- batch: 090 ----
mean loss: 363.71
 ---- batch: 100 ----
mean loss: 372.22
 ---- batch: 110 ----
mean loss: 356.18
train mean loss: 361.55
epoch train time: 0:00:02.167485
elapsed time: 0:06:38.563142
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 00:43:19.664536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.44
 ---- batch: 020 ----
mean loss: 364.01
 ---- batch: 030 ----
mean loss: 363.12
 ---- batch: 040 ----
mean loss: 341.58
 ---- batch: 050 ----
mean loss: 349.00
 ---- batch: 060 ----
mean loss: 368.26
 ---- batch: 070 ----
mean loss: 354.27
 ---- batch: 080 ----
mean loss: 364.66
 ---- batch: 090 ----
mean loss: 369.01
 ---- batch: 100 ----
mean loss: 361.64
 ---- batch: 110 ----
mean loss: 352.56
train mean loss: 360.14
epoch train time: 0:00:02.170816
elapsed time: 0:06:40.734141
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 00:43:21.835556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.47
 ---- batch: 020 ----
mean loss: 373.22
 ---- batch: 030 ----
mean loss: 364.29
 ---- batch: 040 ----
mean loss: 354.71
 ---- batch: 050 ----
mean loss: 357.37
 ---- batch: 060 ----
mean loss: 358.53
 ---- batch: 070 ----
mean loss: 365.97
 ---- batch: 080 ----
mean loss: 358.98
 ---- batch: 090 ----
mean loss: 360.56
 ---- batch: 100 ----
mean loss: 351.94
 ---- batch: 110 ----
mean loss: 347.21
train mean loss: 359.40
epoch train time: 0:00:02.171727
elapsed time: 0:06:42.906060
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 00:43:24.007468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.87
 ---- batch: 020 ----
mean loss: 356.03
 ---- batch: 030 ----
mean loss: 355.94
 ---- batch: 040 ----
mean loss: 352.94
 ---- batch: 050 ----
mean loss: 351.47
 ---- batch: 060 ----
mean loss: 350.30
 ---- batch: 070 ----
mean loss: 352.06
 ---- batch: 080 ----
mean loss: 365.09
 ---- batch: 090 ----
mean loss: 366.68
 ---- batch: 100 ----
mean loss: 358.81
 ---- batch: 110 ----
mean loss: 354.21
train mean loss: 356.87
epoch train time: 0:00:02.167906
elapsed time: 0:06:45.074153
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 00:43:26.175543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.23
 ---- batch: 020 ----
mean loss: 357.07
 ---- batch: 030 ----
mean loss: 370.77
 ---- batch: 040 ----
mean loss: 352.77
 ---- batch: 050 ----
mean loss: 346.85
 ---- batch: 060 ----
mean loss: 357.39
 ---- batch: 070 ----
mean loss: 352.60
 ---- batch: 080 ----
mean loss: 351.80
 ---- batch: 090 ----
mean loss: 366.09
 ---- batch: 100 ----
mean loss: 355.82
 ---- batch: 110 ----
mean loss: 357.48
train mean loss: 356.39
epoch train time: 0:00:02.165011
elapsed time: 0:06:47.239355
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 00:43:28.340732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.66
 ---- batch: 020 ----
mean loss: 358.47
 ---- batch: 030 ----
mean loss: 374.12
 ---- batch: 040 ----
mean loss: 350.02
 ---- batch: 050 ----
mean loss: 353.50
 ---- batch: 060 ----
mean loss: 353.33
 ---- batch: 070 ----
mean loss: 354.36
 ---- batch: 080 ----
mean loss: 351.75
 ---- batch: 090 ----
mean loss: 362.18
 ---- batch: 100 ----
mean loss: 362.81
 ---- batch: 110 ----
mean loss: 356.99
train mean loss: 358.12
epoch train time: 0:00:02.148325
elapsed time: 0:06:49.387817
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 00:43:30.489200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.41
 ---- batch: 020 ----
mean loss: 359.24
 ---- batch: 030 ----
mean loss: 356.19
 ---- batch: 040 ----
mean loss: 352.41
 ---- batch: 050 ----
mean loss: 360.96
 ---- batch: 060 ----
mean loss: 358.31
 ---- batch: 070 ----
mean loss: 363.87
 ---- batch: 080 ----
mean loss: 352.23
 ---- batch: 090 ----
mean loss: 360.30
 ---- batch: 100 ----
mean loss: 363.80
 ---- batch: 110 ----
mean loss: 349.23
train mean loss: 357.89
epoch train time: 0:00:02.140959
elapsed time: 0:06:51.528929
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 00:43:32.630313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.66
 ---- batch: 020 ----
mean loss: 361.06
 ---- batch: 030 ----
mean loss: 339.81
 ---- batch: 040 ----
mean loss: 363.40
 ---- batch: 050 ----
mean loss: 349.35
 ---- batch: 060 ----
mean loss: 359.84
 ---- batch: 070 ----
mean loss: 357.80
 ---- batch: 080 ----
mean loss: 345.72
 ---- batch: 090 ----
mean loss: 352.26
 ---- batch: 100 ----
mean loss: 354.39
 ---- batch: 110 ----
mean loss: 349.83
train mean loss: 353.81
epoch train time: 0:00:02.148006
elapsed time: 0:06:53.677092
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 00:43:34.778500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.45
 ---- batch: 020 ----
mean loss: 366.40
 ---- batch: 030 ----
mean loss: 347.90
 ---- batch: 040 ----
mean loss: 344.15
 ---- batch: 050 ----
mean loss: 354.51
 ---- batch: 060 ----
mean loss: 353.30
 ---- batch: 070 ----
mean loss: 345.06
 ---- batch: 080 ----
mean loss: 351.30
 ---- batch: 090 ----
mean loss: 360.55
 ---- batch: 100 ----
mean loss: 347.42
 ---- batch: 110 ----
mean loss: 347.95
train mean loss: 352.79
epoch train time: 0:00:02.149766
elapsed time: 0:06:55.827039
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 00:43:36.928427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.59
 ---- batch: 020 ----
mean loss: 362.63
 ---- batch: 030 ----
mean loss: 349.70
 ---- batch: 040 ----
mean loss: 342.98
 ---- batch: 050 ----
mean loss: 346.03
 ---- batch: 060 ----
mean loss: 353.26
 ---- batch: 070 ----
mean loss: 358.79
 ---- batch: 080 ----
mean loss: 350.46
 ---- batch: 090 ----
mean loss: 364.59
 ---- batch: 100 ----
mean loss: 348.35
 ---- batch: 110 ----
mean loss: 352.39
train mean loss: 353.84
epoch train time: 0:00:02.144171
elapsed time: 0:06:57.971380
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 00:43:39.072771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.47
 ---- batch: 020 ----
mean loss: 339.14
 ---- batch: 030 ----
mean loss: 342.87
 ---- batch: 040 ----
mean loss: 359.07
 ---- batch: 050 ----
mean loss: 353.18
 ---- batch: 060 ----
mean loss: 336.64
 ---- batch: 070 ----
mean loss: 357.53
 ---- batch: 080 ----
mean loss: 351.45
 ---- batch: 090 ----
mean loss: 348.72
 ---- batch: 100 ----
mean loss: 350.77
 ---- batch: 110 ----
mean loss: 365.45
train mean loss: 349.72
epoch train time: 0:00:02.140619
elapsed time: 0:07:00.112203
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 00:43:41.213624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.19
 ---- batch: 020 ----
mean loss: 358.52
 ---- batch: 030 ----
mean loss: 349.47
 ---- batch: 040 ----
mean loss: 341.95
 ---- batch: 050 ----
mean loss: 351.48
 ---- batch: 060 ----
mean loss: 357.81
 ---- batch: 070 ----
mean loss: 350.88
 ---- batch: 080 ----
mean loss: 351.79
 ---- batch: 090 ----
mean loss: 342.50
 ---- batch: 100 ----
mean loss: 344.35
 ---- batch: 110 ----
mean loss: 342.91
train mean loss: 349.06
epoch train time: 0:00:02.137714
elapsed time: 0:07:02.250110
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 00:43:43.351520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.37
 ---- batch: 020 ----
mean loss: 352.40
 ---- batch: 030 ----
mean loss: 353.67
 ---- batch: 040 ----
mean loss: 343.15
 ---- batch: 050 ----
mean loss: 354.37
 ---- batch: 060 ----
mean loss: 341.30
 ---- batch: 070 ----
mean loss: 340.20
 ---- batch: 080 ----
mean loss: 351.46
 ---- batch: 090 ----
mean loss: 341.00
 ---- batch: 100 ----
mean loss: 359.95
 ---- batch: 110 ----
mean loss: 344.06
train mean loss: 348.50
epoch train time: 0:00:02.137044
elapsed time: 0:07:04.387335
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 00:43:45.488721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.61
 ---- batch: 020 ----
mean loss: 357.41
 ---- batch: 030 ----
mean loss: 344.28
 ---- batch: 040 ----
mean loss: 349.66
 ---- batch: 050 ----
mean loss: 337.45
 ---- batch: 060 ----
mean loss: 349.96
 ---- batch: 070 ----
mean loss: 359.09
 ---- batch: 080 ----
mean loss: 352.49
 ---- batch: 090 ----
mean loss: 347.76
 ---- batch: 100 ----
mean loss: 340.00
 ---- batch: 110 ----
mean loss: 340.03
train mean loss: 348.15
epoch train time: 0:00:02.136954
elapsed time: 0:07:06.524488
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 00:43:47.625872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.04
 ---- batch: 020 ----
mean loss: 350.25
 ---- batch: 030 ----
mean loss: 338.27
 ---- batch: 040 ----
mean loss: 352.27
 ---- batch: 050 ----
mean loss: 348.07
 ---- batch: 060 ----
mean loss: 345.87
 ---- batch: 070 ----
mean loss: 340.99
 ---- batch: 080 ----
mean loss: 339.91
 ---- batch: 090 ----
mean loss: 355.36
 ---- batch: 100 ----
mean loss: 345.53
 ---- batch: 110 ----
mean loss: 343.92
train mean loss: 346.71
epoch train time: 0:00:02.145553
elapsed time: 0:07:08.670196
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 00:43:49.771601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.88
 ---- batch: 020 ----
mean loss: 351.12
 ---- batch: 030 ----
mean loss: 349.46
 ---- batch: 040 ----
mean loss: 342.64
 ---- batch: 050 ----
mean loss: 350.29
 ---- batch: 060 ----
mean loss: 353.16
 ---- batch: 070 ----
mean loss: 351.65
 ---- batch: 080 ----
mean loss: 348.02
 ---- batch: 090 ----
mean loss: 337.48
 ---- batch: 100 ----
mean loss: 352.53
 ---- batch: 110 ----
mean loss: 354.13
train mean loss: 348.18
epoch train time: 0:00:02.134279
elapsed time: 0:07:10.804653
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 00:43:51.906040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.75
 ---- batch: 020 ----
mean loss: 344.71
 ---- batch: 030 ----
mean loss: 350.59
 ---- batch: 040 ----
mean loss: 346.12
 ---- batch: 050 ----
mean loss: 337.31
 ---- batch: 060 ----
mean loss: 346.73
 ---- batch: 070 ----
mean loss: 353.94
 ---- batch: 080 ----
mean loss: 343.26
 ---- batch: 090 ----
mean loss: 350.85
 ---- batch: 100 ----
mean loss: 350.51
 ---- batch: 110 ----
mean loss: 339.51
train mean loss: 345.40
epoch train time: 0:00:02.130327
elapsed time: 0:07:12.935154
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 00:43:54.036562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.19
 ---- batch: 020 ----
mean loss: 352.56
 ---- batch: 030 ----
mean loss: 351.03
 ---- batch: 040 ----
mean loss: 336.14
 ---- batch: 050 ----
mean loss: 346.01
 ---- batch: 060 ----
mean loss: 353.04
 ---- batch: 070 ----
mean loss: 346.05
 ---- batch: 080 ----
mean loss: 346.25
 ---- batch: 090 ----
mean loss: 333.29
 ---- batch: 100 ----
mean loss: 341.31
 ---- batch: 110 ----
mean loss: 340.57
train mean loss: 345.38
epoch train time: 0:00:02.134111
elapsed time: 0:07:15.069439
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 00:43:56.170836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.09
 ---- batch: 020 ----
mean loss: 346.71
 ---- batch: 030 ----
mean loss: 346.18
 ---- batch: 040 ----
mean loss: 338.08
 ---- batch: 050 ----
mean loss: 356.49
 ---- batch: 060 ----
mean loss: 350.35
 ---- batch: 070 ----
mean loss: 347.14
 ---- batch: 080 ----
mean loss: 338.78
 ---- batch: 090 ----
mean loss: 356.80
 ---- batch: 100 ----
mean loss: 343.02
 ---- batch: 110 ----
mean loss: 342.41
train mean loss: 345.67
epoch train time: 0:00:02.137490
elapsed time: 0:07:17.207104
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 00:43:58.308492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.80
 ---- batch: 020 ----
mean loss: 344.23
 ---- batch: 030 ----
mean loss: 351.41
 ---- batch: 040 ----
mean loss: 332.42
 ---- batch: 050 ----
mean loss: 345.89
 ---- batch: 060 ----
mean loss: 341.17
 ---- batch: 070 ----
mean loss: 339.70
 ---- batch: 080 ----
mean loss: 343.15
 ---- batch: 090 ----
mean loss: 348.20
 ---- batch: 100 ----
mean loss: 336.17
 ---- batch: 110 ----
mean loss: 337.91
train mean loss: 342.67
epoch train time: 0:00:02.135330
elapsed time: 0:07:19.342601
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 00:44:00.443988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.83
 ---- batch: 020 ----
mean loss: 343.24
 ---- batch: 030 ----
mean loss: 359.81
 ---- batch: 040 ----
mean loss: 338.69
 ---- batch: 050 ----
mean loss: 340.38
 ---- batch: 060 ----
mean loss: 343.87
 ---- batch: 070 ----
mean loss: 328.45
 ---- batch: 080 ----
mean loss: 341.41
 ---- batch: 090 ----
mean loss: 335.22
 ---- batch: 100 ----
mean loss: 337.60
 ---- batch: 110 ----
mean loss: 340.68
train mean loss: 341.15
epoch train time: 0:00:02.141244
elapsed time: 0:07:21.484017
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 00:44:02.585421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.00
 ---- batch: 020 ----
mean loss: 334.80
 ---- batch: 030 ----
mean loss: 349.90
 ---- batch: 040 ----
mean loss: 335.85
 ---- batch: 050 ----
mean loss: 336.97
 ---- batch: 060 ----
mean loss: 330.72
 ---- batch: 070 ----
mean loss: 346.51
 ---- batch: 080 ----
mean loss: 335.64
 ---- batch: 090 ----
mean loss: 344.27
 ---- batch: 100 ----
mean loss: 339.20
 ---- batch: 110 ----
mean loss: 356.47
train mean loss: 339.42
epoch train time: 0:00:02.156520
elapsed time: 0:07:23.640770
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 00:44:04.742166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.27
 ---- batch: 020 ----
mean loss: 327.28
 ---- batch: 030 ----
mean loss: 331.04
 ---- batch: 040 ----
mean loss: 345.73
 ---- batch: 050 ----
mean loss: 337.53
 ---- batch: 060 ----
mean loss: 345.41
 ---- batch: 070 ----
mean loss: 339.00
 ---- batch: 080 ----
mean loss: 342.27
 ---- batch: 090 ----
mean loss: 345.21
 ---- batch: 100 ----
mean loss: 351.56
 ---- batch: 110 ----
mean loss: 338.83
train mean loss: 340.17
epoch train time: 0:00:02.170316
elapsed time: 0:07:25.811269
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 00:44:06.912656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.41
 ---- batch: 020 ----
mean loss: 329.77
 ---- batch: 030 ----
mean loss: 337.38
 ---- batch: 040 ----
mean loss: 330.26
 ---- batch: 050 ----
mean loss: 341.93
 ---- batch: 060 ----
mean loss: 360.76
 ---- batch: 070 ----
mean loss: 326.39
 ---- batch: 080 ----
mean loss: 337.94
 ---- batch: 090 ----
mean loss: 325.27
 ---- batch: 100 ----
mean loss: 334.65
 ---- batch: 110 ----
mean loss: 341.54
train mean loss: 337.27
epoch train time: 0:00:02.166073
elapsed time: 0:07:27.977503
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 00:44:09.078890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.75
 ---- batch: 020 ----
mean loss: 334.52
 ---- batch: 030 ----
mean loss: 332.84
 ---- batch: 040 ----
mean loss: 339.94
 ---- batch: 050 ----
mean loss: 334.71
 ---- batch: 060 ----
mean loss: 321.61
 ---- batch: 070 ----
mean loss: 339.24
 ---- batch: 080 ----
mean loss: 329.70
 ---- batch: 090 ----
mean loss: 331.10
 ---- batch: 100 ----
mean loss: 334.34
 ---- batch: 110 ----
mean loss: 343.52
train mean loss: 334.30
epoch train time: 0:00:02.175739
elapsed time: 0:07:30.153410
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 00:44:11.254801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.89
 ---- batch: 020 ----
mean loss: 342.31
 ---- batch: 030 ----
mean loss: 338.24
 ---- batch: 040 ----
mean loss: 328.83
 ---- batch: 050 ----
mean loss: 319.22
 ---- batch: 060 ----
mean loss: 334.02
 ---- batch: 070 ----
mean loss: 343.63
 ---- batch: 080 ----
mean loss: 329.85
 ---- batch: 090 ----
mean loss: 324.16
 ---- batch: 100 ----
mean loss: 337.31
 ---- batch: 110 ----
mean loss: 325.06
train mean loss: 332.37
epoch train time: 0:00:02.178258
elapsed time: 0:07:32.331835
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 00:44:13.433221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.01
 ---- batch: 020 ----
mean loss: 338.49
 ---- batch: 030 ----
mean loss: 344.26
 ---- batch: 040 ----
mean loss: 340.00
 ---- batch: 050 ----
mean loss: 332.68
 ---- batch: 060 ----
mean loss: 337.75
 ---- batch: 070 ----
mean loss: 329.53
 ---- batch: 080 ----
mean loss: 355.75
 ---- batch: 090 ----
mean loss: 334.12
 ---- batch: 100 ----
mean loss: 324.69
 ---- batch: 110 ----
mean loss: 333.28
train mean loss: 335.45
epoch train time: 0:00:02.168607
elapsed time: 0:07:34.500640
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 00:44:15.602029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.04
 ---- batch: 020 ----
mean loss: 338.86
 ---- batch: 030 ----
mean loss: 340.54
 ---- batch: 040 ----
mean loss: 355.18
 ---- batch: 050 ----
mean loss: 328.81
 ---- batch: 060 ----
mean loss: 324.63
 ---- batch: 070 ----
mean loss: 335.92
 ---- batch: 080 ----
mean loss: 325.17
 ---- batch: 090 ----
mean loss: 321.35
 ---- batch: 100 ----
mean loss: 332.33
 ---- batch: 110 ----
mean loss: 329.82
train mean loss: 332.91
epoch train time: 0:00:02.166405
elapsed time: 0:07:36.667219
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 00:44:17.768630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.19
 ---- batch: 020 ----
mean loss: 335.65
 ---- batch: 030 ----
mean loss: 338.69
 ---- batch: 040 ----
mean loss: 325.80
 ---- batch: 050 ----
mean loss: 342.41
 ---- batch: 060 ----
mean loss: 324.70
 ---- batch: 070 ----
mean loss: 342.59
 ---- batch: 080 ----
mean loss: 340.69
 ---- batch: 090 ----
mean loss: 325.57
 ---- batch: 100 ----
mean loss: 326.43
 ---- batch: 110 ----
mean loss: 316.60
train mean loss: 331.78
epoch train time: 0:00:02.165605
elapsed time: 0:07:38.833016
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 00:44:19.934688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.36
 ---- batch: 020 ----
mean loss: 335.60
 ---- batch: 030 ----
mean loss: 336.48
 ---- batch: 040 ----
mean loss: 338.17
 ---- batch: 050 ----
mean loss: 332.54
 ---- batch: 060 ----
mean loss: 343.70
 ---- batch: 070 ----
mean loss: 317.84
 ---- batch: 080 ----
mean loss: 325.40
 ---- batch: 090 ----
mean loss: 318.10
 ---- batch: 100 ----
mean loss: 336.24
 ---- batch: 110 ----
mean loss: 321.47
train mean loss: 331.00
epoch train time: 0:00:02.158561
elapsed time: 0:07:40.992029
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 00:44:22.093416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.26
 ---- batch: 020 ----
mean loss: 317.41
 ---- batch: 030 ----
mean loss: 327.57
 ---- batch: 040 ----
mean loss: 310.24
 ---- batch: 050 ----
mean loss: 328.21
 ---- batch: 060 ----
mean loss: 336.12
 ---- batch: 070 ----
mean loss: 332.31
 ---- batch: 080 ----
mean loss: 333.39
 ---- batch: 090 ----
mean loss: 322.22
 ---- batch: 100 ----
mean loss: 330.36
 ---- batch: 110 ----
mean loss: 332.96
train mean loss: 327.29
epoch train time: 0:00:02.169664
elapsed time: 0:07:43.161885
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 00:44:24.263272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.88
 ---- batch: 020 ----
mean loss: 319.66
 ---- batch: 030 ----
mean loss: 323.66
 ---- batch: 040 ----
mean loss: 330.39
 ---- batch: 050 ----
mean loss: 337.04
 ---- batch: 060 ----
mean loss: 338.75
 ---- batch: 070 ----
mean loss: 327.19
 ---- batch: 080 ----
mean loss: 317.98
 ---- batch: 090 ----
mean loss: 320.27
 ---- batch: 100 ----
mean loss: 330.10
 ---- batch: 110 ----
mean loss: 333.47
train mean loss: 327.62
epoch train time: 0:00:02.162669
elapsed time: 0:07:45.324774
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 00:44:26.426164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 324.37
 ---- batch: 020 ----
mean loss: 324.04
 ---- batch: 030 ----
mean loss: 325.11
 ---- batch: 040 ----
mean loss: 316.21
 ---- batch: 050 ----
mean loss: 314.34
 ---- batch: 060 ----
mean loss: 324.76
 ---- batch: 070 ----
mean loss: 306.82
 ---- batch: 080 ----
mean loss: 322.09
 ---- batch: 090 ----
mean loss: 326.29
 ---- batch: 100 ----
mean loss: 333.93
 ---- batch: 110 ----
mean loss: 318.50
train mean loss: 320.90
epoch train time: 0:00:02.166200
elapsed time: 0:07:47.491177
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 00:44:28.592551
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 327.43
 ---- batch: 020 ----
mean loss: 313.51
 ---- batch: 030 ----
mean loss: 315.89
 ---- batch: 040 ----
mean loss: 329.49
 ---- batch: 050 ----
mean loss: 319.41
 ---- batch: 060 ----
mean loss: 326.25
 ---- batch: 070 ----
mean loss: 321.19
 ---- batch: 080 ----
mean loss: 329.21
 ---- batch: 090 ----
mean loss: 329.14
 ---- batch: 100 ----
mean loss: 315.95
 ---- batch: 110 ----
mean loss: 314.20
train mean loss: 321.92
epoch train time: 0:00:02.167219
elapsed time: 0:07:49.658577
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 00:44:30.759968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.83
 ---- batch: 020 ----
mean loss: 312.39
 ---- batch: 030 ----
mean loss: 322.67
 ---- batch: 040 ----
mean loss: 316.17
 ---- batch: 050 ----
mean loss: 322.84
 ---- batch: 060 ----
mean loss: 329.94
 ---- batch: 070 ----
mean loss: 312.94
 ---- batch: 080 ----
mean loss: 328.14
 ---- batch: 090 ----
mean loss: 315.84
 ---- batch: 100 ----
mean loss: 324.17
 ---- batch: 110 ----
mean loss: 320.88
train mean loss: 319.88
epoch train time: 0:00:02.160456
elapsed time: 0:07:51.819249
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 00:44:32.920662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 327.31
 ---- batch: 020 ----
mean loss: 316.82
 ---- batch: 030 ----
mean loss: 321.03
 ---- batch: 040 ----
mean loss: 321.49
 ---- batch: 050 ----
mean loss: 313.53
 ---- batch: 060 ----
mean loss: 317.52
 ---- batch: 070 ----
mean loss: 323.00
 ---- batch: 080 ----
mean loss: 313.91
 ---- batch: 090 ----
mean loss: 321.60
 ---- batch: 100 ----
mean loss: 319.69
 ---- batch: 110 ----
mean loss: 315.17
train mean loss: 318.80
epoch train time: 0:00:02.165300
elapsed time: 0:07:53.984746
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 00:44:35.086135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.51
 ---- batch: 020 ----
mean loss: 320.83
 ---- batch: 030 ----
mean loss: 323.27
 ---- batch: 040 ----
mean loss: 316.16
 ---- batch: 050 ----
mean loss: 327.85
 ---- batch: 060 ----
mean loss: 319.76
 ---- batch: 070 ----
mean loss: 328.85
 ---- batch: 080 ----
mean loss: 316.13
 ---- batch: 090 ----
mean loss: 324.12
 ---- batch: 100 ----
mean loss: 306.51
 ---- batch: 110 ----
mean loss: 325.31
train mean loss: 320.90
epoch train time: 0:00:02.164605
elapsed time: 0:07:56.149517
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 00:44:37.250904
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 314.91
 ---- batch: 020 ----
mean loss: 306.53
 ---- batch: 030 ----
mean loss: 318.77
 ---- batch: 040 ----
mean loss: 323.54
 ---- batch: 050 ----
mean loss: 313.58
 ---- batch: 060 ----
mean loss: 319.85
 ---- batch: 070 ----
mean loss: 324.94
 ---- batch: 080 ----
mean loss: 318.99
 ---- batch: 090 ----
mean loss: 316.71
 ---- batch: 100 ----
mean loss: 310.20
 ---- batch: 110 ----
mean loss: 322.37
train mean loss: 317.46
epoch train time: 0:00:02.161718
elapsed time: 0:07:58.311403
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 00:44:39.412790
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 326.34
 ---- batch: 020 ----
mean loss: 324.81
 ---- batch: 030 ----
mean loss: 316.45
 ---- batch: 040 ----
mean loss: 327.07
 ---- batch: 050 ----
mean loss: 315.39
 ---- batch: 060 ----
mean loss: 319.88
 ---- batch: 070 ----
mean loss: 312.36
 ---- batch: 080 ----
mean loss: 334.34
 ---- batch: 090 ----
mean loss: 319.51
 ---- batch: 100 ----
mean loss: 342.50
 ---- batch: 110 ----
mean loss: 312.40
train mean loss: 322.98
epoch train time: 0:00:02.162576
elapsed time: 0:08:00.474166
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 00:44:41.575558
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 311.32
 ---- batch: 020 ----
mean loss: 316.27
 ---- batch: 030 ----
mean loss: 315.44
 ---- batch: 040 ----
mean loss: 323.26
 ---- batch: 050 ----
mean loss: 317.44
 ---- batch: 060 ----
mean loss: 328.89
 ---- batch: 070 ----
mean loss: 313.70
 ---- batch: 080 ----
mean loss: 320.24
 ---- batch: 090 ----
mean loss: 303.50
 ---- batch: 100 ----
mean loss: 311.47
 ---- batch: 110 ----
mean loss: 316.21
train mean loss: 315.84
epoch train time: 0:00:02.175417
elapsed time: 0:08:02.649764
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 00:44:43.751149
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 324.11
 ---- batch: 020 ----
mean loss: 298.35
 ---- batch: 030 ----
mean loss: 322.81
 ---- batch: 040 ----
mean loss: 306.40
 ---- batch: 050 ----
mean loss: 310.49
 ---- batch: 060 ----
mean loss: 322.65
 ---- batch: 070 ----
mean loss: 319.95
 ---- batch: 080 ----
mean loss: 304.95
 ---- batch: 090 ----
mean loss: 313.66
 ---- batch: 100 ----
mean loss: 316.23
 ---- batch: 110 ----
mean loss: 329.02
train mean loss: 315.02
epoch train time: 0:00:02.171006
elapsed time: 0:08:04.820953
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 00:44:45.922365
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 322.09
 ---- batch: 020 ----
mean loss: 317.86
 ---- batch: 030 ----
mean loss: 320.29
 ---- batch: 040 ----
mean loss: 317.52
 ---- batch: 050 ----
mean loss: 328.07
 ---- batch: 060 ----
mean loss: 332.67
 ---- batch: 070 ----
mean loss: 316.24
 ---- batch: 080 ----
mean loss: 320.48
 ---- batch: 090 ----
mean loss: 318.02
 ---- batch: 100 ----
mean loss: 318.52
 ---- batch: 110 ----
mean loss: 315.82
train mean loss: 321.18
epoch train time: 0:00:02.165160
elapsed time: 0:08:06.986301
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 00:44:48.087687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 327.73
 ---- batch: 020 ----
mean loss: 330.37
 ---- batch: 030 ----
mean loss: 332.78
 ---- batch: 040 ----
mean loss: 319.03
 ---- batch: 050 ----
mean loss: 321.72
 ---- batch: 060 ----
mean loss: 313.81
 ---- batch: 070 ----
mean loss: 317.56
 ---- batch: 080 ----
mean loss: 307.91
 ---- batch: 090 ----
mean loss: 318.52
 ---- batch: 100 ----
mean loss: 314.60
 ---- batch: 110 ----
mean loss: 316.96
train mean loss: 320.25
epoch train time: 0:00:02.160262
elapsed time: 0:08:09.146724
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 00:44:50.248111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.17
 ---- batch: 020 ----
mean loss: 324.07
 ---- batch: 030 ----
mean loss: 318.93
 ---- batch: 040 ----
mean loss: 318.34
 ---- batch: 050 ----
mean loss: 322.50
 ---- batch: 060 ----
mean loss: 324.48
 ---- batch: 070 ----
mean loss: 320.89
 ---- batch: 080 ----
mean loss: 320.89
 ---- batch: 090 ----
mean loss: 318.16
 ---- batch: 100 ----
mean loss: 309.78
 ---- batch: 110 ----
mean loss: 316.36
train mean loss: 319.14
epoch train time: 0:00:02.163868
elapsed time: 0:08:11.310764
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 00:44:52.412152
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.79
 ---- batch: 020 ----
mean loss: 315.42
 ---- batch: 030 ----
mean loss: 313.65
 ---- batch: 040 ----
mean loss: 331.41
 ---- batch: 050 ----
mean loss: 311.80
 ---- batch: 060 ----
mean loss: 323.74
 ---- batch: 070 ----
mean loss: 322.93
 ---- batch: 080 ----
mean loss: 322.37
 ---- batch: 090 ----
mean loss: 308.91
 ---- batch: 100 ----
mean loss: 332.34
 ---- batch: 110 ----
mean loss: 327.83
train mean loss: 320.24
epoch train time: 0:00:02.160867
elapsed time: 0:08:13.471814
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 00:44:54.573233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.12
 ---- batch: 020 ----
mean loss: 329.71
 ---- batch: 030 ----
mean loss: 309.68
 ---- batch: 040 ----
mean loss: 307.31
 ---- batch: 050 ----
mean loss: 316.74
 ---- batch: 060 ----
mean loss: 311.75
 ---- batch: 070 ----
mean loss: 323.73
 ---- batch: 080 ----
mean loss: 327.30
 ---- batch: 090 ----
mean loss: 309.91
 ---- batch: 100 ----
mean loss: 315.83
 ---- batch: 110 ----
mean loss: 324.05
train mean loss: 318.00
epoch train time: 0:00:02.165217
elapsed time: 0:08:15.637276
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 00:44:56.738672
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.74
 ---- batch: 020 ----
mean loss: 324.62
 ---- batch: 030 ----
mean loss: 334.31
 ---- batch: 040 ----
mean loss: 320.50
 ---- batch: 050 ----
mean loss: 306.02
 ---- batch: 060 ----
mean loss: 309.72
 ---- batch: 070 ----
mean loss: 318.57
 ---- batch: 080 ----
mean loss: 319.88
 ---- batch: 090 ----
mean loss: 308.25
 ---- batch: 100 ----
mean loss: 314.89
 ---- batch: 110 ----
mean loss: 321.36
train mean loss: 318.15
epoch train time: 0:00:02.163999
elapsed time: 0:08:17.801451
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 00:44:58.902854
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.07
 ---- batch: 020 ----
mean loss: 313.32
 ---- batch: 030 ----
mean loss: 319.33
 ---- batch: 040 ----
mean loss: 319.41
 ---- batch: 050 ----
mean loss: 314.35
 ---- batch: 060 ----
mean loss: 310.82
 ---- batch: 070 ----
mean loss: 321.27
 ---- batch: 080 ----
mean loss: 324.60
 ---- batch: 090 ----
mean loss: 328.51
 ---- batch: 100 ----
mean loss: 309.97
 ---- batch: 110 ----
mean loss: 309.23
train mean loss: 317.38
epoch train time: 0:00:02.168749
elapsed time: 0:08:19.970382
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 00:45:01.071790
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.50
 ---- batch: 020 ----
mean loss: 309.99
 ---- batch: 030 ----
mean loss: 322.58
 ---- batch: 040 ----
mean loss: 318.97
 ---- batch: 050 ----
mean loss: 324.94
 ---- batch: 060 ----
mean loss: 324.10
 ---- batch: 070 ----
mean loss: 318.62
 ---- batch: 080 ----
mean loss: 310.55
 ---- batch: 090 ----
mean loss: 324.67
 ---- batch: 100 ----
mean loss: 325.91
 ---- batch: 110 ----
mean loss: 310.21
train mean loss: 318.17
epoch train time: 0:00:02.168626
elapsed time: 0:08:22.139218
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 00:45:03.240610
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.89
 ---- batch: 020 ----
mean loss: 323.57
 ---- batch: 030 ----
mean loss: 320.66
 ---- batch: 040 ----
mean loss: 319.33
 ---- batch: 050 ----
mean loss: 315.15
 ---- batch: 060 ----
mean loss: 318.52
 ---- batch: 070 ----
mean loss: 310.34
 ---- batch: 080 ----
mean loss: 311.53
 ---- batch: 090 ----
mean loss: 310.69
 ---- batch: 100 ----
mean loss: 320.82
 ---- batch: 110 ----
mean loss: 319.57
train mean loss: 316.78
epoch train time: 0:00:02.161379
elapsed time: 0:08:24.300828
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 00:45:05.402225
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 312.90
 ---- batch: 020 ----
mean loss: 332.10
 ---- batch: 030 ----
mean loss: 316.00
 ---- batch: 040 ----
mean loss: 317.60
 ---- batch: 050 ----
mean loss: 323.34
 ---- batch: 060 ----
mean loss: 314.48
 ---- batch: 070 ----
mean loss: 318.59
 ---- batch: 080 ----
mean loss: 334.22
 ---- batch: 090 ----
mean loss: 319.18
 ---- batch: 100 ----
mean loss: 317.60
 ---- batch: 110 ----
mean loss: 310.43
train mean loss: 319.04
epoch train time: 0:00:02.173752
elapsed time: 0:08:26.474784
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 00:45:07.576174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.92
 ---- batch: 020 ----
mean loss: 320.82
 ---- batch: 030 ----
mean loss: 318.98
 ---- batch: 040 ----
mean loss: 322.41
 ---- batch: 050 ----
mean loss: 325.23
 ---- batch: 060 ----
mean loss: 318.97
 ---- batch: 070 ----
mean loss: 324.65
 ---- batch: 080 ----
mean loss: 311.60
 ---- batch: 090 ----
mean loss: 321.18
 ---- batch: 100 ----
mean loss: 313.22
 ---- batch: 110 ----
mean loss: 311.35
train mean loss: 318.20
epoch train time: 0:00:02.157698
elapsed time: 0:08:28.632665
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 00:45:09.734053
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.54
 ---- batch: 020 ----
mean loss: 311.66
 ---- batch: 030 ----
mean loss: 323.67
 ---- batch: 040 ----
mean loss: 325.94
 ---- batch: 050 ----
mean loss: 310.52
 ---- batch: 060 ----
mean loss: 313.50
 ---- batch: 070 ----
mean loss: 322.13
 ---- batch: 080 ----
mean loss: 325.15
 ---- batch: 090 ----
mean loss: 315.69
 ---- batch: 100 ----
mean loss: 322.00
 ---- batch: 110 ----
mean loss: 314.76
train mean loss: 318.31
epoch train time: 0:00:02.170202
elapsed time: 0:08:30.803056
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 00:45:11.904450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.00
 ---- batch: 020 ----
mean loss: 313.45
 ---- batch: 030 ----
mean loss: 322.97
 ---- batch: 040 ----
mean loss: 321.74
 ---- batch: 050 ----
mean loss: 315.85
 ---- batch: 060 ----
mean loss: 327.47
 ---- batch: 070 ----
mean loss: 308.40
 ---- batch: 080 ----
mean loss: 318.82
 ---- batch: 090 ----
mean loss: 318.30
 ---- batch: 100 ----
mean loss: 315.51
 ---- batch: 110 ----
mean loss: 322.56
train mean loss: 318.47
epoch train time: 0:00:02.165723
elapsed time: 0:08:32.968983
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 00:45:14.070395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.66
 ---- batch: 020 ----
mean loss: 319.69
 ---- batch: 030 ----
mean loss: 311.70
 ---- batch: 040 ----
mean loss: 323.01
 ---- batch: 050 ----
mean loss: 329.21
 ---- batch: 060 ----
mean loss: 312.85
 ---- batch: 070 ----
mean loss: 315.81
 ---- batch: 080 ----
mean loss: 314.88
 ---- batch: 090 ----
mean loss: 306.63
 ---- batch: 100 ----
mean loss: 323.77
 ---- batch: 110 ----
mean loss: 313.11
train mean loss: 316.70
epoch train time: 0:00:02.162565
elapsed time: 0:08:35.131734
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 00:45:16.233124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.18
 ---- batch: 020 ----
mean loss: 322.94
 ---- batch: 030 ----
mean loss: 312.57
 ---- batch: 040 ----
mean loss: 315.58
 ---- batch: 050 ----
mean loss: 324.12
 ---- batch: 060 ----
mean loss: 326.62
 ---- batch: 070 ----
mean loss: 319.67
 ---- batch: 080 ----
mean loss: 316.27
 ---- batch: 090 ----
mean loss: 311.40
 ---- batch: 100 ----
mean loss: 307.20
 ---- batch: 110 ----
mean loss: 310.24
train mean loss: 316.16
epoch train time: 0:00:02.169714
elapsed time: 0:08:37.301643
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 00:45:18.403039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.64
 ---- batch: 020 ----
mean loss: 317.72
 ---- batch: 030 ----
mean loss: 318.19
 ---- batch: 040 ----
mean loss: 327.93
 ---- batch: 050 ----
mean loss: 303.51
 ---- batch: 060 ----
mean loss: 334.44
 ---- batch: 070 ----
mean loss: 322.61
 ---- batch: 080 ----
mean loss: 311.39
 ---- batch: 090 ----
mean loss: 317.85
 ---- batch: 100 ----
mean loss: 314.62
 ---- batch: 110 ----
mean loss: 309.39
train mean loss: 318.42
epoch train time: 0:00:02.164392
elapsed time: 0:08:39.466233
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 00:45:20.567624
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.83
 ---- batch: 020 ----
mean loss: 322.57
 ---- batch: 030 ----
mean loss: 318.17
 ---- batch: 040 ----
mean loss: 312.54
 ---- batch: 050 ----
mean loss: 318.33
 ---- batch: 060 ----
mean loss: 303.93
 ---- batch: 070 ----
mean loss: 326.75
 ---- batch: 080 ----
mean loss: 312.53
 ---- batch: 090 ----
mean loss: 323.90
 ---- batch: 100 ----
mean loss: 322.84
 ---- batch: 110 ----
mean loss: 314.79
train mean loss: 317.52
epoch train time: 0:00:02.168451
elapsed time: 0:08:41.634861
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 00:45:22.736273
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.97
 ---- batch: 020 ----
mean loss: 323.74
 ---- batch: 030 ----
mean loss: 313.15
 ---- batch: 040 ----
mean loss: 317.90
 ---- batch: 050 ----
mean loss: 310.16
 ---- batch: 060 ----
mean loss: 327.29
 ---- batch: 070 ----
mean loss: 334.73
 ---- batch: 080 ----
mean loss: 322.16
 ---- batch: 090 ----
mean loss: 313.31
 ---- batch: 100 ----
mean loss: 316.02
 ---- batch: 110 ----
mean loss: 312.20
train mean loss: 317.49
epoch train time: 0:00:02.166553
elapsed time: 0:08:43.801626
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 00:45:24.903040
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.71
 ---- batch: 020 ----
mean loss: 309.76
 ---- batch: 030 ----
mean loss: 310.22
 ---- batch: 040 ----
mean loss: 320.71
 ---- batch: 050 ----
mean loss: 318.46
 ---- batch: 060 ----
mean loss: 314.74
 ---- batch: 070 ----
mean loss: 323.37
 ---- batch: 080 ----
mean loss: 316.04
 ---- batch: 090 ----
mean loss: 318.73
 ---- batch: 100 ----
mean loss: 314.07
 ---- batch: 110 ----
mean loss: 317.40
train mean loss: 316.43
epoch train time: 0:00:02.165256
elapsed time: 0:08:45.967088
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 00:45:27.068482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 314.70
 ---- batch: 020 ----
mean loss: 324.98
 ---- batch: 030 ----
mean loss: 323.71
 ---- batch: 040 ----
mean loss: 314.82
 ---- batch: 050 ----
mean loss: 314.66
 ---- batch: 060 ----
mean loss: 319.51
 ---- batch: 070 ----
mean loss: 324.15
 ---- batch: 080 ----
mean loss: 304.94
 ---- batch: 090 ----
mean loss: 313.02
 ---- batch: 100 ----
mean loss: 314.64
 ---- batch: 110 ----
mean loss: 303.85
train mean loss: 315.88
epoch train time: 0:00:02.165183
elapsed time: 0:08:48.132487
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 00:45:29.233902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 312.86
 ---- batch: 020 ----
mean loss: 316.86
 ---- batch: 030 ----
mean loss: 316.04
 ---- batch: 040 ----
mean loss: 330.49
 ---- batch: 050 ----
mean loss: 306.31
 ---- batch: 060 ----
mean loss: 315.71
 ---- batch: 070 ----
mean loss: 319.13
 ---- batch: 080 ----
mean loss: 316.29
 ---- batch: 090 ----
mean loss: 315.74
 ---- batch: 100 ----
mean loss: 308.47
 ---- batch: 110 ----
mean loss: 321.94
train mean loss: 316.94
epoch train time: 0:00:02.164529
elapsed time: 0:08:50.297236
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 00:45:31.398630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.63
 ---- batch: 020 ----
mean loss: 308.78
 ---- batch: 030 ----
mean loss: 304.61
 ---- batch: 040 ----
mean loss: 319.30
 ---- batch: 050 ----
mean loss: 327.89
 ---- batch: 060 ----
mean loss: 314.34
 ---- batch: 070 ----
mean loss: 315.51
 ---- batch: 080 ----
mean loss: 320.45
 ---- batch: 090 ----
mean loss: 321.77
 ---- batch: 100 ----
mean loss: 319.65
 ---- batch: 110 ----
mean loss: 321.79
train mean loss: 316.18
epoch train time: 0:00:02.159112
elapsed time: 0:08:52.456538
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 00:45:33.557929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.95
 ---- batch: 020 ----
mean loss: 309.41
 ---- batch: 030 ----
mean loss: 320.87
 ---- batch: 040 ----
mean loss: 325.20
 ---- batch: 050 ----
mean loss: 300.94
 ---- batch: 060 ----
mean loss: 320.43
 ---- batch: 070 ----
mean loss: 312.79
 ---- batch: 080 ----
mean loss: 323.50
 ---- batch: 090 ----
mean loss: 316.20
 ---- batch: 100 ----
mean loss: 321.64
 ---- batch: 110 ----
mean loss: 311.05
train mean loss: 316.30
epoch train time: 0:00:02.156488
elapsed time: 0:08:54.613232
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 00:45:35.714658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.77
 ---- batch: 020 ----
mean loss: 312.36
 ---- batch: 030 ----
mean loss: 314.01
 ---- batch: 040 ----
mean loss: 322.17
 ---- batch: 050 ----
mean loss: 310.64
 ---- batch: 060 ----
mean loss: 317.55
 ---- batch: 070 ----
mean loss: 320.11
 ---- batch: 080 ----
mean loss: 318.19
 ---- batch: 090 ----
mean loss: 310.73
 ---- batch: 100 ----
mean loss: 323.45
 ---- batch: 110 ----
mean loss: 321.36
train mean loss: 317.26
epoch train time: 0:00:02.149406
elapsed time: 0:08:56.762862
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 00:45:37.864239
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.48
 ---- batch: 020 ----
mean loss: 305.22
 ---- batch: 030 ----
mean loss: 309.87
 ---- batch: 040 ----
mean loss: 313.73
 ---- batch: 050 ----
mean loss: 322.30
 ---- batch: 060 ----
mean loss: 316.78
 ---- batch: 070 ----
mean loss: 321.85
 ---- batch: 080 ----
mean loss: 311.49
 ---- batch: 090 ----
mean loss: 321.54
 ---- batch: 100 ----
mean loss: 318.88
 ---- batch: 110 ----
mean loss: 312.11
train mean loss: 315.52
epoch train time: 0:00:02.142309
elapsed time: 0:08:58.905323
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 00:45:40.006711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.14
 ---- batch: 020 ----
mean loss: 307.77
 ---- batch: 030 ----
mean loss: 320.70
 ---- batch: 040 ----
mean loss: 324.56
 ---- batch: 050 ----
mean loss: 311.99
 ---- batch: 060 ----
mean loss: 322.02
 ---- batch: 070 ----
mean loss: 318.21
 ---- batch: 080 ----
mean loss: 312.30
 ---- batch: 090 ----
mean loss: 316.58
 ---- batch: 100 ----
mean loss: 310.22
 ---- batch: 110 ----
mean loss: 315.46
train mean loss: 316.39
epoch train time: 0:00:02.141739
elapsed time: 0:09:01.047217
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 00:45:42.148625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 311.40
 ---- batch: 020 ----
mean loss: 315.96
 ---- batch: 030 ----
mean loss: 311.12
 ---- batch: 040 ----
mean loss: 315.38
 ---- batch: 050 ----
mean loss: 313.19
 ---- batch: 060 ----
mean loss: 321.79
 ---- batch: 070 ----
mean loss: 323.97
 ---- batch: 080 ----
mean loss: 306.69
 ---- batch: 090 ----
mean loss: 314.81
 ---- batch: 100 ----
mean loss: 315.19
 ---- batch: 110 ----
mean loss: 321.77
train mean loss: 315.27
epoch train time: 0:00:02.148683
elapsed time: 0:09:03.196090
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 00:45:44.297478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 311.41
 ---- batch: 020 ----
mean loss: 330.15
 ---- batch: 030 ----
mean loss: 323.71
 ---- batch: 040 ----
mean loss: 316.33
 ---- batch: 050 ----
mean loss: 314.28
 ---- batch: 060 ----
mean loss: 317.03
 ---- batch: 070 ----
mean loss: 311.00
 ---- batch: 080 ----
mean loss: 315.94
 ---- batch: 090 ----
mean loss: 319.60
 ---- batch: 100 ----
mean loss: 315.93
 ---- batch: 110 ----
mean loss: 309.06
train mean loss: 317.23
epoch train time: 0:00:02.142573
elapsed time: 0:09:05.338820
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 00:45:46.440206
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.07
 ---- batch: 020 ----
mean loss: 315.18
 ---- batch: 030 ----
mean loss: 314.21
 ---- batch: 040 ----
mean loss: 306.03
 ---- batch: 050 ----
mean loss: 318.10
 ---- batch: 060 ----
mean loss: 316.78
 ---- batch: 070 ----
mean loss: 325.66
 ---- batch: 080 ----
mean loss: 308.09
 ---- batch: 090 ----
mean loss: 306.00
 ---- batch: 100 ----
mean loss: 312.73
 ---- batch: 110 ----
mean loss: 313.22
train mean loss: 314.02
epoch train time: 0:00:02.143940
elapsed time: 0:09:07.482927
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 00:45:48.584314
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 307.32
 ---- batch: 020 ----
mean loss: 311.61
 ---- batch: 030 ----
mean loss: 321.10
 ---- batch: 040 ----
mean loss: 309.79
 ---- batch: 050 ----
mean loss: 309.07
 ---- batch: 060 ----
mean loss: 328.98
 ---- batch: 070 ----
mean loss: 311.89
 ---- batch: 080 ----
mean loss: 320.68
 ---- batch: 090 ----
mean loss: 304.24
 ---- batch: 100 ----
mean loss: 311.12
 ---- batch: 110 ----
mean loss: 314.78
train mean loss: 313.29
epoch train time: 0:00:02.139755
elapsed time: 0:09:09.622846
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 00:45:50.724232
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.19
 ---- batch: 020 ----
mean loss: 315.24
 ---- batch: 030 ----
mean loss: 308.26
 ---- batch: 040 ----
mean loss: 303.33
 ---- batch: 050 ----
mean loss: 303.27
 ---- batch: 060 ----
mean loss: 318.96
 ---- batch: 070 ----
mean loss: 320.90
 ---- batch: 080 ----
mean loss: 323.33
 ---- batch: 090 ----
mean loss: 324.13
 ---- batch: 100 ----
mean loss: 310.12
 ---- batch: 110 ----
mean loss: 337.27
train mean loss: 316.23
epoch train time: 0:00:02.144941
elapsed time: 0:09:11.767958
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 00:45:52.869351
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.69
 ---- batch: 020 ----
mean loss: 303.67
 ---- batch: 030 ----
mean loss: 321.51
 ---- batch: 040 ----
mean loss: 310.34
 ---- batch: 050 ----
mean loss: 317.93
 ---- batch: 060 ----
mean loss: 320.56
 ---- batch: 070 ----
mean loss: 311.46
 ---- batch: 080 ----
mean loss: 317.21
 ---- batch: 090 ----
mean loss: 318.30
 ---- batch: 100 ----
mean loss: 324.29
 ---- batch: 110 ----
mean loss: 306.84
train mean loss: 315.05
epoch train time: 0:00:02.139997
elapsed time: 0:09:13.908116
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 00:45:55.009500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 318.85
 ---- batch: 020 ----
mean loss: 331.22
 ---- batch: 030 ----
mean loss: 328.21
 ---- batch: 040 ----
mean loss: 308.50
 ---- batch: 050 ----
mean loss: 305.82
 ---- batch: 060 ----
mean loss: 307.60
 ---- batch: 070 ----
mean loss: 321.45
 ---- batch: 080 ----
mean loss: 307.53
 ---- batch: 090 ----
mean loss: 320.18
 ---- batch: 100 ----
mean loss: 313.60
 ---- batch: 110 ----
mean loss: 312.13
train mean loss: 315.44
epoch train time: 0:00:02.136498
elapsed time: 0:09:16.044770
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 00:45:57.146187
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.25
 ---- batch: 020 ----
mean loss: 323.76
 ---- batch: 030 ----
mean loss: 307.95
 ---- batch: 040 ----
mean loss: 321.31
 ---- batch: 050 ----
mean loss: 321.82
 ---- batch: 060 ----
mean loss: 316.24
 ---- batch: 070 ----
mean loss: 324.43
 ---- batch: 080 ----
mean loss: 305.34
 ---- batch: 090 ----
mean loss: 299.58
 ---- batch: 100 ----
mean loss: 315.75
 ---- batch: 110 ----
mean loss: 311.20
train mean loss: 314.17
epoch train time: 0:00:02.136731
elapsed time: 0:09:18.181696
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 00:45:59.283086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.69
 ---- batch: 020 ----
mean loss: 311.94
 ---- batch: 030 ----
mean loss: 310.97
 ---- batch: 040 ----
mean loss: 315.45
 ---- batch: 050 ----
mean loss: 326.32
 ---- batch: 060 ----
mean loss: 311.32
 ---- batch: 070 ----
mean loss: 319.30
 ---- batch: 080 ----
mean loss: 320.49
 ---- batch: 090 ----
mean loss: 308.40
 ---- batch: 100 ----
mean loss: 315.42
 ---- batch: 110 ----
mean loss: 320.26
train mean loss: 316.11
epoch train time: 0:00:02.137714
elapsed time: 0:09:20.319572
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 00:46:01.420961
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.91
 ---- batch: 020 ----
mean loss: 315.95
 ---- batch: 030 ----
mean loss: 319.04
 ---- batch: 040 ----
mean loss: 316.43
 ---- batch: 050 ----
mean loss: 302.87
 ---- batch: 060 ----
mean loss: 315.87
 ---- batch: 070 ----
mean loss: 325.79
 ---- batch: 080 ----
mean loss: 311.10
 ---- batch: 090 ----
mean loss: 323.54
 ---- batch: 100 ----
mean loss: 317.26
 ---- batch: 110 ----
mean loss: 317.34
train mean loss: 315.54
epoch train time: 0:00:02.133566
elapsed time: 0:09:22.453340
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 00:46:03.554732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.87
 ---- batch: 020 ----
mean loss: 316.06
 ---- batch: 030 ----
mean loss: 309.41
 ---- batch: 040 ----
mean loss: 315.71
 ---- batch: 050 ----
mean loss: 318.08
 ---- batch: 060 ----
mean loss: 308.70
 ---- batch: 070 ----
mean loss: 316.34
 ---- batch: 080 ----
mean loss: 319.24
 ---- batch: 090 ----
mean loss: 314.70
 ---- batch: 100 ----
mean loss: 312.68
 ---- batch: 110 ----
mean loss: 323.62
train mean loss: 316.05
epoch train time: 0:00:02.150293
elapsed time: 0:09:24.603799
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 00:46:05.705183
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.61
 ---- batch: 020 ----
mean loss: 318.82
 ---- batch: 030 ----
mean loss: 314.52
 ---- batch: 040 ----
mean loss: 315.98
 ---- batch: 050 ----
mean loss: 312.97
 ---- batch: 060 ----
mean loss: 319.49
 ---- batch: 070 ----
mean loss: 312.44
 ---- batch: 080 ----
mean loss: 317.03
 ---- batch: 090 ----
mean loss: 311.50
 ---- batch: 100 ----
mean loss: 321.73
 ---- batch: 110 ----
mean loss: 322.59
train mean loss: 314.78
epoch train time: 0:00:02.164170
elapsed time: 0:09:26.768189
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 00:46:07.869636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.46
 ---- batch: 020 ----
mean loss: 312.83
 ---- batch: 030 ----
mean loss: 309.66
 ---- batch: 040 ----
mean loss: 314.98
 ---- batch: 050 ----
mean loss: 313.69
 ---- batch: 060 ----
mean loss: 309.97
 ---- batch: 070 ----
mean loss: 309.84
 ---- batch: 080 ----
mean loss: 321.34
 ---- batch: 090 ----
mean loss: 310.75
 ---- batch: 100 ----
mean loss: 324.60
 ---- batch: 110 ----
mean loss: 311.46
train mean loss: 314.00
epoch train time: 0:00:02.171234
elapsed time: 0:09:28.939663
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 00:46:10.041055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 307.92
 ---- batch: 020 ----
mean loss: 311.98
 ---- batch: 030 ----
mean loss: 302.35
 ---- batch: 040 ----
mean loss: 307.13
 ---- batch: 050 ----
mean loss: 310.11
 ---- batch: 060 ----
mean loss: 319.07
 ---- batch: 070 ----
mean loss: 315.13
 ---- batch: 080 ----
mean loss: 312.75
 ---- batch: 090 ----
mean loss: 321.36
 ---- batch: 100 ----
mean loss: 320.81
 ---- batch: 110 ----
mean loss: 314.64
train mean loss: 313.11
epoch train time: 0:00:02.158632
elapsed time: 0:09:31.102049
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_5/checkpoint.pth.tar
**** end time: 2019-09-27 00:46:12.203399 ****
