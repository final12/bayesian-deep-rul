Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_0', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 22406
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-10-01 14:19:47.244770 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-10-01 14:19:47.254485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4420.20
 ---- batch: 020 ----
mean loss: 4149.18
train mean loss: 4271.21
epoch train time: 0:01:23.809706
elapsed time: 0:01:23.825829
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-10-01 14:21:11.070642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3929.57
 ---- batch: 020 ----
mean loss: 3732.23
train mean loss: 3813.81
epoch train time: 0:00:00.365801
elapsed time: 0:01:24.191803
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-10-01 14:21:11.436645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3500.08
 ---- batch: 020 ----
mean loss: 3487.57
train mean loss: 3491.97
epoch train time: 0:00:00.356885
elapsed time: 0:01:24.548880
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-10-01 14:21:11.793711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3308.17
 ---- batch: 020 ----
mean loss: 3169.76
train mean loss: 3217.17
epoch train time: 0:00:00.370905
elapsed time: 0:01:24.919982
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-10-01 14:21:12.164821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3054.72
 ---- batch: 020 ----
mean loss: 2941.47
train mean loss: 3001.49
epoch train time: 0:00:00.363385
elapsed time: 0:01:25.283570
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-10-01 14:21:12.528404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2853.52
 ---- batch: 020 ----
mean loss: 2751.38
train mean loss: 2790.61
epoch train time: 0:00:00.384853
elapsed time: 0:01:25.668606
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-10-01 14:21:12.913465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2677.76
 ---- batch: 020 ----
mean loss: 2604.09
train mean loss: 2634.71
epoch train time: 0:00:00.362765
elapsed time: 0:01:26.031637
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-10-01 14:21:13.276481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2543.10
 ---- batch: 020 ----
mean loss: 2445.32
train mean loss: 2491.44
epoch train time: 0:00:00.359614
elapsed time: 0:01:26.391436
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-10-01 14:21:13.636262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2405.41
 ---- batch: 020 ----
mean loss: 2336.13
train mean loss: 2357.93
epoch train time: 0:00:00.359204
elapsed time: 0:01:26.750830
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-10-01 14:21:13.995666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2296.92
 ---- batch: 020 ----
mean loss: 2246.57
train mean loss: 2259.44
epoch train time: 0:00:00.364515
elapsed time: 0:01:27.115539
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-10-01 14:21:14.360389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2153.96
 ---- batch: 020 ----
mean loss: 2169.42
train mean loss: 2164.72
epoch train time: 0:00:00.365360
elapsed time: 0:01:27.481163
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-10-01 14:21:14.726026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2115.63
 ---- batch: 020 ----
mean loss: 2034.04
train mean loss: 2077.50
epoch train time: 0:00:00.365671
elapsed time: 0:01:27.847053
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-10-01 14:21:15.091888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1994.88
 ---- batch: 020 ----
mean loss: 1997.83
train mean loss: 1989.37
epoch train time: 0:00:00.362012
elapsed time: 0:01:28.209247
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-10-01 14:21:15.454084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1933.42
 ---- batch: 020 ----
mean loss: 1896.30
train mean loss: 1911.47
epoch train time: 0:00:00.366968
elapsed time: 0:01:28.576404
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-10-01 14:21:15.821249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1876.59
 ---- batch: 020 ----
mean loss: 1810.79
train mean loss: 1846.83
epoch train time: 0:00:00.364471
elapsed time: 0:01:28.941094
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-10-01 14:21:16.185933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1809.25
 ---- batch: 020 ----
mean loss: 1755.09
train mean loss: 1778.65
epoch train time: 0:00:00.358708
elapsed time: 0:01:29.299992
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-10-01 14:21:16.544826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1738.74
 ---- batch: 020 ----
mean loss: 1707.91
train mean loss: 1715.68
epoch train time: 0:00:00.360362
elapsed time: 0:01:29.660564
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-10-01 14:21:16.905397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1677.60
 ---- batch: 020 ----
mean loss: 1628.02
train mean loss: 1657.63
epoch train time: 0:00:00.361535
elapsed time: 0:01:30.022285
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-10-01 14:21:17.267134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1605.67
 ---- batch: 020 ----
mean loss: 1593.12
train mean loss: 1597.54
epoch train time: 0:00:00.375645
elapsed time: 0:01:30.398126
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-10-01 14:21:17.642960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1539.68
 ---- batch: 020 ----
mean loss: 1550.75
train mean loss: 1537.38
epoch train time: 0:00:00.391293
elapsed time: 0:01:30.789648
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-10-01 14:21:18.034513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1500.28
 ---- batch: 020 ----
mean loss: 1467.77
train mean loss: 1477.34
epoch train time: 0:00:00.368775
elapsed time: 0:01:31.158642
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-10-01 14:21:18.403472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1438.96
 ---- batch: 020 ----
mean loss: 1407.09
train mean loss: 1414.94
epoch train time: 0:00:00.361736
elapsed time: 0:01:31.520553
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-10-01 14:21:18.765383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1387.45
 ---- batch: 020 ----
mean loss: 1347.49
train mean loss: 1354.71
epoch train time: 0:00:00.367481
elapsed time: 0:01:31.888216
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-10-01 14:21:19.133049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1315.07
 ---- batch: 020 ----
mean loss: 1295.12
train mean loss: 1299.59
epoch train time: 0:00:00.362314
elapsed time: 0:01:32.250768
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-10-01 14:21:19.495603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1256.05
 ---- batch: 020 ----
mean loss: 1229.74
train mean loss: 1250.24
epoch train time: 0:00:00.392058
elapsed time: 0:01:32.643020
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-10-01 14:21:19.887852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1212.58
 ---- batch: 020 ----
mean loss: 1196.11
train mean loss: 1195.02
epoch train time: 0:00:00.354622
elapsed time: 0:01:32.997855
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-10-01 14:21:20.242687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1162.39
 ---- batch: 020 ----
mean loss: 1151.28
train mean loss: 1149.24
epoch train time: 0:00:00.355709
elapsed time: 0:01:33.353753
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-10-01 14:21:20.598604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1129.45
 ---- batch: 020 ----
mean loss: 1095.49
train mean loss: 1110.93
epoch train time: 0:00:00.358381
elapsed time: 0:01:33.712328
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-10-01 14:21:20.957158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1082.16
 ---- batch: 020 ----
mean loss: 1045.43
train mean loss: 1058.38
epoch train time: 0:00:00.354022
elapsed time: 0:01:34.066555
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-10-01 14:21:21.311386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1037.58
 ---- batch: 020 ----
mean loss: 1018.29
train mean loss: 1027.32
epoch train time: 0:00:00.357440
elapsed time: 0:01:34.424198
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-10-01 14:21:21.669059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 983.64
 ---- batch: 020 ----
mean loss: 979.72
train mean loss: 982.79
epoch train time: 0:00:00.356322
elapsed time: 0:01:34.780729
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-10-01 14:21:22.025554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.20
 ---- batch: 020 ----
mean loss: 940.17
train mean loss: 942.96
epoch train time: 0:00:00.354941
elapsed time: 0:01:35.135836
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-10-01 14:21:22.380665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.91
 ---- batch: 020 ----
mean loss: 895.78
train mean loss: 904.59
epoch train time: 0:00:00.352811
elapsed time: 0:01:35.488824
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-10-01 14:21:22.733657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.81
 ---- batch: 020 ----
mean loss: 861.90
train mean loss: 863.75
epoch train time: 0:00:00.367026
elapsed time: 0:01:35.856073
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-10-01 14:21:23.100912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.88
 ---- batch: 020 ----
mean loss: 827.80
train mean loss: 832.37
epoch train time: 0:00:00.353717
elapsed time: 0:01:36.209983
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-10-01 14:21:23.454815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.21
 ---- batch: 020 ----
mean loss: 791.66
train mean loss: 797.58
epoch train time: 0:00:00.355355
elapsed time: 0:01:36.565526
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-10-01 14:21:23.810399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 777.33
 ---- batch: 020 ----
mean loss: 752.02
train mean loss: 762.90
epoch train time: 0:00:00.382708
elapsed time: 0:01:36.948481
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-10-01 14:21:24.193316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.03
 ---- batch: 020 ----
mean loss: 715.59
train mean loss: 726.06
epoch train time: 0:00:00.360594
elapsed time: 0:01:37.309269
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-10-01 14:21:24.554103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.05
 ---- batch: 020 ----
mean loss: 687.03
train mean loss: 701.39
epoch train time: 0:00:00.360788
elapsed time: 0:01:37.670269
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-10-01 14:21:24.915099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.58
 ---- batch: 020 ----
mean loss: 668.47
train mean loss: 671.48
epoch train time: 0:00:00.350346
elapsed time: 0:01:38.020789
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-10-01 14:21:25.265632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.95
 ---- batch: 020 ----
mean loss: 634.28
train mean loss: 641.55
epoch train time: 0:00:00.346557
elapsed time: 0:01:38.367532
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-10-01 14:21:25.612364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 610.78
 ---- batch: 020 ----
mean loss: 611.59
train mean loss: 610.09
epoch train time: 0:00:00.375318
elapsed time: 0:01:38.743026
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-10-01 14:21:25.987899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.12
 ---- batch: 020 ----
mean loss: 577.41
train mean loss: 587.12
epoch train time: 0:00:00.349401
elapsed time: 0:01:39.092642
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-10-01 14:21:26.337472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.11
 ---- batch: 020 ----
mean loss: 560.89
train mean loss: 563.58
epoch train time: 0:00:00.352598
elapsed time: 0:01:39.445416
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-10-01 14:21:26.690251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 550.69
 ---- batch: 020 ----
mean loss: 541.70
train mean loss: 546.43
epoch train time: 0:00:00.349240
elapsed time: 0:01:39.794863
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-10-01 14:21:27.039692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 536.51
 ---- batch: 020 ----
mean loss: 516.57
train mean loss: 523.13
epoch train time: 0:00:00.351674
elapsed time: 0:01:40.146810
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-10-01 14:21:27.391673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.82
 ---- batch: 020 ----
mean loss: 501.68
train mean loss: 502.86
epoch train time: 0:00:00.377438
elapsed time: 0:01:40.524517
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-10-01 14:21:27.769373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.44
 ---- batch: 020 ----
mean loss: 474.89
train mean loss: 481.40
epoch train time: 0:00:00.380102
elapsed time: 0:01:40.904824
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-10-01 14:21:28.149661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.47
 ---- batch: 020 ----
mean loss: 467.51
train mean loss: 462.83
epoch train time: 0:00:00.362226
elapsed time: 0:01:41.267235
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-10-01 14:21:28.512081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.45
 ---- batch: 020 ----
mean loss: 436.45
train mean loss: 439.26
epoch train time: 0:00:00.368316
elapsed time: 0:01:41.635745
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-10-01 14:21:28.880578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.15
 ---- batch: 020 ----
mean loss: 419.89
train mean loss: 426.46
epoch train time: 0:00:00.354953
elapsed time: 0:01:41.990872
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-10-01 14:21:29.235735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.25
 ---- batch: 020 ----
mean loss: 398.72
train mean loss: 408.38
epoch train time: 0:00:00.350333
elapsed time: 0:01:42.341409
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-10-01 14:21:29.586237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.61
 ---- batch: 020 ----
mean loss: 387.82
train mean loss: 391.52
epoch train time: 0:00:00.365028
elapsed time: 0:01:42.706607
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-10-01 14:21:29.951437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.04
 ---- batch: 020 ----
mean loss: 373.98
train mean loss: 378.17
epoch train time: 0:00:00.352077
elapsed time: 0:01:43.058863
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-10-01 14:21:30.303693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.87
 ---- batch: 020 ----
mean loss: 367.26
train mean loss: 366.00
epoch train time: 0:00:00.353891
elapsed time: 0:01:43.412925
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-10-01 14:21:30.657780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.35
 ---- batch: 020 ----
mean loss: 341.71
train mean loss: 349.70
epoch train time: 0:00:00.360003
elapsed time: 0:01:43.773136
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-10-01 14:21:31.017987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.95
 ---- batch: 020 ----
mean loss: 336.20
train mean loss: 341.72
epoch train time: 0:00:00.359204
elapsed time: 0:01:44.132546
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-10-01 14:21:31.377376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.61
 ---- batch: 020 ----
mean loss: 325.27
train mean loss: 326.82
epoch train time: 0:00:00.360189
elapsed time: 0:01:44.492916
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-10-01 14:21:31.737788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.15
 ---- batch: 020 ----
mean loss: 312.51
train mean loss: 314.62
epoch train time: 0:00:00.364640
elapsed time: 0:01:44.857777
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-10-01 14:21:32.102607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.46
 ---- batch: 020 ----
mean loss: 307.62
train mean loss: 306.67
epoch train time: 0:00:00.357450
elapsed time: 0:01:45.215405
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-10-01 14:21:32.460234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.29
 ---- batch: 020 ----
mean loss: 287.77
train mean loss: 290.23
epoch train time: 0:00:00.365823
elapsed time: 0:01:45.581409
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-10-01 14:21:32.826250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.12
 ---- batch: 020 ----
mean loss: 283.02
train mean loss: 282.29
epoch train time: 0:00:00.362821
elapsed time: 0:01:45.944415
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-10-01 14:21:33.189257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.99
 ---- batch: 020 ----
mean loss: 267.62
train mean loss: 272.28
epoch train time: 0:00:00.361356
elapsed time: 0:01:46.305963
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-10-01 14:21:33.550793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.98
 ---- batch: 020 ----
mean loss: 266.99
train mean loss: 263.33
epoch train time: 0:00:00.364972
elapsed time: 0:01:46.671113
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-10-01 14:21:33.915954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.77
 ---- batch: 020 ----
mean loss: 251.74
train mean loss: 253.85
epoch train time: 0:00:00.361561
elapsed time: 0:01:47.032880
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-10-01 14:21:34.277712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.02
 ---- batch: 020 ----
mean loss: 249.60
train mean loss: 246.59
epoch train time: 0:00:00.363582
elapsed time: 0:01:47.396655
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-10-01 14:21:34.641484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.21
 ---- batch: 020 ----
mean loss: 236.30
train mean loss: 237.31
epoch train time: 0:00:00.371476
elapsed time: 0:01:47.768308
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-10-01 14:21:35.013137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.60
 ---- batch: 020 ----
mean loss: 230.58
train mean loss: 232.64
epoch train time: 0:00:00.363409
elapsed time: 0:01:48.131908
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-10-01 14:21:35.376749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.60
 ---- batch: 020 ----
mean loss: 224.94
train mean loss: 224.20
epoch train time: 0:00:00.367246
elapsed time: 0:01:48.499367
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-10-01 14:21:35.744213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.23
 ---- batch: 020 ----
mean loss: 216.43
train mean loss: 218.07
epoch train time: 0:00:00.373786
elapsed time: 0:01:48.873400
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-10-01 14:21:36.118227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.14
 ---- batch: 020 ----
mean loss: 213.51
train mean loss: 212.34
epoch train time: 0:00:00.351979
elapsed time: 0:01:49.225553
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-10-01 14:21:36.470379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.89
 ---- batch: 020 ----
mean loss: 197.25
train mean loss: 201.34
epoch train time: 0:00:00.351519
elapsed time: 0:01:49.577254
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-10-01 14:21:36.822087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.81
 ---- batch: 020 ----
mean loss: 195.07
train mean loss: 196.37
epoch train time: 0:00:00.365811
elapsed time: 0:01:49.943286
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-10-01 14:21:37.188132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.51
 ---- batch: 020 ----
mean loss: 190.87
train mean loss: 192.39
epoch train time: 0:00:00.366255
elapsed time: 0:01:50.309753
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-10-01 14:21:37.554582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.97
 ---- batch: 020 ----
mean loss: 186.09
train mean loss: 186.14
epoch train time: 0:00:00.382820
elapsed time: 0:01:50.692749
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-10-01 14:21:37.937576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.07
 ---- batch: 020 ----
mean loss: 178.53
train mean loss: 180.76
epoch train time: 0:00:00.352069
elapsed time: 0:01:51.045009
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-10-01 14:21:38.289836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.55
 ---- batch: 020 ----
mean loss: 175.13
train mean loss: 174.14
epoch train time: 0:00:00.350721
elapsed time: 0:01:51.395926
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-10-01 14:21:38.640768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.28
 ---- batch: 020 ----
mean loss: 171.26
train mean loss: 170.91
epoch train time: 0:00:00.360159
elapsed time: 0:01:51.756305
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-10-01 14:21:39.001135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.56
 ---- batch: 020 ----
mean loss: 165.67
train mean loss: 166.39
epoch train time: 0:00:00.350178
elapsed time: 0:01:52.106658
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-10-01 14:21:39.351520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.83
 ---- batch: 020 ----
mean loss: 161.85
train mean loss: 162.59
epoch train time: 0:00:00.348785
elapsed time: 0:01:52.455665
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-10-01 14:21:39.700502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.79
 ---- batch: 020 ----
mean loss: 157.60
train mean loss: 157.49
epoch train time: 0:00:00.360757
elapsed time: 0:01:52.816612
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-10-01 14:21:40.061437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.29
 ---- batch: 020 ----
mean loss: 153.88
train mean loss: 152.90
epoch train time: 0:00:00.356911
elapsed time: 0:01:53.173706
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-10-01 14:21:40.418536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.96
 ---- batch: 020 ----
mean loss: 151.56
train mean loss: 150.56
epoch train time: 0:00:00.357702
elapsed time: 0:01:53.531581
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-10-01 14:21:40.776412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.41
 ---- batch: 020 ----
mean loss: 140.50
train mean loss: 146.00
epoch train time: 0:00:00.357663
elapsed time: 0:01:53.889423
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-10-01 14:21:41.134252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.48
 ---- batch: 020 ----
mean loss: 144.23
train mean loss: 143.87
epoch train time: 0:00:00.351207
elapsed time: 0:01:54.240863
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-10-01 14:21:41.485699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.77
 ---- batch: 020 ----
mean loss: 141.48
train mean loss: 141.08
epoch train time: 0:00:00.363717
elapsed time: 0:01:54.604765
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-10-01 14:21:41.849596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.21
 ---- batch: 020 ----
mean loss: 136.38
train mean loss: 136.55
epoch train time: 0:00:00.352515
elapsed time: 0:01:54.957491
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-10-01 14:21:42.202321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.43
 ---- batch: 020 ----
mean loss: 132.96
train mean loss: 133.73
epoch train time: 0:00:00.350487
elapsed time: 0:01:55.308161
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-10-01 14:21:42.552998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.38
 ---- batch: 020 ----
mean loss: 129.20
train mean loss: 131.84
epoch train time: 0:00:00.361783
elapsed time: 0:01:55.670123
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-10-01 14:21:42.914963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.35
 ---- batch: 020 ----
mean loss: 129.65
train mean loss: 128.14
epoch train time: 0:00:00.348833
elapsed time: 0:01:56.019169
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-10-01 14:21:43.264051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.69
 ---- batch: 020 ----
mean loss: 127.76
train mean loss: 126.20
epoch train time: 0:00:00.346097
elapsed time: 0:01:56.365497
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-10-01 14:21:43.610327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.48
 ---- batch: 020 ----
mean loss: 122.82
train mean loss: 124.65
epoch train time: 0:00:00.372216
elapsed time: 0:01:56.737887
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-10-01 14:21:43.982734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.73
 ---- batch: 020 ----
mean loss: 121.40
train mean loss: 120.98
epoch train time: 0:00:00.351766
elapsed time: 0:01:57.090241
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-10-01 14:21:44.335082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.44
 ---- batch: 020 ----
mean loss: 118.53
train mean loss: 120.21
epoch train time: 0:00:00.357294
elapsed time: 0:01:57.447724
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-10-01 14:21:44.692558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.46
 ---- batch: 020 ----
mean loss: 110.69
train mean loss: 114.54
epoch train time: 0:00:00.368578
elapsed time: 0:01:57.816485
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-10-01 14:21:45.061319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.78
 ---- batch: 020 ----
mean loss: 114.25
train mean loss: 114.37
epoch train time: 0:00:00.359737
elapsed time: 0:01:58.176407
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-10-01 14:21:45.421272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.96
 ---- batch: 020 ----
mean loss: 111.91
train mean loss: 113.75
epoch train time: 0:00:00.343349
elapsed time: 0:01:58.519968
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-10-01 14:21:45.764872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.69
 ---- batch: 020 ----
mean loss: 113.72
train mean loss: 110.60
epoch train time: 0:00:00.367100
elapsed time: 0:01:58.887330
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-10-01 14:21:46.132167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.93
 ---- batch: 020 ----
mean loss: 108.29
train mean loss: 110.47
epoch train time: 0:00:00.362310
elapsed time: 0:01:59.249827
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-10-01 14:21:46.494661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.25
 ---- batch: 020 ----
mean loss: 108.22
train mean loss: 108.65
epoch train time: 0:00:00.360553
elapsed time: 0:01:59.610563
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-10-01 14:21:46.855444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.80
 ---- batch: 020 ----
mean loss: 106.44
train mean loss: 105.74
epoch train time: 0:00:00.354377
elapsed time: 0:01:59.965189
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-10-01 14:21:47.210019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.95
 ---- batch: 020 ----
mean loss: 102.72
train mean loss: 103.93
epoch train time: 0:00:00.359787
elapsed time: 0:02:00.325153
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-10-01 14:21:47.569979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.49
 ---- batch: 020 ----
mean loss: 106.13
train mean loss: 102.69
epoch train time: 0:00:00.387516
elapsed time: 0:02:00.712850
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-10-01 14:21:47.957682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.58
 ---- batch: 020 ----
mean loss: 99.78
train mean loss: 100.94
epoch train time: 0:00:00.352335
elapsed time: 0:02:01.065393
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-10-01 14:21:48.310262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.88
 ---- batch: 020 ----
mean loss: 102.14
train mean loss: 101.41
epoch train time: 0:00:00.349698
elapsed time: 0:02:01.415319
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-10-01 14:21:48.660166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.75
 ---- batch: 020 ----
mean loss: 101.04
train mean loss: 98.98
epoch train time: 0:00:00.371060
elapsed time: 0:02:01.786568
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-10-01 14:21:49.031397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.22
 ---- batch: 020 ----
mean loss: 97.78
train mean loss: 97.21
epoch train time: 0:00:00.359548
elapsed time: 0:02:02.146336
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-10-01 14:21:49.391148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.38
 ---- batch: 020 ----
mean loss: 100.03
train mean loss: 95.04
epoch train time: 0:00:00.370029
elapsed time: 0:02:02.516564
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-10-01 14:21:49.761392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.04
 ---- batch: 020 ----
mean loss: 94.82
train mean loss: 96.86
epoch train time: 0:00:00.367491
elapsed time: 0:02:02.884249
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-10-01 14:21:50.129170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.78
 ---- batch: 020 ----
mean loss: 98.35
train mean loss: 95.97
epoch train time: 0:00:00.357151
elapsed time: 0:02:03.241719
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-10-01 14:21:50.486549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.08
 ---- batch: 020 ----
mean loss: 91.74
train mean loss: 91.04
epoch train time: 0:00:00.365041
elapsed time: 0:02:03.607038
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-10-01 14:21:50.851878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.49
 ---- batch: 020 ----
mean loss: 88.68
train mean loss: 93.06
epoch train time: 0:00:00.375982
elapsed time: 0:02:03.983239
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-10-01 14:21:51.228086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.79
 ---- batch: 020 ----
mean loss: 92.19
train mean loss: 90.50
epoch train time: 0:00:00.343626
elapsed time: 0:02:04.327087
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-10-01 14:21:51.571912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.18
 ---- batch: 020 ----
mean loss: 92.82
train mean loss: 90.16
epoch train time: 0:00:00.356966
elapsed time: 0:02:04.684228
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-10-01 14:21:51.929051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.43
 ---- batch: 020 ----
mean loss: 88.40
train mean loss: 89.65
epoch train time: 0:00:00.345425
elapsed time: 0:02:05.029820
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-10-01 14:21:52.274652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.37
 ---- batch: 020 ----
mean loss: 89.55
train mean loss: 88.27
epoch train time: 0:00:00.348593
elapsed time: 0:02:05.378650
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-10-01 14:21:52.623484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.77
 ---- batch: 020 ----
mean loss: 83.05
train mean loss: 86.47
epoch train time: 0:00:00.361922
elapsed time: 0:02:05.740750
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-10-01 14:21:52.985594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.16
 ---- batch: 020 ----
mean loss: 87.33
train mean loss: 86.84
epoch train time: 0:00:00.343914
elapsed time: 0:02:06.084849
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-10-01 14:21:53.329678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.65
 ---- batch: 020 ----
mean loss: 86.75
train mean loss: 87.17
epoch train time: 0:00:00.348168
elapsed time: 0:02:06.433244
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-10-01 14:21:53.678075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.10
 ---- batch: 020 ----
mean loss: 85.94
train mean loss: 85.05
epoch train time: 0:00:00.364115
elapsed time: 0:02:06.797869
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-10-01 14:21:54.042736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.20
 ---- batch: 020 ----
mean loss: 86.14
train mean loss: 84.40
epoch train time: 0:00:00.363999
elapsed time: 0:02:07.162083
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-10-01 14:21:54.406917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.76
 ---- batch: 020 ----
mean loss: 82.39
train mean loss: 83.77
epoch train time: 0:00:00.375321
elapsed time: 0:02:07.537584
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-10-01 14:21:54.782415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.02
 ---- batch: 020 ----
mean loss: 82.63
train mean loss: 82.22
epoch train time: 0:00:00.358874
elapsed time: 0:02:07.896633
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-10-01 14:21:55.141477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.90
 ---- batch: 020 ----
mean loss: 80.09
train mean loss: 82.19
epoch train time: 0:00:00.353310
elapsed time: 0:02:08.250136
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-10-01 14:21:55.494960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.45
 ---- batch: 020 ----
mean loss: 82.84
train mean loss: 81.88
epoch train time: 0:00:00.368555
elapsed time: 0:02:08.618864
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-10-01 14:21:55.863698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.27
 ---- batch: 020 ----
mean loss: 77.97
train mean loss: 78.92
epoch train time: 0:00:00.355459
elapsed time: 0:02:08.974506
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-10-01 14:21:56.219334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.71
 ---- batch: 020 ----
mean loss: 79.43
train mean loss: 78.24
epoch train time: 0:00:00.355823
elapsed time: 0:02:09.330577
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-10-01 14:21:56.575387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.65
 ---- batch: 020 ----
mean loss: 77.72
train mean loss: 78.77
epoch train time: 0:00:00.363383
elapsed time: 0:02:09.694118
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-10-01 14:21:56.938948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.47
 ---- batch: 020 ----
mean loss: 78.88
train mean loss: 80.16
epoch train time: 0:00:00.360233
elapsed time: 0:02:10.054537
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-10-01 14:21:57.299369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.73
 ---- batch: 020 ----
mean loss: 80.20
train mean loss: 78.45
epoch train time: 0:00:00.358335
elapsed time: 0:02:10.413073
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-10-01 14:21:57.657909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.42
 ---- batch: 020 ----
mean loss: 77.67
train mean loss: 76.62
epoch train time: 0:00:00.365288
elapsed time: 0:02:10.778543
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-10-01 14:21:58.023377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.24
 ---- batch: 020 ----
mean loss: 75.93
train mean loss: 76.45
epoch train time: 0:00:00.363305
elapsed time: 0:02:11.142038
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-10-01 14:21:58.386872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.21
 ---- batch: 020 ----
mean loss: 80.67
train mean loss: 77.32
epoch train time: 0:00:00.362111
elapsed time: 0:02:11.504348
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-10-01 14:21:58.749197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.19
 ---- batch: 020 ----
mean loss: 77.19
train mean loss: 77.53
epoch train time: 0:00:00.364051
elapsed time: 0:02:11.868595
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-10-01 14:21:59.113446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.73
 ---- batch: 020 ----
mean loss: 75.03
train mean loss: 74.80
epoch train time: 0:00:00.364994
elapsed time: 0:02:12.233793
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-10-01 14:21:59.478630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.24
 ---- batch: 020 ----
mean loss: 81.45
train mean loss: 75.59
epoch train time: 0:00:00.351988
elapsed time: 0:02:12.585958
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-10-01 14:21:59.830801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.87
 ---- batch: 020 ----
mean loss: 73.87
train mean loss: 74.32
epoch train time: 0:00:00.347675
elapsed time: 0:02:12.933825
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-10-01 14:22:00.178652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.62
 ---- batch: 020 ----
mean loss: 75.06
train mean loss: 74.94
epoch train time: 0:00:00.343063
elapsed time: 0:02:13.277082
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-10-01 14:22:00.521914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.68
 ---- batch: 020 ----
mean loss: 72.84
train mean loss: 73.89
epoch train time: 0:00:00.352559
elapsed time: 0:02:13.629827
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-10-01 14:22:00.874655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.27
 ---- batch: 020 ----
mean loss: 72.22
train mean loss: 72.90
epoch train time: 0:00:00.356878
elapsed time: 0:02:13.986894
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-10-01 14:22:01.231766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.82
 ---- batch: 020 ----
mean loss: 78.25
train mean loss: 73.80
epoch train time: 0:00:00.356344
elapsed time: 0:02:14.343457
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-10-01 14:22:01.588287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.70
 ---- batch: 020 ----
mean loss: 70.71
train mean loss: 73.20
epoch train time: 0:00:00.371186
elapsed time: 0:02:14.714817
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-10-01 14:22:01.959646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.69
 ---- batch: 020 ----
mean loss: 71.49
train mean loss: 74.15
epoch train time: 0:00:00.349434
elapsed time: 0:02:15.064427
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-10-01 14:22:02.309290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.69
 ---- batch: 020 ----
mean loss: 71.84
train mean loss: 73.28
epoch train time: 0:00:00.351845
elapsed time: 0:02:15.416483
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-10-01 14:22:02.661316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.26
 ---- batch: 020 ----
mean loss: 73.42
train mean loss: 72.20
epoch train time: 0:00:00.357664
elapsed time: 0:02:15.774326
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-10-01 14:22:03.019157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.63
 ---- batch: 020 ----
mean loss: 71.33
train mean loss: 71.09
epoch train time: 0:00:00.352033
elapsed time: 0:02:16.126532
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-10-01 14:22:03.371362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.96
 ---- batch: 020 ----
mean loss: 70.84
train mean loss: 70.24
epoch train time: 0:00:00.360310
elapsed time: 0:02:16.487058
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-10-01 14:22:03.731899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.11
 ---- batch: 020 ----
mean loss: 66.70
train mean loss: 70.53
epoch train time: 0:00:00.357916
elapsed time: 0:02:16.845226
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-10-01 14:22:04.090089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.31
 ---- batch: 020 ----
mean loss: 70.42
train mean loss: 71.36
epoch train time: 0:00:00.351854
elapsed time: 0:02:17.197366
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-10-01 14:22:04.443190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.77
 ---- batch: 020 ----
mean loss: 69.35
train mean loss: 69.49
epoch train time: 0:00:00.351161
elapsed time: 0:02:17.549694
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-10-01 14:22:04.794538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.25
 ---- batch: 020 ----
mean loss: 69.36
train mean loss: 69.72
epoch train time: 0:00:00.352067
elapsed time: 0:02:17.901994
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-10-01 14:22:05.146858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.30
 ---- batch: 020 ----
mean loss: 70.93
train mean loss: 69.97
epoch train time: 0:00:00.348024
elapsed time: 0:02:18.250235
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-10-01 14:22:05.495081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.36
 ---- batch: 020 ----
mean loss: 68.76
train mean loss: 68.81
epoch train time: 0:00:00.360464
elapsed time: 0:02:18.610893
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-10-01 14:22:05.855740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.96
 ---- batch: 020 ----
mean loss: 66.65
train mean loss: 67.57
epoch train time: 0:00:00.350980
elapsed time: 0:02:18.962081
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-10-01 14:22:06.206918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.22
 ---- batch: 020 ----
mean loss: 66.96
train mean loss: 68.72
epoch train time: 0:00:00.344422
elapsed time: 0:02:19.306692
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-10-01 14:22:06.551546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.00
 ---- batch: 020 ----
mean loss: 69.88
train mean loss: 68.26
epoch train time: 0:00:00.358828
elapsed time: 0:02:19.665737
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-10-01 14:22:06.910586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.54
 ---- batch: 020 ----
mean loss: 68.35
train mean loss: 67.22
epoch train time: 0:00:00.360650
elapsed time: 0:02:20.026589
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-10-01 14:22:07.271422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.47
 ---- batch: 020 ----
mean loss: 65.68
train mean loss: 67.22
epoch train time: 0:00:00.351086
elapsed time: 0:02:20.377860
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-10-01 14:22:07.622688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.23
 ---- batch: 020 ----
mean loss: 64.96
train mean loss: 66.63
epoch train time: 0:00:00.371995
elapsed time: 0:02:20.750042
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-10-01 14:22:07.994875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.69
 ---- batch: 020 ----
mean loss: 68.42
train mean loss: 67.74
epoch train time: 0:00:00.364100
elapsed time: 0:02:21.114322
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-10-01 14:22:08.359150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.68
 ---- batch: 020 ----
mean loss: 65.96
train mean loss: 66.99
epoch train time: 0:00:00.362408
elapsed time: 0:02:21.476926
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-10-01 14:22:08.721806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.03
 ---- batch: 020 ----
mean loss: 68.32
train mean loss: 67.13
epoch train time: 0:00:00.371140
elapsed time: 0:02:21.848307
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-10-01 14:22:09.093140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.29
 ---- batch: 020 ----
mean loss: 67.39
train mean loss: 65.76
epoch train time: 0:00:00.357918
elapsed time: 0:02:22.206861
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-10-01 14:22:09.451701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.52
 ---- batch: 020 ----
mean loss: 63.37
train mean loss: 64.67
epoch train time: 0:00:00.369420
elapsed time: 0:02:22.576473
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-10-01 14:22:09.821306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.20
 ---- batch: 020 ----
mean loss: 66.41
train mean loss: 66.04
epoch train time: 0:00:00.353176
elapsed time: 0:02:22.929828
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-10-01 14:22:10.174655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.32
 ---- batch: 020 ----
mean loss: 63.36
train mean loss: 64.50
epoch train time: 0:00:00.346110
elapsed time: 0:02:23.276105
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-10-01 14:22:10.520933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.44
 ---- batch: 020 ----
mean loss: 61.63
train mean loss: 64.65
epoch train time: 0:00:00.363069
elapsed time: 0:02:23.639344
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-10-01 14:22:10.884186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.26
 ---- batch: 020 ----
mean loss: 62.72
train mean loss: 64.17
epoch train time: 0:00:00.344836
elapsed time: 0:02:23.984377
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-10-01 14:22:11.229203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.24
 ---- batch: 020 ----
mean loss: 63.71
train mean loss: 63.96
epoch train time: 0:00:00.350402
elapsed time: 0:02:24.334954
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-10-01 14:22:11.579784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.62
 ---- batch: 020 ----
mean loss: 63.12
train mean loss: 64.13
epoch train time: 0:00:00.374474
elapsed time: 0:02:24.709609
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-10-01 14:22:11.954441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.31
 ---- batch: 020 ----
mean loss: 65.96
train mean loss: 63.80
epoch train time: 0:00:00.361978
elapsed time: 0:02:25.071770
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-10-01 14:22:12.316602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.26
 ---- batch: 020 ----
mean loss: 63.07
train mean loss: 63.17
epoch train time: 0:00:00.360441
elapsed time: 0:02:25.432390
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-10-01 14:22:12.677226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.46
 ---- batch: 020 ----
mean loss: 63.24
train mean loss: 62.31
epoch train time: 0:00:00.360445
elapsed time: 0:02:25.793044
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-10-01 14:22:13.037879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.38
 ---- batch: 020 ----
mean loss: 61.84
train mean loss: 62.78
epoch train time: 0:00:00.352989
elapsed time: 0:02:26.146244
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-10-01 14:22:13.391070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.05
 ---- batch: 020 ----
mean loss: 63.48
train mean loss: 61.37
epoch train time: 0:00:00.352323
elapsed time: 0:02:26.498746
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-10-01 14:22:13.743576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.69
 ---- batch: 020 ----
mean loss: 62.97
train mean loss: 62.10
epoch train time: 0:00:00.370750
elapsed time: 0:02:26.869672
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-10-01 14:22:14.114500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.14
 ---- batch: 020 ----
mean loss: 62.89
train mean loss: 62.67
epoch train time: 0:00:00.357381
elapsed time: 0:02:27.227774
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-10-01 14:22:14.472612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.34
 ---- batch: 020 ----
mean loss: 60.12
train mean loss: 60.36
epoch train time: 0:00:00.356281
elapsed time: 0:02:27.584255
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-10-01 14:22:14.829086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.51
 ---- batch: 020 ----
mean loss: 62.72
train mean loss: 61.26
epoch train time: 0:00:00.356811
elapsed time: 0:02:27.941245
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-10-01 14:22:15.186071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.87
 ---- batch: 020 ----
mean loss: 61.56
train mean loss: 61.52
epoch train time: 0:00:00.355946
elapsed time: 0:02:28.297386
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-10-01 14:22:15.542244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.66
 ---- batch: 020 ----
mean loss: 62.30
train mean loss: 60.64
epoch train time: 0:00:00.347881
elapsed time: 0:02:28.645465
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-10-01 14:22:15.890322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.19
 ---- batch: 020 ----
mean loss: 60.77
train mean loss: 60.85
epoch train time: 0:00:00.362997
elapsed time: 0:02:29.008695
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-10-01 14:22:16.253529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.55
 ---- batch: 020 ----
mean loss: 62.24
train mean loss: 61.35
epoch train time: 0:00:00.355903
elapsed time: 0:02:29.364779
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-10-01 14:22:16.609646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.52
 ---- batch: 020 ----
mean loss: 59.83
train mean loss: 60.15
epoch train time: 0:00:00.376154
elapsed time: 0:02:29.741155
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-10-01 14:22:16.985986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.58
 ---- batch: 020 ----
mean loss: 58.49
train mean loss: 59.95
epoch train time: 0:00:00.364733
elapsed time: 0:02:30.106070
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-10-01 14:22:17.350904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.63
 ---- batch: 020 ----
mean loss: 60.03
train mean loss: 59.66
epoch train time: 0:00:00.357046
elapsed time: 0:02:30.463295
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-10-01 14:22:17.708125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.57
 ---- batch: 020 ----
mean loss: 60.44
train mean loss: 59.65
epoch train time: 0:00:00.354888
elapsed time: 0:02:30.818355
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-10-01 14:22:18.063184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.68
 ---- batch: 020 ----
mean loss: 60.38
train mean loss: 59.22
epoch train time: 0:00:00.354842
elapsed time: 0:02:31.173386
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-10-01 14:22:18.418244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.52
 ---- batch: 020 ----
mean loss: 60.73
train mean loss: 59.80
epoch train time: 0:00:00.356430
elapsed time: 0:02:31.530030
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-10-01 14:22:18.774896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.24
 ---- batch: 020 ----
mean loss: 58.02
train mean loss: 58.07
epoch train time: 0:00:00.359747
elapsed time: 0:02:31.889990
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-10-01 14:22:19.134826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.33
 ---- batch: 020 ----
mean loss: 58.53
train mean loss: 57.62
epoch train time: 0:00:00.368595
elapsed time: 0:02:32.258793
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-10-01 14:22:19.503630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.98
 ---- batch: 020 ----
mean loss: 57.30
train mean loss: 58.92
epoch train time: 0:00:00.357264
elapsed time: 0:02:32.616265
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-10-01 14:22:19.861104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.26
 ---- batch: 020 ----
mean loss: 56.23
train mean loss: 58.75
epoch train time: 0:00:00.362226
elapsed time: 0:02:32.978691
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-10-01 14:22:20.223524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.50
 ---- batch: 020 ----
mean loss: 58.70
train mean loss: 57.19
epoch train time: 0:00:00.357504
elapsed time: 0:02:33.336417
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-10-01 14:22:20.581246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.38
 ---- batch: 020 ----
mean loss: 60.67
train mean loss: 57.66
epoch train time: 0:00:00.375751
elapsed time: 0:02:33.712345
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-10-01 14:22:20.957176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.75
 ---- batch: 020 ----
mean loss: 57.97
train mean loss: 57.05
epoch train time: 0:00:00.359800
elapsed time: 0:02:34.072353
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-10-01 14:22:21.317228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.72
 ---- batch: 020 ----
mean loss: 56.69
train mean loss: 55.77
epoch train time: 0:00:00.357461
elapsed time: 0:02:34.430076
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-10-01 14:22:21.674910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.54
 ---- batch: 020 ----
mean loss: 57.44
train mean loss: 56.32
epoch train time: 0:00:00.349726
elapsed time: 0:02:34.779985
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-10-01 14:22:22.024819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.83
 ---- batch: 020 ----
mean loss: 57.09
train mean loss: 56.74
epoch train time: 0:00:00.350553
elapsed time: 0:02:35.130716
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-10-01 14:22:22.375558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.42
 ---- batch: 020 ----
mean loss: 57.34
train mean loss: 56.19
epoch train time: 0:00:00.351694
elapsed time: 0:02:35.482607
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-10-01 14:22:22.727434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.26
 ---- batch: 020 ----
mean loss: 56.85
train mean loss: 56.45
epoch train time: 0:00:00.354954
elapsed time: 0:02:35.837731
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-10-01 14:22:23.082560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.55
 ---- batch: 020 ----
mean loss: 54.93
train mean loss: 55.41
epoch train time: 0:00:00.353429
elapsed time: 0:02:36.191360
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-10-01 14:22:23.436170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.43
 ---- batch: 020 ----
mean loss: 58.24
train mean loss: 55.91
epoch train time: 0:00:00.358894
elapsed time: 0:02:36.550408
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-10-01 14:22:23.795240
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.31
 ---- batch: 020 ----
mean loss: 55.48
train mean loss: 55.18
epoch train time: 0:00:00.370866
elapsed time: 0:02:36.921468
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-10-01 14:22:24.166306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.63
 ---- batch: 020 ----
mean loss: 55.61
train mean loss: 55.79
epoch train time: 0:00:00.364956
elapsed time: 0:02:37.286607
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-10-01 14:22:24.531439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.16
 ---- batch: 020 ----
mean loss: 56.17
train mean loss: 54.69
epoch train time: 0:00:00.357198
elapsed time: 0:02:37.644001
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-10-01 14:22:24.888840
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.75
 ---- batch: 020 ----
mean loss: 56.63
train mean loss: 54.97
epoch train time: 0:00:00.347666
elapsed time: 0:02:37.991855
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-10-01 14:22:25.236687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.06
 ---- batch: 020 ----
mean loss: 53.95
train mean loss: 54.31
epoch train time: 0:00:00.346228
elapsed time: 0:02:38.338269
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-10-01 14:22:25.583096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.19
 ---- batch: 020 ----
mean loss: 53.77
train mean loss: 54.72
epoch train time: 0:00:00.367498
elapsed time: 0:02:38.705952
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-10-01 14:22:25.950788
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.75
 ---- batch: 020 ----
mean loss: 54.76
train mean loss: 55.04
epoch train time: 0:00:00.357416
elapsed time: 0:02:39.063553
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-10-01 14:22:26.308384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.43
 ---- batch: 020 ----
mean loss: 55.23
train mean loss: 54.16
epoch train time: 0:00:00.352775
elapsed time: 0:02:39.416509
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-10-01 14:22:26.661357
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.02
 ---- batch: 020 ----
mean loss: 53.73
train mean loss: 54.72
epoch train time: 0:00:00.358011
elapsed time: 0:02:39.774731
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-10-01 14:22:27.019562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.10
 ---- batch: 020 ----
mean loss: 53.88
train mean loss: 54.87
epoch train time: 0:00:00.353941
elapsed time: 0:02:40.128866
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-10-01 14:22:27.373774
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.23
 ---- batch: 020 ----
mean loss: 55.82
train mean loss: 54.47
epoch train time: 0:00:00.353515
elapsed time: 0:02:40.482653
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-10-01 14:22:27.727493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.95
 ---- batch: 020 ----
mean loss: 55.20
train mean loss: 54.53
epoch train time: 0:00:00.365153
elapsed time: 0:02:40.847989
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-10-01 14:22:28.092834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.01
 ---- batch: 020 ----
mean loss: 54.85
train mean loss: 54.77
epoch train time: 0:00:00.351814
elapsed time: 0:02:41.199985
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-10-01 14:22:28.444844
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.66
 ---- batch: 020 ----
mean loss: 55.57
train mean loss: 54.04
epoch train time: 0:00:00.344133
elapsed time: 0:02:41.544316
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-10-01 14:22:28.789142
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.56
 ---- batch: 020 ----
mean loss: 56.33
train mean loss: 54.92
epoch train time: 0:00:00.355880
elapsed time: 0:02:41.900398
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-10-01 14:22:29.145245
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.38
 ---- batch: 020 ----
mean loss: 55.21
train mean loss: 54.95
epoch train time: 0:00:00.361808
elapsed time: 0:02:42.262916
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-10-01 14:22:29.507759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.18
 ---- batch: 020 ----
mean loss: 56.16
train mean loss: 55.29
epoch train time: 0:00:00.356534
elapsed time: 0:02:42.619661
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-10-01 14:22:29.864493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.84
 ---- batch: 020 ----
mean loss: 53.97
train mean loss: 54.95
epoch train time: 0:00:00.353296
elapsed time: 0:02:42.973139
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-10-01 14:22:30.217972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.84
 ---- batch: 020 ----
mean loss: 54.73
train mean loss: 54.71
epoch train time: 0:00:00.363916
elapsed time: 0:02:43.337256
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-10-01 14:22:30.582092
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.10
 ---- batch: 020 ----
mean loss: 54.64
train mean loss: 54.38
epoch train time: 0:00:00.355008
elapsed time: 0:02:43.692439
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-10-01 14:22:30.937263
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.59
 ---- batch: 020 ----
mean loss: 53.66
train mean loss: 53.92
epoch train time: 0:00:00.353405
elapsed time: 0:02:44.046015
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-10-01 14:22:31.290853
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.57
 ---- batch: 020 ----
mean loss: 54.52
train mean loss: 54.35
epoch train time: 0:00:00.353909
elapsed time: 0:02:44.400127
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-10-01 14:22:31.644961
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.11
 ---- batch: 020 ----
mean loss: 55.79
train mean loss: 54.56
epoch train time: 0:00:00.365172
elapsed time: 0:02:44.765504
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-10-01 14:22:32.010354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.83
 ---- batch: 020 ----
mean loss: 54.09
train mean loss: 53.77
epoch train time: 0:00:00.367854
elapsed time: 0:02:45.133611
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-10-01 14:22:32.378442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.05
 ---- batch: 020 ----
mean loss: 54.17
train mean loss: 54.57
epoch train time: 0:00:00.361211
elapsed time: 0:02:45.495016
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-10-01 14:22:32.739854
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 51.33
 ---- batch: 020 ----
mean loss: 56.99
train mean loss: 54.85
epoch train time: 0:00:00.370030
elapsed time: 0:02:45.865231
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-10-01 14:22:33.110059
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.05
 ---- batch: 020 ----
mean loss: 55.98
train mean loss: 54.95
epoch train time: 0:00:00.361429
elapsed time: 0:02:46.226846
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-10-01 14:22:33.471702
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.33
 ---- batch: 020 ----
mean loss: 55.80
train mean loss: 54.56
epoch train time: 0:00:00.357864
elapsed time: 0:02:46.584912
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-10-01 14:22:33.829746
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.49
 ---- batch: 020 ----
mean loss: 56.30
train mean loss: 54.13
epoch train time: 0:00:00.361873
elapsed time: 0:02:46.946971
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-10-01 14:22:34.191806
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.80
 ---- batch: 020 ----
mean loss: 53.89
train mean loss: 54.95
epoch train time: 0:00:00.359960
elapsed time: 0:02:47.307110
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-10-01 14:22:34.551941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.94
 ---- batch: 020 ----
mean loss: 53.03
train mean loss: 54.35
epoch train time: 0:00:00.367411
elapsed time: 0:02:47.674731
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-10-01 14:22:34.919542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.76
 ---- batch: 020 ----
mean loss: 54.86
train mean loss: 53.72
epoch train time: 0:00:00.361880
elapsed time: 0:02:48.036769
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-10-01 14:22:35.281599
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.41
 ---- batch: 020 ----
mean loss: 53.82
train mean loss: 52.67
epoch train time: 0:00:00.345897
elapsed time: 0:02:48.382843
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-10-01 14:22:35.627679
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.88
 ---- batch: 020 ----
mean loss: 54.42
train mean loss: 54.07
epoch train time: 0:00:00.350237
elapsed time: 0:02:48.733277
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-10-01 14:22:35.978106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.66
 ---- batch: 020 ----
mean loss: 54.31
train mean loss: 53.20
epoch train time: 0:00:00.350519
elapsed time: 0:02:49.083963
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-10-01 14:22:36.328792
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.50
 ---- batch: 020 ----
mean loss: 51.41
train mean loss: 53.62
epoch train time: 0:00:00.341252
elapsed time: 0:02:49.425413
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-10-01 14:22:36.670246
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.46
 ---- batch: 020 ----
mean loss: 56.01
train mean loss: 53.67
epoch train time: 0:00:00.366965
elapsed time: 0:02:49.792578
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-10-01 14:22:37.037431
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.53
 ---- batch: 020 ----
mean loss: 52.50
train mean loss: 53.86
epoch train time: 0:00:00.364504
elapsed time: 0:02:50.157285
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-10-01 14:22:37.402118
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.91
 ---- batch: 020 ----
mean loss: 54.51
train mean loss: 53.94
epoch train time: 0:00:00.364312
elapsed time: 0:02:50.521784
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-10-01 14:22:37.766620
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.39
 ---- batch: 020 ----
mean loss: 55.10
train mean loss: 54.38
epoch train time: 0:00:00.364510
elapsed time: 0:02:50.886480
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-10-01 14:22:38.131326
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.32
 ---- batch: 020 ----
mean loss: 54.75
train mean loss: 54.91
epoch train time: 0:00:00.358189
elapsed time: 0:02:51.244865
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-10-01 14:22:38.489697
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.53
 ---- batch: 020 ----
mean loss: 54.96
train mean loss: 53.05
epoch train time: 0:00:00.358854
elapsed time: 0:02:51.603893
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-10-01 14:22:38.848723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.45
 ---- batch: 020 ----
mean loss: 56.23
train mean loss: 54.37
epoch train time: 0:00:00.353444
elapsed time: 0:02:51.957529
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-10-01 14:22:39.202376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.11
 ---- batch: 020 ----
mean loss: 52.22
train mean loss: 53.14
epoch train time: 0:00:00.355153
elapsed time: 0:02:52.312876
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-10-01 14:22:39.557710
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.47
 ---- batch: 020 ----
mean loss: 53.65
train mean loss: 54.13
epoch train time: 0:00:00.357949
elapsed time: 0:02:52.671017
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-10-01 14:22:39.915847
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.14
 ---- batch: 020 ----
mean loss: 53.36
train mean loss: 53.08
epoch train time: 0:00:00.361971
elapsed time: 0:02:53.033169
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-10-01 14:22:40.278017
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 51.63
 ---- batch: 020 ----
mean loss: 55.27
train mean loss: 53.97
epoch train time: 0:00:00.358073
elapsed time: 0:02:53.399358
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_0/checkpoint.pth.tar
**** end time: 2019-10-01 14:22:40.644145 ****
