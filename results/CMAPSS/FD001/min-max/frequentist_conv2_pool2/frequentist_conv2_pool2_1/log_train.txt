Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_1', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31504
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:41:04.346491 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:41:04.351532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4197.01
 ---- batch: 020 ----
mean loss: 4032.15
 ---- batch: 030 ----
mean loss: 4083.76
train mean loss: 4099.52
epoch train time: 0:00:12.551502
elapsed time: 0:00:12.557892
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:41:16.904423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4027.40
 ---- batch: 020 ----
mean loss: 3947.04
 ---- batch: 030 ----
mean loss: 3924.83
train mean loss: 3960.40
epoch train time: 0:00:00.179983
elapsed time: 0:00:12.738016
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:41:17.084566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3836.13
 ---- batch: 020 ----
mean loss: 3756.71
 ---- batch: 030 ----
mean loss: 3699.90
train mean loss: 3741.18
epoch train time: 0:00:00.173808
elapsed time: 0:00:12.911971
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:41:17.258534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3546.71
 ---- batch: 020 ----
mean loss: 3446.38
 ---- batch: 030 ----
mean loss: 3408.61
train mean loss: 3456.89
epoch train time: 0:00:00.175195
elapsed time: 0:00:13.087341
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:41:17.433881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3228.29
 ---- batch: 020 ----
mean loss: 3148.91
 ---- batch: 030 ----
mean loss: 3197.64
train mean loss: 3174.51
epoch train time: 0:00:00.177439
elapsed time: 0:00:13.264950
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:41:17.611492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3009.20
 ---- batch: 020 ----
mean loss: 2925.17
 ---- batch: 030 ----
mean loss: 2895.88
train mean loss: 2929.14
epoch train time: 0:00:00.179653
elapsed time: 0:00:13.444745
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:41:17.791287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2806.22
 ---- batch: 020 ----
mean loss: 2718.00
 ---- batch: 030 ----
mean loss: 2654.50
train mean loss: 2712.43
epoch train time: 0:00:00.178259
elapsed time: 0:00:13.623199
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:41:17.969763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2583.06
 ---- batch: 020 ----
mean loss: 2552.29
 ---- batch: 030 ----
mean loss: 2454.71
train mean loss: 2518.35
epoch train time: 0:00:00.179128
elapsed time: 0:00:13.802496
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:41:18.149052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2400.80
 ---- batch: 020 ----
mean loss: 2355.65
 ---- batch: 030 ----
mean loss: 2316.75
train mean loss: 2346.47
epoch train time: 0:00:00.174140
elapsed time: 0:00:13.976824
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:41:18.323364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2258.17
 ---- batch: 020 ----
mean loss: 2192.37
 ---- batch: 030 ----
mean loss: 2161.65
train mean loss: 2190.01
epoch train time: 0:00:00.173478
elapsed time: 0:00:14.150526
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:41:18.497067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2084.54
 ---- batch: 020 ----
mean loss: 2071.78
 ---- batch: 030 ----
mean loss: 2011.82
train mean loss: 2045.22
epoch train time: 0:00:00.175491
elapsed time: 0:00:14.326166
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:41:18.672708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1953.55
 ---- batch: 020 ----
mean loss: 1935.87
 ---- batch: 030 ----
mean loss: 1865.79
train mean loss: 1915.99
epoch train time: 0:00:00.195648
elapsed time: 0:00:14.521967
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:41:18.868512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1844.57
 ---- batch: 020 ----
mean loss: 1789.81
 ---- batch: 030 ----
mean loss: 1788.41
train mean loss: 1793.66
epoch train time: 0:00:00.188212
elapsed time: 0:00:14.710343
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:41:19.056904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1750.90
 ---- batch: 020 ----
mean loss: 1697.29
 ---- batch: 030 ----
mean loss: 1638.64
train mean loss: 1681.54
epoch train time: 0:00:00.180768
elapsed time: 0:00:14.891275
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:41:19.237818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1609.87
 ---- batch: 020 ----
mean loss: 1588.98
 ---- batch: 030 ----
mean loss: 1559.49
train mean loss: 1579.38
epoch train time: 0:00:00.177822
elapsed time: 0:00:15.069273
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:41:19.415815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1509.27
 ---- batch: 020 ----
mean loss: 1483.75
 ---- batch: 030 ----
mean loss: 1471.97
train mean loss: 1483.05
epoch train time: 0:00:00.180164
elapsed time: 0:00:15.249606
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:41:19.596177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1429.09
 ---- batch: 020 ----
mean loss: 1393.49
 ---- batch: 030 ----
mean loss: 1375.88
train mean loss: 1393.84
epoch train time: 0:00:00.180265
elapsed time: 0:00:15.430074
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:41:19.776620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1343.38
 ---- batch: 020 ----
mean loss: 1320.34
 ---- batch: 030 ----
mean loss: 1276.83
train mean loss: 1312.83
epoch train time: 0:00:00.179063
elapsed time: 0:00:15.609283
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:41:19.955825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1268.06
 ---- batch: 020 ----
mean loss: 1246.27
 ---- batch: 030 ----
mean loss: 1213.19
train mean loss: 1236.56
epoch train time: 0:00:00.178933
elapsed time: 0:00:15.788388
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:41:20.134964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1195.41
 ---- batch: 020 ----
mean loss: 1167.26
 ---- batch: 030 ----
mean loss: 1148.68
train mean loss: 1166.05
epoch train time: 0:00:00.178838
elapsed time: 0:00:15.967403
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:41:20.313945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1112.32
 ---- batch: 020 ----
mean loss: 1105.19
 ---- batch: 030 ----
mean loss: 1087.07
train mean loss: 1102.19
epoch train time: 0:00:00.178068
elapsed time: 0:00:16.145618
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:41:20.492163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1055.26
 ---- batch: 020 ----
mean loss: 1066.59
 ---- batch: 030 ----
mean loss: 1030.62
train mean loss: 1042.07
epoch train time: 0:00:00.178555
elapsed time: 0:00:16.324330
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:41:20.670880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1015.00
 ---- batch: 020 ----
mean loss: 989.74
 ---- batch: 030 ----
mean loss: 979.14
train mean loss: 986.85
epoch train time: 0:00:00.179496
elapsed time: 0:00:16.503987
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:41:20.850530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.40
 ---- batch: 020 ----
mean loss: 952.64
 ---- batch: 030 ----
mean loss: 936.00
train mean loss: 935.41
epoch train time: 0:00:00.170726
elapsed time: 0:00:16.674857
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:41:21.021410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.82
 ---- batch: 020 ----
mean loss: 905.12
 ---- batch: 030 ----
mean loss: 871.58
train mean loss: 888.75
epoch train time: 0:00:00.168590
elapsed time: 0:00:16.843601
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:41:21.190140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.46
 ---- batch: 020 ----
mean loss: 863.93
 ---- batch: 030 ----
mean loss: 841.87
train mean loss: 846.53
epoch train time: 0:00:00.168038
elapsed time: 0:00:17.011776
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:41:21.358316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 823.25
 ---- batch: 020 ----
mean loss: 813.67
 ---- batch: 030 ----
mean loss: 783.53
train mean loss: 807.48
epoch train time: 0:00:00.169302
elapsed time: 0:00:17.181216
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:41:21.527756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 786.60
 ---- batch: 020 ----
mean loss: 773.42
 ---- batch: 030 ----
mean loss: 755.03
train mean loss: 769.95
epoch train time: 0:00:00.168829
elapsed time: 0:00:17.350226
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:41:21.696785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 741.27
 ---- batch: 020 ----
mean loss: 753.74
 ---- batch: 030 ----
mean loss: 728.49
train mean loss: 735.99
epoch train time: 0:00:00.181500
elapsed time: 0:00:17.531888
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:41:21.878430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.35
 ---- batch: 020 ----
mean loss: 705.00
 ---- batch: 030 ----
mean loss: 699.26
train mean loss: 706.02
epoch train time: 0:00:00.176960
elapsed time: 0:00:17.709003
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:41:22.055569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 683.48
 ---- batch: 020 ----
mean loss: 670.79
 ---- batch: 030 ----
mean loss: 686.34
train mean loss: 678.10
epoch train time: 0:00:00.180163
elapsed time: 0:00:17.889375
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:41:22.235931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 662.18
 ---- batch: 020 ----
mean loss: 658.28
 ---- batch: 030 ----
mean loss: 642.41
train mean loss: 651.17
epoch train time: 0:00:00.180945
elapsed time: 0:00:18.070491
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:41:22.417056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 639.50
 ---- batch: 020 ----
mean loss: 626.60
 ---- batch: 030 ----
mean loss: 613.88
train mean loss: 628.04
epoch train time: 0:00:00.178307
elapsed time: 0:00:18.248979
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:41:22.595522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 614.35
 ---- batch: 020 ----
mean loss: 615.34
 ---- batch: 030 ----
mean loss: 596.07
train mean loss: 606.57
epoch train time: 0:00:00.176472
elapsed time: 0:00:18.425603
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:41:22.772146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 588.09
 ---- batch: 020 ----
mean loss: 594.95
 ---- batch: 030 ----
mean loss: 580.20
train mean loss: 586.63
epoch train time: 0:00:00.176774
elapsed time: 0:00:18.602518
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:41:22.949058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.92
 ---- batch: 020 ----
mean loss: 565.95
 ---- batch: 030 ----
mean loss: 574.99
train mean loss: 569.01
epoch train time: 0:00:00.180129
elapsed time: 0:00:18.782789
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:41:23.129331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.17
 ---- batch: 020 ----
mean loss: 554.90
 ---- batch: 030 ----
mean loss: 548.64
train mean loss: 551.44
epoch train time: 0:00:00.173042
elapsed time: 0:00:18.955972
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:41:23.302511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.28
 ---- batch: 020 ----
mean loss: 533.05
 ---- batch: 030 ----
mean loss: 537.26
train mean loss: 536.50
epoch train time: 0:00:00.176836
elapsed time: 0:00:19.132966
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:41:23.479530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.73
 ---- batch: 020 ----
mean loss: 523.89
 ---- batch: 030 ----
mean loss: 517.22
train mean loss: 522.93
epoch train time: 0:00:00.178458
elapsed time: 0:00:19.311592
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:41:23.658133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.22
 ---- batch: 020 ----
mean loss: 509.37
 ---- batch: 030 ----
mean loss: 510.39
train mean loss: 510.00
epoch train time: 0:00:00.179703
elapsed time: 0:00:19.491461
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:41:23.838003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.82
 ---- batch: 020 ----
mean loss: 495.38
 ---- batch: 030 ----
mean loss: 496.78
train mean loss: 498.24
epoch train time: 0:00:00.183111
elapsed time: 0:00:19.674723
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:41:24.021268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.40
 ---- batch: 020 ----
mean loss: 481.05
 ---- batch: 030 ----
mean loss: 488.99
train mean loss: 487.26
epoch train time: 0:00:00.185737
elapsed time: 0:00:19.860604
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:41:24.207147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.46
 ---- batch: 020 ----
mean loss: 475.76
 ---- batch: 030 ----
mean loss: 472.99
train mean loss: 477.19
epoch train time: 0:00:00.181544
elapsed time: 0:00:20.042321
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:41:24.388886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.43
 ---- batch: 020 ----
mean loss: 476.83
 ---- batch: 030 ----
mean loss: 467.00
train mean loss: 468.45
epoch train time: 0:00:00.181852
elapsed time: 0:00:20.224349
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:41:24.570888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.28
 ---- batch: 020 ----
mean loss: 455.51
 ---- batch: 030 ----
mean loss: 462.74
train mean loss: 459.83
epoch train time: 0:00:00.185217
elapsed time: 0:00:20.409703
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:41:24.756249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.50
 ---- batch: 020 ----
mean loss: 450.38
 ---- batch: 030 ----
mean loss: 448.61
train mean loss: 452.39
epoch train time: 0:00:00.184423
elapsed time: 0:00:20.594276
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:41:24.940819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.80
 ---- batch: 020 ----
mean loss: 445.69
 ---- batch: 030 ----
mean loss: 444.33
train mean loss: 444.60
epoch train time: 0:00:00.181383
elapsed time: 0:00:20.775813
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:41:25.122365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.46
 ---- batch: 020 ----
mean loss: 443.19
 ---- batch: 030 ----
mean loss: 435.41
train mean loss: 438.00
epoch train time: 0:00:00.179214
elapsed time: 0:00:20.955203
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:41:25.301748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.75
 ---- batch: 020 ----
mean loss: 432.01
 ---- batch: 030 ----
mean loss: 425.84
train mean loss: 431.43
epoch train time: 0:00:00.178565
elapsed time: 0:00:21.133957
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:41:25.480498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.12
 ---- batch: 020 ----
mean loss: 425.79
 ---- batch: 030 ----
mean loss: 422.37
train mean loss: 426.12
epoch train time: 0:00:00.178473
elapsed time: 0:00:21.312571
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:41:25.659126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.47
 ---- batch: 020 ----
mean loss: 418.97
 ---- batch: 030 ----
mean loss: 422.69
train mean loss: 420.42
epoch train time: 0:00:00.182498
elapsed time: 0:00:21.495228
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:41:25.841784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.63
 ---- batch: 020 ----
mean loss: 407.83
 ---- batch: 030 ----
mean loss: 419.54
train mean loss: 415.57
epoch train time: 0:00:00.187230
elapsed time: 0:00:21.682627
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:41:26.029178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.85
 ---- batch: 020 ----
mean loss: 414.57
 ---- batch: 030 ----
mean loss: 402.79
train mean loss: 410.62
epoch train time: 0:00:00.187104
elapsed time: 0:00:21.869884
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:41:26.216438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.89
 ---- batch: 020 ----
mean loss: 404.55
 ---- batch: 030 ----
mean loss: 408.73
train mean loss: 406.57
epoch train time: 0:00:00.180835
elapsed time: 0:00:22.050875
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:41:26.397417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.84
 ---- batch: 020 ----
mean loss: 405.71
 ---- batch: 030 ----
mean loss: 396.58
train mean loss: 402.74
epoch train time: 0:00:00.179107
elapsed time: 0:00:22.230126
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:41:26.576668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.36
 ---- batch: 020 ----
mean loss: 398.30
 ---- batch: 030 ----
mean loss: 401.41
train mean loss: 399.28
epoch train time: 0:00:00.184451
elapsed time: 0:00:22.414725
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:41:26.761268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.87
 ---- batch: 020 ----
mean loss: 403.85
 ---- batch: 030 ----
mean loss: 393.00
train mean loss: 396.59
epoch train time: 0:00:00.182295
elapsed time: 0:00:22.597172
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:41:26.943745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.52
 ---- batch: 020 ----
mean loss: 393.49
 ---- batch: 030 ----
mean loss: 394.11
train mean loss: 393.53
epoch train time: 0:00:00.179045
elapsed time: 0:00:22.776398
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:41:27.122949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.12
 ---- batch: 020 ----
mean loss: 400.49
 ---- batch: 030 ----
mean loss: 391.39
train mean loss: 391.06
epoch train time: 0:00:00.179691
elapsed time: 0:00:22.956292
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:41:27.302834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.05
 ---- batch: 020 ----
mean loss: 391.83
 ---- batch: 030 ----
mean loss: 383.72
train mean loss: 388.98
epoch train time: 0:00:00.183140
elapsed time: 0:00:23.139578
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:41:27.486121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.67
 ---- batch: 020 ----
mean loss: 380.30
 ---- batch: 030 ----
mean loss: 383.99
train mean loss: 387.50
epoch train time: 0:00:00.177802
elapsed time: 0:00:23.317534
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:41:27.664077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.56
 ---- batch: 020 ----
mean loss: 384.00
 ---- batch: 030 ----
mean loss: 391.67
train mean loss: 385.44
epoch train time: 0:00:00.179521
elapsed time: 0:00:23.497202
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:41:27.843743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.30
 ---- batch: 020 ----
mean loss: 387.17
 ---- batch: 030 ----
mean loss: 380.18
train mean loss: 383.90
epoch train time: 0:00:00.175732
elapsed time: 0:00:23.673074
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:41:28.019615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.92
 ---- batch: 020 ----
mean loss: 384.21
 ---- batch: 030 ----
mean loss: 381.11
train mean loss: 382.14
epoch train time: 0:00:00.179234
elapsed time: 0:00:23.852458
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:41:28.199000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.54
 ---- batch: 020 ----
mean loss: 377.40
 ---- batch: 030 ----
mean loss: 380.49
train mean loss: 381.18
epoch train time: 0:00:00.176759
elapsed time: 0:00:24.029359
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:41:28.375900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.33
 ---- batch: 020 ----
mean loss: 375.53
 ---- batch: 030 ----
mean loss: 378.48
train mean loss: 379.95
epoch train time: 0:00:00.178910
elapsed time: 0:00:24.208438
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:41:28.554980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.16
 ---- batch: 020 ----
mean loss: 375.78
 ---- batch: 030 ----
mean loss: 375.80
train mean loss: 378.62
epoch train time: 0:00:00.177256
elapsed time: 0:00:24.385837
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:41:28.732377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.68
 ---- batch: 020 ----
mean loss: 374.37
 ---- batch: 030 ----
mean loss: 378.97
train mean loss: 377.92
epoch train time: 0:00:00.175701
elapsed time: 0:00:24.561684
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:41:28.908227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.72
 ---- batch: 020 ----
mean loss: 380.11
 ---- batch: 030 ----
mean loss: 376.38
train mean loss: 376.53
epoch train time: 0:00:00.176689
elapsed time: 0:00:24.738517
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:41:29.085059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.95
 ---- batch: 020 ----
mean loss: 376.13
 ---- batch: 030 ----
mean loss: 375.70
train mean loss: 376.06
epoch train time: 0:00:00.190655
elapsed time: 0:00:24.929338
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:41:29.275898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.13
 ---- batch: 020 ----
mean loss: 367.02
 ---- batch: 030 ----
mean loss: 380.15
train mean loss: 374.93
epoch train time: 0:00:00.181680
elapsed time: 0:00:25.111203
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:41:29.457755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.74
 ---- batch: 020 ----
mean loss: 372.95
 ---- batch: 030 ----
mean loss: 368.62
train mean loss: 374.60
epoch train time: 0:00:00.177096
elapsed time: 0:00:25.288456
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:41:29.635000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.39
 ---- batch: 020 ----
mean loss: 374.93
 ---- batch: 030 ----
mean loss: 370.38
train mean loss: 373.59
epoch train time: 0:00:00.180097
elapsed time: 0:00:25.468759
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:41:29.815314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.01
 ---- batch: 020 ----
mean loss: 380.93
 ---- batch: 030 ----
mean loss: 369.52
train mean loss: 372.99
epoch train time: 0:00:00.178600
elapsed time: 0:00:25.647536
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:41:29.994080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.00
 ---- batch: 020 ----
mean loss: 368.82
 ---- batch: 030 ----
mean loss: 375.42
train mean loss: 372.76
epoch train time: 0:00:00.173832
elapsed time: 0:00:25.821517
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:41:30.168060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.76
 ---- batch: 020 ----
mean loss: 376.06
 ---- batch: 030 ----
mean loss: 372.91
train mean loss: 372.22
epoch train time: 0:00:00.176284
elapsed time: 0:00:25.997963
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:41:30.344507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.11
 ---- batch: 020 ----
mean loss: 367.16
 ---- batch: 030 ----
mean loss: 372.10
train mean loss: 371.94
epoch train time: 0:00:00.173082
elapsed time: 0:00:26.171213
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:41:30.517755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.73
 ---- batch: 020 ----
mean loss: 374.43
 ---- batch: 030 ----
mean loss: 374.59
train mean loss: 371.14
epoch train time: 0:00:00.174359
elapsed time: 0:00:26.345725
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:41:30.692276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.45
 ---- batch: 020 ----
mean loss: 364.96
 ---- batch: 030 ----
mean loss: 366.91
train mean loss: 371.16
epoch train time: 0:00:00.178682
elapsed time: 0:00:26.524561
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:41:30.871101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.49
 ---- batch: 020 ----
mean loss: 370.64
 ---- batch: 030 ----
mean loss: 373.67
train mean loss: 370.76
epoch train time: 0:00:00.170614
elapsed time: 0:00:26.695321
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:41:31.041863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.80
 ---- batch: 020 ----
mean loss: 369.18
 ---- batch: 030 ----
mean loss: 381.37
train mean loss: 369.87
epoch train time: 0:00:00.173894
elapsed time: 0:00:26.869356
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:41:31.215897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.80
 ---- batch: 020 ----
mean loss: 371.69
 ---- batch: 030 ----
mean loss: 369.48
train mean loss: 369.78
epoch train time: 0:00:00.172009
elapsed time: 0:00:27.041508
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:41:31.388048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.89
 ---- batch: 020 ----
mean loss: 363.77
 ---- batch: 030 ----
mean loss: 370.86
train mean loss: 369.59
epoch train time: 0:00:00.171500
elapsed time: 0:00:27.213151
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:41:31.559692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.41
 ---- batch: 020 ----
mean loss: 372.59
 ---- batch: 030 ----
mean loss: 367.25
train mean loss: 369.07
epoch train time: 0:00:00.176520
elapsed time: 0:00:27.389815
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:41:31.736377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.10
 ---- batch: 020 ----
mean loss: 364.73
 ---- batch: 030 ----
mean loss: 368.98
train mean loss: 369.11
epoch train time: 0:00:00.173977
elapsed time: 0:00:27.563960
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:41:31.910505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.31
 ---- batch: 020 ----
mean loss: 374.03
 ---- batch: 030 ----
mean loss: 367.33
train mean loss: 368.40
epoch train time: 0:00:00.175080
elapsed time: 0:00:27.739215
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:41:32.085760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.64
 ---- batch: 020 ----
mean loss: 369.23
 ---- batch: 030 ----
mean loss: 364.95
train mean loss: 368.18
epoch train time: 0:00:00.173009
elapsed time: 0:00:27.912375
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:41:32.258918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.00
 ---- batch: 020 ----
mean loss: 378.22
 ---- batch: 030 ----
mean loss: 359.90
train mean loss: 367.89
epoch train time: 0:00:00.172030
elapsed time: 0:00:28.084550
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:41:32.431092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.39
 ---- batch: 020 ----
mean loss: 363.48
 ---- batch: 030 ----
mean loss: 362.83
train mean loss: 367.82
epoch train time: 0:00:00.172352
elapsed time: 0:00:28.257048
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:41:32.603591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.12
 ---- batch: 020 ----
mean loss: 363.90
 ---- batch: 030 ----
mean loss: 371.29
train mean loss: 367.27
epoch train time: 0:00:00.169044
elapsed time: 0:00:28.426233
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:41:32.772773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.00
 ---- batch: 020 ----
mean loss: 369.91
 ---- batch: 030 ----
mean loss: 370.46
train mean loss: 367.26
epoch train time: 0:00:00.173267
elapsed time: 0:00:28.599643
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:41:32.946182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.66
 ---- batch: 020 ----
mean loss: 366.73
 ---- batch: 030 ----
mean loss: 369.88
train mean loss: 366.68
epoch train time: 0:00:00.170141
elapsed time: 0:00:28.769931
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:41:33.116474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.84
 ---- batch: 020 ----
mean loss: 367.00
 ---- batch: 030 ----
mean loss: 363.57
train mean loss: 366.64
epoch train time: 0:00:00.171633
elapsed time: 0:00:28.941711
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:41:33.288254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.34
 ---- batch: 020 ----
mean loss: 364.95
 ---- batch: 030 ----
mean loss: 367.44
train mean loss: 366.53
epoch train time: 0:00:00.170537
elapsed time: 0:00:29.112393
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:41:33.458945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.99
 ---- batch: 020 ----
mean loss: 360.48
 ---- batch: 030 ----
mean loss: 373.93
train mean loss: 366.55
epoch train time: 0:00:00.170734
elapsed time: 0:00:29.283285
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:41:33.629826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.09
 ---- batch: 020 ----
mean loss: 363.35
 ---- batch: 030 ----
mean loss: 371.33
train mean loss: 365.77
epoch train time: 0:00:00.174957
elapsed time: 0:00:29.458384
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:41:33.804926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.58
 ---- batch: 020 ----
mean loss: 365.17
 ---- batch: 030 ----
mean loss: 370.33
train mean loss: 365.69
epoch train time: 0:00:00.173179
elapsed time: 0:00:29.631708
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:41:33.978250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.91
 ---- batch: 020 ----
mean loss: 363.27
 ---- batch: 030 ----
mean loss: 362.09
train mean loss: 365.67
epoch train time: 0:00:00.180041
elapsed time: 0:00:29.811903
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:41:34.158469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.55
 ---- batch: 020 ----
mean loss: 364.09
 ---- batch: 030 ----
mean loss: 362.68
train mean loss: 365.71
epoch train time: 0:00:00.170874
elapsed time: 0:00:29.982957
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:41:34.329501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.24
 ---- batch: 020 ----
mean loss: 361.66
 ---- batch: 030 ----
mean loss: 367.22
train mean loss: 364.98
epoch train time: 0:00:00.172576
elapsed time: 0:00:30.155681
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:41:34.502223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.69
 ---- batch: 020 ----
mean loss: 367.81
 ---- batch: 030 ----
mean loss: 359.61
train mean loss: 365.35
epoch train time: 0:00:00.169135
elapsed time: 0:00:30.324976
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:41:34.671519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.92
 ---- batch: 020 ----
mean loss: 369.69
 ---- batch: 030 ----
mean loss: 365.15
train mean loss: 364.84
epoch train time: 0:00:00.180279
elapsed time: 0:00:30.505399
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:41:34.851941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.99
 ---- batch: 020 ----
mean loss: 365.35
 ---- batch: 030 ----
mean loss: 365.03
train mean loss: 364.28
epoch train time: 0:00:00.179414
elapsed time: 0:00:30.684972
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:41:35.031513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.48
 ---- batch: 020 ----
mean loss: 370.07
 ---- batch: 030 ----
mean loss: 364.98
train mean loss: 364.53
epoch train time: 0:00:00.174853
elapsed time: 0:00:30.859973
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:41:35.206514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.53
 ---- batch: 020 ----
mean loss: 370.69
 ---- batch: 030 ----
mean loss: 366.24
train mean loss: 364.08
epoch train time: 0:00:00.173980
elapsed time: 0:00:31.034093
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:41:35.380632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.07
 ---- batch: 020 ----
mean loss: 364.63
 ---- batch: 030 ----
mean loss: 370.10
train mean loss: 364.07
epoch train time: 0:00:00.172270
elapsed time: 0:00:31.206505
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:41:35.553045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.56
 ---- batch: 020 ----
mean loss: 366.53
 ---- batch: 030 ----
mean loss: 365.98
train mean loss: 363.91
epoch train time: 0:00:00.171439
elapsed time: 0:00:31.378121
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:41:35.724650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.90
 ---- batch: 020 ----
mean loss: 356.17
 ---- batch: 030 ----
mean loss: 367.61
train mean loss: 363.67
epoch train time: 0:00:00.182536
elapsed time: 0:00:31.560816
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:41:35.907389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.50
 ---- batch: 020 ----
mean loss: 370.50
 ---- batch: 030 ----
mean loss: 358.88
train mean loss: 363.74
epoch train time: 0:00:00.181696
elapsed time: 0:00:31.742703
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:41:36.089245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.56
 ---- batch: 020 ----
mean loss: 357.48
 ---- batch: 030 ----
mean loss: 365.70
train mean loss: 363.54
epoch train time: 0:00:00.178215
elapsed time: 0:00:31.921061
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:41:36.267613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.71
 ---- batch: 020 ----
mean loss: 362.49
 ---- batch: 030 ----
mean loss: 363.93
train mean loss: 362.78
epoch train time: 0:00:00.179479
elapsed time: 0:00:32.100725
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:41:36.447286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.40
 ---- batch: 020 ----
mean loss: 362.07
 ---- batch: 030 ----
mean loss: 365.76
train mean loss: 362.83
epoch train time: 0:00:00.179811
elapsed time: 0:00:32.280701
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:41:36.627243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.87
 ---- batch: 020 ----
mean loss: 359.73
 ---- batch: 030 ----
mean loss: 361.37
train mean loss: 363.30
epoch train time: 0:00:00.191371
elapsed time: 0:00:32.472224
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:41:36.818790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.01
 ---- batch: 020 ----
mean loss: 362.04
 ---- batch: 030 ----
mean loss: 366.31
train mean loss: 362.50
epoch train time: 0:00:00.180292
elapsed time: 0:00:32.652747
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:41:36.999303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.14
 ---- batch: 020 ----
mean loss: 365.12
 ---- batch: 030 ----
mean loss: 370.49
train mean loss: 362.46
epoch train time: 0:00:00.177840
elapsed time: 0:00:32.830754
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:41:37.177310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.22
 ---- batch: 020 ----
mean loss: 363.41
 ---- batch: 030 ----
mean loss: 360.96
train mean loss: 362.76
epoch train time: 0:00:00.175503
elapsed time: 0:00:33.006411
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:41:37.352949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.87
 ---- batch: 020 ----
mean loss: 363.16
 ---- batch: 030 ----
mean loss: 363.72
train mean loss: 361.89
epoch train time: 0:00:00.176292
elapsed time: 0:00:33.182836
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:41:37.529375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.59
 ---- batch: 020 ----
mean loss: 364.82
 ---- batch: 030 ----
mean loss: 360.43
train mean loss: 361.42
epoch train time: 0:00:00.178263
elapsed time: 0:00:33.361263
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:41:37.707804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.89
 ---- batch: 020 ----
mean loss: 365.47
 ---- batch: 030 ----
mean loss: 360.88
train mean loss: 361.91
epoch train time: 0:00:00.182740
elapsed time: 0:00:33.544174
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:41:37.890727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.97
 ---- batch: 020 ----
mean loss: 359.05
 ---- batch: 030 ----
mean loss: 360.20
train mean loss: 361.62
epoch train time: 0:00:00.179782
elapsed time: 0:00:33.724109
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:41:38.070649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.06
 ---- batch: 020 ----
mean loss: 358.29
 ---- batch: 030 ----
mean loss: 359.24
train mean loss: 361.58
epoch train time: 0:00:00.177678
elapsed time: 0:00:33.901927
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:41:38.248468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.03
 ---- batch: 020 ----
mean loss: 359.10
 ---- batch: 030 ----
mean loss: 368.20
train mean loss: 361.11
epoch train time: 0:00:00.179313
elapsed time: 0:00:34.081384
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:41:38.427933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.61
 ---- batch: 020 ----
mean loss: 359.44
 ---- batch: 030 ----
mean loss: 365.01
train mean loss: 361.24
epoch train time: 0:00:00.177961
elapsed time: 0:00:34.259496
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:41:38.606035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.33
 ---- batch: 020 ----
mean loss: 362.89
 ---- batch: 030 ----
mean loss: 359.66
train mean loss: 360.82
epoch train time: 0:00:00.178514
elapsed time: 0:00:34.438148
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:41:38.784693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.00
 ---- batch: 020 ----
mean loss: 355.30
 ---- batch: 030 ----
mean loss: 363.75
train mean loss: 360.55
epoch train time: 0:00:00.178020
elapsed time: 0:00:34.616331
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:41:38.962906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.41
 ---- batch: 020 ----
mean loss: 358.62
 ---- batch: 030 ----
mean loss: 352.36
train mean loss: 360.94
epoch train time: 0:00:00.181526
elapsed time: 0:00:34.798069
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:41:39.144625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.46
 ---- batch: 020 ----
mean loss: 366.50
 ---- batch: 030 ----
mean loss: 361.13
train mean loss: 360.41
epoch train time: 0:00:00.178396
elapsed time: 0:00:34.976635
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:41:39.323165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.46
 ---- batch: 020 ----
mean loss: 359.28
 ---- batch: 030 ----
mean loss: 364.87
train mean loss: 360.35
epoch train time: 0:00:00.179150
elapsed time: 0:00:35.155927
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:41:39.502470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.38
 ---- batch: 020 ----
mean loss: 363.12
 ---- batch: 030 ----
mean loss: 361.35
train mean loss: 359.96
epoch train time: 0:00:00.180461
elapsed time: 0:00:35.336559
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:41:39.683100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.65
 ---- batch: 020 ----
mean loss: 361.67
 ---- batch: 030 ----
mean loss: 361.97
train mean loss: 359.80
epoch train time: 0:00:00.183389
elapsed time: 0:00:35.520092
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:41:39.866662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.64
 ---- batch: 020 ----
mean loss: 359.62
 ---- batch: 030 ----
mean loss: 370.35
train mean loss: 360.13
epoch train time: 0:00:00.174276
elapsed time: 0:00:35.694536
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:41:40.041076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.96
 ---- batch: 020 ----
mean loss: 372.17
 ---- batch: 030 ----
mean loss: 352.93
train mean loss: 359.72
epoch train time: 0:00:00.176085
elapsed time: 0:00:35.870777
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:41:40.217317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.13
 ---- batch: 020 ----
mean loss: 363.85
 ---- batch: 030 ----
mean loss: 367.10
train mean loss: 359.59
epoch train time: 0:00:00.176691
elapsed time: 0:00:36.047609
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:41:40.394162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.63
 ---- batch: 020 ----
mean loss: 357.52
 ---- batch: 030 ----
mean loss: 352.80
train mean loss: 359.38
epoch train time: 0:00:00.176665
elapsed time: 0:00:36.224427
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:41:40.570965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.33
 ---- batch: 020 ----
mean loss: 356.12
 ---- batch: 030 ----
mean loss: 365.70
train mean loss: 359.09
epoch train time: 0:00:00.178459
elapsed time: 0:00:36.403024
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:41:40.749564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.41
 ---- batch: 020 ----
mean loss: 361.47
 ---- batch: 030 ----
mean loss: 366.59
train mean loss: 359.27
epoch train time: 0:00:00.176153
elapsed time: 0:00:36.579336
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:41:40.926056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.65
 ---- batch: 020 ----
mean loss: 354.88
 ---- batch: 030 ----
mean loss: 354.93
train mean loss: 359.11
epoch train time: 0:00:00.184170
elapsed time: 0:00:36.763829
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:41:41.110371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.15
 ---- batch: 020 ----
mean loss: 357.40
 ---- batch: 030 ----
mean loss: 360.68
train mean loss: 358.88
epoch train time: 0:00:00.182032
elapsed time: 0:00:36.946003
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:41:41.292564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.11
 ---- batch: 020 ----
mean loss: 355.58
 ---- batch: 030 ----
mean loss: 361.84
train mean loss: 358.35
epoch train time: 0:00:00.182112
elapsed time: 0:00:37.128278
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:41:41.474819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.52
 ---- batch: 020 ----
mean loss: 355.33
 ---- batch: 030 ----
mean loss: 351.65
train mean loss: 358.65
epoch train time: 0:00:00.181311
elapsed time: 0:00:37.309732
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:41:41.656275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.83
 ---- batch: 020 ----
mean loss: 360.73
 ---- batch: 030 ----
mean loss: 353.48
train mean loss: 358.68
epoch train time: 0:00:00.182366
elapsed time: 0:00:37.492242
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:41:41.838783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.53
 ---- batch: 020 ----
mean loss: 356.91
 ---- batch: 030 ----
mean loss: 357.85
train mean loss: 358.54
epoch train time: 0:00:00.181887
elapsed time: 0:00:37.674274
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:41:42.020815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.97
 ---- batch: 020 ----
mean loss: 363.11
 ---- batch: 030 ----
mean loss: 357.58
train mean loss: 358.13
epoch train time: 0:00:00.180986
elapsed time: 0:00:37.855436
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:41:42.201980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.45
 ---- batch: 020 ----
mean loss: 365.19
 ---- batch: 030 ----
mean loss: 353.82
train mean loss: 357.90
epoch train time: 0:00:00.181054
elapsed time: 0:00:38.036646
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:41:42.383189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.69
 ---- batch: 020 ----
mean loss: 357.84
 ---- batch: 030 ----
mean loss: 357.70
train mean loss: 357.65
epoch train time: 0:00:00.178652
elapsed time: 0:00:38.215448
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:41:42.561991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.13
 ---- batch: 020 ----
mean loss: 359.48
 ---- batch: 030 ----
mean loss: 355.32
train mean loss: 357.70
epoch train time: 0:00:00.180905
elapsed time: 0:00:38.396514
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:41:42.743057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.55
 ---- batch: 020 ----
mean loss: 358.45
 ---- batch: 030 ----
mean loss: 358.72
train mean loss: 357.38
epoch train time: 0:00:00.171665
elapsed time: 0:00:38.568325
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:41:42.914867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.41
 ---- batch: 020 ----
mean loss: 353.09
 ---- batch: 030 ----
mean loss: 357.25
train mean loss: 357.13
epoch train time: 0:00:00.171140
elapsed time: 0:00:38.739608
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:41:43.086150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.32
 ---- batch: 020 ----
mean loss: 353.96
 ---- batch: 030 ----
mean loss: 361.22
train mean loss: 357.09
epoch train time: 0:00:00.174275
elapsed time: 0:00:38.914039
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:41:43.260570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.22
 ---- batch: 020 ----
mean loss: 357.35
 ---- batch: 030 ----
mean loss: 363.71
train mean loss: 356.93
epoch train time: 0:00:00.169904
elapsed time: 0:00:39.084091
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:41:43.430636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.43
 ---- batch: 020 ----
mean loss: 362.15
 ---- batch: 030 ----
mean loss: 351.09
train mean loss: 356.86
epoch train time: 0:00:00.171440
elapsed time: 0:00:39.255678
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:41:43.602218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.60
 ---- batch: 020 ----
mean loss: 363.66
 ---- batch: 030 ----
mean loss: 353.23
train mean loss: 356.73
epoch train time: 0:00:00.173447
elapsed time: 0:00:39.429270
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:41:43.775832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.09
 ---- batch: 020 ----
mean loss: 354.99
 ---- batch: 030 ----
mean loss: 353.82
train mean loss: 357.00
epoch train time: 0:00:00.176765
elapsed time: 0:00:39.606204
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:41:43.952742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.13
 ---- batch: 020 ----
mean loss: 355.39
 ---- batch: 030 ----
mean loss: 360.99
train mean loss: 356.39
epoch train time: 0:00:00.181769
elapsed time: 0:00:39.788113
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:41:44.134654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.92
 ---- batch: 020 ----
mean loss: 358.80
 ---- batch: 030 ----
mean loss: 360.24
train mean loss: 356.21
epoch train time: 0:00:00.185201
elapsed time: 0:00:39.973462
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:41:44.320004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.17
 ---- batch: 020 ----
mean loss: 353.60
 ---- batch: 030 ----
mean loss: 353.91
train mean loss: 356.47
epoch train time: 0:00:00.184578
elapsed time: 0:00:40.158187
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:41:44.504728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.42
 ---- batch: 020 ----
mean loss: 356.60
 ---- batch: 030 ----
mean loss: 352.24
train mean loss: 356.00
epoch train time: 0:00:00.185938
elapsed time: 0:00:40.344270
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:41:44.690828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.20
 ---- batch: 020 ----
mean loss: 360.46
 ---- batch: 030 ----
mean loss: 357.94
train mean loss: 356.01
epoch train time: 0:00:00.185096
elapsed time: 0:00:40.529530
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:41:44.876101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.45
 ---- batch: 020 ----
mean loss: 351.25
 ---- batch: 030 ----
mean loss: 356.17
train mean loss: 355.67
epoch train time: 0:00:00.180682
elapsed time: 0:00:40.710383
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:41:45.056925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.40
 ---- batch: 020 ----
mean loss: 358.96
 ---- batch: 030 ----
mean loss: 352.33
train mean loss: 355.74
epoch train time: 0:00:00.173979
elapsed time: 0:00:40.884511
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:41:45.231051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.45
 ---- batch: 020 ----
mean loss: 365.69
 ---- batch: 030 ----
mean loss: 348.39
train mean loss: 354.97
epoch train time: 0:00:00.176038
elapsed time: 0:00:41.060686
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:41:45.407223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.68
 ---- batch: 020 ----
mean loss: 351.78
 ---- batch: 030 ----
mean loss: 359.81
train mean loss: 355.17
epoch train time: 0:00:00.175597
elapsed time: 0:00:41.236513
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:41:45.583054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.99
 ---- batch: 020 ----
mean loss: 358.61
 ---- batch: 030 ----
mean loss: 358.48
train mean loss: 355.33
epoch train time: 0:00:00.176789
elapsed time: 0:00:41.413441
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:41:45.759981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.88
 ---- batch: 020 ----
mean loss: 357.49
 ---- batch: 030 ----
mean loss: 354.53
train mean loss: 355.19
epoch train time: 0:00:00.176420
elapsed time: 0:00:41.590016
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:41:45.936557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.34
 ---- batch: 020 ----
mean loss: 361.28
 ---- batch: 030 ----
mean loss: 352.88
train mean loss: 354.84
epoch train time: 0:00:00.175605
elapsed time: 0:00:41.765771
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:41:46.112328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.20
 ---- batch: 020 ----
mean loss: 360.37
 ---- batch: 030 ----
mean loss: 349.37
train mean loss: 354.30
epoch train time: 0:00:00.172068
elapsed time: 0:00:41.937992
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:41:46.284530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.35
 ---- batch: 020 ----
mean loss: 354.12
 ---- batch: 030 ----
mean loss: 355.55
train mean loss: 354.50
epoch train time: 0:00:00.174252
elapsed time: 0:00:42.112391
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:41:46.458930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.23
 ---- batch: 020 ----
mean loss: 347.18
 ---- batch: 030 ----
mean loss: 363.04
train mean loss: 354.86
epoch train time: 0:00:00.174940
elapsed time: 0:00:42.287472
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:41:46.634012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.14
 ---- batch: 020 ----
mean loss: 352.68
 ---- batch: 030 ----
mean loss: 354.71
train mean loss: 354.43
epoch train time: 0:00:00.182399
elapsed time: 0:00:42.470041
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:41:46.816590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.11
 ---- batch: 020 ----
mean loss: 357.47
 ---- batch: 030 ----
mean loss: 351.22
train mean loss: 354.15
epoch train time: 0:00:00.177663
elapsed time: 0:00:42.647853
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:41:46.994394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.44
 ---- batch: 020 ----
mean loss: 353.64
 ---- batch: 030 ----
mean loss: 352.29
train mean loss: 353.93
epoch train time: 0:00:00.177088
elapsed time: 0:00:42.825079
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:41:47.171618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.71
 ---- batch: 020 ----
mean loss: 360.47
 ---- batch: 030 ----
mean loss: 361.21
train mean loss: 353.78
epoch train time: 0:00:00.176386
elapsed time: 0:00:43.001603
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:41:47.348143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.16
 ---- batch: 020 ----
mean loss: 348.64
 ---- batch: 030 ----
mean loss: 355.58
train mean loss: 354.17
epoch train time: 0:00:00.173565
elapsed time: 0:00:43.175316
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:41:47.521858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.83
 ---- batch: 020 ----
mean loss: 357.03
 ---- batch: 030 ----
mean loss: 350.30
train mean loss: 353.79
epoch train time: 0:00:00.174263
elapsed time: 0:00:43.349744
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:41:47.696275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.73
 ---- batch: 020 ----
mean loss: 356.73
 ---- batch: 030 ----
mean loss: 354.40
train mean loss: 353.39
epoch train time: 0:00:00.184214
elapsed time: 0:00:43.534098
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:41:47.880640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.83
 ---- batch: 020 ----
mean loss: 355.69
 ---- batch: 030 ----
mean loss: 350.82
train mean loss: 353.30
epoch train time: 0:00:00.175283
elapsed time: 0:00:43.709522
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:41:48.056090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.45
 ---- batch: 020 ----
mean loss: 348.02
 ---- batch: 030 ----
mean loss: 359.25
train mean loss: 353.06
epoch train time: 0:00:00.173819
elapsed time: 0:00:43.883508
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:41:48.230101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.25
 ---- batch: 020 ----
mean loss: 351.73
 ---- batch: 030 ----
mean loss: 346.85
train mean loss: 352.88
epoch train time: 0:00:00.173667
elapsed time: 0:00:44.057365
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:41:48.403904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.60
 ---- batch: 020 ----
mean loss: 359.36
 ---- batch: 030 ----
mean loss: 346.67
train mean loss: 353.09
epoch train time: 0:00:00.174873
elapsed time: 0:00:44.232417
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:41:48.579003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.30
 ---- batch: 020 ----
mean loss: 352.56
 ---- batch: 030 ----
mean loss: 356.64
train mean loss: 352.61
epoch train time: 0:00:00.182101
elapsed time: 0:00:44.414706
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:41:48.761247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.21
 ---- batch: 020 ----
mean loss: 357.19
 ---- batch: 030 ----
mean loss: 347.18
train mean loss: 352.91
epoch train time: 0:00:00.181436
elapsed time: 0:00:44.596283
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:41:48.942839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.88
 ---- batch: 020 ----
mean loss: 350.85
 ---- batch: 030 ----
mean loss: 357.11
train mean loss: 352.33
epoch train time: 0:00:00.177642
elapsed time: 0:00:44.774079
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:41:49.120622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.26
 ---- batch: 020 ----
mean loss: 349.49
 ---- batch: 030 ----
mean loss: 353.12
train mean loss: 352.49
epoch train time: 0:00:00.173880
elapsed time: 0:00:44.948116
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:41:49.294656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.33
 ---- batch: 020 ----
mean loss: 352.18
 ---- batch: 030 ----
mean loss: 354.26
train mean loss: 352.37
epoch train time: 0:00:00.173619
elapsed time: 0:00:45.121890
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:41:49.468478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.13
 ---- batch: 020 ----
mean loss: 347.38
 ---- batch: 030 ----
mean loss: 351.67
train mean loss: 352.15
epoch train time: 0:00:00.177292
elapsed time: 0:00:45.299370
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:41:49.645910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.83
 ---- batch: 020 ----
mean loss: 357.78
 ---- batch: 030 ----
mean loss: 343.08
train mean loss: 352.15
epoch train time: 0:00:00.178268
elapsed time: 0:00:45.477785
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:41:49.824325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.97
 ---- batch: 020 ----
mean loss: 356.49
 ---- batch: 030 ----
mean loss: 347.76
train mean loss: 351.78
epoch train time: 0:00:00.176285
elapsed time: 0:00:45.654210
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:41:50.000751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.49
 ---- batch: 020 ----
mean loss: 358.41
 ---- batch: 030 ----
mean loss: 351.39
train mean loss: 351.73
epoch train time: 0:00:00.176212
elapsed time: 0:00:45.830575
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:41:50.177114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.11
 ---- batch: 020 ----
mean loss: 345.42
 ---- batch: 030 ----
mean loss: 356.89
train mean loss: 351.40
epoch train time: 0:00:00.173319
elapsed time: 0:00:46.004032
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:41:50.350572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.81
 ---- batch: 020 ----
mean loss: 354.47
 ---- batch: 030 ----
mean loss: 355.29
train mean loss: 351.11
epoch train time: 0:00:00.174858
elapsed time: 0:00:46.179028
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:41:50.525570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.88
 ---- batch: 020 ----
mean loss: 354.76
 ---- batch: 030 ----
mean loss: 351.46
train mean loss: 351.17
epoch train time: 0:00:00.175052
elapsed time: 0:00:46.354277
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:41:50.700827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.92
 ---- batch: 020 ----
mean loss: 356.52
 ---- batch: 030 ----
mean loss: 347.96
train mean loss: 350.78
epoch train time: 0:00:00.178954
elapsed time: 0:00:46.533379
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:41:50.879947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.18
 ---- batch: 020 ----
mean loss: 354.94
 ---- batch: 030 ----
mean loss: 345.74
train mean loss: 350.68
epoch train time: 0:00:00.174661
elapsed time: 0:00:46.708207
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:41:51.054747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.61
 ---- batch: 020 ----
mean loss: 350.21
 ---- batch: 030 ----
mean loss: 348.61
train mean loss: 350.73
epoch train time: 0:00:00.174380
elapsed time: 0:00:46.882725
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:41:51.229265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.94
 ---- batch: 020 ----
mean loss: 353.84
 ---- batch: 030 ----
mean loss: 349.66
train mean loss: 350.53
epoch train time: 0:00:00.171887
elapsed time: 0:00:47.054748
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:41:51.401287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.01
 ---- batch: 020 ----
mean loss: 345.43
 ---- batch: 030 ----
mean loss: 350.03
train mean loss: 350.24
epoch train time: 0:00:00.175696
elapsed time: 0:00:47.230626
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:41:51.577214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.59
 ---- batch: 020 ----
mean loss: 350.02
 ---- batch: 030 ----
mean loss: 347.22
train mean loss: 349.84
epoch train time: 0:00:00.174997
elapsed time: 0:00:47.405826
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:41:51.752366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.07
 ---- batch: 020 ----
mean loss: 354.07
 ---- batch: 030 ----
mean loss: 355.98
train mean loss: 349.98
epoch train time: 0:00:00.177875
elapsed time: 0:00:47.583841
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:41:51.930382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.86
 ---- batch: 020 ----
mean loss: 348.60
 ---- batch: 030 ----
mean loss: 349.25
train mean loss: 349.86
epoch train time: 0:00:00.180814
elapsed time: 0:00:47.764799
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:41:52.111340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.47
 ---- batch: 020 ----
mean loss: 350.32
 ---- batch: 030 ----
mean loss: 352.07
train mean loss: 349.21
epoch train time: 0:00:00.175517
elapsed time: 0:00:47.940456
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:41:52.286996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.98
 ---- batch: 020 ----
mean loss: 357.62
 ---- batch: 030 ----
mean loss: 345.55
train mean loss: 349.72
epoch train time: 0:00:00.176612
elapsed time: 0:00:48.117216
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:41:52.463758
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.17
 ---- batch: 020 ----
mean loss: 345.58
 ---- batch: 030 ----
mean loss: 351.84
train mean loss: 349.42
epoch train time: 0:00:00.175800
elapsed time: 0:00:48.293191
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:41:52.639721
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.21
 ---- batch: 020 ----
mean loss: 350.69
 ---- batch: 030 ----
mean loss: 354.47
train mean loss: 349.49
epoch train time: 0:00:00.179087
elapsed time: 0:00:48.472418
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:41:52.818965
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.82
 ---- batch: 020 ----
mean loss: 352.15
 ---- batch: 030 ----
mean loss: 342.81
train mean loss: 349.58
epoch train time: 0:00:00.182580
elapsed time: 0:00:48.655166
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:41:53.001723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.77
 ---- batch: 020 ----
mean loss: 345.26
 ---- batch: 030 ----
mean loss: 356.53
train mean loss: 349.39
epoch train time: 0:00:00.174446
elapsed time: 0:00:48.829767
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:41:53.176306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.97
 ---- batch: 020 ----
mean loss: 348.61
 ---- batch: 030 ----
mean loss: 352.64
train mean loss: 349.09
epoch train time: 0:00:00.172576
elapsed time: 0:00:49.002502
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:41:53.349061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.41
 ---- batch: 020 ----
mean loss: 348.20
 ---- batch: 030 ----
mean loss: 346.88
train mean loss: 349.38
epoch train time: 0:00:00.173733
elapsed time: 0:00:49.176396
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:41:53.522935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.42
 ---- batch: 020 ----
mean loss: 350.16
 ---- batch: 030 ----
mean loss: 355.47
train mean loss: 349.15
epoch train time: 0:00:00.179427
elapsed time: 0:00:49.355969
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:41:53.702519
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.69
 ---- batch: 020 ----
mean loss: 342.91
 ---- batch: 030 ----
mean loss: 350.58
train mean loss: 349.51
epoch train time: 0:00:00.180038
elapsed time: 0:00:49.536175
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:41:53.882717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.76
 ---- batch: 020 ----
mean loss: 356.63
 ---- batch: 030 ----
mean loss: 350.69
train mean loss: 349.08
epoch train time: 0:00:00.182715
elapsed time: 0:00:49.719039
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:41:54.065595
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.67
 ---- batch: 020 ----
mean loss: 346.84
 ---- batch: 030 ----
mean loss: 352.61
train mean loss: 349.83
epoch train time: 0:00:00.179432
elapsed time: 0:00:49.898636
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:41:54.245196
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.96
 ---- batch: 020 ----
mean loss: 348.74
 ---- batch: 030 ----
mean loss: 355.79
train mean loss: 348.96
epoch train time: 0:00:00.177644
elapsed time: 0:00:50.076474
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:41:54.423014
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.07
 ---- batch: 020 ----
mean loss: 356.24
 ---- batch: 030 ----
mean loss: 346.34
train mean loss: 348.85
epoch train time: 0:00:00.177315
elapsed time: 0:00:50.253941
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:41:54.600481
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.54
 ---- batch: 020 ----
mean loss: 347.15
 ---- batch: 030 ----
mean loss: 345.26
train mean loss: 348.75
epoch train time: 0:00:00.180908
elapsed time: 0:00:50.434999
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:41:54.781541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.85
 ---- batch: 020 ----
mean loss: 345.70
 ---- batch: 030 ----
mean loss: 350.44
train mean loss: 349.60
epoch train time: 0:00:00.180105
elapsed time: 0:00:50.615254
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:41:54.961808
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.23
 ---- batch: 020 ----
mean loss: 344.70
 ---- batch: 030 ----
mean loss: 349.95
train mean loss: 349.04
epoch train time: 0:00:00.177673
elapsed time: 0:00:50.793137
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:41:55.139676
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.09
 ---- batch: 020 ----
mean loss: 350.54
 ---- batch: 030 ----
mean loss: 346.54
train mean loss: 348.78
epoch train time: 0:00:00.173189
elapsed time: 0:00:50.966480
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:41:55.313020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.17
 ---- batch: 020 ----
mean loss: 346.54
 ---- batch: 030 ----
mean loss: 351.77
train mean loss: 349.47
epoch train time: 0:00:00.180383
elapsed time: 0:00:51.147003
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:41:55.493544
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.66
 ---- batch: 020 ----
mean loss: 349.00
 ---- batch: 030 ----
mean loss: 345.91
train mean loss: 349.29
epoch train time: 0:00:00.174892
elapsed time: 0:00:51.322057
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:41:55.668626
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.00
 ---- batch: 020 ----
mean loss: 351.78
 ---- batch: 030 ----
mean loss: 344.06
train mean loss: 349.04
epoch train time: 0:00:00.178538
elapsed time: 0:00:51.500775
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:41:55.847314
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.58
 ---- batch: 020 ----
mean loss: 344.98
 ---- batch: 030 ----
mean loss: 347.44
train mean loss: 348.93
epoch train time: 0:00:00.180828
elapsed time: 0:00:51.681740
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:41:56.028280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.34
 ---- batch: 020 ----
mean loss: 347.46
 ---- batch: 030 ----
mean loss: 345.69
train mean loss: 349.05
epoch train time: 0:00:00.174528
elapsed time: 0:00:51.856418
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:41:56.202960
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.37
 ---- batch: 020 ----
mean loss: 336.21
 ---- batch: 030 ----
mean loss: 352.28
train mean loss: 348.81
epoch train time: 0:00:00.174011
elapsed time: 0:00:52.030571
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:41:56.377111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.71
 ---- batch: 020 ----
mean loss: 346.80
 ---- batch: 030 ----
mean loss: 349.94
train mean loss: 348.90
epoch train time: 0:00:00.176409
elapsed time: 0:00:52.207186
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:41:56.553744
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.36
 ---- batch: 020 ----
mean loss: 349.51
 ---- batch: 030 ----
mean loss: 351.34
train mean loss: 349.10
epoch train time: 0:00:00.187326
elapsed time: 0:00:52.394666
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:41:56.741206
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.72
 ---- batch: 020 ----
mean loss: 347.82
 ---- batch: 030 ----
mean loss: 345.41
train mean loss: 349.03
epoch train time: 0:00:00.180067
elapsed time: 0:00:52.574875
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:41:56.921427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.58
 ---- batch: 020 ----
mean loss: 344.43
 ---- batch: 030 ----
mean loss: 346.94
train mean loss: 348.74
epoch train time: 0:00:00.180544
elapsed time: 0:00:52.755571
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:41:57.102111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.77
 ---- batch: 020 ----
mean loss: 342.53
 ---- batch: 030 ----
mean loss: 352.38
train mean loss: 348.75
epoch train time: 0:00:00.177112
elapsed time: 0:00:52.932818
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:41:57.279357
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.82
 ---- batch: 020 ----
mean loss: 358.09
 ---- batch: 030 ----
mean loss: 350.21
train mean loss: 348.74
epoch train time: 0:00:00.176307
elapsed time: 0:00:53.109290
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:41:57.455831
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.11
 ---- batch: 020 ----
mean loss: 351.32
 ---- batch: 030 ----
mean loss: 349.15
train mean loss: 349.42
epoch train time: 0:00:00.179738
elapsed time: 0:00:53.289182
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:41:57.635749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.74
 ---- batch: 020 ----
mean loss: 356.01
 ---- batch: 030 ----
mean loss: 340.89
train mean loss: 348.40
epoch train time: 0:00:00.186758
elapsed time: 0:00:53.476109
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:41:57.822650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.93
 ---- batch: 020 ----
mean loss: 339.39
 ---- batch: 030 ----
mean loss: 356.66
train mean loss: 348.66
epoch train time: 0:00:00.179508
elapsed time: 0:00:53.655761
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:41:58.002303
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.27
 ---- batch: 020 ----
mean loss: 342.69
 ---- batch: 030 ----
mean loss: 356.31
train mean loss: 348.92
epoch train time: 0:00:00.176626
elapsed time: 0:00:53.832541
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:41:58.179097
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.29
 ---- batch: 020 ----
mean loss: 352.42
 ---- batch: 030 ----
mean loss: 351.86
train mean loss: 348.67
epoch train time: 0:00:00.174754
elapsed time: 0:00:54.007523
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:41:58.354054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.54
 ---- batch: 020 ----
mean loss: 348.50
 ---- batch: 030 ----
mean loss: 350.56
train mean loss: 349.01
epoch train time: 0:00:00.175248
elapsed time: 0:00:54.182913
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:41:58.529461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.58
 ---- batch: 020 ----
mean loss: 347.78
 ---- batch: 030 ----
mean loss: 352.93
train mean loss: 348.49
epoch train time: 0:00:00.174015
elapsed time: 0:00:54.357092
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:41:58.703647
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.13
 ---- batch: 020 ----
mean loss: 351.50
 ---- batch: 030 ----
mean loss: 343.25
train mean loss: 348.85
epoch train time: 0:00:00.175537
elapsed time: 0:00:54.532785
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:41:58.879326
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.69
 ---- batch: 020 ----
mean loss: 354.34
 ---- batch: 030 ----
mean loss: 347.42
train mean loss: 348.74
epoch train time: 0:00:00.176922
elapsed time: 0:00:54.709855
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:41:59.056415
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.11
 ---- batch: 020 ----
mean loss: 355.04
 ---- batch: 030 ----
mean loss: 337.73
train mean loss: 348.79
epoch train time: 0:00:00.181312
elapsed time: 0:00:54.891330
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:41:59.237873
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.60
 ---- batch: 020 ----
mean loss: 354.18
 ---- batch: 030 ----
mean loss: 347.05
train mean loss: 348.75
epoch train time: 0:00:00.177590
elapsed time: 0:00:55.069081
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:41:59.415625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.46
 ---- batch: 020 ----
mean loss: 343.65
 ---- batch: 030 ----
mean loss: 346.80
train mean loss: 348.23
epoch train time: 0:00:00.181270
elapsed time: 0:00:55.250498
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:41:59.597039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.03
 ---- batch: 020 ----
mean loss: 348.09
 ---- batch: 030 ----
mean loss: 347.77
train mean loss: 348.49
epoch train time: 0:00:00.178636
elapsed time: 0:00:55.429285
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:41:59.775825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.07
 ---- batch: 020 ----
mean loss: 344.54
 ---- batch: 030 ----
mean loss: 348.84
train mean loss: 348.96
epoch train time: 0:00:00.176443
elapsed time: 0:00:55.605889
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:41:59.952446
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.13
 ---- batch: 020 ----
mean loss: 354.22
 ---- batch: 030 ----
mean loss: 346.64
train mean loss: 348.31
epoch train time: 0:00:00.177907
elapsed time: 0:00:55.783953
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:42:00.130493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.92
 ---- batch: 020 ----
mean loss: 350.82
 ---- batch: 030 ----
mean loss: 341.52
train mean loss: 348.76
epoch train time: 0:00:00.174546
elapsed time: 0:00:55.958638
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:42:00.305177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.21
 ---- batch: 020 ----
mean loss: 345.24
 ---- batch: 030 ----
mean loss: 348.70
train mean loss: 348.68
epoch train time: 0:00:00.176385
elapsed time: 0:00:56.135197
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:42:00.481759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.66
 ---- batch: 020 ----
mean loss: 359.87
 ---- batch: 030 ----
mean loss: 343.83
train mean loss: 348.73
epoch train time: 0:00:00.174392
elapsed time: 0:00:56.309752
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:42:00.656290
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.90
 ---- batch: 020 ----
mean loss: 350.12
 ---- batch: 030 ----
mean loss: 342.96
train mean loss: 348.74
epoch train time: 0:00:00.182438
elapsed time: 0:00:56.492329
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:42:00.838884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.06
 ---- batch: 020 ----
mean loss: 346.63
 ---- batch: 030 ----
mean loss: 354.37
train mean loss: 348.54
epoch train time: 0:00:00.176041
elapsed time: 0:00:56.668525
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:42:01.015067
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.91
 ---- batch: 020 ----
mean loss: 343.29
 ---- batch: 030 ----
mean loss: 350.28
train mean loss: 348.31
epoch train time: 0:00:00.178174
elapsed time: 0:00:56.849151
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_1/checkpoint.pth.tar
**** end time: 2019-09-27 16:42:01.195690 ****
