Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 28581
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-20 11:50:49.543625 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 31, 14]             200
           Sigmoid-2           [-1, 10, 31, 14]               0
    BayesianConv2d-3           [-1, 10, 30, 14]           2,000
           Sigmoid-4           [-1, 10, 30, 14]               0
    BayesianConv2d-5           [-1, 10, 31, 14]           2,000
           Sigmoid-6           [-1, 10, 31, 14]               0
    BayesianConv2d-7           [-1, 10, 30, 14]           2,000
           Sigmoid-8           [-1, 10, 30, 14]               0
    BayesianConv2d-9            [-1, 1, 30, 14]              60
         Softplus-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
   BayesianLinear-12                  [-1, 100]          84,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 90,460
Trainable params: 90,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 11:50:49.559313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1807.23
 ---- batch: 020 ----
mean loss: 1312.42
 ---- batch: 030 ----
mean loss: 1158.88
 ---- batch: 040 ----
mean loss: 1107.80
train mean loss: 1329.51
epoch train time: 0:00:20.382361
elapsed time: 0:00:20.405665
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 11:51:09.949331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1034.08
 ---- batch: 020 ----
mean loss: 1024.63
 ---- batch: 030 ----
mean loss: 1038.34
 ---- batch: 040 ----
mean loss: 992.41
train mean loss: 1022.21
epoch train time: 0:00:08.214501
elapsed time: 0:00:28.620493
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 11:51:18.164315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1000.73
 ---- batch: 020 ----
mean loss: 991.76
 ---- batch: 030 ----
mean loss: 977.01
 ---- batch: 040 ----
mean loss: 1039.20
train mean loss: 1001.60
epoch train time: 0:00:08.002079
elapsed time: 0:00:36.623053
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 11:51:26.166843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 990.09
 ---- batch: 020 ----
mean loss: 964.37
 ---- batch: 030 ----
mean loss: 1027.35
 ---- batch: 040 ----
mean loss: 974.03
train mean loss: 991.94
epoch train time: 0:00:08.013528
elapsed time: 0:00:44.636995
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 11:51:34.180776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1000.41
 ---- batch: 020 ----
mean loss: 945.70
 ---- batch: 030 ----
mean loss: 981.99
 ---- batch: 040 ----
mean loss: 968.21
train mean loss: 975.52
epoch train time: 0:00:08.000755
elapsed time: 0:00:52.638147
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 11:51:42.181918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 958.56
 ---- batch: 020 ----
mean loss: 954.29
 ---- batch: 030 ----
mean loss: 971.26
 ---- batch: 040 ----
mean loss: 963.63
train mean loss: 960.89
epoch train time: 0:00:08.015501
elapsed time: 0:01:00.654081
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 11:51:50.197907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.19
 ---- batch: 020 ----
mean loss: 964.83
 ---- batch: 030 ----
mean loss: 942.81
 ---- batch: 040 ----
mean loss: 949.63
train mean loss: 953.71
epoch train time: 0:00:07.925432
elapsed time: 0:01:08.579980
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 11:51:58.123757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.89
 ---- batch: 020 ----
mean loss: 922.98
 ---- batch: 030 ----
mean loss: 920.25
 ---- batch: 040 ----
mean loss: 886.67
train mean loss: 910.94
epoch train time: 0:00:07.969424
elapsed time: 0:01:16.549870
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 11:52:06.093722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.46
 ---- batch: 020 ----
mean loss: 746.06
 ---- batch: 030 ----
mean loss: 661.23
 ---- batch: 040 ----
mean loss: 561.44
train mean loss: 685.10
epoch train time: 0:00:07.914286
elapsed time: 0:01:24.464654
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 11:52:14.008484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.34
 ---- batch: 020 ----
mean loss: 484.68
 ---- batch: 030 ----
mean loss: 458.72
 ---- batch: 040 ----
mean loss: 456.58
train mean loss: 469.01
epoch train time: 0:00:08.006617
elapsed time: 0:01:32.471747
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 11:52:22.015536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.82
 ---- batch: 020 ----
mean loss: 427.47
 ---- batch: 030 ----
mean loss: 412.14
 ---- batch: 040 ----
mean loss: 388.35
train mean loss: 416.94
epoch train time: 0:00:07.799229
elapsed time: 0:01:40.271435
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 11:52:29.815215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.78
 ---- batch: 020 ----
mean loss: 391.60
 ---- batch: 030 ----
mean loss: 376.57
 ---- batch: 040 ----
mean loss: 368.32
train mean loss: 382.96
epoch train time: 0:00:07.804315
elapsed time: 0:01:48.076220
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 11:52:37.619995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.13
 ---- batch: 020 ----
mean loss: 351.49
 ---- batch: 030 ----
mean loss: 346.56
 ---- batch: 040 ----
mean loss: 342.22
train mean loss: 350.80
epoch train time: 0:00:07.806604
elapsed time: 0:01:55.883247
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 11:52:45.427029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.15
 ---- batch: 020 ----
mean loss: 330.43
 ---- batch: 030 ----
mean loss: 331.67
 ---- batch: 040 ----
mean loss: 327.62
train mean loss: 332.11
epoch train time: 0:00:07.790532
elapsed time: 0:02:03.674197
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 11:52:53.217975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.84
 ---- batch: 020 ----
mean loss: 328.39
 ---- batch: 030 ----
mean loss: 318.38
 ---- batch: 040 ----
mean loss: 313.22
train mean loss: 323.08
epoch train time: 0:00:07.982007
elapsed time: 0:02:11.656581
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 11:53:01.200350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.67
 ---- batch: 020 ----
mean loss: 311.62
 ---- batch: 030 ----
mean loss: 307.76
 ---- batch: 040 ----
mean loss: 303.96
train mean loss: 311.67
epoch train time: 0:00:07.990442
elapsed time: 0:02:19.647410
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 11:53:09.191185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.92
 ---- batch: 020 ----
mean loss: 308.14
 ---- batch: 030 ----
mean loss: 306.75
 ---- batch: 040 ----
mean loss: 304.53
train mean loss: 309.14
epoch train time: 0:00:07.992403
elapsed time: 0:02:27.640238
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 11:53:17.184027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.88
 ---- batch: 020 ----
mean loss: 295.11
 ---- batch: 030 ----
mean loss: 293.47
 ---- batch: 040 ----
mean loss: 287.04
train mean loss: 291.56
epoch train time: 0:00:07.964288
elapsed time: 0:02:35.605027
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 11:53:25.148843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.94
 ---- batch: 020 ----
mean loss: 295.65
 ---- batch: 030 ----
mean loss: 288.63
 ---- batch: 040 ----
mean loss: 274.11
train mean loss: 287.68
epoch train time: 0:00:07.971963
elapsed time: 0:02:43.577442
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 11:53:33.121211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.85
 ---- batch: 020 ----
mean loss: 283.18
 ---- batch: 030 ----
mean loss: 276.82
 ---- batch: 040 ----
mean loss: 286.28
train mean loss: 283.37
epoch train time: 0:00:08.000338
elapsed time: 0:02:51.578175
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 11:53:41.121982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.57
 ---- batch: 020 ----
mean loss: 278.44
 ---- batch: 030 ----
mean loss: 271.46
 ---- batch: 040 ----
mean loss: 264.39
train mean loss: 272.05
epoch train time: 0:00:07.852073
elapsed time: 0:02:59.430715
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 11:53:48.974503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.30
 ---- batch: 020 ----
mean loss: 273.77
 ---- batch: 030 ----
mean loss: 254.15
 ---- batch: 040 ----
mean loss: 262.70
train mean loss: 263.98
epoch train time: 0:00:07.846483
elapsed time: 0:03:07.277632
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 11:53:56.821369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.91
 ---- batch: 020 ----
mean loss: 260.87
 ---- batch: 030 ----
mean loss: 251.81
 ---- batch: 040 ----
mean loss: 254.04
train mean loss: 258.53
epoch train time: 0:00:07.926346
elapsed time: 0:03:15.204376
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 11:54:04.748158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.56
 ---- batch: 020 ----
mean loss: 249.78
 ---- batch: 030 ----
mean loss: 251.28
 ---- batch: 040 ----
mean loss: 258.76
train mean loss: 253.56
epoch train time: 0:00:07.758227
elapsed time: 0:03:22.963033
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 11:54:12.506816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.90
 ---- batch: 020 ----
mean loss: 241.23
 ---- batch: 030 ----
mean loss: 247.91
 ---- batch: 040 ----
mean loss: 249.59
train mean loss: 244.05
epoch train time: 0:00:07.771025
elapsed time: 0:03:30.734489
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 11:54:20.278269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.05
 ---- batch: 020 ----
mean loss: 237.99
 ---- batch: 030 ----
mean loss: 233.94
 ---- batch: 040 ----
mean loss: 239.92
train mean loss: 239.18
epoch train time: 0:00:07.758192
elapsed time: 0:03:38.493112
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 11:54:28.036886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.67
 ---- batch: 020 ----
mean loss: 227.69
 ---- batch: 030 ----
mean loss: 237.94
 ---- batch: 040 ----
mean loss: 231.38
train mean loss: 231.85
epoch train time: 0:00:07.724596
elapsed time: 0:03:46.218136
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 11:54:35.761933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.62
 ---- batch: 020 ----
mean loss: 235.07
 ---- batch: 030 ----
mean loss: 226.39
 ---- batch: 040 ----
mean loss: 213.88
train mean loss: 223.56
epoch train time: 0:00:07.764214
elapsed time: 0:03:53.982789
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 11:54:43.526560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.13
 ---- batch: 020 ----
mean loss: 221.70
 ---- batch: 030 ----
mean loss: 218.60
 ---- batch: 040 ----
mean loss: 219.07
train mean loss: 218.68
epoch train time: 0:00:07.902899
elapsed time: 0:04:01.886130
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 11:54:51.429948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.99
 ---- batch: 020 ----
mean loss: 227.37
 ---- batch: 030 ----
mean loss: 206.67
 ---- batch: 040 ----
mean loss: 208.08
train mean loss: 215.37
epoch train time: 0:00:07.854921
elapsed time: 0:04:09.741511
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 11:54:59.285302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.13
 ---- batch: 020 ----
mean loss: 201.96
 ---- batch: 030 ----
mean loss: 207.84
 ---- batch: 040 ----
mean loss: 211.77
train mean loss: 208.39
epoch train time: 0:00:07.871794
elapsed time: 0:04:17.613747
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 11:55:07.157524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.38
 ---- batch: 020 ----
mean loss: 200.74
 ---- batch: 030 ----
mean loss: 203.23
 ---- batch: 040 ----
mean loss: 198.63
train mean loss: 203.03
epoch train time: 0:00:07.836725
elapsed time: 0:04:25.450949
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 11:55:14.994810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.31
 ---- batch: 020 ----
mean loss: 200.62
 ---- batch: 030 ----
mean loss: 196.49
 ---- batch: 040 ----
mean loss: 199.10
train mean loss: 200.88
epoch train time: 0:00:07.815507
elapsed time: 0:04:33.266961
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 11:55:22.810750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.48
 ---- batch: 020 ----
mean loss: 198.00
 ---- batch: 030 ----
mean loss: 194.71
 ---- batch: 040 ----
mean loss: 192.49
train mean loss: 196.29
epoch train time: 0:00:07.805666
elapsed time: 0:04:41.073049
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 11:55:30.616839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.43
 ---- batch: 020 ----
mean loss: 194.86
 ---- batch: 030 ----
mean loss: 196.92
 ---- batch: 040 ----
mean loss: 193.65
train mean loss: 195.39
epoch train time: 0:00:07.818488
elapsed time: 0:04:48.891959
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 11:55:38.435824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.80
 ---- batch: 020 ----
mean loss: 188.60
 ---- batch: 030 ----
mean loss: 193.66
 ---- batch: 040 ----
mean loss: 187.38
train mean loss: 188.35
epoch train time: 0:00:07.590146
elapsed time: 0:04:56.482599
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 11:55:46.026381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.84
 ---- batch: 020 ----
mean loss: 184.90
 ---- batch: 030 ----
mean loss: 185.61
 ---- batch: 040 ----
mean loss: 186.07
train mean loss: 185.13
epoch train time: 0:00:07.739682
elapsed time: 0:05:04.222735
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 11:55:53.766506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.08
 ---- batch: 020 ----
mean loss: 183.45
 ---- batch: 030 ----
mean loss: 181.83
 ---- batch: 040 ----
mean loss: 182.16
train mean loss: 183.75
epoch train time: 0:00:07.693657
elapsed time: 0:05:11.916834
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 11:56:01.460578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.98
 ---- batch: 020 ----
mean loss: 175.01
 ---- batch: 030 ----
mean loss: 177.75
 ---- batch: 040 ----
mean loss: 180.23
train mean loss: 179.84
epoch train time: 0:00:07.692169
elapsed time: 0:05:19.609358
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 11:56:09.153131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.99
 ---- batch: 020 ----
mean loss: 179.41
 ---- batch: 030 ----
mean loss: 180.35
 ---- batch: 040 ----
mean loss: 173.89
train mean loss: 176.15
epoch train time: 0:00:07.707164
elapsed time: 0:05:27.316982
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 11:56:16.860773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.88
 ---- batch: 020 ----
mean loss: 177.72
 ---- batch: 030 ----
mean loss: 178.55
 ---- batch: 040 ----
mean loss: 169.64
train mean loss: 175.94
epoch train time: 0:00:07.711329
elapsed time: 0:05:35.028723
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 11:56:24.572536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.59
 ---- batch: 020 ----
mean loss: 170.94
 ---- batch: 030 ----
mean loss: 179.77
 ---- batch: 040 ----
mean loss: 166.67
train mean loss: 173.89
epoch train time: 0:00:07.693466
elapsed time: 0:05:42.722661
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 11:56:32.266444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.21
 ---- batch: 020 ----
mean loss: 166.68
 ---- batch: 030 ----
mean loss: 171.50
 ---- batch: 040 ----
mean loss: 168.51
train mean loss: 169.37
epoch train time: 0:00:07.729660
elapsed time: 0:05:50.452716
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 11:56:39.996504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.69
 ---- batch: 020 ----
mean loss: 164.70
 ---- batch: 030 ----
mean loss: 158.66
 ---- batch: 040 ----
mean loss: 169.80
train mean loss: 167.58
epoch train time: 0:00:07.722610
elapsed time: 0:05:58.175746
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 11:56:47.719486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.76
 ---- batch: 020 ----
mean loss: 171.01
 ---- batch: 030 ----
mean loss: 166.33
 ---- batch: 040 ----
mean loss: 166.83
train mean loss: 167.08
epoch train time: 0:00:07.712704
elapsed time: 0:06:05.888822
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 11:56:55.432594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.96
 ---- batch: 020 ----
mean loss: 170.73
 ---- batch: 030 ----
mean loss: 162.60
 ---- batch: 040 ----
mean loss: 165.35
train mean loss: 165.02
epoch train time: 0:00:07.716072
elapsed time: 0:06:13.605286
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 11:57:03.149061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.97
 ---- batch: 020 ----
mean loss: 159.86
 ---- batch: 030 ----
mean loss: 167.66
 ---- batch: 040 ----
mean loss: 170.49
train mean loss: 165.00
epoch train time: 0:00:07.718974
elapsed time: 0:06:21.324654
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 11:57:10.868498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.44
 ---- batch: 020 ----
mean loss: 156.76
 ---- batch: 030 ----
mean loss: 168.83
 ---- batch: 040 ----
mean loss: 157.43
train mean loss: 160.52
epoch train time: 0:00:07.682303
elapsed time: 0:06:29.007400
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 11:57:18.551168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.33
 ---- batch: 020 ----
mean loss: 159.32
 ---- batch: 030 ----
mean loss: 153.47
 ---- batch: 040 ----
mean loss: 160.95
train mean loss: 158.11
epoch train time: 0:00:07.673265
elapsed time: 0:06:36.681109
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 11:57:26.224905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.83
 ---- batch: 020 ----
mean loss: 156.38
 ---- batch: 030 ----
mean loss: 150.84
 ---- batch: 040 ----
mean loss: 156.39
train mean loss: 156.12
epoch train time: 0:00:07.676539
elapsed time: 0:06:44.358097
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 11:57:33.901883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.99
 ---- batch: 020 ----
mean loss: 156.48
 ---- batch: 030 ----
mean loss: 154.14
 ---- batch: 040 ----
mean loss: 158.32
train mean loss: 155.66
epoch train time: 0:00:07.644207
elapsed time: 0:06:52.002704
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 11:57:41.546484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.97
 ---- batch: 020 ----
mean loss: 154.35
 ---- batch: 030 ----
mean loss: 154.31
 ---- batch: 040 ----
mean loss: 152.31
train mean loss: 153.21
epoch train time: 0:00:07.657287
elapsed time: 0:06:59.660429
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 11:57:49.204258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.82
 ---- batch: 020 ----
mean loss: 148.34
 ---- batch: 030 ----
mean loss: 154.53
 ---- batch: 040 ----
mean loss: 147.06
train mean loss: 152.41
epoch train time: 0:00:07.693521
elapsed time: 0:07:07.354403
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 11:57:56.898186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.97
 ---- batch: 020 ----
mean loss: 148.74
 ---- batch: 030 ----
mean loss: 148.30
 ---- batch: 040 ----
mean loss: 150.89
train mean loss: 150.01
epoch train time: 0:00:07.696945
elapsed time: 0:07:15.051795
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 11:58:04.595565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.86
 ---- batch: 020 ----
mean loss: 152.50
 ---- batch: 030 ----
mean loss: 152.39
 ---- batch: 040 ----
mean loss: 146.20
train mean loss: 150.10
epoch train time: 0:00:07.687936
elapsed time: 0:07:22.740146
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 11:58:12.283947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.12
 ---- batch: 020 ----
mean loss: 150.69
 ---- batch: 030 ----
mean loss: 150.82
 ---- batch: 040 ----
mean loss: 140.51
train mean loss: 148.35
epoch train time: 0:00:07.607395
elapsed time: 0:07:30.347975
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 11:58:19.891784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.65
 ---- batch: 020 ----
mean loss: 146.21
 ---- batch: 030 ----
mean loss: 144.90
 ---- batch: 040 ----
mean loss: 147.75
train mean loss: 147.29
epoch train time: 0:00:07.553823
elapsed time: 0:07:37.902252
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 11:58:27.446078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.68
 ---- batch: 020 ----
mean loss: 144.46
 ---- batch: 030 ----
mean loss: 142.01
 ---- batch: 040 ----
mean loss: 153.89
train mean loss: 147.26
epoch train time: 0:00:07.624182
elapsed time: 0:07:45.526900
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 11:58:35.070674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.79
 ---- batch: 020 ----
mean loss: 145.07
 ---- batch: 030 ----
mean loss: 145.20
 ---- batch: 040 ----
mean loss: 149.74
train mean loss: 145.72
epoch train time: 0:00:07.625084
elapsed time: 0:07:53.152405
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 11:58:42.696184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.16
 ---- batch: 020 ----
mean loss: 151.90
 ---- batch: 030 ----
mean loss: 139.82
 ---- batch: 040 ----
mean loss: 142.63
train mean loss: 143.68
epoch train time: 0:00:07.680442
elapsed time: 0:08:00.833319
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 11:58:50.377116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.77
 ---- batch: 020 ----
mean loss: 137.25
 ---- batch: 030 ----
mean loss: 146.91
 ---- batch: 040 ----
mean loss: 143.25
train mean loss: 141.83
epoch train time: 0:00:07.692762
elapsed time: 0:08:08.526578
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 11:58:58.070363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.93
 ---- batch: 020 ----
mean loss: 140.24
 ---- batch: 030 ----
mean loss: 136.84
 ---- batch: 040 ----
mean loss: 142.41
train mean loss: 139.85
epoch train time: 0:00:07.727939
elapsed time: 0:08:16.254953
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 11:59:05.798758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.67
 ---- batch: 020 ----
mean loss: 142.69
 ---- batch: 030 ----
mean loss: 139.41
 ---- batch: 040 ----
mean loss: 143.40
train mean loss: 140.77
epoch train time: 0:00:07.746067
elapsed time: 0:08:24.001487
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 11:59:13.545288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.21
 ---- batch: 020 ----
mean loss: 136.90
 ---- batch: 030 ----
mean loss: 137.81
 ---- batch: 040 ----
mean loss: 138.02
train mean loss: 137.89
epoch train time: 0:00:07.689452
elapsed time: 0:08:31.691356
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 11:59:21.235129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.87
 ---- batch: 020 ----
mean loss: 135.66
 ---- batch: 030 ----
mean loss: 141.45
 ---- batch: 040 ----
mean loss: 133.86
train mean loss: 138.75
epoch train time: 0:00:07.666083
elapsed time: 0:08:39.357815
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 11:59:28.901567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.35
 ---- batch: 020 ----
mean loss: 143.61
 ---- batch: 030 ----
mean loss: 133.59
 ---- batch: 040 ----
mean loss: 138.14
train mean loss: 137.60
epoch train time: 0:00:07.696384
elapsed time: 0:08:47.054576
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 11:59:36.598388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.63
 ---- batch: 020 ----
mean loss: 131.66
 ---- batch: 030 ----
mean loss: 137.84
 ---- batch: 040 ----
mean loss: 140.98
train mean loss: 136.12
epoch train time: 0:00:07.684070
elapsed time: 0:08:54.739175
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 11:59:44.283042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.14
 ---- batch: 020 ----
mean loss: 132.37
 ---- batch: 030 ----
mean loss: 134.91
 ---- batch: 040 ----
mean loss: 133.54
train mean loss: 133.68
epoch train time: 0:00:07.646792
elapsed time: 0:09:02.386450
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 11:59:51.930228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.60
 ---- batch: 020 ----
mean loss: 130.18
 ---- batch: 030 ----
mean loss: 130.59
 ---- batch: 040 ----
mean loss: 136.62
train mean loss: 133.38
epoch train time: 0:00:07.702869
elapsed time: 0:09:10.089703
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 11:59:59.633475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.74
 ---- batch: 020 ----
mean loss: 130.31
 ---- batch: 030 ----
mean loss: 134.64
 ---- batch: 040 ----
mean loss: 131.63
train mean loss: 132.90
epoch train time: 0:00:07.711935
elapsed time: 0:09:17.802138
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 12:00:07.345920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.38
 ---- batch: 020 ----
mean loss: 126.89
 ---- batch: 030 ----
mean loss: 131.62
 ---- batch: 040 ----
mean loss: 129.45
train mean loss: 130.26
epoch train time: 0:00:07.690033
elapsed time: 0:09:25.492584
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 12:00:15.036366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.24
 ---- batch: 020 ----
mean loss: 132.72
 ---- batch: 030 ----
mean loss: 131.26
 ---- batch: 040 ----
mean loss: 123.82
train mean loss: 130.07
epoch train time: 0:00:07.697589
elapsed time: 0:09:33.190559
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 12:00:22.734338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.97
 ---- batch: 020 ----
mean loss: 133.74
 ---- batch: 030 ----
mean loss: 129.34
 ---- batch: 040 ----
mean loss: 128.39
train mean loss: 128.80
epoch train time: 0:00:07.701130
elapsed time: 0:09:40.892105
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 12:00:30.435877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.38
 ---- batch: 020 ----
mean loss: 127.23
 ---- batch: 030 ----
mean loss: 130.14
 ---- batch: 040 ----
mean loss: 128.40
train mean loss: 129.57
epoch train time: 0:00:07.756259
elapsed time: 0:09:48.648736
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 12:00:38.192513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.24
 ---- batch: 020 ----
mean loss: 128.35
 ---- batch: 030 ----
mean loss: 128.70
 ---- batch: 040 ----
mean loss: 126.83
train mean loss: 127.39
epoch train time: 0:00:07.682389
elapsed time: 0:09:56.331535
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 12:00:45.875310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.51
 ---- batch: 020 ----
mean loss: 125.86
 ---- batch: 030 ----
mean loss: 124.46
 ---- batch: 040 ----
mean loss: 123.07
train mean loss: 124.85
epoch train time: 0:00:07.694087
elapsed time: 0:10:04.025996
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 12:00:53.569761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.39
 ---- batch: 020 ----
mean loss: 121.76
 ---- batch: 030 ----
mean loss: 128.49
 ---- batch: 040 ----
mean loss: 128.19
train mean loss: 125.88
epoch train time: 0:00:07.728480
elapsed time: 0:10:11.754864
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 12:01:01.298689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.46
 ---- batch: 020 ----
mean loss: 128.68
 ---- batch: 030 ----
mean loss: 125.33
 ---- batch: 040 ----
mean loss: 124.87
train mean loss: 125.22
epoch train time: 0:00:07.793089
elapsed time: 0:10:19.548400
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 12:01:09.092177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.46
 ---- batch: 020 ----
mean loss: 119.94
 ---- batch: 030 ----
mean loss: 125.10
 ---- batch: 040 ----
mean loss: 123.91
train mean loss: 123.51
epoch train time: 0:00:07.722649
elapsed time: 0:10:27.271446
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 12:01:16.815204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.54
 ---- batch: 020 ----
mean loss: 121.09
 ---- batch: 030 ----
mean loss: 124.76
 ---- batch: 040 ----
mean loss: 124.03
train mean loss: 122.91
epoch train time: 0:00:07.688416
elapsed time: 0:10:34.960287
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 12:01:24.504085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.39
 ---- batch: 020 ----
mean loss: 122.82
 ---- batch: 030 ----
mean loss: 123.20
 ---- batch: 040 ----
mean loss: 120.12
train mean loss: 122.85
epoch train time: 0:00:07.737388
elapsed time: 0:10:42.698097
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 12:01:32.241882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.55
 ---- batch: 020 ----
mean loss: 123.69
 ---- batch: 030 ----
mean loss: 119.56
 ---- batch: 040 ----
mean loss: 118.94
train mean loss: 119.99
epoch train time: 0:00:07.810130
elapsed time: 0:10:50.508635
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 12:01:40.052426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.48
 ---- batch: 020 ----
mean loss: 119.94
 ---- batch: 030 ----
mean loss: 118.83
 ---- batch: 040 ----
mean loss: 118.28
train mean loss: 119.90
epoch train time: 0:00:07.521648
elapsed time: 0:10:58.030706
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 12:01:47.574471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.04
 ---- batch: 020 ----
mean loss: 117.18
 ---- batch: 030 ----
mean loss: 117.32
 ---- batch: 040 ----
mean loss: 119.41
train mean loss: 116.65
epoch train time: 0:00:07.560842
elapsed time: 0:11:05.591980
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 12:01:55.135772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.72
 ---- batch: 020 ----
mean loss: 118.54
 ---- batch: 030 ----
mean loss: 117.53
 ---- batch: 040 ----
mean loss: 114.23
train mean loss: 117.54
epoch train time: 0:00:07.400556
elapsed time: 0:11:12.993025
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 12:02:02.536825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.32
 ---- batch: 020 ----
mean loss: 117.31
 ---- batch: 030 ----
mean loss: 118.41
 ---- batch: 040 ----
mean loss: 120.48
train mean loss: 117.01
epoch train time: 0:00:07.428001
elapsed time: 0:11:20.421511
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 12:02:09.965288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.81
 ---- batch: 020 ----
mean loss: 117.08
 ---- batch: 030 ----
mean loss: 114.81
 ---- batch: 040 ----
mean loss: 113.78
train mean loss: 115.87
epoch train time: 0:00:07.588875
elapsed time: 0:11:28.010824
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 12:02:17.554664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.91
 ---- batch: 020 ----
mean loss: 114.76
 ---- batch: 030 ----
mean loss: 115.42
 ---- batch: 040 ----
mean loss: 110.12
train mean loss: 114.06
epoch train time: 0:00:07.724415
elapsed time: 0:11:35.735731
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 12:02:25.279511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.55
 ---- batch: 020 ----
mean loss: 114.88
 ---- batch: 030 ----
mean loss: 113.46
 ---- batch: 040 ----
mean loss: 109.58
train mean loss: 113.91
epoch train time: 0:00:07.723315
elapsed time: 0:11:43.459506
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 12:02:33.003300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.02
 ---- batch: 020 ----
mean loss: 112.98
 ---- batch: 030 ----
mean loss: 114.62
 ---- batch: 040 ----
mean loss: 118.55
train mean loss: 115.09
epoch train time: 0:00:07.772643
elapsed time: 0:11:51.232591
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 12:02:40.776377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.99
 ---- batch: 020 ----
mean loss: 117.78
 ---- batch: 030 ----
mean loss: 107.59
 ---- batch: 040 ----
mean loss: 112.49
train mean loss: 113.10
epoch train time: 0:00:07.828263
elapsed time: 0:11:59.061359
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 12:02:48.605150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.58
 ---- batch: 020 ----
mean loss: 115.58
 ---- batch: 030 ----
mean loss: 108.05
 ---- batch: 040 ----
mean loss: 109.31
train mean loss: 111.05
epoch train time: 0:00:07.780899
elapsed time: 0:12:06.842738
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 12:02:56.386551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.57
 ---- batch: 020 ----
mean loss: 110.40
 ---- batch: 030 ----
mean loss: 110.59
 ---- batch: 040 ----
mean loss: 113.20
train mean loss: 111.06
epoch train time: 0:00:07.933148
elapsed time: 0:12:14.776441
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 12:03:04.320250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.05
 ---- batch: 020 ----
mean loss: 110.61
 ---- batch: 030 ----
mean loss: 111.44
 ---- batch: 040 ----
mean loss: 111.31
train mean loss: 111.30
epoch train time: 0:00:07.958600
elapsed time: 0:12:22.735535
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 12:03:12.279321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.31
 ---- batch: 020 ----
mean loss: 116.02
 ---- batch: 030 ----
mean loss: 104.38
 ---- batch: 040 ----
mean loss: 115.25
train mean loss: 110.51
epoch train time: 0:00:07.954732
elapsed time: 0:12:30.690738
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 12:03:20.234566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.17
 ---- batch: 020 ----
mean loss: 112.36
 ---- batch: 030 ----
mean loss: 108.89
 ---- batch: 040 ----
mean loss: 109.67
train mean loss: 109.84
epoch train time: 0:00:07.958521
elapsed time: 0:12:38.649818
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 12:03:28.193601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.50
 ---- batch: 020 ----
mean loss: 107.30
 ---- batch: 030 ----
mean loss: 105.15
 ---- batch: 040 ----
mean loss: 111.40
train mean loss: 108.67
epoch train time: 0:00:07.992094
elapsed time: 0:12:46.642328
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 12:03:36.186092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.89
 ---- batch: 020 ----
mean loss: 106.51
 ---- batch: 030 ----
mean loss: 108.21
 ---- batch: 040 ----
mean loss: 106.03
train mean loss: 108.06
epoch train time: 0:00:07.793433
elapsed time: 0:12:54.436135
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 12:03:43.979909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.72
 ---- batch: 020 ----
mean loss: 107.41
 ---- batch: 030 ----
mean loss: 107.10
 ---- batch: 040 ----
mean loss: 109.08
train mean loss: 106.48
epoch train time: 0:00:08.007696
elapsed time: 0:13:02.444232
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 12:03:51.988006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.77
 ---- batch: 020 ----
mean loss: 109.42
 ---- batch: 030 ----
mean loss: 106.97
 ---- batch: 040 ----
mean loss: 104.38
train mean loss: 106.94
epoch train time: 0:00:08.077528
elapsed time: 0:13:10.522203
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 12:04:00.066008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.83
 ---- batch: 020 ----
mean loss: 107.70
 ---- batch: 030 ----
mean loss: 107.07
 ---- batch: 040 ----
mean loss: 105.48
train mean loss: 106.17
epoch train time: 0:00:08.109883
elapsed time: 0:13:18.632495
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 12:04:08.176261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.55
 ---- batch: 020 ----
mean loss: 105.22
 ---- batch: 030 ----
mean loss: 104.28
 ---- batch: 040 ----
mean loss: 104.59
train mean loss: 104.96
epoch train time: 0:00:08.085634
elapsed time: 0:13:26.718555
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 12:04:16.262298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.96
 ---- batch: 020 ----
mean loss: 98.39
 ---- batch: 030 ----
mean loss: 103.12
 ---- batch: 040 ----
mean loss: 108.75
train mean loss: 103.68
epoch train time: 0:00:07.854934
elapsed time: 0:13:34.573923
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 12:04:24.117692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.70
 ---- batch: 020 ----
mean loss: 101.23
 ---- batch: 030 ----
mean loss: 105.28
 ---- batch: 040 ----
mean loss: 99.37
train mean loss: 102.64
epoch train time: 0:00:07.800890
elapsed time: 0:13:42.375303
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 12:04:31.919151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.37
 ---- batch: 020 ----
mean loss: 102.34
 ---- batch: 030 ----
mean loss: 99.10
 ---- batch: 040 ----
mean loss: 105.39
train mean loss: 102.75
epoch train time: 0:00:07.880268
elapsed time: 0:13:50.256114
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 12:04:39.799913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.97
 ---- batch: 020 ----
mean loss: 100.96
 ---- batch: 030 ----
mean loss: 100.18
 ---- batch: 040 ----
mean loss: 104.34
train mean loss: 103.08
epoch train time: 0:00:07.936027
elapsed time: 0:13:58.192619
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 12:04:47.736430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.72
 ---- batch: 020 ----
mean loss: 101.75
 ---- batch: 030 ----
mean loss: 102.29
 ---- batch: 040 ----
mean loss: 100.26
train mean loss: 101.25
epoch train time: 0:00:07.963143
elapsed time: 0:14:06.156331
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 12:04:55.700101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.63
 ---- batch: 020 ----
mean loss: 100.75
 ---- batch: 030 ----
mean loss: 102.30
 ---- batch: 040 ----
mean loss: 100.63
train mean loss: 101.35
epoch train time: 0:00:08.033612
elapsed time: 0:14:14.190434
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 12:05:03.734241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.29
 ---- batch: 020 ----
mean loss: 101.85
 ---- batch: 030 ----
mean loss: 96.39
 ---- batch: 040 ----
mean loss: 105.97
train mean loss: 100.90
epoch train time: 0:00:08.155531
elapsed time: 0:14:22.346524
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 12:05:11.890299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.30
 ---- batch: 020 ----
mean loss: 99.78
 ---- batch: 030 ----
mean loss: 104.08
 ---- batch: 040 ----
mean loss: 94.35
train mean loss: 99.87
epoch train time: 0:00:08.065452
elapsed time: 0:14:30.412454
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 12:05:19.956274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.50
 ---- batch: 020 ----
mean loss: 99.83
 ---- batch: 030 ----
mean loss: 102.23
 ---- batch: 040 ----
mean loss: 97.28
train mean loss: 99.57
epoch train time: 0:00:07.960207
elapsed time: 0:14:38.373121
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 12:05:27.916884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.70
 ---- batch: 020 ----
mean loss: 100.09
 ---- batch: 030 ----
mean loss: 97.40
 ---- batch: 040 ----
mean loss: 98.11
train mean loss: 98.44
epoch train time: 0:00:07.972830
elapsed time: 0:14:46.346368
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 12:05:35.890137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.66
 ---- batch: 020 ----
mean loss: 99.15
 ---- batch: 030 ----
mean loss: 99.13
 ---- batch: 040 ----
mean loss: 92.99
train mean loss: 98.48
epoch train time: 0:00:07.964502
elapsed time: 0:14:54.311364
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 12:05:43.855153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.33
 ---- batch: 020 ----
mean loss: 94.63
 ---- batch: 030 ----
mean loss: 98.93
 ---- batch: 040 ----
mean loss: 93.32
train mean loss: 96.69
epoch train time: 0:00:08.085110
elapsed time: 0:15:02.397021
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 12:05:51.940809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.15
 ---- batch: 020 ----
mean loss: 94.26
 ---- batch: 030 ----
mean loss: 101.27
 ---- batch: 040 ----
mean loss: 97.56
train mean loss: 98.24
epoch train time: 0:00:07.956579
elapsed time: 0:15:10.354081
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 12:05:59.897854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.66
 ---- batch: 020 ----
mean loss: 96.42
 ---- batch: 030 ----
mean loss: 99.86
 ---- batch: 040 ----
mean loss: 94.57
train mean loss: 96.91
epoch train time: 0:00:07.958910
elapsed time: 0:15:18.313499
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 12:06:07.857335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.24
 ---- batch: 020 ----
mean loss: 93.22
 ---- batch: 030 ----
mean loss: 97.31
 ---- batch: 040 ----
mean loss: 98.76
train mean loss: 96.14
epoch train time: 0:00:07.843732
elapsed time: 0:15:26.157687
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 12:06:15.701461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.74
 ---- batch: 020 ----
mean loss: 97.40
 ---- batch: 030 ----
mean loss: 91.72
 ---- batch: 040 ----
mean loss: 93.15
train mean loss: 94.76
epoch train time: 0:00:07.865547
elapsed time: 0:15:34.023629
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 12:06:23.567403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.30
 ---- batch: 020 ----
mean loss: 94.33
 ---- batch: 030 ----
mean loss: 97.27
 ---- batch: 040 ----
mean loss: 93.65
train mean loss: 95.45
epoch train time: 0:00:07.839912
elapsed time: 0:15:41.863999
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 12:06:31.407809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.94
 ---- batch: 020 ----
mean loss: 96.25
 ---- batch: 030 ----
mean loss: 96.81
 ---- batch: 040 ----
mean loss: 92.26
train mean loss: 94.77
epoch train time: 0:00:07.952863
elapsed time: 0:15:49.817375
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 12:06:39.361149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.29
 ---- batch: 020 ----
mean loss: 96.60
 ---- batch: 030 ----
mean loss: 101.12
 ---- batch: 040 ----
mean loss: 94.41
train mean loss: 95.26
epoch train time: 0:00:07.904049
elapsed time: 0:15:57.721963
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 12:06:47.265799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.36
 ---- batch: 020 ----
mean loss: 99.04
 ---- batch: 030 ----
mean loss: 91.76
 ---- batch: 040 ----
mean loss: 94.43
train mean loss: 94.27
epoch train time: 0:00:07.914875
elapsed time: 0:16:05.637366
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 12:06:55.181139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.84
 ---- batch: 020 ----
mean loss: 94.47
 ---- batch: 030 ----
mean loss: 94.22
 ---- batch: 040 ----
mean loss: 92.89
train mean loss: 93.20
epoch train time: 0:00:07.951365
elapsed time: 0:16:13.589226
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 12:07:03.133013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.41
 ---- batch: 020 ----
mean loss: 92.96
 ---- batch: 030 ----
mean loss: 93.40
 ---- batch: 040 ----
mean loss: 89.19
train mean loss: 92.64
epoch train time: 0:00:07.945268
elapsed time: 0:16:21.534996
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 12:07:11.078769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.39
 ---- batch: 020 ----
mean loss: 92.35
 ---- batch: 030 ----
mean loss: 90.71
 ---- batch: 040 ----
mean loss: 91.83
train mean loss: 92.15
epoch train time: 0:00:07.952244
elapsed time: 0:16:29.487718
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 12:07:19.031518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.59
 ---- batch: 020 ----
mean loss: 90.78
 ---- batch: 030 ----
mean loss: 98.56
 ---- batch: 040 ----
mean loss: 93.32
train mean loss: 92.25
epoch train time: 0:00:07.930079
elapsed time: 0:16:37.418240
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 12:07:26.962022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.06
 ---- batch: 020 ----
mean loss: 91.70
 ---- batch: 030 ----
mean loss: 91.54
 ---- batch: 040 ----
mean loss: 91.26
train mean loss: 91.40
epoch train time: 0:00:07.921800
elapsed time: 0:16:45.340565
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 12:07:34.884250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.45
 ---- batch: 020 ----
mean loss: 88.69
 ---- batch: 030 ----
mean loss: 90.29
 ---- batch: 040 ----
mean loss: 91.75
train mean loss: 91.88
epoch train time: 0:00:07.968680
elapsed time: 0:16:53.309608
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 12:07:42.853360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.08
 ---- batch: 020 ----
mean loss: 92.10
 ---- batch: 030 ----
mean loss: 92.11
 ---- batch: 040 ----
mean loss: 91.41
train mean loss: 90.97
epoch train time: 0:00:07.965335
elapsed time: 0:17:01.275319
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 12:07:50.819047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.09
 ---- batch: 020 ----
mean loss: 89.66
 ---- batch: 030 ----
mean loss: 91.60
 ---- batch: 040 ----
mean loss: 92.54
train mean loss: 92.01
epoch train time: 0:00:08.018644
elapsed time: 0:17:09.294336
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 12:07:58.838118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.69
 ---- batch: 020 ----
mean loss: 91.82
 ---- batch: 030 ----
mean loss: 88.69
 ---- batch: 040 ----
mean loss: 88.60
train mean loss: 90.46
epoch train time: 0:00:08.006837
elapsed time: 0:17:17.301701
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 12:08:06.845535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.46
 ---- batch: 020 ----
mean loss: 89.84
 ---- batch: 030 ----
mean loss: 90.95
 ---- batch: 040 ----
mean loss: 90.57
train mean loss: 90.92
epoch train time: 0:00:07.921052
elapsed time: 0:17:25.223273
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 12:08:14.767103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.38
 ---- batch: 020 ----
mean loss: 90.28
 ---- batch: 030 ----
mean loss: 89.04
 ---- batch: 040 ----
mean loss: 85.58
train mean loss: 89.20
epoch train time: 0:00:08.069171
elapsed time: 0:17:33.293144
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 12:08:22.836968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.01
 ---- batch: 020 ----
mean loss: 88.28
 ---- batch: 030 ----
mean loss: 90.12
 ---- batch: 040 ----
mean loss: 85.22
train mean loss: 87.27
epoch train time: 0:00:08.078472
elapsed time: 0:17:41.372127
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 12:08:30.915939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.42
 ---- batch: 020 ----
mean loss: 90.11
 ---- batch: 030 ----
mean loss: 87.12
 ---- batch: 040 ----
mean loss: 87.16
train mean loss: 87.77
epoch train time: 0:00:08.090552
elapsed time: 0:17:49.463168
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 12:08:39.006987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.23
 ---- batch: 020 ----
mean loss: 84.71
 ---- batch: 030 ----
mean loss: 84.44
 ---- batch: 040 ----
mean loss: 90.00
train mean loss: 85.96
epoch train time: 0:00:08.066054
elapsed time: 0:17:57.529727
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 12:08:47.073550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.16
 ---- batch: 020 ----
mean loss: 88.77
 ---- batch: 030 ----
mean loss: 84.04
 ---- batch: 040 ----
mean loss: 85.36
train mean loss: 86.93
epoch train time: 0:00:07.907312
elapsed time: 0:18:05.437492
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 12:08:54.981270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.50
 ---- batch: 020 ----
mean loss: 85.70
 ---- batch: 030 ----
mean loss: 87.94
 ---- batch: 040 ----
mean loss: 87.87
train mean loss: 86.71
epoch train time: 0:00:07.891072
elapsed time: 0:18:13.329058
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 12:09:02.872846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.00
 ---- batch: 020 ----
mean loss: 90.94
 ---- batch: 030 ----
mean loss: 86.41
 ---- batch: 040 ----
mean loss: 82.98
train mean loss: 85.58
epoch train time: 0:00:07.929268
elapsed time: 0:18:21.258853
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 12:09:10.802663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.41
 ---- batch: 020 ----
mean loss: 84.52
 ---- batch: 030 ----
mean loss: 86.64
 ---- batch: 040 ----
mean loss: 86.07
train mean loss: 85.99
epoch train time: 0:00:08.028030
elapsed time: 0:18:29.287342
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 12:09:18.831142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.08
 ---- batch: 020 ----
mean loss: 81.72
 ---- batch: 030 ----
mean loss: 85.83
 ---- batch: 040 ----
mean loss: 86.31
train mean loss: 84.96
epoch train time: 0:00:08.072245
elapsed time: 0:18:37.360046
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 12:09:26.903857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.80
 ---- batch: 020 ----
mean loss: 86.57
 ---- batch: 030 ----
mean loss: 85.97
 ---- batch: 040 ----
mean loss: 87.23
train mean loss: 85.17
epoch train time: 0:00:08.102913
elapsed time: 0:18:45.463483
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 12:09:35.007302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.39
 ---- batch: 020 ----
mean loss: 88.83
 ---- batch: 030 ----
mean loss: 85.89
 ---- batch: 040 ----
mean loss: 80.80
train mean loss: 84.17
epoch train time: 0:00:07.910389
elapsed time: 0:18:53.374339
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 12:09:42.918120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.38
 ---- batch: 020 ----
mean loss: 83.61
 ---- batch: 030 ----
mean loss: 83.43
 ---- batch: 040 ----
mean loss: 84.00
train mean loss: 83.79
epoch train time: 0:00:07.939037
elapsed time: 0:19:01.313795
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 12:09:50.857577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.08
 ---- batch: 020 ----
mean loss: 83.92
 ---- batch: 030 ----
mean loss: 85.33
 ---- batch: 040 ----
mean loss: 80.16
train mean loss: 83.28
epoch train time: 0:00:07.912326
elapsed time: 0:19:09.226694
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 12:09:58.770536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.29
 ---- batch: 020 ----
mean loss: 80.79
 ---- batch: 030 ----
mean loss: 85.14
 ---- batch: 040 ----
mean loss: 83.15
train mean loss: 83.12
epoch train time: 0:00:07.933143
elapsed time: 0:19:17.160432
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 12:10:06.704230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.64
 ---- batch: 020 ----
mean loss: 85.40
 ---- batch: 030 ----
mean loss: 79.97
 ---- batch: 040 ----
mean loss: 83.92
train mean loss: 82.00
epoch train time: 0:00:07.980442
elapsed time: 0:19:25.141371
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 12:10:14.685220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.70
 ---- batch: 020 ----
mean loss: 76.91
 ---- batch: 030 ----
mean loss: 84.90
 ---- batch: 040 ----
mean loss: 80.64
train mean loss: 81.00
epoch train time: 0:00:08.078851
elapsed time: 0:19:33.220795
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 12:10:22.764615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.26
 ---- batch: 020 ----
mean loss: 85.18
 ---- batch: 030 ----
mean loss: 81.81
 ---- batch: 040 ----
mean loss: 78.52
train mean loss: 81.56
epoch train time: 0:00:07.999345
elapsed time: 0:19:41.220717
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 12:10:30.764425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.32
 ---- batch: 020 ----
mean loss: 78.44
 ---- batch: 030 ----
mean loss: 80.83
 ---- batch: 040 ----
mean loss: 79.25
train mean loss: 80.76
epoch train time: 0:00:08.041606
elapsed time: 0:19:49.262773
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 12:10:38.806593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.35
 ---- batch: 020 ----
mean loss: 82.07
 ---- batch: 030 ----
mean loss: 79.44
 ---- batch: 040 ----
mean loss: 79.27
train mean loss: 80.12
epoch train time: 0:00:07.995485
elapsed time: 0:19:57.258772
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 12:10:46.802560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.75
 ---- batch: 020 ----
mean loss: 80.16
 ---- batch: 030 ----
mean loss: 79.25
 ---- batch: 040 ----
mean loss: 81.39
train mean loss: 80.82
epoch train time: 0:00:08.030824
elapsed time: 0:20:05.290056
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 12:10:54.833848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.56
 ---- batch: 020 ----
mean loss: 79.08
 ---- batch: 030 ----
mean loss: 77.79
 ---- batch: 040 ----
mean loss: 81.73
train mean loss: 79.23
epoch train time: 0:00:08.034869
elapsed time: 0:20:13.325355
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 12:11:02.869133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.77
 ---- batch: 020 ----
mean loss: 81.19
 ---- batch: 030 ----
mean loss: 74.86
 ---- batch: 040 ----
mean loss: 80.34
train mean loss: 79.39
epoch train time: 0:00:08.035846
elapsed time: 0:20:21.361659
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 12:11:10.905438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.40
 ---- batch: 020 ----
mean loss: 79.15
 ---- batch: 030 ----
mean loss: 78.40
 ---- batch: 040 ----
mean loss: 81.77
train mean loss: 78.98
epoch train time: 0:00:08.048058
elapsed time: 0:20:29.410150
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 12:11:18.953923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.89
 ---- batch: 020 ----
mean loss: 79.12
 ---- batch: 030 ----
mean loss: 84.48
 ---- batch: 040 ----
mean loss: 77.30
train mean loss: 79.36
epoch train time: 0:00:08.074973
elapsed time: 0:20:37.485560
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 12:11:27.029335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.75
 ---- batch: 020 ----
mean loss: 78.40
 ---- batch: 030 ----
mean loss: 75.72
 ---- batch: 040 ----
mean loss: 80.46
train mean loss: 77.98
epoch train time: 0:00:08.055879
elapsed time: 0:20:45.541959
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 12:11:35.085744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.58
 ---- batch: 020 ----
mean loss: 79.92
 ---- batch: 030 ----
mean loss: 77.08
 ---- batch: 040 ----
mean loss: 75.64
train mean loss: 77.38
epoch train time: 0:00:08.041504
elapsed time: 0:20:53.583928
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 12:11:43.127713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.10
 ---- batch: 020 ----
mean loss: 77.20
 ---- batch: 030 ----
mean loss: 79.45
 ---- batch: 040 ----
mean loss: 76.46
train mean loss: 77.68
epoch train time: 0:00:08.046282
elapsed time: 0:21:01.630637
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 12:11:51.174426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.61
 ---- batch: 020 ----
mean loss: 77.36
 ---- batch: 030 ----
mean loss: 74.63
 ---- batch: 040 ----
mean loss: 80.01
train mean loss: 77.20
epoch train time: 0:00:08.010899
elapsed time: 0:21:09.642072
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 12:11:59.185888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.92
 ---- batch: 020 ----
mean loss: 75.20
 ---- batch: 030 ----
mean loss: 75.66
 ---- batch: 040 ----
mean loss: 79.35
train mean loss: 76.43
epoch train time: 0:00:08.012714
elapsed time: 0:21:17.655195
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 12:12:07.198968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.33
 ---- batch: 020 ----
mean loss: 76.57
 ---- batch: 030 ----
mean loss: 75.29
 ---- batch: 040 ----
mean loss: 78.55
train mean loss: 76.75
epoch train time: 0:00:07.997825
elapsed time: 0:21:25.653481
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 12:12:15.197285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.71
 ---- batch: 020 ----
mean loss: 76.12
 ---- batch: 030 ----
mean loss: 74.01
 ---- batch: 040 ----
mean loss: 76.36
train mean loss: 75.81
epoch train time: 0:00:08.028443
elapsed time: 0:21:33.682432
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 12:12:23.226224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.23
 ---- batch: 020 ----
mean loss: 74.68
 ---- batch: 030 ----
mean loss: 80.13
 ---- batch: 040 ----
mean loss: 75.53
train mean loss: 76.18
epoch train time: 0:00:07.999098
elapsed time: 0:21:41.681947
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 12:12:31.225733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.62
 ---- batch: 020 ----
mean loss: 75.91
 ---- batch: 030 ----
mean loss: 76.96
 ---- batch: 040 ----
mean loss: 76.95
train mean loss: 75.56
epoch train time: 0:00:08.027950
elapsed time: 0:21:49.710340
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 12:12:39.254119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.24
 ---- batch: 020 ----
mean loss: 72.58
 ---- batch: 030 ----
mean loss: 76.04
 ---- batch: 040 ----
mean loss: 75.60
train mean loss: 75.64
epoch train time: 0:00:08.137764
elapsed time: 0:21:57.848505
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 12:12:47.392273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.84
 ---- batch: 020 ----
mean loss: 74.68
 ---- batch: 030 ----
mean loss: 75.11
 ---- batch: 040 ----
mean loss: 72.41
train mean loss: 74.35
epoch train time: 0:00:07.844041
elapsed time: 0:22:05.693110
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 12:12:55.236893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.38
 ---- batch: 020 ----
mean loss: 73.25
 ---- batch: 030 ----
mean loss: 76.25
 ---- batch: 040 ----
mean loss: 75.91
train mean loss: 74.53
epoch train time: 0:00:07.501855
elapsed time: 0:22:13.195410
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 12:13:02.739184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.50
 ---- batch: 020 ----
mean loss: 73.92
 ---- batch: 030 ----
mean loss: 80.39
 ---- batch: 040 ----
mean loss: 75.57
train mean loss: 76.55
epoch train time: 0:00:07.547559
elapsed time: 0:22:20.743365
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 12:13:10.287135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.18
 ---- batch: 020 ----
mean loss: 74.07
 ---- batch: 030 ----
mean loss: 79.41
 ---- batch: 040 ----
mean loss: 74.39
train mean loss: 75.82
epoch train time: 0:00:07.549302
elapsed time: 0:22:28.293130
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 12:13:17.836923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.98
 ---- batch: 020 ----
mean loss: 71.98
 ---- batch: 030 ----
mean loss: 73.51
 ---- batch: 040 ----
mean loss: 73.47
train mean loss: 73.28
epoch train time: 0:00:07.611805
elapsed time: 0:22:35.905385
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 12:13:25.449199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.59
 ---- batch: 020 ----
mean loss: 69.97
 ---- batch: 030 ----
mean loss: 77.25
 ---- batch: 040 ----
mean loss: 71.49
train mean loss: 72.98
epoch train time: 0:00:07.718089
elapsed time: 0:22:43.623932
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 12:13:33.167753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.75
 ---- batch: 020 ----
mean loss: 78.40
 ---- batch: 030 ----
mean loss: 73.57
 ---- batch: 040 ----
mean loss: 76.63
train mean loss: 77.33
epoch train time: 0:00:07.748659
elapsed time: 0:22:51.373113
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 12:13:40.916886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.11
 ---- batch: 020 ----
mean loss: 76.21
 ---- batch: 030 ----
mean loss: 74.33
 ---- batch: 040 ----
mean loss: 72.03
train mean loss: 73.63
epoch train time: 0:00:07.690136
elapsed time: 0:22:59.063865
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 12:13:48.607566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.77
 ---- batch: 020 ----
mean loss: 73.47
 ---- batch: 030 ----
mean loss: 71.57
 ---- batch: 040 ----
mean loss: 69.41
train mean loss: 72.19
epoch train time: 0:00:07.702267
elapsed time: 0:23:06.766448
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 12:13:56.310291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.84
 ---- batch: 020 ----
mean loss: 71.34
 ---- batch: 030 ----
mean loss: 72.84
 ---- batch: 040 ----
mean loss: 73.23
train mean loss: 72.23
epoch train time: 0:00:07.701029
elapsed time: 0:23:14.467999
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 12:14:04.011792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.42
 ---- batch: 020 ----
mean loss: 71.72
 ---- batch: 030 ----
mean loss: 72.74
 ---- batch: 040 ----
mean loss: 72.65
train mean loss: 71.61
epoch train time: 0:00:07.758130
elapsed time: 0:23:22.226599
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 12:14:11.770401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.32
 ---- batch: 020 ----
mean loss: 69.90
 ---- batch: 030 ----
mean loss: 70.18
 ---- batch: 040 ----
mean loss: 72.56
train mean loss: 71.95
epoch train time: 0:00:07.738567
elapsed time: 0:23:29.965594
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 12:14:19.509383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.96
 ---- batch: 020 ----
mean loss: 70.37
 ---- batch: 030 ----
mean loss: 69.97
 ---- batch: 040 ----
mean loss: 69.70
train mean loss: 70.92
epoch train time: 0:00:07.707457
elapsed time: 0:23:37.673482
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 12:14:27.217240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.35
 ---- batch: 020 ----
mean loss: 77.38
 ---- batch: 030 ----
mean loss: 75.07
 ---- batch: 040 ----
mean loss: 73.42
train mean loss: 74.65
epoch train time: 0:00:07.738662
elapsed time: 0:23:45.412571
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 12:14:34.956482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.49
 ---- batch: 020 ----
mean loss: 71.04
 ---- batch: 030 ----
mean loss: 70.51
 ---- batch: 040 ----
mean loss: 69.71
train mean loss: 70.41
epoch train time: 0:00:07.780558
elapsed time: 0:23:53.193671
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 12:14:42.737460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.27
 ---- batch: 020 ----
mean loss: 69.08
 ---- batch: 030 ----
mean loss: 69.19
 ---- batch: 040 ----
mean loss: 71.21
train mean loss: 70.28
epoch train time: 0:00:07.840126
elapsed time: 0:24:01.034287
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 12:14:50.578083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.57
 ---- batch: 020 ----
mean loss: 74.20
 ---- batch: 030 ----
mean loss: 69.48
 ---- batch: 040 ----
mean loss: 72.28
train mean loss: 70.96
epoch train time: 0:00:07.677496
elapsed time: 0:24:08.712260
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 12:14:58.256069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.38
 ---- batch: 020 ----
mean loss: 72.67
 ---- batch: 030 ----
mean loss: 70.54
 ---- batch: 040 ----
mean loss: 71.50
train mean loss: 70.54
epoch train time: 0:00:07.518580
elapsed time: 0:24:16.231323
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 12:15:05.775102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.12
 ---- batch: 020 ----
mean loss: 72.32
 ---- batch: 030 ----
mean loss: 66.21
 ---- batch: 040 ----
mean loss: 73.48
train mean loss: 70.24
epoch train time: 0:00:07.483450
elapsed time: 0:24:23.715242
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 12:15:13.259032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.35
 ---- batch: 020 ----
mean loss: 64.69
 ---- batch: 030 ----
mean loss: 72.36
 ---- batch: 040 ----
mean loss: 69.05
train mean loss: 69.45
epoch train time: 0:00:07.521098
elapsed time: 0:24:31.236860
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 12:15:20.780646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.55
 ---- batch: 020 ----
mean loss: 72.56
 ---- batch: 030 ----
mean loss: 69.90
 ---- batch: 040 ----
mean loss: 64.92
train mean loss: 69.43
epoch train time: 0:00:07.642561
elapsed time: 0:24:38.879868
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 12:15:28.423674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.72
 ---- batch: 020 ----
mean loss: 67.66
 ---- batch: 030 ----
mean loss: 67.52
 ---- batch: 040 ----
mean loss: 70.15
train mean loss: 68.19
epoch train time: 0:00:07.515794
elapsed time: 0:24:46.396095
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 12:15:35.939886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.41
 ---- batch: 020 ----
mean loss: 70.63
 ---- batch: 030 ----
mean loss: 68.76
 ---- batch: 040 ----
mean loss: 68.38
train mean loss: 68.17
epoch train time: 0:00:07.512134
elapsed time: 0:24:53.908663
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 12:15:43.452504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.10
 ---- batch: 020 ----
mean loss: 68.73
 ---- batch: 030 ----
mean loss: 67.86
 ---- batch: 040 ----
mean loss: 71.49
train mean loss: 69.24
epoch train time: 0:00:07.508639
elapsed time: 0:25:01.417776
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 12:15:50.961556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.15
 ---- batch: 020 ----
mean loss: 69.18
 ---- batch: 030 ----
mean loss: 68.38
 ---- batch: 040 ----
mean loss: 70.62
train mean loss: 69.06
epoch train time: 0:00:07.490687
elapsed time: 0:25:08.908926
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 12:15:58.452712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.73
 ---- batch: 020 ----
mean loss: 65.76
 ---- batch: 030 ----
mean loss: 70.54
 ---- batch: 040 ----
mean loss: 66.07
train mean loss: 67.24
epoch train time: 0:00:07.506847
elapsed time: 0:25:16.416210
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 12:16:05.959982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.08
 ---- batch: 020 ----
mean loss: 67.13
 ---- batch: 030 ----
mean loss: 70.77
 ---- batch: 040 ----
mean loss: 68.38
train mean loss: 68.56
epoch train time: 0:00:07.580599
elapsed time: 0:25:23.997305
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 12:16:13.541107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.06
 ---- batch: 020 ----
mean loss: 67.33
 ---- batch: 030 ----
mean loss: 68.64
 ---- batch: 040 ----
mean loss: 69.69
train mean loss: 67.65
epoch train time: 0:00:07.693569
elapsed time: 0:25:31.691357
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 12:16:21.235136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.87
 ---- batch: 020 ----
mean loss: 68.65
 ---- batch: 030 ----
mean loss: 67.15
 ---- batch: 040 ----
mean loss: 66.46
train mean loss: 67.00
epoch train time: 0:00:07.545724
elapsed time: 0:25:39.237488
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 12:16:28.781273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.61
 ---- batch: 020 ----
mean loss: 68.98
 ---- batch: 030 ----
mean loss: 69.88
 ---- batch: 040 ----
mean loss: 68.70
train mean loss: 68.38
epoch train time: 0:00:07.667814
elapsed time: 0:25:46.905747
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 12:16:36.449524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.82
 ---- batch: 020 ----
mean loss: 71.74
 ---- batch: 030 ----
mean loss: 65.40
 ---- batch: 040 ----
mean loss: 66.82
train mean loss: 68.08
epoch train time: 0:00:07.566597
elapsed time: 0:25:54.472795
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 12:16:44.016585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.89
 ---- batch: 020 ----
mean loss: 65.94
 ---- batch: 030 ----
mean loss: 67.57
 ---- batch: 040 ----
mean loss: 64.50
train mean loss: 66.70
epoch train time: 0:00:07.437096
elapsed time: 0:26:01.910392
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 12:16:51.454243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.96
 ---- batch: 020 ----
mean loss: 67.13
 ---- batch: 030 ----
mean loss: 67.29
 ---- batch: 040 ----
mean loss: 67.32
train mean loss: 66.75
epoch train time: 0:00:07.632410
elapsed time: 0:26:09.543348
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 12:16:59.087162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.87
 ---- batch: 020 ----
mean loss: 70.16
 ---- batch: 030 ----
mean loss: 66.54
 ---- batch: 040 ----
mean loss: 66.26
train mean loss: 66.59
epoch train time: 0:00:07.653162
elapsed time: 0:26:17.197098
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 12:17:06.740892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.70
 ---- batch: 020 ----
mean loss: 66.84
 ---- batch: 030 ----
mean loss: 65.96
 ---- batch: 040 ----
mean loss: 65.73
train mean loss: 65.92
epoch train time: 0:00:07.677284
elapsed time: 0:26:24.874850
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 12:17:14.418670
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.93
 ---- batch: 020 ----
mean loss: 64.13
 ---- batch: 030 ----
mean loss: 65.64
 ---- batch: 040 ----
mean loss: 66.81
train mean loss: 64.82
epoch train time: 0:00:07.745268
elapsed time: 0:26:32.620715
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 12:17:22.164405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.81
 ---- batch: 020 ----
mean loss: 65.88
 ---- batch: 030 ----
mean loss: 63.30
 ---- batch: 040 ----
mean loss: 63.84
train mean loss: 64.25
epoch train time: 0:00:07.723298
elapsed time: 0:26:40.344336
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 12:17:29.888116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.27
 ---- batch: 020 ----
mean loss: 66.84
 ---- batch: 030 ----
mean loss: 64.40
 ---- batch: 040 ----
mean loss: 63.06
train mean loss: 64.65
epoch train time: 0:00:07.746927
elapsed time: 0:26:48.091660
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 12:17:37.635423
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.74
 ---- batch: 020 ----
mean loss: 63.97
 ---- batch: 030 ----
mean loss: 64.86
 ---- batch: 040 ----
mean loss: 66.73
train mean loss: 64.57
epoch train time: 0:00:07.636705
elapsed time: 0:26:55.728800
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 12:17:45.272602
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.47
 ---- batch: 020 ----
mean loss: 65.74
 ---- batch: 030 ----
mean loss: 65.08
 ---- batch: 040 ----
mean loss: 63.82
train mean loss: 65.00
epoch train time: 0:00:07.771344
elapsed time: 0:27:03.500613
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 12:17:53.044414
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.66
 ---- batch: 020 ----
mean loss: 64.34
 ---- batch: 030 ----
mean loss: 64.39
 ---- batch: 040 ----
mean loss: 62.25
train mean loss: 64.62
epoch train time: 0:00:07.627185
elapsed time: 0:27:11.128234
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 12:18:00.672039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.01
 ---- batch: 020 ----
mean loss: 64.81
 ---- batch: 030 ----
mean loss: 65.16
 ---- batch: 040 ----
mean loss: 63.66
train mean loss: 65.22
epoch train time: 0:00:07.642166
elapsed time: 0:27:18.770854
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 12:18:08.314619
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.84
 ---- batch: 020 ----
mean loss: 66.22
 ---- batch: 030 ----
mean loss: 62.13
 ---- batch: 040 ----
mean loss: 65.41
train mean loss: 64.49
epoch train time: 0:00:07.480240
elapsed time: 0:27:26.251496
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 12:18:15.795264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.26
 ---- batch: 020 ----
mean loss: 63.77
 ---- batch: 030 ----
mean loss: 62.14
 ---- batch: 040 ----
mean loss: 66.41
train mean loss: 64.00
epoch train time: 0:00:07.508596
elapsed time: 0:27:33.760469
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 12:18:23.304241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.61
 ---- batch: 020 ----
mean loss: 68.04
 ---- batch: 030 ----
mean loss: 63.66
 ---- batch: 040 ----
mean loss: 60.36
train mean loss: 64.48
epoch train time: 0:00:07.518917
elapsed time: 0:27:41.279874
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 12:18:30.823687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.23
 ---- batch: 020 ----
mean loss: 64.53
 ---- batch: 030 ----
mean loss: 61.42
 ---- batch: 040 ----
mean loss: 65.90
train mean loss: 64.80
epoch train time: 0:00:07.881775
elapsed time: 0:27:49.162111
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 12:18:38.705901
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.07
 ---- batch: 020 ----
mean loss: 63.96
 ---- batch: 030 ----
mean loss: 61.34
 ---- batch: 040 ----
mean loss: 63.94
train mean loss: 63.79
epoch train time: 0:00:07.892715
elapsed time: 0:27:57.055338
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 12:18:46.599176
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.73
 ---- batch: 020 ----
mean loss: 61.29
 ---- batch: 030 ----
mean loss: 62.41
 ---- batch: 040 ----
mean loss: 68.81
train mean loss: 64.32
epoch train time: 0:00:07.904986
elapsed time: 0:28:04.960896
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 12:18:54.504672
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.88
 ---- batch: 020 ----
mean loss: 66.04
 ---- batch: 030 ----
mean loss: 65.12
 ---- batch: 040 ----
mean loss: 62.70
train mean loss: 64.76
epoch train time: 0:00:07.878594
elapsed time: 0:28:12.839912
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 12:19:02.383685
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.70
 ---- batch: 020 ----
mean loss: 64.49
 ---- batch: 030 ----
mean loss: 65.11
 ---- batch: 040 ----
mean loss: 63.85
train mean loss: 64.32
epoch train time: 0:00:07.916281
elapsed time: 0:28:20.756601
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 12:19:10.300409
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.65
 ---- batch: 020 ----
mean loss: 64.44
 ---- batch: 030 ----
mean loss: 64.32
 ---- batch: 040 ----
mean loss: 60.58
train mean loss: 64.38
epoch train time: 0:00:07.783300
elapsed time: 0:28:28.540336
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 12:19:18.084143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.27
 ---- batch: 020 ----
mean loss: 62.41
 ---- batch: 030 ----
mean loss: 67.00
 ---- batch: 040 ----
mean loss: 65.11
train mean loss: 64.25
epoch train time: 0:00:07.787574
elapsed time: 0:28:36.328330
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 12:19:25.872111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.90
 ---- batch: 020 ----
mean loss: 65.22
 ---- batch: 030 ----
mean loss: 65.37
 ---- batch: 040 ----
mean loss: 63.98
train mean loss: 64.10
epoch train time: 0:00:07.770405
elapsed time: 0:28:44.099181
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 12:19:33.642970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.64
 ---- batch: 020 ----
mean loss: 65.03
 ---- batch: 030 ----
mean loss: 63.90
 ---- batch: 040 ----
mean loss: 64.20
train mean loss: 64.39
epoch train time: 0:00:07.784125
elapsed time: 0:28:51.883724
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 12:19:41.427522
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.00
 ---- batch: 020 ----
mean loss: 65.19
 ---- batch: 030 ----
mean loss: 65.88
 ---- batch: 040 ----
mean loss: 64.21
train mean loss: 64.48
epoch train time: 0:00:07.783082
elapsed time: 0:28:59.667240
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 12:19:49.211019
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.22
 ---- batch: 020 ----
mean loss: 64.01
 ---- batch: 030 ----
mean loss: 61.99
 ---- batch: 040 ----
mean loss: 65.24
train mean loss: 64.02
epoch train time: 0:00:07.738053
elapsed time: 0:29:07.405741
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 12:19:56.949475
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.28
 ---- batch: 020 ----
mean loss: 66.34
 ---- batch: 030 ----
mean loss: 64.58
 ---- batch: 040 ----
mean loss: 63.81
train mean loss: 64.30
epoch train time: 0:00:07.826848
elapsed time: 0:29:15.233002
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 12:20:04.776742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.78
 ---- batch: 020 ----
mean loss: 63.06
 ---- batch: 030 ----
mean loss: 64.90
 ---- batch: 040 ----
mean loss: 63.25
train mean loss: 63.82
epoch train time: 0:00:07.671190
elapsed time: 0:29:22.904529
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 12:20:12.448303
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.83
 ---- batch: 020 ----
mean loss: 66.35
 ---- batch: 030 ----
mean loss: 62.99
 ---- batch: 040 ----
mean loss: 63.97
train mean loss: 64.14
epoch train time: 0:00:07.687563
elapsed time: 0:29:30.592509
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 12:20:20.136315
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.42
 ---- batch: 020 ----
mean loss: 62.00
 ---- batch: 030 ----
mean loss: 64.93
 ---- batch: 040 ----
mean loss: 66.02
train mean loss: 63.98
epoch train time: 0:00:07.704546
elapsed time: 0:29:38.297557
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 12:20:27.841347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.10
 ---- batch: 020 ----
mean loss: 64.66
 ---- batch: 030 ----
mean loss: 61.61
 ---- batch: 040 ----
mean loss: 65.23
train mean loss: 64.01
epoch train time: 0:00:07.703846
elapsed time: 0:29:46.001869
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 12:20:35.545721
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.93
 ---- batch: 020 ----
mean loss: 64.04
 ---- batch: 030 ----
mean loss: 65.70
 ---- batch: 040 ----
mean loss: 61.75
train mean loss: 64.01
epoch train time: 0:00:07.799319
elapsed time: 0:29:53.801684
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 12:20:43.345465
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.39
 ---- batch: 020 ----
mean loss: 61.18
 ---- batch: 030 ----
mean loss: 66.21
 ---- batch: 040 ----
mean loss: 63.68
train mean loss: 64.22
epoch train time: 0:00:07.720189
elapsed time: 0:30:01.522282
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 12:20:51.066078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.09
 ---- batch: 020 ----
mean loss: 63.39
 ---- batch: 030 ----
mean loss: 63.18
 ---- batch: 040 ----
mean loss: 64.81
train mean loss: 63.78
epoch train time: 0:00:07.734650
elapsed time: 0:30:09.257461
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 12:20:58.801272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.66
 ---- batch: 020 ----
mean loss: 65.32
 ---- batch: 030 ----
mean loss: 64.48
 ---- batch: 040 ----
mean loss: 61.63
train mean loss: 64.26
epoch train time: 0:00:07.717162
elapsed time: 0:30:16.975105
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 12:21:06.518881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.88
 ---- batch: 020 ----
mean loss: 62.19
 ---- batch: 030 ----
mean loss: 63.69
 ---- batch: 040 ----
mean loss: 63.73
train mean loss: 63.98
epoch train time: 0:00:07.745477
elapsed time: 0:30:24.720990
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 12:21:14.264770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.37
 ---- batch: 020 ----
mean loss: 63.41
 ---- batch: 030 ----
mean loss: 65.95
 ---- batch: 040 ----
mean loss: 61.22
train mean loss: 63.22
epoch train time: 0:00:07.718106
elapsed time: 0:30:32.439527
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 12:21:21.983244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.86
 ---- batch: 020 ----
mean loss: 65.29
 ---- batch: 030 ----
mean loss: 63.52
 ---- batch: 040 ----
mean loss: 62.48
train mean loss: 64.15
epoch train time: 0:00:07.775733
elapsed time: 0:30:40.215729
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 12:21:29.759396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.68
 ---- batch: 020 ----
mean loss: 64.34
 ---- batch: 030 ----
mean loss: 65.40
 ---- batch: 040 ----
mean loss: 65.19
train mean loss: 64.43
epoch train time: 0:00:07.695751
elapsed time: 0:30:47.911767
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 12:21:37.455571
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.88
 ---- batch: 020 ----
mean loss: 62.62
 ---- batch: 030 ----
mean loss: 62.31
 ---- batch: 040 ----
mean loss: 65.39
train mean loss: 63.86
epoch train time: 0:00:07.587625
elapsed time: 0:30:55.499896
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 12:21:45.043693
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.64
 ---- batch: 020 ----
mean loss: 64.72
 ---- batch: 030 ----
mean loss: 63.73
 ---- batch: 040 ----
mean loss: 61.20
train mean loss: 63.39
epoch train time: 0:00:07.584538
elapsed time: 0:31:03.084851
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 12:21:52.628648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.48
 ---- batch: 020 ----
mean loss: 60.98
 ---- batch: 030 ----
mean loss: 64.75
 ---- batch: 040 ----
mean loss: 63.62
train mean loss: 63.86
epoch train time: 0:00:07.553493
elapsed time: 0:31:10.638872
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 12:22:00.182681
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.50
 ---- batch: 020 ----
mean loss: 62.91
 ---- batch: 030 ----
mean loss: 64.53
 ---- batch: 040 ----
mean loss: 62.81
train mean loss: 63.59
epoch train time: 0:00:07.564842
elapsed time: 0:31:18.204137
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 12:22:07.747930
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.95
 ---- batch: 020 ----
mean loss: 63.67
 ---- batch: 030 ----
mean loss: 64.99
 ---- batch: 040 ----
mean loss: 64.10
train mean loss: 63.50
epoch train time: 0:00:07.616626
elapsed time: 0:31:25.821194
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 12:22:15.364980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.41
 ---- batch: 020 ----
mean loss: 64.65
 ---- batch: 030 ----
mean loss: 60.06
 ---- batch: 040 ----
mean loss: 63.92
train mean loss: 63.61
epoch train time: 0:00:07.711706
elapsed time: 0:31:33.533324
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 12:22:23.077111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.02
 ---- batch: 020 ----
mean loss: 65.13
 ---- batch: 030 ----
mean loss: 63.08
 ---- batch: 040 ----
mean loss: 63.53
train mean loss: 63.90
epoch train time: 0:00:07.919589
elapsed time: 0:31:41.453352
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 12:22:30.997148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.14
 ---- batch: 020 ----
mean loss: 62.84
 ---- batch: 030 ----
mean loss: 63.71
 ---- batch: 040 ----
mean loss: 63.70
train mean loss: 63.61
epoch train time: 0:00:07.969508
elapsed time: 0:31:49.423323
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 12:22:38.967132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.52
 ---- batch: 020 ----
mean loss: 64.15
 ---- batch: 030 ----
mean loss: 62.74
 ---- batch: 040 ----
mean loss: 64.84
train mean loss: 63.50
epoch train time: 0:00:07.758899
elapsed time: 0:31:57.182680
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 12:22:46.726505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.69
 ---- batch: 020 ----
mean loss: 65.25
 ---- batch: 030 ----
mean loss: 63.94
 ---- batch: 040 ----
mean loss: 62.89
train mean loss: 63.49
epoch train time: 0:00:07.733121
elapsed time: 0:32:04.916289
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 12:22:54.460082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.35
 ---- batch: 020 ----
mean loss: 67.28
 ---- batch: 030 ----
mean loss: 61.66
 ---- batch: 040 ----
mean loss: 64.55
train mean loss: 63.69
epoch train time: 0:00:07.716710
elapsed time: 0:32:12.633453
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 12:23:02.177190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.99
 ---- batch: 020 ----
mean loss: 62.65
 ---- batch: 030 ----
mean loss: 65.67
 ---- batch: 040 ----
mean loss: 63.42
train mean loss: 63.70
epoch train time: 0:00:07.744692
elapsed time: 0:32:20.378578
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 12:23:09.922407
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.23
 ---- batch: 020 ----
mean loss: 64.01
 ---- batch: 030 ----
mean loss: 62.74
 ---- batch: 040 ----
mean loss: 62.59
train mean loss: 63.52
epoch train time: 0:00:07.915907
elapsed time: 0:32:28.295059
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 12:23:17.838836
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.51
 ---- batch: 020 ----
mean loss: 62.53
 ---- batch: 030 ----
mean loss: 65.47
 ---- batch: 040 ----
mean loss: 61.77
train mean loss: 63.80
epoch train time: 0:00:07.717865
elapsed time: 0:32:36.013319
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 12:23:25.557109
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.25
 ---- batch: 020 ----
mean loss: 62.69
 ---- batch: 030 ----
mean loss: 61.47
 ---- batch: 040 ----
mean loss: 62.15
train mean loss: 62.82
epoch train time: 0:00:07.593847
elapsed time: 0:32:43.616363
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_2/checkpoint.pth.tar
**** end time: 2019-09-20 12:23:33.160004 ****
