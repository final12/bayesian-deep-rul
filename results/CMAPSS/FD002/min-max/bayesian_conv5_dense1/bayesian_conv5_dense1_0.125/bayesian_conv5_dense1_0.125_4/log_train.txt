Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.125/bayesian_conv5_dense1_0.125_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.125, resume=False, step_size=200, visualize_step=50)
pid: 17351
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 16:27:11.352449 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 16:27:11.369872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2271.66
train mean loss: 2187.62
epoch train time: 0:00:06.009777
elapsed time: 0:00:06.035269
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 16:27:17.387767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1550.18
train mean loss: 1509.55
epoch train time: 0:00:02.127980
elapsed time: 0:00:08.163505
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 16:27:19.516042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1304.10
train mean loss: 1299.56
epoch train time: 0:00:02.116313
elapsed time: 0:00:10.280643
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 16:27:21.633177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1211.62
train mean loss: 1205.86
epoch train time: 0:00:02.122767
elapsed time: 0:00:12.403698
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 16:27:23.756220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1134.21
train mean loss: 1149.21
epoch train time: 0:00:02.124626
elapsed time: 0:00:14.528604
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 16:27:25.881126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1121.87
train mean loss: 1112.30
epoch train time: 0:00:02.115985
elapsed time: 0:00:16.644862
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 16:27:27.997385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1097.75
train mean loss: 1097.36
epoch train time: 0:00:02.119603
elapsed time: 0:00:18.764726
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 16:27:30.117262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1090.11
train mean loss: 1079.22
epoch train time: 0:00:02.114150
elapsed time: 0:00:20.879164
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 16:27:32.231685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1055.89
train mean loss: 1058.12
epoch train time: 0:00:02.118423
elapsed time: 0:00:22.997899
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 16:27:34.350423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1054.89
train mean loss: 1065.14
epoch train time: 0:00:02.120992
elapsed time: 0:00:25.119151
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 16:27:36.471677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1049.63
train mean loss: 1046.57
epoch train time: 0:00:02.115602
elapsed time: 0:00:27.235034
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 16:27:38.587556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1013.86
train mean loss: 1020.76
epoch train time: 0:00:02.124952
elapsed time: 0:00:29.360258
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 16:27:40.712789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1037.57
train mean loss: 1032.34
epoch train time: 0:00:02.137023
elapsed time: 0:00:31.497647
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 16:27:42.850173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1049.37
train mean loss: 1037.04
epoch train time: 0:00:02.139065
elapsed time: 0:00:33.636998
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 16:27:44.989534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1021.66
train mean loss: 1017.10
epoch train time: 0:00:02.138220
elapsed time: 0:00:35.775496
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 16:27:47.128033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1033.82
train mean loss: 1021.33
epoch train time: 0:00:02.142096
elapsed time: 0:00:37.917877
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 16:27:49.270424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1016.26
train mean loss: 1027.36
epoch train time: 0:00:02.119913
elapsed time: 0:00:40.038087
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 16:27:51.390623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1028.37
train mean loss: 1012.64
epoch train time: 0:00:02.111635
elapsed time: 0:00:42.149998
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 16:27:53.502540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1016.45
train mean loss: 1014.90
epoch train time: 0:00:02.118880
elapsed time: 0:00:44.269192
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 16:27:55.621721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1013.29
train mean loss: 1017.97
epoch train time: 0:00:02.119384
elapsed time: 0:00:46.388882
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 16:27:57.741407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1006.39
train mean loss: 1011.38
epoch train time: 0:00:02.130036
elapsed time: 0:00:48.519227
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 16:27:59.871818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 997.03
train mean loss: 999.17
epoch train time: 0:00:02.124770
elapsed time: 0:00:50.644347
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 16:28:01.996870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 994.91
train mean loss: 998.33
epoch train time: 0:00:02.115556
elapsed time: 0:00:52.760193
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 16:28:04.112736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1002.83
train mean loss: 1007.74
epoch train time: 0:00:02.120423
elapsed time: 0:00:54.880939
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 16:28:06.233518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1000.79
train mean loss: 997.32
epoch train time: 0:00:02.120387
elapsed time: 0:00:57.001628
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 16:28:08.354182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1015.21
train mean loss: 1012.82
epoch train time: 0:00:02.114809
elapsed time: 0:00:59.116733
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 16:28:10.469255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 987.48
train mean loss: 983.28
epoch train time: 0:00:02.120998
elapsed time: 0:01:01.238024
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 16:28:12.590550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 988.36
train mean loss: 995.90
epoch train time: 0:00:02.110700
elapsed time: 0:01:03.349015
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 16:28:14.701542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.93
train mean loss: 999.91
epoch train time: 0:00:02.119838
elapsed time: 0:01:05.469235
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 16:28:16.821765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.23
train mean loss: 993.23
epoch train time: 0:00:02.129014
elapsed time: 0:01:07.598588
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 16:28:18.951126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 966.13
train mean loss: 981.34
epoch train time: 0:00:02.113992
elapsed time: 0:01:09.712944
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 16:28:21.065495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 990.71
train mean loss: 986.47
epoch train time: 0:00:02.113507
elapsed time: 0:01:11.826818
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 16:28:23.179348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 994.13
train mean loss: 988.43
epoch train time: 0:00:02.117794
elapsed time: 0:01:13.944879
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 16:28:25.297427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 975.20
train mean loss: 976.62
epoch train time: 0:00:02.112777
elapsed time: 0:01:16.057955
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 16:28:27.410497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.49
train mean loss: 976.10
epoch train time: 0:00:02.110409
elapsed time: 0:01:18.168676
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 16:28:29.521205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.98
train mean loss: 985.25
epoch train time: 0:00:02.119319
elapsed time: 0:01:20.288268
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 16:28:31.640816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.20
train mean loss: 980.65
epoch train time: 0:00:02.114129
elapsed time: 0:01:22.402710
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 16:28:33.755236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.55
train mean loss: 981.27
epoch train time: 0:00:02.111188
elapsed time: 0:01:24.514170
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 16:28:35.866694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 991.32
train mean loss: 988.66
epoch train time: 0:00:02.116647
elapsed time: 0:01:26.631084
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 16:28:37.983625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 991.42
train mean loss: 988.61
epoch train time: 0:00:02.116643
elapsed time: 0:01:28.748025
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 16:28:40.100556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 965.17
train mean loss: 973.69
epoch train time: 0:00:02.106607
elapsed time: 0:01:30.854902
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 16:28:42.207434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.13
train mean loss: 981.40
epoch train time: 0:00:02.112429
elapsed time: 0:01:32.967623
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 16:28:44.320169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 971.48
train mean loss: 979.27
epoch train time: 0:00:02.110715
elapsed time: 0:01:35.078649
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 16:28:46.431168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 967.81
train mean loss: 970.96
epoch train time: 0:00:02.113741
elapsed time: 0:01:37.192665
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 16:28:48.545220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 977.74
train mean loss: 980.76
epoch train time: 0:00:02.117186
elapsed time: 0:01:39.310203
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 16:28:50.662743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.59
train mean loss: 980.06
epoch train time: 0:00:02.118393
elapsed time: 0:01:41.428905
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 16:28:52.781436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 975.19
train mean loss: 969.20
epoch train time: 0:00:02.109749
elapsed time: 0:01:43.538974
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 16:28:54.891508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 965.31
train mean loss: 965.43
epoch train time: 0:00:02.112493
elapsed time: 0:01:45.651754
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 16:28:57.004287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 977.67
train mean loss: 969.60
epoch train time: 0:00:02.117350
elapsed time: 0:01:47.769391
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 16:28:59.121913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 976.63
train mean loss: 975.14
epoch train time: 0:00:02.115758
elapsed time: 0:01:49.885440
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 16:29:01.237969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 962.82
train mean loss: 959.79
epoch train time: 0:00:02.118428
elapsed time: 0:01:52.004160
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 16:29:03.356688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.43
train mean loss: 968.99
epoch train time: 0:00:02.116161
elapsed time: 0:01:54.120730
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 16:29:05.473267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.25
train mean loss: 976.06
epoch train time: 0:00:02.116842
elapsed time: 0:01:56.237863
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 16:29:07.590390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 954.53
train mean loss: 960.09
epoch train time: 0:00:02.115574
elapsed time: 0:01:58.353743
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 16:29:09.706267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.52
train mean loss: 964.51
epoch train time: 0:00:02.117443
elapsed time: 0:02:00.471514
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 16:29:11.824042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 952.39
train mean loss: 954.13
epoch train time: 0:00:02.117904
elapsed time: 0:02:02.589726
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 16:29:13.942251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.90
train mean loss: 958.30
epoch train time: 0:00:02.123046
elapsed time: 0:02:04.713052
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 16:29:16.065585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 959.97
train mean loss: 965.84
epoch train time: 0:00:02.121490
elapsed time: 0:02:06.834843
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 16:29:18.187383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.71
train mean loss: 955.46
epoch train time: 0:00:02.124230
elapsed time: 0:02:08.959361
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 16:29:20.311883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 962.05
train mean loss: 959.45
epoch train time: 0:00:02.114842
elapsed time: 0:02:11.074493
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 16:29:22.427016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 952.07
train mean loss: 965.37
epoch train time: 0:00:02.116376
elapsed time: 0:02:13.191148
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 16:29:24.543680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 971.19
train mean loss: 965.97
epoch train time: 0:00:02.115235
elapsed time: 0:02:15.306666
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 16:29:26.659191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.95
train mean loss: 960.79
epoch train time: 0:00:02.116247
elapsed time: 0:02:17.423231
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 16:29:28.775756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.14
train mean loss: 957.43
epoch train time: 0:00:02.117025
elapsed time: 0:02:19.540538
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 16:29:30.893065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.17
train mean loss: 947.80
epoch train time: 0:00:02.111884
elapsed time: 0:02:21.652717
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 16:29:33.005250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.69
train mean loss: 959.31
epoch train time: 0:00:02.115658
elapsed time: 0:02:23.768699
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 16:29:35.121220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 945.34
train mean loss: 956.22
epoch train time: 0:00:02.119553
elapsed time: 0:02:25.888543
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 16:29:37.241072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.41
train mean loss: 950.42
epoch train time: 0:00:02.108676
elapsed time: 0:02:27.997486
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 16:29:39.350015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.76
train mean loss: 955.96
epoch train time: 0:00:02.116208
elapsed time: 0:02:30.113980
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 16:29:41.466507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.81
train mean loss: 950.43
epoch train time: 0:00:02.103041
elapsed time: 0:02:32.217331
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 16:29:43.569860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 954.42
train mean loss: 947.93
epoch train time: 0:00:02.115701
elapsed time: 0:02:34.333329
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 16:29:45.685924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 953.68
train mean loss: 959.66
epoch train time: 0:00:02.118397
elapsed time: 0:02:36.452094
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 16:29:47.804620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.77
train mean loss: 944.48
epoch train time: 0:00:02.115840
elapsed time: 0:02:38.568243
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 16:29:49.920769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 954.38
train mean loss: 958.29
epoch train time: 0:00:02.122722
elapsed time: 0:02:40.691271
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 16:29:52.043798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 952.38
train mean loss: 953.13
epoch train time: 0:00:02.112354
elapsed time: 0:02:42.803913
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 16:29:54.156449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.42
train mean loss: 952.41
epoch train time: 0:00:02.113554
elapsed time: 0:02:44.917807
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 16:29:56.270327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.15
train mean loss: 937.38
epoch train time: 0:00:02.120044
elapsed time: 0:02:47.038114
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 16:29:58.390637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.56
train mean loss: 941.55
epoch train time: 0:00:02.120403
elapsed time: 0:02:49.158790
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 16:30:00.511321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 954.10
train mean loss: 953.43
epoch train time: 0:00:02.120317
elapsed time: 0:02:51.279426
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 16:30:02.631971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.34
train mean loss: 939.57
epoch train time: 0:00:02.116600
elapsed time: 0:02:53.396328
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 16:30:04.748872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.42
train mean loss: 953.16
epoch train time: 0:00:02.118666
elapsed time: 0:02:55.515297
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 16:30:06.867869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.59
train mean loss: 937.88
epoch train time: 0:00:02.135367
elapsed time: 0:02:57.651042
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 16:30:09.003610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.09
train mean loss: 940.62
epoch train time: 0:00:02.138068
elapsed time: 0:02:59.789429
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 16:30:11.141963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.84
train mean loss: 936.23
epoch train time: 0:00:02.114606
elapsed time: 0:03:01.904338
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 16:30:13.256877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.41
train mean loss: 937.62
epoch train time: 0:00:02.110998
elapsed time: 0:03:04.015622
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 16:30:15.368143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.76
train mean loss: 938.78
epoch train time: 0:00:02.111724
elapsed time: 0:03:06.127614
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 16:30:17.480140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.27
train mean loss: 936.29
epoch train time: 0:00:02.118287
elapsed time: 0:03:08.246217
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 16:30:19.598742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.42
train mean loss: 933.90
epoch train time: 0:00:02.113115
elapsed time: 0:03:10.359651
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 16:30:21.712189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.49
train mean loss: 936.27
epoch train time: 0:00:02.117117
elapsed time: 0:03:12.477089
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 16:30:23.829676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.40
train mean loss: 926.57
epoch train time: 0:00:02.114020
elapsed time: 0:03:14.591455
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 16:30:25.943987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.10
train mean loss: 934.66
epoch train time: 0:00:02.112355
elapsed time: 0:03:16.704143
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 16:30:28.056700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.21
train mean loss: 929.68
epoch train time: 0:00:02.114572
elapsed time: 0:03:18.819010
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 16:30:30.171541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.47
train mean loss: 920.70
epoch train time: 0:00:02.115906
elapsed time: 0:03:20.935183
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 16:30:32.287706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.21
train mean loss: 923.28
epoch train time: 0:00:02.103369
elapsed time: 0:03:23.038878
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 16:30:34.391402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.22
train mean loss: 920.77
epoch train time: 0:00:02.115089
elapsed time: 0:03:25.154247
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 16:30:36.506779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.07
train mean loss: 927.00
epoch train time: 0:00:02.114945
elapsed time: 0:03:27.269487
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 16:30:38.622021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.73
train mean loss: 921.67
epoch train time: 0:00:02.116914
elapsed time: 0:03:29.386678
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 16:30:40.739203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.37
train mean loss: 915.64
epoch train time: 0:00:02.120424
elapsed time: 0:03:31.507389
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 16:30:42.859919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.84
train mean loss: 928.79
epoch train time: 0:00:02.114491
elapsed time: 0:03:33.622155
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 16:30:44.974690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.98
train mean loss: 932.80
epoch train time: 0:00:02.110543
elapsed time: 0:03:35.732996
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 16:30:47.085524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.55
train mean loss: 915.35
epoch train time: 0:00:02.112852
elapsed time: 0:03:37.846172
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 16:30:49.198720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.68
train mean loss: 918.56
epoch train time: 0:00:02.120408
elapsed time: 0:03:39.966873
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 16:30:51.319397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.53
train mean loss: 922.03
epoch train time: 0:00:02.117339
elapsed time: 0:03:42.084509
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 16:30:53.437031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.46
train mean loss: 914.38
epoch train time: 0:00:02.110928
elapsed time: 0:03:44.195698
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 16:30:55.548220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.79
train mean loss: 905.81
epoch train time: 0:00:02.108642
elapsed time: 0:03:46.304598
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 16:30:57.657129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.37
train mean loss: 895.26
epoch train time: 0:00:02.114194
elapsed time: 0:03:48.419103
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 16:30:59.771642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.86
train mean loss: 895.51
epoch train time: 0:00:02.120501
elapsed time: 0:03:50.539970
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 16:31:01.892483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.30
train mean loss: 899.78
epoch train time: 0:00:02.120910
elapsed time: 0:03:52.661184
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 16:31:04.013714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.01
train mean loss: 883.64
epoch train time: 0:00:02.115017
elapsed time: 0:03:54.776502
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 16:31:06.129020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.14
train mean loss: 879.15
epoch train time: 0:00:02.117682
elapsed time: 0:03:56.894439
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 16:31:08.246973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.51
train mean loss: 878.25
epoch train time: 0:00:02.120429
elapsed time: 0:03:59.015172
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 16:31:10.367699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.61
train mean loss: 856.42
epoch train time: 0:00:02.114476
elapsed time: 0:04:01.129951
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 16:31:12.482490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.23
train mean loss: 845.22
epoch train time: 0:00:02.117678
elapsed time: 0:04:03.247943
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 16:31:14.600471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 812.07
train mean loss: 815.10
epoch train time: 0:00:02.124716
elapsed time: 0:04:05.372960
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 16:31:16.725496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 799.97
train mean loss: 794.18
epoch train time: 0:00:02.127240
elapsed time: 0:04:07.500579
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 16:31:18.853162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.33
train mean loss: 765.26
epoch train time: 0:00:02.116337
elapsed time: 0:04:09.617298
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 16:31:20.969824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.27
train mean loss: 761.50
epoch train time: 0:00:02.130237
elapsed time: 0:04:11.747809
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 16:31:23.100394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.79
train mean loss: 741.78
epoch train time: 0:00:02.112133
elapsed time: 0:04:13.860276
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 16:31:25.212804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 728.49
train mean loss: 724.30
epoch train time: 0:00:02.116225
elapsed time: 0:04:15.976768
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 16:31:27.329290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 724.69
train mean loss: 722.25
epoch train time: 0:00:02.119482
elapsed time: 0:04:18.096565
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 16:31:29.449105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.88
train mean loss: 712.06
epoch train time: 0:00:02.119790
elapsed time: 0:04:20.216643
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 16:31:31.569166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.80
train mean loss: 700.29
epoch train time: 0:00:02.121843
elapsed time: 0:04:22.338776
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 16:31:33.691302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.94
train mean loss: 697.90
epoch train time: 0:00:02.116602
elapsed time: 0:04:24.455658
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 16:31:35.808195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.81
train mean loss: 683.93
epoch train time: 0:00:02.125119
elapsed time: 0:04:26.581114
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 16:31:37.933663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.73
train mean loss: 683.30
epoch train time: 0:00:02.116423
elapsed time: 0:04:28.697868
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 16:31:40.050447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.14
train mean loss: 669.32
epoch train time: 0:00:02.114607
elapsed time: 0:04:30.812811
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 16:31:42.165357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 672.32
train mean loss: 672.43
epoch train time: 0:00:02.125082
elapsed time: 0:04:32.938224
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 16:31:44.290715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.61
train mean loss: 672.32
epoch train time: 0:00:02.111934
elapsed time: 0:04:35.050381
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 16:31:46.402908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.51
train mean loss: 656.50
epoch train time: 0:00:02.115248
elapsed time: 0:04:37.165923
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 16:31:48.518457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 644.72
train mean loss: 644.73
epoch train time: 0:00:02.122243
elapsed time: 0:04:39.288451
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 16:31:50.640980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.15
train mean loss: 642.95
epoch train time: 0:00:02.119403
elapsed time: 0:04:41.408154
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 16:31:52.760676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.58
train mean loss: 628.91
epoch train time: 0:00:02.123282
elapsed time: 0:04:43.531713
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 16:31:54.884242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 629.50
train mean loss: 624.15
epoch train time: 0:00:02.114113
elapsed time: 0:04:45.646161
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 16:31:56.998701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 633.70
train mean loss: 629.71
epoch train time: 0:00:02.118995
elapsed time: 0:04:47.765495
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 16:31:59.118035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 615.32
train mean loss: 613.66
epoch train time: 0:00:02.126702
elapsed time: 0:04:49.892512
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 16:32:01.245046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.28
train mean loss: 595.26
epoch train time: 0:00:02.118108
elapsed time: 0:04:52.010909
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 16:32:03.363433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.00
train mean loss: 606.20
epoch train time: 0:00:02.111181
elapsed time: 0:04:54.122360
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 16:32:05.474882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.92
train mean loss: 593.77
epoch train time: 0:00:02.120710
elapsed time: 0:04:56.243388
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 16:32:07.595920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.60
train mean loss: 575.16
epoch train time: 0:00:02.122407
elapsed time: 0:04:58.366113
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 16:32:09.718646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.81
train mean loss: 572.50
epoch train time: 0:00:02.121725
elapsed time: 0:05:00.488170
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 16:32:11.840706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 591.83
train mean loss: 583.30
epoch train time: 0:00:02.116029
elapsed time: 0:05:02.604562
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 16:32:13.957114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 568.26
train mean loss: 564.25
epoch train time: 0:00:02.124448
elapsed time: 0:05:04.729345
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 16:32:16.081882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.19
train mean loss: 552.44
epoch train time: 0:00:02.112316
elapsed time: 0:05:06.841958
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 16:32:18.194484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.04
train mean loss: 555.13
epoch train time: 0:00:02.122394
elapsed time: 0:05:08.964764
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 16:32:20.317377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 563.35
train mean loss: 558.56
epoch train time: 0:00:02.110462
elapsed time: 0:05:11.075599
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 16:32:22.428128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 548.25
train mean loss: 547.51
epoch train time: 0:00:02.115363
elapsed time: 0:05:13.191244
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 16:32:24.543776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 546.19
train mean loss: 546.20
epoch train time: 0:00:02.112327
elapsed time: 0:05:15.303840
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 16:32:26.656384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 531.01
train mean loss: 533.68
epoch train time: 0:00:02.119653
elapsed time: 0:05:17.423801
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 16:32:28.776337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.72
train mean loss: 524.06
epoch train time: 0:00:02.121210
elapsed time: 0:05:19.545357
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 16:32:30.897859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 517.04
train mean loss: 515.76
epoch train time: 0:00:02.122372
elapsed time: 0:05:21.668033
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 16:32:33.020576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 521.55
train mean loss: 516.17
epoch train time: 0:00:02.116708
elapsed time: 0:05:23.785035
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 16:32:35.137561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 521.52
train mean loss: 521.15
epoch train time: 0:00:02.118058
elapsed time: 0:05:25.903356
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 16:32:37.255878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.48
train mean loss: 508.78
epoch train time: 0:00:02.107549
elapsed time: 0:05:28.011214
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 16:32:39.363740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.58
train mean loss: 507.12
epoch train time: 0:00:02.108441
elapsed time: 0:05:30.119911
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 16:32:41.472448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.42
train mean loss: 497.09
epoch train time: 0:00:02.119918
elapsed time: 0:05:32.240100
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 16:32:43.592632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.93
train mean loss: 497.80
epoch train time: 0:00:02.117693
elapsed time: 0:05:34.358137
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 16:32:45.710700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.29
train mean loss: 501.66
epoch train time: 0:00:02.114881
elapsed time: 0:05:36.473411
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 16:32:47.825979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.79
train mean loss: 491.11
epoch train time: 0:00:02.124531
elapsed time: 0:05:38.598320
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 16:32:49.950846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.52
train mean loss: 491.74
epoch train time: 0:00:02.127215
elapsed time: 0:05:40.725828
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 16:32:52.078357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.58
train mean loss: 491.30
epoch train time: 0:00:02.146168
elapsed time: 0:05:42.872280
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 16:32:54.224807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.07
train mean loss: 483.04
epoch train time: 0:00:02.143305
elapsed time: 0:05:45.015956
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 16:32:56.368480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.34
train mean loss: 486.16
epoch train time: 0:00:02.142981
elapsed time: 0:05:47.159211
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 16:32:58.511736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.17
train mean loss: 478.73
epoch train time: 0:00:02.136846
elapsed time: 0:05:49.296332
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 16:33:00.648879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.44
train mean loss: 467.90
epoch train time: 0:00:02.125679
elapsed time: 0:05:51.422313
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 16:33:02.774837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.21
train mean loss: 469.69
epoch train time: 0:00:02.111297
elapsed time: 0:05:53.533891
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 16:33:04.886428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.26
train mean loss: 468.71
epoch train time: 0:00:02.118039
elapsed time: 0:05:55.652219
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 16:33:07.004745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.04
train mean loss: 475.14
epoch train time: 0:00:02.114620
elapsed time: 0:05:57.767159
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 16:33:09.119715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.90
train mean loss: 464.82
epoch train time: 0:00:02.115360
elapsed time: 0:05:59.882838
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 16:33:11.235365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.25
train mean loss: 464.22
epoch train time: 0:00:02.116375
elapsed time: 0:06:01.999493
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 16:33:13.352018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.04
train mean loss: 452.84
epoch train time: 0:00:02.120033
elapsed time: 0:06:04.119800
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 16:33:15.472352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.67
train mean loss: 456.42
epoch train time: 0:00:02.118364
elapsed time: 0:06:06.238463
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 16:33:17.590986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.08
train mean loss: 451.79
epoch train time: 0:00:02.113050
elapsed time: 0:06:08.351780
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 16:33:19.704310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.55
train mean loss: 457.36
epoch train time: 0:00:02.114549
elapsed time: 0:06:10.466608
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 16:33:21.819132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.33
train mean loss: 448.70
epoch train time: 0:00:02.124980
elapsed time: 0:06:12.591980
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 16:33:23.944479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.44
train mean loss: 443.79
epoch train time: 0:00:02.113995
elapsed time: 0:06:14.706323
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 16:33:26.058876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.94
train mean loss: 437.09
epoch train time: 0:00:02.118205
elapsed time: 0:06:16.824840
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 16:33:28.177364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.85
train mean loss: 434.53
epoch train time: 0:00:02.117196
elapsed time: 0:06:18.942318
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 16:33:30.294846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.23
train mean loss: 441.51
epoch train time: 0:00:02.115490
elapsed time: 0:06:21.058082
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 16:33:32.410632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.54
train mean loss: 433.54
epoch train time: 0:00:02.108290
elapsed time: 0:06:23.166640
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 16:33:34.519188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.75
train mean loss: 426.42
epoch train time: 0:00:02.117130
elapsed time: 0:06:25.284069
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 16:33:36.636596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.74
train mean loss: 424.53
epoch train time: 0:00:02.119163
elapsed time: 0:06:27.403528
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 16:33:38.756075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.05
train mean loss: 428.39
epoch train time: 0:00:02.115037
elapsed time: 0:06:29.518868
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 16:33:40.871392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.56
train mean loss: 429.99
epoch train time: 0:00:02.115331
elapsed time: 0:06:31.634490
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 16:33:42.987015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.41
train mean loss: 420.61
epoch train time: 0:00:02.117053
elapsed time: 0:06:33.751812
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 16:33:45.104335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.19
train mean loss: 436.48
epoch train time: 0:00:02.117832
elapsed time: 0:06:35.869909
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 16:33:47.222432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.35
train mean loss: 430.69
epoch train time: 0:00:02.112363
elapsed time: 0:06:37.982551
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 16:33:49.335087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.59
train mean loss: 419.95
epoch train time: 0:00:02.113997
elapsed time: 0:06:40.096824
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 16:33:51.449344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.94
train mean loss: 421.04
epoch train time: 0:00:02.122377
elapsed time: 0:06:42.219485
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 16:33:53.572021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.88
train mean loss: 419.69
epoch train time: 0:00:02.117151
elapsed time: 0:06:44.336923
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 16:33:55.689449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.83
train mean loss: 418.00
epoch train time: 0:00:02.115236
elapsed time: 0:06:46.452432
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 16:33:57.804967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.40
train mean loss: 412.09
epoch train time: 0:00:02.117663
elapsed time: 0:06:48.570412
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 16:33:59.922949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.40
train mean loss: 414.22
epoch train time: 0:00:02.121965
elapsed time: 0:06:50.692679
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 16:34:02.045207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.78
train mean loss: 420.54
epoch train time: 0:00:02.112153
elapsed time: 0:06:52.805136
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 16:34:04.157682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.25
train mean loss: 412.32
epoch train time: 0:00:02.103362
elapsed time: 0:06:54.908788
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 16:34:06.261310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.73
train mean loss: 411.68
epoch train time: 0:00:02.112325
elapsed time: 0:06:57.021401
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 16:34:08.373943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.73
train mean loss: 410.66
epoch train time: 0:00:02.110018
elapsed time: 0:06:59.131727
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 16:34:10.484254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.36
train mean loss: 408.00
epoch train time: 0:00:02.119416
elapsed time: 0:07:01.251433
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 16:34:12.603968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.93
train mean loss: 405.65
epoch train time: 0:00:02.107052
elapsed time: 0:07:03.358799
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 16:34:14.711333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.23
train mean loss: 405.06
epoch train time: 0:00:02.105325
elapsed time: 0:07:05.464446
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 16:34:16.817000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.74
train mean loss: 398.34
epoch train time: 0:00:02.113919
elapsed time: 0:07:07.578664
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 16:34:18.931220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.23
train mean loss: 402.79
epoch train time: 0:00:02.106784
elapsed time: 0:07:09.685765
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 16:34:21.038295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 401.14
train mean loss: 401.97
epoch train time: 0:00:02.111721
elapsed time: 0:07:11.797850
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 16:34:23.150343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 399.90
train mean loss: 397.58
epoch train time: 0:00:02.102685
elapsed time: 0:07:13.900782
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 16:34:25.253313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 400.60
train mean loss: 403.37
epoch train time: 0:00:02.107501
elapsed time: 0:07:16.008561
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 16:34:27.361086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 401.94
train mean loss: 399.68
epoch train time: 0:00:02.104777
elapsed time: 0:07:18.113604
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 16:34:29.466134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 398.52
train mean loss: 394.52
epoch train time: 0:00:02.106539
elapsed time: 0:07:20.220440
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 16:34:31.572975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 394.01
train mean loss: 395.71
epoch train time: 0:00:02.111735
elapsed time: 0:07:22.332470
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 16:34:33.685015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 393.78
train mean loss: 391.54
epoch train time: 0:00:02.108995
elapsed time: 0:07:24.441762
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 16:34:35.794284
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 382.55
train mean loss: 387.93
epoch train time: 0:00:02.111023
elapsed time: 0:07:26.553056
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 16:34:37.905577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 397.36
train mean loss: 399.13
epoch train time: 0:00:02.105096
elapsed time: 0:07:28.658525
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 16:34:40.011054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.69
train mean loss: 399.12
epoch train time: 0:00:02.115545
elapsed time: 0:07:30.774350
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 16:34:42.126872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 402.51
train mean loss: 400.25
epoch train time: 0:00:02.123214
elapsed time: 0:07:32.897836
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 16:34:44.250381
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 398.10
train mean loss: 399.95
epoch train time: 0:00:02.106531
elapsed time: 0:07:35.004664
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 16:34:46.357191
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.27
train mean loss: 394.75
epoch train time: 0:00:02.110624
elapsed time: 0:07:37.115581
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 16:34:48.468106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 401.25
train mean loss: 401.87
epoch train time: 0:00:02.105120
elapsed time: 0:07:39.220978
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 16:34:50.573503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 398.30
train mean loss: 396.11
epoch train time: 0:00:02.114465
elapsed time: 0:07:41.335726
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 16:34:52.688284
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 403.23
train mean loss: 400.25
epoch train time: 0:00:02.106070
elapsed time: 0:07:43.442119
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 16:34:54.794648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 394.94
train mean loss: 395.68
epoch train time: 0:00:02.108256
elapsed time: 0:07:45.550682
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 16:34:56.903242
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 403.25
train mean loss: 400.22
epoch train time: 0:00:02.111641
elapsed time: 0:07:47.662675
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 16:34:59.015245
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 408.98
train mean loss: 403.63
epoch train time: 0:00:02.116428
elapsed time: 0:07:49.779426
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 16:35:01.131962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 386.84
train mean loss: 394.89
epoch train time: 0:00:02.117751
elapsed time: 0:07:51.897471
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 16:35:03.250002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.80
train mean loss: 394.83
epoch train time: 0:00:02.111667
elapsed time: 0:07:54.009421
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 16:35:05.361943
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 393.70
train mean loss: 396.69
epoch train time: 0:00:02.116052
elapsed time: 0:07:56.125742
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 16:35:07.478265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 400.92
train mean loss: 400.42
epoch train time: 0:00:02.111528
elapsed time: 0:07:58.237563
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 16:35:09.590109
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 396.08
train mean loss: 397.06
epoch train time: 0:00:02.111617
elapsed time: 0:08:00.349493
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 16:35:11.702026
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 399.78
train mean loss: 399.39
epoch train time: 0:00:02.118959
elapsed time: 0:08:02.468762
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 16:35:13.821295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 405.16
train mean loss: 399.81
epoch train time: 0:00:02.142464
elapsed time: 0:08:04.611527
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 16:35:15.964053
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 397.35
train mean loss: 395.23
epoch train time: 0:00:02.119498
elapsed time: 0:08:06.731301
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 16:35:18.083827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 404.68
train mean loss: 399.67
epoch train time: 0:00:02.115153
elapsed time: 0:08:08.846722
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 16:35:20.199249
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 392.67
train mean loss: 394.01
epoch train time: 0:00:02.111894
elapsed time: 0:08:10.958876
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 16:35:22.311413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 389.44
train mean loss: 388.86
epoch train time: 0:00:02.110076
elapsed time: 0:08:13.069256
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 16:35:24.421790
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 392.43
train mean loss: 391.50
epoch train time: 0:00:02.105928
elapsed time: 0:08:15.175477
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 16:35:26.527997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 399.42
train mean loss: 397.97
epoch train time: 0:00:02.105988
elapsed time: 0:08:17.281731
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 16:35:28.634254
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 396.95
train mean loss: 392.52
epoch train time: 0:00:02.113769
elapsed time: 0:08:19.395802
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 16:35:30.748294
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 390.10
train mean loss: 390.54
epoch train time: 0:00:02.111626
elapsed time: 0:08:21.507669
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 16:35:32.860198
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 386.53
train mean loss: 390.02
epoch train time: 0:00:02.111719
elapsed time: 0:08:23.619687
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 16:35:34.972230
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 401.53
train mean loss: 398.16
epoch train time: 0:00:02.117729
elapsed time: 0:08:25.737709
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 16:35:37.090242
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 392.68
train mean loss: 389.56
epoch train time: 0:00:02.105267
elapsed time: 0:08:27.843264
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 16:35:39.195803
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.95
train mean loss: 397.11
epoch train time: 0:00:02.106240
elapsed time: 0:08:29.949795
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 16:35:41.302320
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 391.74
train mean loss: 396.48
epoch train time: 0:00:02.108833
elapsed time: 0:08:32.058887
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 16:35:43.411427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 401.40
train mean loss: 396.60
epoch train time: 0:00:02.108960
elapsed time: 0:08:34.168144
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 16:35:45.520676
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 387.36
train mean loss: 388.79
epoch train time: 0:00:02.113602
elapsed time: 0:08:36.282010
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 16:35:47.634564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.18
train mean loss: 392.93
epoch train time: 0:00:02.113748
elapsed time: 0:08:38.396063
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 16:35:49.748590
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 390.98
train mean loss: 392.10
epoch train time: 0:00:02.116008
elapsed time: 0:08:40.512352
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 16:35:51.864876
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 384.66
train mean loss: 390.81
epoch train time: 0:00:02.119656
elapsed time: 0:08:42.632294
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 16:35:53.984855
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 389.89
train mean loss: 391.46
epoch train time: 0:00:02.113780
elapsed time: 0:08:44.746378
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 16:35:56.098912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 400.96
train mean loss: 398.47
epoch train time: 0:00:02.107592
elapsed time: 0:08:46.854260
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 16:35:58.206784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 391.78
train mean loss: 389.34
epoch train time: 0:00:02.109664
elapsed time: 0:08:48.964197
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 16:36:00.316723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.20
train mean loss: 393.52
epoch train time: 0:00:02.113359
elapsed time: 0:08:51.077851
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 16:36:02.430386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 391.78
train mean loss: 389.14
epoch train time: 0:00:02.113093
elapsed time: 0:08:53.200238
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.125/bayesian_conv5_dense1_0.125_4/checkpoint.pth.tar
**** end time: 2019-09-25 16:36:04.552704 ****
