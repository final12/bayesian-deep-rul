Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_7', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31833
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:48:21.012982 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:48:21.018343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4123.14
 ---- batch: 020 ----
mean loss: 3956.68
 ---- batch: 030 ----
mean loss: 4002.58
train mean loss: 4021.45
epoch train time: 0:00:12.472071
elapsed time: 0:00:12.478833
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:48:33.491852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3937.23
 ---- batch: 020 ----
mean loss: 3850.05
 ---- batch: 030 ----
mean loss: 3819.23
train mean loss: 3860.61
epoch train time: 0:00:00.179672
elapsed time: 0:00:12.658644
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:48:33.671689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3717.80
 ---- batch: 020 ----
mean loss: 3632.80
 ---- batch: 030 ----
mean loss: 3573.43
train mean loss: 3617.95
epoch train time: 0:00:00.176281
elapsed time: 0:00:12.835079
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:48:33.848132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3423.43
 ---- batch: 020 ----
mean loss: 3326.75
 ---- batch: 030 ----
mean loss: 3291.87
train mean loss: 3337.77
epoch train time: 0:00:00.177999
elapsed time: 0:00:13.013245
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:48:34.026277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3120.13
 ---- batch: 020 ----
mean loss: 3044.91
 ---- batch: 030 ----
mean loss: 3095.01
train mean loss: 3070.57
epoch train time: 0:00:00.175195
elapsed time: 0:00:13.188595
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:48:34.201652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2913.89
 ---- batch: 020 ----
mean loss: 2834.29
 ---- batch: 030 ----
mean loss: 2806.83
train mean loss: 2838.22
epoch train time: 0:00:00.179890
elapsed time: 0:00:13.368691
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:48:34.381758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2721.83
 ---- batch: 020 ----
mean loss: 2636.61
 ---- batch: 030 ----
mean loss: 2576.00
train mean loss: 2631.65
epoch train time: 0:00:00.176758
elapsed time: 0:00:13.545625
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:48:34.558670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2507.75
 ---- batch: 020 ----
mean loss: 2478.78
 ---- batch: 030 ----
mean loss: 2383.96
train mean loss: 2445.62
epoch train time: 0:00:00.180912
elapsed time: 0:00:13.726691
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:48:34.739756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2332.61
 ---- batch: 020 ----
mean loss: 2288.92
 ---- batch: 030 ----
mean loss: 2251.75
train mean loss: 2280.21
epoch train time: 0:00:00.172831
elapsed time: 0:00:13.899712
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:48:34.912743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2195.52
 ---- batch: 020 ----
mean loss: 2131.35
 ---- batch: 030 ----
mean loss: 2101.43
train mean loss: 2129.13
epoch train time: 0:00:00.170266
elapsed time: 0:00:14.070158
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:48:35.083186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2026.89
 ---- batch: 020 ----
mean loss: 2015.12
 ---- batch: 030 ----
mean loss: 1956.47
train mean loss: 1988.98
epoch train time: 0:00:00.169381
elapsed time: 0:00:14.239680
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:48:35.252709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1900.05
 ---- batch: 020 ----
mean loss: 1883.01
 ---- batch: 030 ----
mean loss: 1814.83
train mean loss: 1863.74
epoch train time: 0:00:00.168870
elapsed time: 0:00:14.408709
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:48:35.421741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1794.46
 ---- batch: 020 ----
mean loss: 1741.27
 ---- batch: 030 ----
mean loss: 1740.10
train mean loss: 1745.07
epoch train time: 0:00:00.173288
elapsed time: 0:00:14.582169
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:48:35.595202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1703.78
 ---- batch: 020 ----
mean loss: 1651.69
 ---- batch: 030 ----
mean loss: 1594.44
train mean loss: 1636.29
epoch train time: 0:00:00.174709
elapsed time: 0:00:14.757046
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:48:35.770079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1566.60
 ---- batch: 020 ----
mean loss: 1546.51
 ---- batch: 030 ----
mean loss: 1517.92
train mean loss: 1537.16
epoch train time: 0:00:00.177765
elapsed time: 0:00:14.934984
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:48:35.948014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1469.25
 ---- batch: 020 ----
mean loss: 1444.24
 ---- batch: 030 ----
mean loss: 1433.03
train mean loss: 1443.69
epoch train time: 0:00:00.173546
elapsed time: 0:00:15.108736
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:48:36.121771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1391.60
 ---- batch: 020 ----
mean loss: 1356.76
 ---- batch: 030 ----
mean loss: 1339.70
train mean loss: 1357.15
epoch train time: 0:00:00.174910
elapsed time: 0:00:15.283807
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:48:36.296838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1308.02
 ---- batch: 020 ----
mean loss: 1286.05
 ---- batch: 030 ----
mean loss: 1243.78
train mean loss: 1278.60
epoch train time: 0:00:00.172549
elapsed time: 0:00:15.456507
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:48:36.469576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1235.16
 ---- batch: 020 ----
mean loss: 1214.12
 ---- batch: 030 ----
mean loss: 1182.05
train mean loss: 1204.66
epoch train time: 0:00:00.173558
elapsed time: 0:00:15.630243
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:48:36.643274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1164.79
 ---- batch: 020 ----
mean loss: 1137.48
 ---- batch: 030 ----
mean loss: 1119.45
train mean loss: 1136.33
epoch train time: 0:00:00.174173
elapsed time: 0:00:15.804555
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:48:36.817586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1084.29
 ---- batch: 020 ----
mean loss: 1077.52
 ---- batch: 030 ----
mean loss: 1059.69
train mean loss: 1074.46
epoch train time: 0:00:00.185159
elapsed time: 0:00:15.989854
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:48:37.002920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1029.04
 ---- batch: 020 ----
mean loss: 1040.07
 ---- batch: 030 ----
mean loss: 1005.20
train mean loss: 1016.26
epoch train time: 0:00:00.172094
elapsed time: 0:00:16.162136
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:48:37.175185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 990.11
 ---- batch: 020 ----
mean loss: 965.49
 ---- batch: 030 ----
mean loss: 955.41
train mean loss: 962.82
epoch train time: 0:00:00.172410
elapsed time: 0:00:16.334749
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:48:37.347791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.85
 ---- batch: 020 ----
mean loss: 929.88
 ---- batch: 030 ----
mean loss: 913.35
train mean loss: 913.04
epoch train time: 0:00:00.175068
elapsed time: 0:00:16.509966
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:48:37.523010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.19
 ---- batch: 020 ----
mean loss: 883.82
 ---- batch: 030 ----
mean loss: 851.39
train mean loss: 867.93
epoch train time: 0:00:00.175320
elapsed time: 0:00:16.685444
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:48:37.698475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.71
 ---- batch: 020 ----
mean loss: 844.31
 ---- batch: 030 ----
mean loss: 822.66
train mean loss: 827.13
epoch train time: 0:00:00.177068
elapsed time: 0:00:16.862653
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:48:37.875686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 804.70
 ---- batch: 020 ----
mean loss: 795.30
 ---- batch: 030 ----
mean loss: 766.28
train mean loss: 789.43
epoch train time: 0:00:00.172027
elapsed time: 0:00:17.034823
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:48:38.047853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 769.28
 ---- batch: 020 ----
mean loss: 756.47
 ---- batch: 030 ----
mean loss: 738.69
train mean loss: 753.21
epoch train time: 0:00:00.174122
elapsed time: 0:00:17.209086
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:48:38.222117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.45
 ---- batch: 020 ----
mean loss: 737.62
 ---- batch: 030 ----
mean loss: 713.32
train mean loss: 720.43
epoch train time: 0:00:00.169172
elapsed time: 0:00:17.378394
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:48:38.391435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 699.46
 ---- batch: 020 ----
mean loss: 690.56
 ---- batch: 030 ----
mean loss: 685.08
train mean loss: 691.55
epoch train time: 0:00:00.170414
elapsed time: 0:00:17.548994
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:48:38.562045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.64
 ---- batch: 020 ----
mean loss: 657.75
 ---- batch: 030 ----
mean loss: 672.66
train mean loss: 664.65
epoch train time: 0:00:00.173393
elapsed time: 0:00:17.722545
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:48:38.735574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.44
 ---- batch: 020 ----
mean loss: 645.53
 ---- batch: 030 ----
mean loss: 630.22
train mean loss: 638.70
epoch train time: 0:00:00.177072
elapsed time: 0:00:17.899753
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:48:38.912791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.54
 ---- batch: 020 ----
mean loss: 614.99
 ---- batch: 030 ----
mean loss: 602.71
train mean loss: 616.44
epoch train time: 0:00:00.173549
elapsed time: 0:00:18.073472
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:48:39.086504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 603.19
 ---- batch: 020 ----
mean loss: 604.07
 ---- batch: 030 ----
mean loss: 585.73
train mean loss: 595.76
epoch train time: 0:00:00.172194
elapsed time: 0:00:18.245806
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:48:39.258883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 577.83
 ---- batch: 020 ----
mean loss: 584.85
 ---- batch: 030 ----
mean loss: 570.27
train mean loss: 576.58
epoch train time: 0:00:00.170030
elapsed time: 0:00:18.416020
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:48:39.429051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 561.47
 ---- batch: 020 ----
mean loss: 556.74
 ---- batch: 030 ----
mean loss: 565.36
train mean loss: 559.61
epoch train time: 0:00:00.170698
elapsed time: 0:00:18.586885
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:48:39.599924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.19
 ---- batch: 020 ----
mean loss: 546.13
 ---- batch: 030 ----
mean loss: 539.96
train mean loss: 542.70
epoch train time: 0:00:00.176737
elapsed time: 0:00:18.763833
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:48:39.776865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 534.92
 ---- batch: 020 ----
mean loss: 524.87
 ---- batch: 030 ----
mean loss: 529.28
train mean loss: 528.32
epoch train time: 0:00:00.173256
elapsed time: 0:00:18.937248
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:48:39.950279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 522.60
 ---- batch: 020 ----
mean loss: 516.15
 ---- batch: 030 ----
mean loss: 509.83
train mean loss: 515.29
epoch train time: 0:00:00.174102
elapsed time: 0:00:19.111491
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:48:40.124522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.03
 ---- batch: 020 ----
mean loss: 502.46
 ---- batch: 030 ----
mean loss: 503.03
train mean loss: 502.85
epoch train time: 0:00:00.176396
elapsed time: 0:00:19.288027
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:48:40.301067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.89
 ---- batch: 020 ----
mean loss: 488.98
 ---- batch: 030 ----
mean loss: 489.98
train mean loss: 491.52
epoch train time: 0:00:00.174777
elapsed time: 0:00:19.462955
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:48:40.475984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.14
 ---- batch: 020 ----
mean loss: 474.74
 ---- batch: 030 ----
mean loss: 482.45
train mean loss: 480.97
epoch train time: 0:00:00.174838
elapsed time: 0:00:19.637942
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:48:40.650975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.31
 ---- batch: 020 ----
mean loss: 469.73
 ---- batch: 030 ----
mean loss: 467.39
train mean loss: 471.27
epoch train time: 0:00:00.180030
elapsed time: 0:00:19.818115
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:48:40.831152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.82
 ---- batch: 020 ----
mean loss: 470.94
 ---- batch: 030 ----
mean loss: 461.64
train mean loss: 462.87
epoch train time: 0:00:00.175005
elapsed time: 0:00:19.993291
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:48:41.006368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.72
 ---- batch: 020 ----
mean loss: 450.28
 ---- batch: 030 ----
mean loss: 457.58
train mean loss: 454.53
epoch train time: 0:00:00.173820
elapsed time: 0:00:20.167296
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:48:41.180325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.29
 ---- batch: 020 ----
mean loss: 445.52
 ---- batch: 030 ----
mean loss: 443.48
train mean loss: 447.34
epoch train time: 0:00:00.170415
elapsed time: 0:00:20.337847
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:48:41.350877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.06
 ---- batch: 020 ----
mean loss: 440.79
 ---- batch: 030 ----
mean loss: 439.47
train mean loss: 439.77
epoch train time: 0:00:00.173851
elapsed time: 0:00:20.511837
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:48:41.524868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.55
 ---- batch: 020 ----
mean loss: 438.64
 ---- batch: 030 ----
mean loss: 430.57
train mean loss: 433.40
epoch train time: 0:00:00.174090
elapsed time: 0:00:20.686066
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:48:41.699115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.17
 ---- batch: 020 ----
mean loss: 427.44
 ---- batch: 030 ----
mean loss: 421.57
train mean loss: 426.99
epoch train time: 0:00:00.176396
elapsed time: 0:00:20.863374
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:48:41.876443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.82
 ---- batch: 020 ----
mean loss: 421.58
 ---- batch: 030 ----
mean loss: 418.17
train mean loss: 421.89
epoch train time: 0:00:00.173365
elapsed time: 0:00:21.036921
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:48:42.049952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.37
 ---- batch: 020 ----
mean loss: 415.18
 ---- batch: 030 ----
mean loss: 418.44
train mean loss: 416.44
epoch train time: 0:00:00.172517
elapsed time: 0:00:21.209578
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:48:42.222611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.84
 ---- batch: 020 ----
mean loss: 403.86
 ---- batch: 030 ----
mean loss: 415.99
train mean loss: 411.87
epoch train time: 0:00:00.171695
elapsed time: 0:00:21.381416
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:48:42.394445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.22
 ---- batch: 020 ----
mean loss: 411.19
 ---- batch: 030 ----
mean loss: 399.62
train mean loss: 407.23
epoch train time: 0:00:00.173553
elapsed time: 0:00:21.555106
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:48:42.568135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.85
 ---- batch: 020 ----
mean loss: 401.06
 ---- batch: 030 ----
mean loss: 405.88
train mean loss: 403.51
epoch train time: 0:00:00.176723
elapsed time: 0:00:21.731973
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:48:42.745001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.09
 ---- batch: 020 ----
mean loss: 403.15
 ---- batch: 030 ----
mean loss: 393.56
train mean loss: 399.96
epoch train time: 0:00:00.174375
elapsed time: 0:00:21.906538
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:48:42.919612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.75
 ---- batch: 020 ----
mean loss: 395.65
 ---- batch: 030 ----
mean loss: 399.04
train mean loss: 396.75
epoch train time: 0:00:00.173827
elapsed time: 0:00:22.080561
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:48:43.093591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.46
 ---- batch: 020 ----
mean loss: 401.41
 ---- batch: 030 ----
mean loss: 390.85
train mean loss: 394.30
epoch train time: 0:00:00.172118
elapsed time: 0:00:22.252820
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:48:43.265850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.30
 ---- batch: 020 ----
mean loss: 391.45
 ---- batch: 030 ----
mean loss: 391.96
train mean loss: 391.41
epoch train time: 0:00:00.175121
elapsed time: 0:00:22.428080
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:48:43.441109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.12
 ---- batch: 020 ----
mean loss: 398.60
 ---- batch: 030 ----
mean loss: 389.41
train mean loss: 389.13
epoch train time: 0:00:00.173753
elapsed time: 0:00:22.601973
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:48:43.615000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.30
 ---- batch: 020 ----
mean loss: 389.99
 ---- batch: 030 ----
mean loss: 381.97
train mean loss: 387.21
epoch train time: 0:00:00.174709
elapsed time: 0:00:22.776822
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:48:43.789853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.09
 ---- batch: 020 ----
mean loss: 378.70
 ---- batch: 030 ----
mean loss: 382.27
train mean loss: 385.88
epoch train time: 0:00:00.173541
elapsed time: 0:00:22.950502
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:48:43.963550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.06
 ---- batch: 020 ----
mean loss: 382.56
 ---- batch: 030 ----
mean loss: 390.08
train mean loss: 383.95
epoch train time: 0:00:00.169511
elapsed time: 0:00:23.120169
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:48:44.133198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.87
 ---- batch: 020 ----
mean loss: 385.85
 ---- batch: 030 ----
mean loss: 378.78
train mean loss: 382.52
epoch train time: 0:00:00.170946
elapsed time: 0:00:23.291265
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:48:44.304294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.65
 ---- batch: 020 ----
mean loss: 382.87
 ---- batch: 030 ----
mean loss: 379.87
train mean loss: 380.87
epoch train time: 0:00:00.173353
elapsed time: 0:00:23.464758
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:48:44.477788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.40
 ---- batch: 020 ----
mean loss: 376.22
 ---- batch: 030 ----
mean loss: 379.28
train mean loss: 380.01
epoch train time: 0:00:00.173624
elapsed time: 0:00:23.638536
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:48:44.651570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.26
 ---- batch: 020 ----
mean loss: 374.41
 ---- batch: 030 ----
mean loss: 377.33
train mean loss: 378.87
epoch train time: 0:00:00.186433
elapsed time: 0:00:23.825114
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:48:44.838146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.18
 ---- batch: 020 ----
mean loss: 374.69
 ---- batch: 030 ----
mean loss: 374.77
train mean loss: 377.61
epoch train time: 0:00:00.170873
elapsed time: 0:00:23.996127
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:48:45.009157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.67
 ---- batch: 020 ----
mean loss: 373.40
 ---- batch: 030 ----
mean loss: 378.08
train mean loss: 376.98
epoch train time: 0:00:00.172178
elapsed time: 0:00:24.168444
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:48:45.181473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.81
 ---- batch: 020 ----
mean loss: 379.22
 ---- batch: 030 ----
mean loss: 375.55
train mean loss: 375.65
epoch train time: 0:00:00.168487
elapsed time: 0:00:24.337078
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:48:45.350107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.18
 ---- batch: 020 ----
mean loss: 375.26
 ---- batch: 030 ----
mean loss: 374.87
train mean loss: 375.24
epoch train time: 0:00:00.171464
elapsed time: 0:00:24.508700
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:48:45.521733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.43
 ---- batch: 020 ----
mean loss: 366.18
 ---- batch: 030 ----
mean loss: 379.41
train mean loss: 374.16
epoch train time: 0:00:00.168842
elapsed time: 0:00:24.677697
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:48:45.690727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.06
 ---- batch: 020 ----
mean loss: 372.22
 ---- batch: 030 ----
mean loss: 367.83
train mean loss: 373.88
epoch train time: 0:00:00.179658
elapsed time: 0:00:24.857504
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:48:45.870538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.78
 ---- batch: 020 ----
mean loss: 374.19
 ---- batch: 030 ----
mean loss: 369.71
train mean loss: 372.90
epoch train time: 0:00:00.174782
elapsed time: 0:00:25.032448
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:48:46.045481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.37
 ---- batch: 020 ----
mean loss: 380.34
 ---- batch: 030 ----
mean loss: 368.84
train mean loss: 372.35
epoch train time: 0:00:00.176033
elapsed time: 0:00:25.208654
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:48:46.221704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.38
 ---- batch: 020 ----
mean loss: 368.18
 ---- batch: 030 ----
mean loss: 374.81
train mean loss: 372.14
epoch train time: 0:00:00.171818
elapsed time: 0:00:25.380667
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:48:46.393721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.09
 ---- batch: 020 ----
mean loss: 375.47
 ---- batch: 030 ----
mean loss: 372.39
train mean loss: 371.64
epoch train time: 0:00:00.173468
elapsed time: 0:00:25.554313
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:48:46.567343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.54
 ---- batch: 020 ----
mean loss: 366.64
 ---- batch: 030 ----
mean loss: 371.56
train mean loss: 371.38
epoch train time: 0:00:00.172112
elapsed time: 0:00:25.726563
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:48:46.739592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.15
 ---- batch: 020 ----
mean loss: 373.92
 ---- batch: 030 ----
mean loss: 374.06
train mean loss: 370.61
epoch train time: 0:00:00.173863
elapsed time: 0:00:25.900562
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:48:46.913650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.95
 ---- batch: 020 ----
mean loss: 364.42
 ---- batch: 030 ----
mean loss: 366.44
train mean loss: 370.65
epoch train time: 0:00:00.170762
elapsed time: 0:00:26.071531
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:48:47.084562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.96
 ---- batch: 020 ----
mean loss: 370.21
 ---- batch: 030 ----
mean loss: 373.16
train mean loss: 370.27
epoch train time: 0:00:00.170474
elapsed time: 0:00:26.242158
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:48:47.255191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.29
 ---- batch: 020 ----
mean loss: 368.75
 ---- batch: 030 ----
mean loss: 380.88
train mean loss: 369.39
epoch train time: 0:00:00.169986
elapsed time: 0:00:26.412283
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:48:47.425311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.28
 ---- batch: 020 ----
mean loss: 371.22
 ---- batch: 030 ----
mean loss: 369.07
train mean loss: 369.33
epoch train time: 0:00:00.168628
elapsed time: 0:00:26.581058
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:48:47.594089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.44
 ---- batch: 020 ----
mean loss: 363.31
 ---- batch: 030 ----
mean loss: 370.42
train mean loss: 369.15
epoch train time: 0:00:00.171010
elapsed time: 0:00:26.752221
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:48:47.765258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.99
 ---- batch: 020 ----
mean loss: 372.10
 ---- batch: 030 ----
mean loss: 366.84
train mean loss: 368.65
epoch train time: 0:00:00.178578
elapsed time: 0:00:26.930971
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:48:47.943998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.65
 ---- batch: 020 ----
mean loss: 364.33
 ---- batch: 030 ----
mean loss: 368.54
train mean loss: 368.69
epoch train time: 0:00:00.170311
elapsed time: 0:00:27.101445
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:48:48.114474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.87
 ---- batch: 020 ----
mean loss: 373.64
 ---- batch: 030 ----
mean loss: 366.90
train mean loss: 368.00
epoch train time: 0:00:00.173072
elapsed time: 0:00:27.274661
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:48:48.287691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.23
 ---- batch: 020 ----
mean loss: 368.83
 ---- batch: 030 ----
mean loss: 364.56
train mean loss: 367.79
epoch train time: 0:00:00.170849
elapsed time: 0:00:27.445671
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:48:48.458714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.60
 ---- batch: 020 ----
mean loss: 377.85
 ---- batch: 030 ----
mean loss: 359.49
train mean loss: 367.51
epoch train time: 0:00:00.172313
elapsed time: 0:00:27.618142
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:48:48.631209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.03
 ---- batch: 020 ----
mean loss: 363.11
 ---- batch: 030 ----
mean loss: 362.45
train mean loss: 367.44
epoch train time: 0:00:00.179900
elapsed time: 0:00:27.798224
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:48:48.811254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.70
 ---- batch: 020 ----
mean loss: 363.54
 ---- batch: 030 ----
mean loss: 370.96
train mean loss: 366.90
epoch train time: 0:00:00.183643
elapsed time: 0:00:27.982008
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:48:48.995040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.62
 ---- batch: 020 ----
mean loss: 369.52
 ---- batch: 030 ----
mean loss: 370.14
train mean loss: 366.90
epoch train time: 0:00:00.178663
elapsed time: 0:00:28.160818
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:48:49.173852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.30
 ---- batch: 020 ----
mean loss: 366.36
 ---- batch: 030 ----
mean loss: 369.52
train mean loss: 366.32
epoch train time: 0:00:00.174881
elapsed time: 0:00:28.335845
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:48:49.348875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.49
 ---- batch: 020 ----
mean loss: 366.65
 ---- batch: 030 ----
mean loss: 363.19
train mean loss: 366.29
epoch train time: 0:00:00.177379
elapsed time: 0:00:28.513368
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:48:49.526401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.98
 ---- batch: 020 ----
mean loss: 364.63
 ---- batch: 030 ----
mean loss: 367.09
train mean loss: 366.18
epoch train time: 0:00:00.174932
elapsed time: 0:00:28.688501
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:48:49.701534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.65
 ---- batch: 020 ----
mean loss: 360.12
 ---- batch: 030 ----
mean loss: 373.57
train mean loss: 366.21
epoch train time: 0:00:00.183813
elapsed time: 0:00:28.872458
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:48:49.885489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.77
 ---- batch: 020 ----
mean loss: 362.99
 ---- batch: 030 ----
mean loss: 371.01
train mean loss: 365.43
epoch train time: 0:00:00.168856
elapsed time: 0:00:29.041451
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:48:50.054495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.28
 ---- batch: 020 ----
mean loss: 364.84
 ---- batch: 030 ----
mean loss: 369.98
train mean loss: 365.36
epoch train time: 0:00:00.168911
elapsed time: 0:00:29.210513
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:48:50.223556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.55
 ---- batch: 020 ----
mean loss: 362.97
 ---- batch: 030 ----
mean loss: 361.78
train mean loss: 365.35
epoch train time: 0:00:00.170680
elapsed time: 0:00:29.381359
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:48:50.394388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.25
 ---- batch: 020 ----
mean loss: 363.83
 ---- batch: 030 ----
mean loss: 362.34
train mean loss: 365.38
epoch train time: 0:00:00.169355
elapsed time: 0:00:29.550899
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:48:50.563928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.91
 ---- batch: 020 ----
mean loss: 361.35
 ---- batch: 030 ----
mean loss: 366.89
train mean loss: 364.66
epoch train time: 0:00:00.174061
elapsed time: 0:00:29.725106
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:48:50.738138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.37
 ---- batch: 020 ----
mean loss: 367.54
 ---- batch: 030 ----
mean loss: 359.29
train mean loss: 365.04
epoch train time: 0:00:00.177406
elapsed time: 0:00:29.902655
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:48:50.915687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.62
 ---- batch: 020 ----
mean loss: 369.38
 ---- batch: 030 ----
mean loss: 364.85
train mean loss: 364.53
epoch train time: 0:00:00.172255
elapsed time: 0:00:30.075056
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:48:51.088105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.65
 ---- batch: 020 ----
mean loss: 365.04
 ---- batch: 030 ----
mean loss: 364.74
train mean loss: 363.98
epoch train time: 0:00:00.170950
elapsed time: 0:00:30.246198
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:48:51.259227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.14
 ---- batch: 020 ----
mean loss: 369.77
 ---- batch: 030 ----
mean loss: 364.68
train mean loss: 364.23
epoch train time: 0:00:00.172010
elapsed time: 0:00:30.418344
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:48:51.431374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.23
 ---- batch: 020 ----
mean loss: 370.39
 ---- batch: 030 ----
mean loss: 365.93
train mean loss: 363.78
epoch train time: 0:00:00.174496
elapsed time: 0:00:30.592998
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:48:51.606026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.80
 ---- batch: 020 ----
mean loss: 364.32
 ---- batch: 030 ----
mean loss: 369.79
train mean loss: 363.77
epoch train time: 0:00:00.177906
elapsed time: 0:00:30.771070
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:48:51.784112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.26
 ---- batch: 020 ----
mean loss: 366.24
 ---- batch: 030 ----
mean loss: 365.67
train mean loss: 363.61
epoch train time: 0:00:00.176349
elapsed time: 0:00:30.947587
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:48:51.960610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.58
 ---- batch: 020 ----
mean loss: 355.89
 ---- batch: 030 ----
mean loss: 367.29
train mean loss: 363.38
epoch train time: 0:00:00.169845
elapsed time: 0:00:31.117563
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:48:52.130592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.22
 ---- batch: 020 ----
mean loss: 370.19
 ---- batch: 030 ----
mean loss: 358.58
train mean loss: 363.45
epoch train time: 0:00:00.169220
elapsed time: 0:00:31.286919
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:48:52.299948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.25
 ---- batch: 020 ----
mean loss: 357.21
 ---- batch: 030 ----
mean loss: 365.40
train mean loss: 363.24
epoch train time: 0:00:00.171545
elapsed time: 0:00:31.458600
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:48:52.471628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.43
 ---- batch: 020 ----
mean loss: 362.20
 ---- batch: 030 ----
mean loss: 363.64
train mean loss: 362.49
epoch train time: 0:00:00.167956
elapsed time: 0:00:31.626705
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:48:52.639732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.14
 ---- batch: 020 ----
mean loss: 361.78
 ---- batch: 030 ----
mean loss: 365.46
train mean loss: 362.54
epoch train time: 0:00:00.176977
elapsed time: 0:00:31.803818
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:48:52.816849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.57
 ---- batch: 020 ----
mean loss: 359.47
 ---- batch: 030 ----
mean loss: 361.08
train mean loss: 363.02
epoch train time: 0:00:00.182238
elapsed time: 0:00:31.986209
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:48:52.999241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.73
 ---- batch: 020 ----
mean loss: 361.77
 ---- batch: 030 ----
mean loss: 366.00
train mean loss: 362.21
epoch train time: 0:00:00.177492
elapsed time: 0:00:32.163845
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:48:53.176876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.89
 ---- batch: 020 ----
mean loss: 364.85
 ---- batch: 030 ----
mean loss: 370.19
train mean loss: 362.17
epoch train time: 0:00:00.176692
elapsed time: 0:00:32.340713
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:48:53.353767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.97
 ---- batch: 020 ----
mean loss: 363.09
 ---- batch: 030 ----
mean loss: 360.68
train mean loss: 362.48
epoch train time: 0:00:00.174476
elapsed time: 0:00:32.515400
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:48:53.528431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.62
 ---- batch: 020 ----
mean loss: 362.87
 ---- batch: 030 ----
mean loss: 363.43
train mean loss: 361.61
epoch train time: 0:00:00.174045
elapsed time: 0:00:32.689587
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:48:53.702628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.33
 ---- batch: 020 ----
mean loss: 364.51
 ---- batch: 030 ----
mean loss: 360.15
train mean loss: 361.15
epoch train time: 0:00:00.175812
elapsed time: 0:00:32.865547
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:48:53.878591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.60
 ---- batch: 020 ----
mean loss: 365.19
 ---- batch: 030 ----
mean loss: 360.61
train mean loss: 361.64
epoch train time: 0:00:00.166617
elapsed time: 0:00:33.032325
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:48:54.045362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.70
 ---- batch: 020 ----
mean loss: 358.77
 ---- batch: 030 ----
mean loss: 359.93
train mean loss: 361.35
epoch train time: 0:00:00.167620
elapsed time: 0:00:33.200085
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:48:54.213112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.79
 ---- batch: 020 ----
mean loss: 358.06
 ---- batch: 030 ----
mean loss: 358.94
train mean loss: 361.31
epoch train time: 0:00:00.169162
elapsed time: 0:00:33.369393
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:48:54.382431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.76
 ---- batch: 020 ----
mean loss: 358.81
 ---- batch: 030 ----
mean loss: 367.94
train mean loss: 360.84
epoch train time: 0:00:00.170566
elapsed time: 0:00:33.540107
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:48:54.553137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.36
 ---- batch: 020 ----
mean loss: 359.17
 ---- batch: 030 ----
mean loss: 364.71
train mean loss: 360.97
epoch train time: 0:00:00.172420
elapsed time: 0:00:33.712692
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:48:54.725726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.08
 ---- batch: 020 ----
mean loss: 362.62
 ---- batch: 030 ----
mean loss: 359.37
train mean loss: 360.56
epoch train time: 0:00:00.173961
elapsed time: 0:00:33.886796
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:48:54.899826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.70
 ---- batch: 020 ----
mean loss: 355.05
 ---- batch: 030 ----
mean loss: 363.50
train mean loss: 360.28
epoch train time: 0:00:00.170244
elapsed time: 0:00:34.057194
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:48:55.070225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.12
 ---- batch: 020 ----
mean loss: 358.36
 ---- batch: 030 ----
mean loss: 352.10
train mean loss: 360.67
epoch train time: 0:00:00.172473
elapsed time: 0:00:34.229816
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:48:55.242846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.19
 ---- batch: 020 ----
mean loss: 366.22
 ---- batch: 030 ----
mean loss: 360.90
train mean loss: 360.14
epoch train time: 0:00:00.173217
elapsed time: 0:00:34.403207
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:48:55.416224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.18
 ---- batch: 020 ----
mean loss: 358.98
 ---- batch: 030 ----
mean loss: 364.63
train mean loss: 360.09
epoch train time: 0:00:00.171337
elapsed time: 0:00:34.574673
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:48:55.587702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.11
 ---- batch: 020 ----
mean loss: 362.87
 ---- batch: 030 ----
mean loss: 361.10
train mean loss: 359.69
epoch train time: 0:00:00.173858
elapsed time: 0:00:34.748702
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:48:55.761756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.36
 ---- batch: 020 ----
mean loss: 361.42
 ---- batch: 030 ----
mean loss: 361.71
train mean loss: 359.54
epoch train time: 0:00:00.180614
elapsed time: 0:00:34.929476
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:48:55.942523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.37
 ---- batch: 020 ----
mean loss: 359.41
 ---- batch: 030 ----
mean loss: 370.04
train mean loss: 359.86
epoch train time: 0:00:00.170469
elapsed time: 0:00:35.100099
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:48:56.113127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.72
 ---- batch: 020 ----
mean loss: 371.89
 ---- batch: 030 ----
mean loss: 352.66
train mean loss: 359.46
epoch train time: 0:00:00.172079
elapsed time: 0:00:35.272316
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:48:56.285345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.82
 ---- batch: 020 ----
mean loss: 363.59
 ---- batch: 030 ----
mean loss: 366.85
train mean loss: 359.33
epoch train time: 0:00:00.172226
elapsed time: 0:00:35.444738
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:48:56.457769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.35
 ---- batch: 020 ----
mean loss: 357.27
 ---- batch: 030 ----
mean loss: 352.56
train mean loss: 359.11
epoch train time: 0:00:00.172595
elapsed time: 0:00:35.617504
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:48:56.630535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.03
 ---- batch: 020 ----
mean loss: 355.87
 ---- batch: 030 ----
mean loss: 365.46
train mean loss: 358.83
epoch train time: 0:00:00.174173
elapsed time: 0:00:35.791820
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:48:56.804850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.14
 ---- batch: 020 ----
mean loss: 361.23
 ---- batch: 030 ----
mean loss: 366.32
train mean loss: 359.00
epoch train time: 0:00:00.175217
elapsed time: 0:00:35.967201
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:48:56.980234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.41
 ---- batch: 020 ----
mean loss: 354.60
 ---- batch: 030 ----
mean loss: 354.66
train mean loss: 358.84
epoch train time: 0:00:00.169545
elapsed time: 0:00:36.136890
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:48:57.149920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.88
 ---- batch: 020 ----
mean loss: 357.14
 ---- batch: 030 ----
mean loss: 360.39
train mean loss: 358.61
epoch train time: 0:00:00.172126
elapsed time: 0:00:36.309166
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:48:57.322215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.80
 ---- batch: 020 ----
mean loss: 355.31
 ---- batch: 030 ----
mean loss: 361.62
train mean loss: 358.08
epoch train time: 0:00:00.174724
elapsed time: 0:00:36.484047
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:48:57.497077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.29
 ---- batch: 020 ----
mean loss: 355.05
 ---- batch: 030 ----
mean loss: 351.37
train mean loss: 358.38
epoch train time: 0:00:00.172896
elapsed time: 0:00:36.657080
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:48:57.670134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.57
 ---- batch: 020 ----
mean loss: 360.44
 ---- batch: 030 ----
mean loss: 353.22
train mean loss: 358.42
epoch train time: 0:00:00.173503
elapsed time: 0:00:36.830745
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:48:57.843775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.25
 ---- batch: 020 ----
mean loss: 356.65
 ---- batch: 030 ----
mean loss: 357.57
train mean loss: 358.27
epoch train time: 0:00:00.177542
elapsed time: 0:00:37.008424
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:48:58.021477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.68
 ---- batch: 020 ----
mean loss: 362.88
 ---- batch: 030 ----
mean loss: 357.30
train mean loss: 357.86
epoch train time: 0:00:00.172453
elapsed time: 0:00:37.181042
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:48:58.194077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.19
 ---- batch: 020 ----
mean loss: 364.90
 ---- batch: 030 ----
mean loss: 353.55
train mean loss: 357.63
epoch train time: 0:00:00.172984
elapsed time: 0:00:37.354184
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:48:58.367214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.40
 ---- batch: 020 ----
mean loss: 357.57
 ---- batch: 030 ----
mean loss: 357.45
train mean loss: 357.38
epoch train time: 0:00:00.172192
elapsed time: 0:00:37.526515
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:48:58.539559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.89
 ---- batch: 020 ----
mean loss: 359.17
 ---- batch: 030 ----
mean loss: 355.07
train mean loss: 357.43
epoch train time: 0:00:00.170279
elapsed time: 0:00:37.696957
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:48:58.709990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.30
 ---- batch: 020 ----
mean loss: 358.19
 ---- batch: 030 ----
mean loss: 358.41
train mean loss: 357.11
epoch train time: 0:00:00.175024
elapsed time: 0:00:37.872124
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:48:58.885155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.15
 ---- batch: 020 ----
mean loss: 352.81
 ---- batch: 030 ----
mean loss: 356.96
train mean loss: 356.85
epoch train time: 0:00:00.172853
elapsed time: 0:00:38.045120
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:48:59.058153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.08
 ---- batch: 020 ----
mean loss: 353.65
 ---- batch: 030 ----
mean loss: 360.96
train mean loss: 356.82
epoch train time: 0:00:00.174472
elapsed time: 0:00:38.219744
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:48:59.232763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.98
 ---- batch: 020 ----
mean loss: 357.09
 ---- batch: 030 ----
mean loss: 363.39
train mean loss: 356.66
epoch train time: 0:00:00.171119
elapsed time: 0:00:38.390989
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:48:59.404018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.13
 ---- batch: 020 ----
mean loss: 361.87
 ---- batch: 030 ----
mean loss: 350.82
train mean loss: 356.58
epoch train time: 0:00:00.175325
elapsed time: 0:00:38.566452
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:48:59.579483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.33
 ---- batch: 020 ----
mean loss: 363.34
 ---- batch: 030 ----
mean loss: 352.97
train mean loss: 356.46
epoch train time: 0:00:00.175814
elapsed time: 0:00:38.742406
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:48:59.755440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.80
 ---- batch: 020 ----
mean loss: 354.73
 ---- batch: 030 ----
mean loss: 353.56
train mean loss: 356.72
epoch train time: 0:00:00.177442
elapsed time: 0:00:38.920039
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:48:59.933070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.83
 ---- batch: 020 ----
mean loss: 355.12
 ---- batch: 030 ----
mean loss: 360.72
train mean loss: 356.11
epoch train time: 0:00:00.171098
elapsed time: 0:00:39.091278
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:49:00.104309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.67
 ---- batch: 020 ----
mean loss: 358.52
 ---- batch: 030 ----
mean loss: 359.90
train mean loss: 355.93
epoch train time: 0:00:00.169898
elapsed time: 0:00:39.261316
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:49:00.274346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.94
 ---- batch: 020 ----
mean loss: 353.29
 ---- batch: 030 ----
mean loss: 353.63
train mean loss: 356.18
epoch train time: 0:00:00.169812
elapsed time: 0:00:39.431265
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:49:00.444314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.15
 ---- batch: 020 ----
mean loss: 356.27
 ---- batch: 030 ----
mean loss: 351.95
train mean loss: 355.72
epoch train time: 0:00:00.169073
elapsed time: 0:00:39.600537
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:49:00.613579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.93
 ---- batch: 020 ----
mean loss: 360.18
 ---- batch: 030 ----
mean loss: 357.59
train mean loss: 355.72
epoch train time: 0:00:00.172535
elapsed time: 0:00:39.773220
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:49:00.786246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.16
 ---- batch: 020 ----
mean loss: 351.00
 ---- batch: 030 ----
mean loss: 355.86
train mean loss: 355.38
epoch train time: 0:00:00.171944
elapsed time: 0:00:39.945299
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:49:00.958328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.10
 ---- batch: 020 ----
mean loss: 358.67
 ---- batch: 030 ----
mean loss: 352.02
train mean loss: 355.45
epoch train time: 0:00:00.169553
elapsed time: 0:00:40.114987
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:49:01.128015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.17
 ---- batch: 020 ----
mean loss: 365.41
 ---- batch: 030 ----
mean loss: 348.08
train mean loss: 354.67
epoch train time: 0:00:00.170418
elapsed time: 0:00:40.285553
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:49:01.298579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.37
 ---- batch: 020 ----
mean loss: 351.48
 ---- batch: 030 ----
mean loss: 359.56
train mean loss: 354.88
epoch train time: 0:00:00.170321
elapsed time: 0:00:40.456005
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:49:01.469031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.68
 ---- batch: 020 ----
mean loss: 358.28
 ---- batch: 030 ----
mean loss: 358.21
train mean loss: 355.03
epoch train time: 0:00:00.169962
elapsed time: 0:00:40.626098
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:49:01.639125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.54
 ---- batch: 020 ----
mean loss: 357.18
 ---- batch: 030 ----
mean loss: 354.23
train mean loss: 354.89
epoch train time: 0:00:00.175334
elapsed time: 0:00:40.801581
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:49:01.814612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.08
 ---- batch: 020 ----
mean loss: 360.94
 ---- batch: 030 ----
mean loss: 352.56
train mean loss: 354.54
epoch train time: 0:00:00.181849
elapsed time: 0:00:40.983574
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:49:01.996605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.87
 ---- batch: 020 ----
mean loss: 360.10
 ---- batch: 030 ----
mean loss: 349.10
train mean loss: 354.00
epoch train time: 0:00:00.175943
elapsed time: 0:00:41.159681
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:49:02.172727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.10
 ---- batch: 020 ----
mean loss: 353.75
 ---- batch: 030 ----
mean loss: 355.23
train mean loss: 354.19
epoch train time: 0:00:00.172523
elapsed time: 0:00:41.332358
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:49:02.345389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.90
 ---- batch: 020 ----
mean loss: 346.90
 ---- batch: 030 ----
mean loss: 362.74
train mean loss: 354.55
epoch train time: 0:00:00.174895
elapsed time: 0:00:41.507395
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:49:02.520425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.81
 ---- batch: 020 ----
mean loss: 352.41
 ---- batch: 030 ----
mean loss: 354.39
train mean loss: 354.11
epoch train time: 0:00:00.174530
elapsed time: 0:00:41.682062
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:49:02.695092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.80
 ---- batch: 020 ----
mean loss: 357.12
 ---- batch: 030 ----
mean loss: 350.92
train mean loss: 353.84
epoch train time: 0:00:00.173857
elapsed time: 0:00:41.856099
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:49:02.869169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.18
 ---- batch: 020 ----
mean loss: 353.33
 ---- batch: 030 ----
mean loss: 351.91
train mean loss: 353.62
epoch train time: 0:00:00.176620
elapsed time: 0:00:42.032901
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:49:03.045933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.43
 ---- batch: 020 ----
mean loss: 360.17
 ---- batch: 030 ----
mean loss: 360.84
train mean loss: 353.46
epoch train time: 0:00:00.174026
elapsed time: 0:00:42.207078
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:49:03.220111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.84
 ---- batch: 020 ----
mean loss: 348.33
 ---- batch: 030 ----
mean loss: 355.26
train mean loss: 353.84
epoch train time: 0:00:00.173943
elapsed time: 0:00:42.381165
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:49:03.394213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.50
 ---- batch: 020 ----
mean loss: 356.71
 ---- batch: 030 ----
mean loss: 349.95
train mean loss: 353.46
epoch train time: 0:00:00.174524
elapsed time: 0:00:42.555863
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:49:03.568882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.42
 ---- batch: 020 ----
mean loss: 356.39
 ---- batch: 030 ----
mean loss: 354.06
train mean loss: 353.07
epoch train time: 0:00:00.177805
elapsed time: 0:00:42.733812
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:49:03.746855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.48
 ---- batch: 020 ----
mean loss: 355.34
 ---- batch: 030 ----
mean loss: 350.52
train mean loss: 352.98
epoch train time: 0:00:00.173105
elapsed time: 0:00:42.907074
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:49:03.920105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.13
 ---- batch: 020 ----
mean loss: 347.67
 ---- batch: 030 ----
mean loss: 358.94
train mean loss: 352.73
epoch train time: 0:00:00.173322
elapsed time: 0:00:43.080536
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:49:04.093566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.93
 ---- batch: 020 ----
mean loss: 351.35
 ---- batch: 030 ----
mean loss: 346.57
train mean loss: 352.55
epoch train time: 0:00:00.167729
elapsed time: 0:00:43.248402
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:49:04.261431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.26
 ---- batch: 020 ----
mean loss: 359.02
 ---- batch: 030 ----
mean loss: 346.35
train mean loss: 352.75
epoch train time: 0:00:00.171663
elapsed time: 0:00:43.420203
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:49:04.433233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.97
 ---- batch: 020 ----
mean loss: 352.26
 ---- batch: 030 ----
mean loss: 356.26
train mean loss: 352.27
epoch train time: 0:00:00.174904
elapsed time: 0:00:43.595257
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:49:04.608284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.89
 ---- batch: 020 ----
mean loss: 356.83
 ---- batch: 030 ----
mean loss: 346.87
train mean loss: 352.56
epoch train time: 0:00:00.171827
elapsed time: 0:00:43.767221
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:49:04.780253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.57
 ---- batch: 020 ----
mean loss: 350.48
 ---- batch: 030 ----
mean loss: 356.78
train mean loss: 351.98
epoch train time: 0:00:00.174402
elapsed time: 0:00:43.941765
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:49:04.954799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.90
 ---- batch: 020 ----
mean loss: 349.15
 ---- batch: 030 ----
mean loss: 352.79
train mean loss: 352.15
epoch train time: 0:00:00.175391
elapsed time: 0:00:44.117301
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:49:05.130350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.92
 ---- batch: 020 ----
mean loss: 351.89
 ---- batch: 030 ----
mean loss: 353.89
train mean loss: 352.02
epoch train time: 0:00:00.170835
elapsed time: 0:00:44.288298
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:49:05.301346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.77
 ---- batch: 020 ----
mean loss: 347.00
 ---- batch: 030 ----
mean loss: 351.34
train mean loss: 351.79
epoch train time: 0:00:00.174046
elapsed time: 0:00:44.462511
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:49:05.475542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.56
 ---- batch: 020 ----
mean loss: 357.43
 ---- batch: 030 ----
mean loss: 342.69
train mean loss: 351.79
epoch train time: 0:00:00.171942
elapsed time: 0:00:44.634589
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:49:05.647633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.58
 ---- batch: 020 ----
mean loss: 356.13
 ---- batch: 030 ----
mean loss: 347.48
train mean loss: 351.42
epoch train time: 0:00:00.171618
elapsed time: 0:00:44.806389
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:49:05.819428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.11
 ---- batch: 020 ----
mean loss: 358.03
 ---- batch: 030 ----
mean loss: 351.03
train mean loss: 351.38
epoch train time: 0:00:00.169316
elapsed time: 0:00:44.975853
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:49:05.988884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.73
 ---- batch: 020 ----
mean loss: 345.05
 ---- batch: 030 ----
mean loss: 356.56
train mean loss: 351.04
epoch train time: 0:00:00.170465
elapsed time: 0:00:45.146487
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:49:06.159558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.44
 ---- batch: 020 ----
mean loss: 354.12
 ---- batch: 030 ----
mean loss: 354.93
train mean loss: 350.75
epoch train time: 0:00:00.167899
elapsed time: 0:00:45.314567
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:49:06.327594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.52
 ---- batch: 020 ----
mean loss: 354.40
 ---- batch: 030 ----
mean loss: 351.09
train mean loss: 350.81
epoch train time: 0:00:00.166493
elapsed time: 0:00:45.481192
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:49:06.494237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.51
 ---- batch: 020 ----
mean loss: 356.23
 ---- batch: 030 ----
mean loss: 347.56
train mean loss: 350.41
epoch train time: 0:00:00.174820
elapsed time: 0:00:45.656166
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:49:06.669196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.79
 ---- batch: 020 ----
mean loss: 354.61
 ---- batch: 030 ----
mean loss: 345.38
train mean loss: 350.30
epoch train time: 0:00:00.175030
elapsed time: 0:00:45.831337
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:49:06.844368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.23
 ---- batch: 020 ----
mean loss: 349.81
 ---- batch: 030 ----
mean loss: 348.26
train mean loss: 350.36
epoch train time: 0:00:00.175563
elapsed time: 0:00:46.007039
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:49:07.020071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.55
 ---- batch: 020 ----
mean loss: 353.47
 ---- batch: 030 ----
mean loss: 349.29
train mean loss: 350.15
epoch train time: 0:00:00.173477
elapsed time: 0:00:46.180696
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:49:07.193745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.65
 ---- batch: 020 ----
mean loss: 344.99
 ---- batch: 030 ----
mean loss: 349.66
train mean loss: 349.87
epoch train time: 0:00:00.174261
elapsed time: 0:00:46.355132
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:49:07.368162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.23
 ---- batch: 020 ----
mean loss: 349.62
 ---- batch: 030 ----
mean loss: 346.86
train mean loss: 349.46
epoch train time: 0:00:00.173222
elapsed time: 0:00:46.528492
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:49:07.541556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.66
 ---- batch: 020 ----
mean loss: 353.68
 ---- batch: 030 ----
mean loss: 355.63
train mean loss: 349.60
epoch train time: 0:00:00.173326
elapsed time: 0:00:46.701999
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:49:07.715031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.48
 ---- batch: 020 ----
mean loss: 348.24
 ---- batch: 030 ----
mean loss: 348.89
train mean loss: 349.47
epoch train time: 0:00:00.173617
elapsed time: 0:00:46.875753
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:49:07.888784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.04
 ---- batch: 020 ----
mean loss: 349.94
 ---- batch: 030 ----
mean loss: 351.70
train mean loss: 348.82
epoch train time: 0:00:00.172211
elapsed time: 0:00:47.048103
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:49:08.061162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.58
 ---- batch: 020 ----
mean loss: 357.29
 ---- batch: 030 ----
mean loss: 345.16
train mean loss: 349.33
epoch train time: 0:00:00.169048
elapsed time: 0:00:47.217318
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:49:08.230349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.73
 ---- batch: 020 ----
mean loss: 345.22
 ---- batch: 030 ----
mean loss: 351.49
train mean loss: 349.03
epoch train time: 0:00:00.168540
elapsed time: 0:00:47.386065
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:49:08.399086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.82
 ---- batch: 020 ----
mean loss: 350.29
 ---- batch: 030 ----
mean loss: 354.10
train mean loss: 349.09
epoch train time: 0:00:00.172105
elapsed time: 0:00:47.558299
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:49:08.571329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.43
 ---- batch: 020 ----
mean loss: 351.79
 ---- batch: 030 ----
mean loss: 342.41
train mean loss: 349.18
epoch train time: 0:00:00.171826
elapsed time: 0:00:47.730279
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:49:08.743324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.32
 ---- batch: 020 ----
mean loss: 344.89
 ---- batch: 030 ----
mean loss: 356.17
train mean loss: 349.00
epoch train time: 0:00:00.179211
elapsed time: 0:00:47.909644
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:49:08.922675
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.61
 ---- batch: 020 ----
mean loss: 348.23
 ---- batch: 030 ----
mean loss: 352.23
train mean loss: 348.69
epoch train time: 0:00:00.175865
elapsed time: 0:00:48.085662
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:49:09.098692
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.01
 ---- batch: 020 ----
mean loss: 347.82
 ---- batch: 030 ----
mean loss: 346.51
train mean loss: 348.99
epoch train time: 0:00:00.170825
elapsed time: 0:00:48.256648
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:49:09.269682
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.02
 ---- batch: 020 ----
mean loss: 349.77
 ---- batch: 030 ----
mean loss: 355.12
train mean loss: 348.76
epoch train time: 0:00:00.171041
elapsed time: 0:00:48.427830
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:49:09.440860
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.28
 ---- batch: 020 ----
mean loss: 342.50
 ---- batch: 030 ----
mean loss: 350.21
train mean loss: 349.12
epoch train time: 0:00:00.173298
elapsed time: 0:00:48.601269
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:49:09.614300
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.44
 ---- batch: 020 ----
mean loss: 356.22
 ---- batch: 030 ----
mean loss: 350.31
train mean loss: 348.69
epoch train time: 0:00:00.173942
elapsed time: 0:00:48.775352
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:49:09.788407
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.23
 ---- batch: 020 ----
mean loss: 346.45
 ---- batch: 030 ----
mean loss: 352.23
train mean loss: 349.44
epoch train time: 0:00:00.174931
elapsed time: 0:00:48.950447
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:49:09.963495
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.57
 ---- batch: 020 ----
mean loss: 348.36
 ---- batch: 030 ----
mean loss: 355.39
train mean loss: 348.57
epoch train time: 0:00:00.172813
elapsed time: 0:00:49.123416
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:49:10.136447
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.64
 ---- batch: 020 ----
mean loss: 355.88
 ---- batch: 030 ----
mean loss: 345.94
train mean loss: 348.46
epoch train time: 0:00:00.172802
elapsed time: 0:00:49.296355
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:49:10.309384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.15
 ---- batch: 020 ----
mean loss: 346.74
 ---- batch: 030 ----
mean loss: 344.92
train mean loss: 348.35
epoch train time: 0:00:00.171217
elapsed time: 0:00:49.467709
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:49:10.480737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.50
 ---- batch: 020 ----
mean loss: 345.26
 ---- batch: 030 ----
mean loss: 350.04
train mean loss: 349.21
epoch train time: 0:00:00.169748
elapsed time: 0:00:49.637594
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:49:10.650624
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.84
 ---- batch: 020 ----
mean loss: 344.36
 ---- batch: 030 ----
mean loss: 349.53
train mean loss: 348.65
epoch train time: 0:00:00.177363
elapsed time: 0:00:49.815105
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:49:10.828158
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.64
 ---- batch: 020 ----
mean loss: 350.16
 ---- batch: 030 ----
mean loss: 346.19
train mean loss: 348.39
epoch train time: 0:00:00.177298
elapsed time: 0:00:49.992569
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:49:11.005600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.76
 ---- batch: 020 ----
mean loss: 346.21
 ---- batch: 030 ----
mean loss: 351.39
train mean loss: 349.08
epoch train time: 0:00:00.173948
elapsed time: 0:00:50.166670
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:49:11.179715
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.29
 ---- batch: 020 ----
mean loss: 348.61
 ---- batch: 030 ----
mean loss: 345.49
train mean loss: 348.90
epoch train time: 0:00:00.171324
elapsed time: 0:00:50.338147
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:49:11.351192
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.57
 ---- batch: 020 ----
mean loss: 351.39
 ---- batch: 030 ----
mean loss: 343.64
train mean loss: 348.65
epoch train time: 0:00:00.170834
elapsed time: 0:00:50.509156
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:49:11.522186
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.11
 ---- batch: 020 ----
mean loss: 344.53
 ---- batch: 030 ----
mean loss: 347.16
train mean loss: 348.54
epoch train time: 0:00:00.169615
elapsed time: 0:00:50.678920
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:49:11.692027
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.95
 ---- batch: 020 ----
mean loss: 347.04
 ---- batch: 030 ----
mean loss: 345.33
train mean loss: 348.66
epoch train time: 0:00:00.183213
elapsed time: 0:00:50.862360
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:49:11.875406
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.94
 ---- batch: 020 ----
mean loss: 335.83
 ---- batch: 030 ----
mean loss: 351.89
train mean loss: 348.41
epoch train time: 0:00:00.178805
elapsed time: 0:00:51.041324
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:49:12.054355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.34
 ---- batch: 020 ----
mean loss: 346.42
 ---- batch: 030 ----
mean loss: 349.48
train mean loss: 348.50
epoch train time: 0:00:00.178433
elapsed time: 0:00:51.219898
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:49:12.232958
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.94
 ---- batch: 020 ----
mean loss: 349.13
 ---- batch: 030 ----
mean loss: 350.93
train mean loss: 348.70
epoch train time: 0:00:00.173784
elapsed time: 0:00:51.393854
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:49:12.406885
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.32
 ---- batch: 020 ----
mean loss: 347.42
 ---- batch: 030 ----
mean loss: 345.01
train mean loss: 348.64
epoch train time: 0:00:00.175759
elapsed time: 0:00:51.569752
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:49:12.582798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.20
 ---- batch: 020 ----
mean loss: 344.06
 ---- batch: 030 ----
mean loss: 346.50
train mean loss: 348.35
epoch train time: 0:00:00.174682
elapsed time: 0:00:51.744588
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:49:12.757657
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.32
 ---- batch: 020 ----
mean loss: 342.16
 ---- batch: 030 ----
mean loss: 352.01
train mean loss: 348.36
epoch train time: 0:00:00.181214
elapsed time: 0:00:51.925991
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:49:12.939044
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.42
 ---- batch: 020 ----
mean loss: 357.66
 ---- batch: 030 ----
mean loss: 349.83
train mean loss: 348.35
epoch train time: 0:00:00.174165
elapsed time: 0:00:52.100347
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:49:13.113377
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.69
 ---- batch: 020 ----
mean loss: 350.94
 ---- batch: 030 ----
mean loss: 348.79
train mean loss: 349.02
epoch train time: 0:00:00.172916
elapsed time: 0:00:52.273414
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:49:13.286445
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.31
 ---- batch: 020 ----
mean loss: 355.60
 ---- batch: 030 ----
mean loss: 340.52
train mean loss: 348.01
epoch train time: 0:00:00.172408
elapsed time: 0:00:52.445993
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:49:13.459023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.50
 ---- batch: 020 ----
mean loss: 339.03
 ---- batch: 030 ----
mean loss: 356.24
train mean loss: 348.27
epoch train time: 0:00:00.174471
elapsed time: 0:00:52.620611
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:49:13.633667
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.81
 ---- batch: 020 ----
mean loss: 342.34
 ---- batch: 030 ----
mean loss: 355.96
train mean loss: 348.52
epoch train time: 0:00:00.175014
elapsed time: 0:00:52.795810
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:49:13.808841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.89
 ---- batch: 020 ----
mean loss: 352.00
 ---- batch: 030 ----
mean loss: 351.49
train mean loss: 348.27
epoch train time: 0:00:00.174001
elapsed time: 0:00:52.969992
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:49:13.983040
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.12
 ---- batch: 020 ----
mean loss: 348.14
 ---- batch: 030 ----
mean loss: 350.13
train mean loss: 348.61
epoch train time: 0:00:00.172640
elapsed time: 0:00:53.142791
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:49:14.155821
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.20
 ---- batch: 020 ----
mean loss: 347.38
 ---- batch: 030 ----
mean loss: 352.53
train mean loss: 348.10
epoch train time: 0:00:00.173201
elapsed time: 0:00:53.316132
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:49:14.329164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.72
 ---- batch: 020 ----
mean loss: 351.07
 ---- batch: 030 ----
mean loss: 342.91
train mean loss: 348.46
epoch train time: 0:00:00.173317
elapsed time: 0:00:53.489590
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:49:14.502640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.30
 ---- batch: 020 ----
mean loss: 353.95
 ---- batch: 030 ----
mean loss: 347.03
train mean loss: 348.34
epoch train time: 0:00:00.172789
elapsed time: 0:00:53.662554
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:49:14.675603
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.65
 ---- batch: 020 ----
mean loss: 354.71
 ---- batch: 030 ----
mean loss: 337.34
train mean loss: 348.39
epoch train time: 0:00:00.178281
elapsed time: 0:00:53.840997
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:49:14.854029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.26
 ---- batch: 020 ----
mean loss: 353.74
 ---- batch: 030 ----
mean loss: 346.64
train mean loss: 348.35
epoch train time: 0:00:00.174473
elapsed time: 0:00:54.015612
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:49:15.028656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.08
 ---- batch: 020 ----
mean loss: 343.15
 ---- batch: 030 ----
mean loss: 346.48
train mean loss: 347.84
epoch train time: 0:00:00.171272
elapsed time: 0:00:54.187035
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:49:15.200064
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.62
 ---- batch: 020 ----
mean loss: 347.76
 ---- batch: 030 ----
mean loss: 347.33
train mean loss: 348.10
epoch train time: 0:00:00.170472
elapsed time: 0:00:54.357647
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:49:15.370678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.68
 ---- batch: 020 ----
mean loss: 344.10
 ---- batch: 030 ----
mean loss: 348.40
train mean loss: 348.57
epoch train time: 0:00:00.166469
elapsed time: 0:00:54.524255
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:49:15.537285
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.77
 ---- batch: 020 ----
mean loss: 353.80
 ---- batch: 030 ----
mean loss: 346.25
train mean loss: 347.92
epoch train time: 0:00:00.165195
elapsed time: 0:00:54.689592
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:49:15.702623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.53
 ---- batch: 020 ----
mean loss: 350.48
 ---- batch: 030 ----
mean loss: 341.07
train mean loss: 348.36
epoch train time: 0:00:00.173130
elapsed time: 0:00:54.862893
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:49:15.875929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.80
 ---- batch: 020 ----
mean loss: 344.84
 ---- batch: 030 ----
mean loss: 348.29
train mean loss: 348.29
epoch train time: 0:00:00.173373
elapsed time: 0:00:55.036422
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:49:16.049456
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.28
 ---- batch: 020 ----
mean loss: 359.46
 ---- batch: 030 ----
mean loss: 343.42
train mean loss: 348.33
epoch train time: 0:00:00.172973
elapsed time: 0:00:55.209536
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:49:16.222566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.51
 ---- batch: 020 ----
mean loss: 349.74
 ---- batch: 030 ----
mean loss: 342.50
train mean loss: 348.34
epoch train time: 0:00:00.173055
elapsed time: 0:00:55.382730
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:49:16.395761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.61
 ---- batch: 020 ----
mean loss: 346.29
 ---- batch: 030 ----
mean loss: 353.94
train mean loss: 348.14
epoch train time: 0:00:00.173009
elapsed time: 0:00:55.555880
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:49:16.568910
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.49
 ---- batch: 020 ----
mean loss: 342.96
 ---- batch: 030 ----
mean loss: 349.84
train mean loss: 347.91
epoch train time: 0:00:00.173723
elapsed time: 0:00:55.731741
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_7/checkpoint.pth.tar
**** end time: 2019-09-27 16:49:16.744739 ****
