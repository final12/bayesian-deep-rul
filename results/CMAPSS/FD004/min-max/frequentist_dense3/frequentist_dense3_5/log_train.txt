Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18174
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:19:40.238578 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:19:40.241854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 5009.27
 ---- batch: 020 ----
mean loss: 4896.46
 ---- batch: 030 ----
mean loss: 4801.62
 ---- batch: 040 ----
mean loss: 4708.60
 ---- batch: 050 ----
mean loss: 4653.80
 ---- batch: 060 ----
mean loss: 4559.59
 ---- batch: 070 ----
mean loss: 4534.13
 ---- batch: 080 ----
mean loss: 4468.02
 ---- batch: 090 ----
mean loss: 4404.99
 ---- batch: 100 ----
mean loss: 4382.26
 ---- batch: 110 ----
mean loss: 4354.62
train mean loss: 4607.03
epoch train time: 0:00:32.926715
elapsed time: 0:00:32.932245
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:20:13.170862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4242.65
 ---- batch: 020 ----
mean loss: 4155.95
 ---- batch: 030 ----
mean loss: 4135.12
 ---- batch: 040 ----
mean loss: 4072.24
 ---- batch: 050 ----
mean loss: 4056.25
 ---- batch: 060 ----
mean loss: 3968.71
 ---- batch: 070 ----
mean loss: 3879.61
 ---- batch: 080 ----
mean loss: 3875.53
 ---- batch: 090 ----
mean loss: 3791.22
 ---- batch: 100 ----
mean loss: 3705.31
 ---- batch: 110 ----
mean loss: 3624.35
train mean loss: 3948.27
epoch train time: 0:00:00.551325
elapsed time: 0:00:33.483696
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:20:13.722324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3608.43
 ---- batch: 020 ----
mean loss: 3553.57
 ---- batch: 030 ----
mean loss: 3531.59
 ---- batch: 040 ----
mean loss: 3475.51
 ---- batch: 050 ----
mean loss: 3416.82
 ---- batch: 060 ----
mean loss: 3354.79
 ---- batch: 070 ----
mean loss: 3327.13
 ---- batch: 080 ----
mean loss: 3239.09
 ---- batch: 090 ----
mean loss: 3175.43
 ---- batch: 100 ----
mean loss: 3172.14
 ---- batch: 110 ----
mean loss: 3038.66
train mean loss: 3347.80
epoch train time: 0:00:00.550158
elapsed time: 0:00:34.034017
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:20:14.272687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3022.02
 ---- batch: 020 ----
mean loss: 2931.65
 ---- batch: 030 ----
mean loss: 2963.17
 ---- batch: 040 ----
mean loss: 2922.85
 ---- batch: 050 ----
mean loss: 2832.76
 ---- batch: 060 ----
mean loss: 2818.21
 ---- batch: 070 ----
mean loss: 2763.18
 ---- batch: 080 ----
mean loss: 2758.73
 ---- batch: 090 ----
mean loss: 2652.93
 ---- batch: 100 ----
mean loss: 2632.17
 ---- batch: 110 ----
mean loss: 2588.86
train mean loss: 2801.26
epoch train time: 0:00:00.551878
elapsed time: 0:00:34.586072
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:20:14.824723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2573.36
 ---- batch: 020 ----
mean loss: 2504.57
 ---- batch: 030 ----
mean loss: 2449.15
 ---- batch: 040 ----
mean loss: 2406.96
 ---- batch: 050 ----
mean loss: 2386.00
 ---- batch: 060 ----
mean loss: 2337.82
 ---- batch: 070 ----
mean loss: 2282.79
 ---- batch: 080 ----
mean loss: 2245.95
 ---- batch: 090 ----
mean loss: 2238.88
 ---- batch: 100 ----
mean loss: 2221.19
 ---- batch: 110 ----
mean loss: 2199.50
train mean loss: 2344.04
epoch train time: 0:00:00.556598
elapsed time: 0:00:35.142837
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:20:15.381466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2155.56
 ---- batch: 020 ----
mean loss: 2119.25
 ---- batch: 030 ----
mean loss: 2081.31
 ---- batch: 040 ----
mean loss: 2086.85
 ---- batch: 050 ----
mean loss: 2001.01
 ---- batch: 060 ----
mean loss: 1970.98
 ---- batch: 070 ----
mean loss: 1937.26
 ---- batch: 080 ----
mean loss: 1904.37
 ---- batch: 090 ----
mean loss: 1896.21
 ---- batch: 100 ----
mean loss: 1890.15
 ---- batch: 110 ----
mean loss: 1847.57
train mean loss: 1986.39
epoch train time: 0:00:00.554875
elapsed time: 0:00:35.697845
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:20:15.936512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1822.29
 ---- batch: 020 ----
mean loss: 1788.69
 ---- batch: 030 ----
mean loss: 1753.09
 ---- batch: 040 ----
mean loss: 1740.66
 ---- batch: 050 ----
mean loss: 1756.35
 ---- batch: 060 ----
mean loss: 1658.28
 ---- batch: 070 ----
mean loss: 1650.40
 ---- batch: 080 ----
mean loss: 1621.42
 ---- batch: 090 ----
mean loss: 1614.69
 ---- batch: 100 ----
mean loss: 1571.86
 ---- batch: 110 ----
mean loss: 1590.48
train mean loss: 1684.29
epoch train time: 0:00:00.558237
elapsed time: 0:00:36.256267
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:20:16.494886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1519.64
 ---- batch: 020 ----
mean loss: 1530.07
 ---- batch: 030 ----
mean loss: 1479.41
 ---- batch: 040 ----
mean loss: 1460.15
 ---- batch: 050 ----
mean loss: 1457.25
 ---- batch: 060 ----
mean loss: 1438.90
 ---- batch: 070 ----
mean loss: 1420.85
 ---- batch: 080 ----
mean loss: 1405.99
 ---- batch: 090 ----
mean loss: 1408.95
 ---- batch: 100 ----
mean loss: 1391.78
 ---- batch: 110 ----
mean loss: 1330.74
train mean loss: 1438.40
epoch train time: 0:00:00.571289
elapsed time: 0:00:36.827703
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:20:17.066321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1319.56
 ---- batch: 020 ----
mean loss: 1326.71
 ---- batch: 030 ----
mean loss: 1293.94
 ---- batch: 040 ----
mean loss: 1272.99
 ---- batch: 050 ----
mean loss: 1274.88
 ---- batch: 060 ----
mean loss: 1273.18
 ---- batch: 070 ----
mean loss: 1254.36
 ---- batch: 080 ----
mean loss: 1237.09
 ---- batch: 090 ----
mean loss: 1220.92
 ---- batch: 100 ----
mean loss: 1218.49
 ---- batch: 110 ----
mean loss: 1211.50
train mean loss: 1261.24
epoch train time: 0:00:00.551347
elapsed time: 0:00:37.379180
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:20:17.617807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1186.03
 ---- batch: 020 ----
mean loss: 1181.75
 ---- batch: 030 ----
mean loss: 1154.21
 ---- batch: 040 ----
mean loss: 1140.70
 ---- batch: 050 ----
mean loss: 1136.84
 ---- batch: 060 ----
mean loss: 1135.65
 ---- batch: 070 ----
mean loss: 1130.37
 ---- batch: 080 ----
mean loss: 1113.27
 ---- batch: 090 ----
mean loss: 1109.19
 ---- batch: 100 ----
mean loss: 1083.14
 ---- batch: 110 ----
mean loss: 1097.57
train mean loss: 1132.39
epoch train time: 0:00:00.557494
elapsed time: 0:00:37.936841
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:20:18.175483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1072.80
 ---- batch: 020 ----
mean loss: 1064.24
 ---- batch: 030 ----
mean loss: 1063.65
 ---- batch: 040 ----
mean loss: 1056.37
 ---- batch: 050 ----
mean loss: 1047.97
 ---- batch: 060 ----
mean loss: 1039.33
 ---- batch: 070 ----
mean loss: 1019.91
 ---- batch: 080 ----
mean loss: 1024.39
 ---- batch: 090 ----
mean loss: 1025.78
 ---- batch: 100 ----
mean loss: 1022.73
 ---- batch: 110 ----
mean loss: 1000.09
train mean loss: 1038.83
epoch train time: 0:00:00.564599
elapsed time: 0:00:38.501625
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:20:18.740260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1004.73
 ---- batch: 020 ----
mean loss: 989.34
 ---- batch: 030 ----
mean loss: 982.28
 ---- batch: 040 ----
mean loss: 970.39
 ---- batch: 050 ----
mean loss: 958.08
 ---- batch: 060 ----
mean loss: 965.13
 ---- batch: 070 ----
mean loss: 967.05
 ---- batch: 080 ----
mean loss: 955.22
 ---- batch: 090 ----
mean loss: 955.43
 ---- batch: 100 ----
mean loss: 949.61
 ---- batch: 110 ----
mean loss: 935.62
train mean loss: 966.47
epoch train time: 0:00:00.556569
elapsed time: 0:00:39.058337
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:20:19.296976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.09
 ---- batch: 020 ----
mean loss: 940.33
 ---- batch: 030 ----
mean loss: 932.70
 ---- batch: 040 ----
mean loss: 923.22
 ---- batch: 050 ----
mean loss: 914.77
 ---- batch: 060 ----
mean loss: 899.09
 ---- batch: 070 ----
mean loss: 923.97
 ---- batch: 080 ----
mean loss: 899.03
 ---- batch: 090 ----
mean loss: 904.52
 ---- batch: 100 ----
mean loss: 915.14
 ---- batch: 110 ----
mean loss: 890.46
train mean loss: 916.89
epoch train time: 0:00:00.556088
elapsed time: 0:00:39.614570
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:20:19.853194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.36
 ---- batch: 020 ----
mean loss: 889.35
 ---- batch: 030 ----
mean loss: 890.45
 ---- batch: 040 ----
mean loss: 883.21
 ---- batch: 050 ----
mean loss: 884.87
 ---- batch: 060 ----
mean loss: 895.30
 ---- batch: 070 ----
mean loss: 887.70
 ---- batch: 080 ----
mean loss: 887.23
 ---- batch: 090 ----
mean loss: 875.95
 ---- batch: 100 ----
mean loss: 883.94
 ---- batch: 110 ----
mean loss: 883.59
train mean loss: 886.65
epoch train time: 0:00:00.548688
elapsed time: 0:00:40.163391
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:20:20.402054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.96
 ---- batch: 020 ----
mean loss: 870.65
 ---- batch: 030 ----
mean loss: 886.55
 ---- batch: 040 ----
mean loss: 883.82
 ---- batch: 050 ----
mean loss: 868.76
 ---- batch: 060 ----
mean loss: 861.29
 ---- batch: 070 ----
mean loss: 860.27
 ---- batch: 080 ----
mean loss: 855.92
 ---- batch: 090 ----
mean loss: 864.55
 ---- batch: 100 ----
mean loss: 856.18
 ---- batch: 110 ----
mean loss: 875.15
train mean loss: 868.13
epoch train time: 0:00:00.557097
elapsed time: 0:00:40.720662
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:20:20.959334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.78
 ---- batch: 020 ----
mean loss: 864.92
 ---- batch: 030 ----
mean loss: 856.07
 ---- batch: 040 ----
mean loss: 844.87
 ---- batch: 050 ----
mean loss: 854.97
 ---- batch: 060 ----
mean loss: 860.27
 ---- batch: 070 ----
mean loss: 870.18
 ---- batch: 080 ----
mean loss: 846.46
 ---- batch: 090 ----
mean loss: 849.73
 ---- batch: 100 ----
mean loss: 862.71
 ---- batch: 110 ----
mean loss: 843.41
train mean loss: 857.29
epoch train time: 0:00:00.557703
elapsed time: 0:00:41.278546
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:20:21.517172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.78
 ---- batch: 020 ----
mean loss: 826.45
 ---- batch: 030 ----
mean loss: 851.32
 ---- batch: 040 ----
mean loss: 869.23
 ---- batch: 050 ----
mean loss: 868.91
 ---- batch: 060 ----
mean loss: 865.22
 ---- batch: 070 ----
mean loss: 859.36
 ---- batch: 080 ----
mean loss: 850.99
 ---- batch: 090 ----
mean loss: 837.38
 ---- batch: 100 ----
mean loss: 841.54
 ---- batch: 110 ----
mean loss: 842.95
train mean loss: 851.03
epoch train time: 0:00:00.559185
elapsed time: 0:00:41.837874
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:20:22.076501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.04
 ---- batch: 020 ----
mean loss: 852.21
 ---- batch: 030 ----
mean loss: 836.06
 ---- batch: 040 ----
mean loss: 848.56
 ---- batch: 050 ----
mean loss: 862.14
 ---- batch: 060 ----
mean loss: 828.18
 ---- batch: 070 ----
mean loss: 861.06
 ---- batch: 080 ----
mean loss: 843.60
 ---- batch: 090 ----
mean loss: 844.01
 ---- batch: 100 ----
mean loss: 861.93
 ---- batch: 110 ----
mean loss: 857.49
train mean loss: 847.43
epoch train time: 0:00:00.557294
elapsed time: 0:00:42.395306
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:20:22.633944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.84
 ---- batch: 020 ----
mean loss: 857.20
 ---- batch: 030 ----
mean loss: 844.45
 ---- batch: 040 ----
mean loss: 824.95
 ---- batch: 050 ----
mean loss: 836.72
 ---- batch: 060 ----
mean loss: 852.63
 ---- batch: 070 ----
mean loss: 842.38
 ---- batch: 080 ----
mean loss: 843.32
 ---- batch: 090 ----
mean loss: 853.15
 ---- batch: 100 ----
mean loss: 841.16
 ---- batch: 110 ----
mean loss: 863.50
train mean loss: 845.53
epoch train time: 0:00:00.555005
elapsed time: 0:00:42.950472
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:20:23.189116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.64
 ---- batch: 020 ----
mean loss: 853.34
 ---- batch: 030 ----
mean loss: 855.09
 ---- batch: 040 ----
mean loss: 839.49
 ---- batch: 050 ----
mean loss: 829.03
 ---- batch: 060 ----
mean loss: 851.92
 ---- batch: 070 ----
mean loss: 843.46
 ---- batch: 080 ----
mean loss: 849.05
 ---- batch: 090 ----
mean loss: 838.69
 ---- batch: 100 ----
mean loss: 846.66
 ---- batch: 110 ----
mean loss: 851.69
train mean loss: 844.67
epoch train time: 0:00:00.551342
elapsed time: 0:00:43.501969
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:20:23.740611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.55
 ---- batch: 020 ----
mean loss: 880.60
 ---- batch: 030 ----
mean loss: 843.52
 ---- batch: 040 ----
mean loss: 869.57
 ---- batch: 050 ----
mean loss: 847.94
 ---- batch: 060 ----
mean loss: 859.77
 ---- batch: 070 ----
mean loss: 840.72
 ---- batch: 080 ----
mean loss: 857.44
 ---- batch: 090 ----
mean loss: 830.70
 ---- batch: 100 ----
mean loss: 826.47
 ---- batch: 110 ----
mean loss: 828.10
train mean loss: 844.28
epoch train time: 0:00:00.557425
elapsed time: 0:00:44.059545
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:20:24.298168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.01
 ---- batch: 020 ----
mean loss: 856.45
 ---- batch: 030 ----
mean loss: 828.46
 ---- batch: 040 ----
mean loss: 858.94
 ---- batch: 050 ----
mean loss: 850.60
 ---- batch: 060 ----
mean loss: 841.73
 ---- batch: 070 ----
mean loss: 853.92
 ---- batch: 080 ----
mean loss: 837.75
 ---- batch: 090 ----
mean loss: 835.48
 ---- batch: 100 ----
mean loss: 831.92
 ---- batch: 110 ----
mean loss: 853.11
train mean loss: 844.09
epoch train time: 0:00:00.554803
elapsed time: 0:00:44.614480
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:20:24.853106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.55
 ---- batch: 020 ----
mean loss: 834.78
 ---- batch: 030 ----
mean loss: 819.18
 ---- batch: 040 ----
mean loss: 844.23
 ---- batch: 050 ----
mean loss: 871.36
 ---- batch: 060 ----
mean loss: 835.71
 ---- batch: 070 ----
mean loss: 864.72
 ---- batch: 080 ----
mean loss: 832.29
 ---- batch: 090 ----
mean loss: 846.12
 ---- batch: 100 ----
mean loss: 861.98
 ---- batch: 110 ----
mean loss: 844.25
train mean loss: 844.06
epoch train time: 0:00:00.554160
elapsed time: 0:00:45.168774
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:20:25.407414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.48
 ---- batch: 020 ----
mean loss: 841.66
 ---- batch: 030 ----
mean loss: 856.84
 ---- batch: 040 ----
mean loss: 844.19
 ---- batch: 050 ----
mean loss: 860.67
 ---- batch: 060 ----
mean loss: 834.28
 ---- batch: 070 ----
mean loss: 833.54
 ---- batch: 080 ----
mean loss: 858.24
 ---- batch: 090 ----
mean loss: 841.88
 ---- batch: 100 ----
mean loss: 851.78
 ---- batch: 110 ----
mean loss: 829.30
train mean loss: 844.01
epoch train time: 0:00:00.557874
elapsed time: 0:00:45.726795
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:20:25.965419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.90
 ---- batch: 020 ----
mean loss: 842.30
 ---- batch: 030 ----
mean loss: 827.36
 ---- batch: 040 ----
mean loss: 849.84
 ---- batch: 050 ----
mean loss: 846.93
 ---- batch: 060 ----
mean loss: 858.64
 ---- batch: 070 ----
mean loss: 818.94
 ---- batch: 080 ----
mean loss: 846.63
 ---- batch: 090 ----
mean loss: 854.55
 ---- batch: 100 ----
mean loss: 835.55
 ---- batch: 110 ----
mean loss: 854.67
train mean loss: 844.00
epoch train time: 0:00:00.549223
elapsed time: 0:00:46.276176
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:20:26.514820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.79
 ---- batch: 020 ----
mean loss: 845.52
 ---- batch: 030 ----
mean loss: 843.54
 ---- batch: 040 ----
mean loss: 840.50
 ---- batch: 050 ----
mean loss: 834.65
 ---- batch: 060 ----
mean loss: 852.34
 ---- batch: 070 ----
mean loss: 864.86
 ---- batch: 080 ----
mean loss: 830.97
 ---- batch: 090 ----
mean loss: 848.37
 ---- batch: 100 ----
mean loss: 838.75
 ---- batch: 110 ----
mean loss: 838.40
train mean loss: 843.96
epoch train time: 0:00:00.558413
elapsed time: 0:00:46.834775
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:20:27.073399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.06
 ---- batch: 020 ----
mean loss: 852.37
 ---- batch: 030 ----
mean loss: 858.97
 ---- batch: 040 ----
mean loss: 845.34
 ---- batch: 050 ----
mean loss: 839.65
 ---- batch: 060 ----
mean loss: 819.41
 ---- batch: 070 ----
mean loss: 828.50
 ---- batch: 080 ----
mean loss: 860.86
 ---- batch: 090 ----
mean loss: 863.12
 ---- batch: 100 ----
mean loss: 836.78
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.567096
elapsed time: 0:00:47.402004
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:20:27.640648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.66
 ---- batch: 030 ----
mean loss: 854.13
 ---- batch: 040 ----
mean loss: 865.56
 ---- batch: 050 ----
mean loss: 838.79
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 829.95
 ---- batch: 080 ----
mean loss: 849.24
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 835.78
 ---- batch: 110 ----
mean loss: 847.17
train mean loss: 843.84
epoch train time: 0:00:00.555763
elapsed time: 0:00:47.957919
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:20:28.196554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.61
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.25
 ---- batch: 040 ----
mean loss: 861.72
 ---- batch: 050 ----
mean loss: 858.48
 ---- batch: 060 ----
mean loss: 838.94
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.45
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.94
epoch train time: 0:00:00.550822
elapsed time: 0:00:48.508891
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:20:28.747530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.48
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.56
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.567070
elapsed time: 0:00:49.076113
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:20:29.314752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.22
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.43
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.61
 ---- batch: 090 ----
mean loss: 844.55
 ---- batch: 100 ----
mean loss: 855.06
 ---- batch: 110 ----
mean loss: 856.51
train mean loss: 844.02
epoch train time: 0:00:00.569015
elapsed time: 0:00:49.645293
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:20:29.883915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.89
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.12
 ---- batch: 050 ----
mean loss: 818.03
 ---- batch: 060 ----
mean loss: 833.12
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.87
 ---- batch: 090 ----
mean loss: 842.99
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.566099
elapsed time: 0:00:50.211540
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:20:30.450179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.32
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.55
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.01
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.00
train mean loss: 843.87
epoch train time: 0:00:00.570994
elapsed time: 0:00:50.782678
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:20:31.021302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.60
 ---- batch: 020 ----
mean loss: 848.83
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 824.87
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.42
 ---- batch: 080 ----
mean loss: 858.18
 ---- batch: 090 ----
mean loss: 855.87
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.83
train mean loss: 843.97
epoch train time: 0:00:00.556033
elapsed time: 0:00:51.338841
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:20:31.577472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.73
 ---- batch: 030 ----
mean loss: 838.89
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.73
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.553469
elapsed time: 0:00:51.892450
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:20:32.131074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.12
 ---- batch: 020 ----
mean loss: 861.63
 ---- batch: 030 ----
mean loss: 838.12
 ---- batch: 040 ----
mean loss: 845.01
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.19
 ---- batch: 090 ----
mean loss: 833.50
 ---- batch: 100 ----
mean loss: 834.01
 ---- batch: 110 ----
mean loss: 834.96
train mean loss: 844.04
epoch train time: 0:00:00.548994
elapsed time: 0:00:52.441573
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:20:32.680200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.51
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.74
 ---- batch: 060 ----
mean loss: 850.21
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.14
 ---- batch: 090 ----
mean loss: 864.26
 ---- batch: 100 ----
mean loss: 852.65
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.95
epoch train time: 0:00:00.553898
elapsed time: 0:00:52.995604
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:20:33.234228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.72
 ---- batch: 030 ----
mean loss: 830.98
 ---- batch: 040 ----
mean loss: 838.38
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.24
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.76
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.07
 ---- batch: 110 ----
mean loss: 855.24
train mean loss: 843.94
epoch train time: 0:00:00.549778
elapsed time: 0:00:53.545515
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:20:33.784159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.38
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.66
 ---- batch: 050 ----
mean loss: 817.83
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.04
 ---- batch: 080 ----
mean loss: 854.70
 ---- batch: 090 ----
mean loss: 824.21
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 844.56
train mean loss: 844.09
epoch train time: 0:00:00.570240
elapsed time: 0:00:54.115913
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:20:34.354540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.50
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.46
 ---- batch: 040 ----
mean loss: 845.66
 ---- batch: 050 ----
mean loss: 833.50
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.35
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.95
train mean loss: 844.01
epoch train time: 0:00:00.567515
elapsed time: 0:00:54.683581
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:20:34.922222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.23
 ---- batch: 030 ----
mean loss: 833.32
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.66
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.549833
elapsed time: 0:00:55.233560
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:20:35.472185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.49
 ---- batch: 020 ----
mean loss: 825.58
 ---- batch: 030 ----
mean loss: 839.14
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.23
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.49
 ---- batch: 090 ----
mean loss: 853.99
 ---- batch: 100 ----
mean loss: 844.02
 ---- batch: 110 ----
mean loss: 822.71
train mean loss: 843.96
epoch train time: 0:00:00.553732
elapsed time: 0:00:55.787429
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:20:36.026056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.69
 ---- batch: 030 ----
mean loss: 846.90
 ---- batch: 040 ----
mean loss: 835.58
 ---- batch: 050 ----
mean loss: 868.28
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.95
 ---- batch: 090 ----
mean loss: 841.46
 ---- batch: 100 ----
mean loss: 841.99
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.567604
elapsed time: 0:00:56.355166
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:20:36.593789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.79
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.94
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 809.15
 ---- batch: 080 ----
mean loss: 856.45
 ---- batch: 090 ----
mean loss: 845.25
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.570715
elapsed time: 0:00:56.926014
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:20:37.164639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.81
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.13
 ---- batch: 050 ----
mean loss: 833.72
 ---- batch: 060 ----
mean loss: 865.09
 ---- batch: 070 ----
mean loss: 852.62
 ---- batch: 080 ----
mean loss: 833.10
 ---- batch: 090 ----
mean loss: 822.30
 ---- batch: 100 ----
mean loss: 830.74
 ---- batch: 110 ----
mean loss: 843.89
train mean loss: 844.04
epoch train time: 0:00:00.561926
elapsed time: 0:00:57.488072
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:20:37.726717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.00
 ---- batch: 020 ----
mean loss: 869.89
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.32
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.34
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.566071
elapsed time: 0:00:58.054298
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:20:38.292922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.24
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.66
 ---- batch: 060 ----
mean loss: 825.05
 ---- batch: 070 ----
mean loss: 862.07
 ---- batch: 080 ----
mean loss: 821.71
 ---- batch: 090 ----
mean loss: 868.26
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 844.02
epoch train time: 0:00:00.573694
elapsed time: 0:00:58.628148
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:20:38.866788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.91
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.44
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.58
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.87
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.02
epoch train time: 0:00:00.549032
elapsed time: 0:00:59.177329
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:20:39.415962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.87
 ---- batch: 020 ----
mean loss: 843.81
 ---- batch: 030 ----
mean loss: 851.02
 ---- batch: 040 ----
mean loss: 856.66
 ---- batch: 050 ----
mean loss: 832.84
 ---- batch: 060 ----
mean loss: 851.55
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.21
 ---- batch: 090 ----
mean loss: 833.15
 ---- batch: 100 ----
mean loss: 851.12
 ---- batch: 110 ----
mean loss: 824.81
train mean loss: 844.02
epoch train time: 0:00:00.557800
elapsed time: 0:00:59.735269
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:20:39.973894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.35
 ---- batch: 030 ----
mean loss: 854.58
 ---- batch: 040 ----
mean loss: 833.74
 ---- batch: 050 ----
mean loss: 834.99
 ---- batch: 060 ----
mean loss: 847.39
 ---- batch: 070 ----
mean loss: 828.86
 ---- batch: 080 ----
mean loss: 845.62
 ---- batch: 090 ----
mean loss: 847.91
 ---- batch: 100 ----
mean loss: 839.45
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 844.00
epoch train time: 0:00:00.549544
elapsed time: 0:01:00.284941
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:20:40.523580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 846.50
 ---- batch: 030 ----
mean loss: 862.86
 ---- batch: 040 ----
mean loss: 852.97
 ---- batch: 050 ----
mean loss: 837.18
 ---- batch: 060 ----
mean loss: 840.36
 ---- batch: 070 ----
mean loss: 860.77
 ---- batch: 080 ----
mean loss: 855.69
 ---- batch: 090 ----
mean loss: 833.00
 ---- batch: 100 ----
mean loss: 817.31
 ---- batch: 110 ----
mean loss: 839.15
train mean loss: 843.95
epoch train time: 0:00:00.561299
elapsed time: 0:01:00.846390
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:20:41.085015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 842.82
 ---- batch: 030 ----
mean loss: 842.87
 ---- batch: 040 ----
mean loss: 838.16
 ---- batch: 050 ----
mean loss: 844.73
 ---- batch: 060 ----
mean loss: 863.76
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 808.24
 ---- batch: 090 ----
mean loss: 833.55
 ---- batch: 100 ----
mean loss: 859.01
 ---- batch: 110 ----
mean loss: 849.06
train mean loss: 844.05
epoch train time: 0:00:00.552720
elapsed time: 0:01:01.399242
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:20:41.637887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.96
 ---- batch: 020 ----
mean loss: 835.16
 ---- batch: 030 ----
mean loss: 837.94
 ---- batch: 040 ----
mean loss: 824.13
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 857.77
 ---- batch: 070 ----
mean loss: 822.83
 ---- batch: 080 ----
mean loss: 857.42
 ---- batch: 090 ----
mean loss: 835.62
 ---- batch: 100 ----
mean loss: 851.20
 ---- batch: 110 ----
mean loss: 863.65
train mean loss: 844.00
epoch train time: 0:00:00.567026
elapsed time: 0:01:01.966425
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:20:42.205050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.57
 ---- batch: 020 ----
mean loss: 858.19
 ---- batch: 030 ----
mean loss: 852.89
 ---- batch: 040 ----
mean loss: 839.77
 ---- batch: 050 ----
mean loss: 830.18
 ---- batch: 060 ----
mean loss: 841.44
 ---- batch: 070 ----
mean loss: 855.69
 ---- batch: 080 ----
mean loss: 849.65
 ---- batch: 090 ----
mean loss: 832.65
 ---- batch: 100 ----
mean loss: 836.73
 ---- batch: 110 ----
mean loss: 842.08
train mean loss: 843.93
epoch train time: 0:00:00.566431
elapsed time: 0:01:02.532987
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:20:42.771613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.38
 ---- batch: 020 ----
mean loss: 853.11
 ---- batch: 030 ----
mean loss: 847.68
 ---- batch: 040 ----
mean loss: 843.46
 ---- batch: 050 ----
mean loss: 859.03
 ---- batch: 060 ----
mean loss: 816.26
 ---- batch: 070 ----
mean loss: 860.17
 ---- batch: 080 ----
mean loss: 813.18
 ---- batch: 090 ----
mean loss: 834.16
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 857.17
train mean loss: 843.89
epoch train time: 0:00:00.560706
elapsed time: 0:01:03.093845
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:20:43.332483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.13
 ---- batch: 020 ----
mean loss: 837.08
 ---- batch: 030 ----
mean loss: 843.34
 ---- batch: 040 ----
mean loss: 852.88
 ---- batch: 050 ----
mean loss: 846.10
 ---- batch: 060 ----
mean loss: 837.07
 ---- batch: 070 ----
mean loss: 843.34
 ---- batch: 080 ----
mean loss: 827.01
 ---- batch: 090 ----
mean loss: 848.80
 ---- batch: 100 ----
mean loss: 843.75
 ---- batch: 110 ----
mean loss: 850.03
train mean loss: 843.99
epoch train time: 0:00:00.547987
elapsed time: 0:01:03.641981
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:20:43.880601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.49
 ---- batch: 020 ----
mean loss: 850.57
 ---- batch: 030 ----
mean loss: 833.68
 ---- batch: 040 ----
mean loss: 829.32
 ---- batch: 050 ----
mean loss: 839.98
 ---- batch: 060 ----
mean loss: 842.15
 ---- batch: 070 ----
mean loss: 844.11
 ---- batch: 080 ----
mean loss: 847.44
 ---- batch: 090 ----
mean loss: 835.40
 ---- batch: 100 ----
mean loss: 836.15
 ---- batch: 110 ----
mean loss: 847.37
train mean loss: 843.86
epoch train time: 0:00:00.557837
elapsed time: 0:01:04.199979
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:20:44.438618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.06
 ---- batch: 020 ----
mean loss: 831.80
 ---- batch: 030 ----
mean loss: 793.07
 ---- batch: 040 ----
mean loss: 790.09
 ---- batch: 050 ----
mean loss: 767.71
 ---- batch: 060 ----
mean loss: 752.37
 ---- batch: 070 ----
mean loss: 739.61
 ---- batch: 080 ----
mean loss: 708.95
 ---- batch: 090 ----
mean loss: 734.35
 ---- batch: 100 ----
mean loss: 699.61
 ---- batch: 110 ----
mean loss: 636.42
train mean loss: 749.69
epoch train time: 0:00:00.558705
elapsed time: 0:01:04.758832
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:20:44.997483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 522.24
 ---- batch: 020 ----
mean loss: 451.89
 ---- batch: 030 ----
mean loss: 414.85
 ---- batch: 040 ----
mean loss: 399.03
 ---- batch: 050 ----
mean loss: 380.14
 ---- batch: 060 ----
mean loss: 353.85
 ---- batch: 070 ----
mean loss: 355.02
 ---- batch: 080 ----
mean loss: 360.52
 ---- batch: 090 ----
mean loss: 336.21
 ---- batch: 100 ----
mean loss: 335.90
 ---- batch: 110 ----
mean loss: 316.79
train mean loss: 382.31
epoch train time: 0:00:00.562432
elapsed time: 0:01:05.321422
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:20:45.560044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.95
 ---- batch: 020 ----
mean loss: 299.21
 ---- batch: 030 ----
mean loss: 295.99
 ---- batch: 040 ----
mean loss: 293.45
 ---- batch: 050 ----
mean loss: 277.25
 ---- batch: 060 ----
mean loss: 282.35
 ---- batch: 070 ----
mean loss: 270.88
 ---- batch: 080 ----
mean loss: 279.34
 ---- batch: 090 ----
mean loss: 288.84
 ---- batch: 100 ----
mean loss: 281.77
 ---- batch: 110 ----
mean loss: 276.88
train mean loss: 286.68
epoch train time: 0:00:00.556615
elapsed time: 0:01:05.878200
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:20:46.116835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.01
 ---- batch: 020 ----
mean loss: 257.66
 ---- batch: 030 ----
mean loss: 267.30
 ---- batch: 040 ----
mean loss: 262.42
 ---- batch: 050 ----
mean loss: 258.64
 ---- batch: 060 ----
mean loss: 261.40
 ---- batch: 070 ----
mean loss: 252.23
 ---- batch: 080 ----
mean loss: 258.17
 ---- batch: 090 ----
mean loss: 255.46
 ---- batch: 100 ----
mean loss: 250.41
 ---- batch: 110 ----
mean loss: 248.27
train mean loss: 257.39
epoch train time: 0:00:00.551423
elapsed time: 0:01:06.429765
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:20:46.668390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.32
 ---- batch: 020 ----
mean loss: 251.08
 ---- batch: 030 ----
mean loss: 245.46
 ---- batch: 040 ----
mean loss: 234.33
 ---- batch: 050 ----
mean loss: 245.05
 ---- batch: 060 ----
mean loss: 238.55
 ---- batch: 070 ----
mean loss: 245.17
 ---- batch: 080 ----
mean loss: 234.98
 ---- batch: 090 ----
mean loss: 244.22
 ---- batch: 100 ----
mean loss: 230.31
 ---- batch: 110 ----
mean loss: 245.79
train mean loss: 241.07
epoch train time: 0:00:00.555271
elapsed time: 0:01:06.985165
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:20:47.223801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.93
 ---- batch: 020 ----
mean loss: 222.58
 ---- batch: 030 ----
mean loss: 228.79
 ---- batch: 040 ----
mean loss: 237.62
 ---- batch: 050 ----
mean loss: 218.63
 ---- batch: 060 ----
mean loss: 228.63
 ---- batch: 070 ----
mean loss: 217.08
 ---- batch: 080 ----
mean loss: 238.57
 ---- batch: 090 ----
mean loss: 229.06
 ---- batch: 100 ----
mean loss: 230.51
 ---- batch: 110 ----
mean loss: 227.39
train mean loss: 228.94
epoch train time: 0:00:00.553720
elapsed time: 0:01:07.539050
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:20:47.777681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.87
 ---- batch: 020 ----
mean loss: 216.41
 ---- batch: 030 ----
mean loss: 220.78
 ---- batch: 040 ----
mean loss: 224.94
 ---- batch: 050 ----
mean loss: 213.78
 ---- batch: 060 ----
mean loss: 216.14
 ---- batch: 070 ----
mean loss: 223.95
 ---- batch: 080 ----
mean loss: 217.67
 ---- batch: 090 ----
mean loss: 215.07
 ---- batch: 100 ----
mean loss: 217.22
 ---- batch: 110 ----
mean loss: 231.05
train mean loss: 219.81
epoch train time: 0:00:00.564450
elapsed time: 0:01:08.103643
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:20:48.342284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.99
 ---- batch: 020 ----
mean loss: 214.61
 ---- batch: 030 ----
mean loss: 208.41
 ---- batch: 040 ----
mean loss: 197.26
 ---- batch: 050 ----
mean loss: 211.90
 ---- batch: 060 ----
mean loss: 216.75
 ---- batch: 070 ----
mean loss: 217.95
 ---- batch: 080 ----
mean loss: 217.12
 ---- batch: 090 ----
mean loss: 224.92
 ---- batch: 100 ----
mean loss: 212.16
 ---- batch: 110 ----
mean loss: 205.62
train mean loss: 212.86
epoch train time: 0:00:00.565203
elapsed time: 0:01:08.668997
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:20:48.907640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.99
 ---- batch: 020 ----
mean loss: 200.96
 ---- batch: 030 ----
mean loss: 213.32
 ---- batch: 040 ----
mean loss: 208.36
 ---- batch: 050 ----
mean loss: 217.11
 ---- batch: 060 ----
mean loss: 211.59
 ---- batch: 070 ----
mean loss: 216.02
 ---- batch: 080 ----
mean loss: 206.02
 ---- batch: 090 ----
mean loss: 200.03
 ---- batch: 100 ----
mean loss: 203.64
 ---- batch: 110 ----
mean loss: 211.23
train mean loss: 209.41
epoch train time: 0:00:00.545907
elapsed time: 0:01:09.215075
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:20:49.453702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.82
 ---- batch: 020 ----
mean loss: 199.22
 ---- batch: 030 ----
mean loss: 200.40
 ---- batch: 040 ----
mean loss: 198.44
 ---- batch: 050 ----
mean loss: 193.20
 ---- batch: 060 ----
mean loss: 208.35
 ---- batch: 070 ----
mean loss: 202.31
 ---- batch: 080 ----
mean loss: 206.71
 ---- batch: 090 ----
mean loss: 198.73
 ---- batch: 100 ----
mean loss: 203.86
 ---- batch: 110 ----
mean loss: 201.74
train mean loss: 201.35
epoch train time: 0:00:00.552911
elapsed time: 0:01:09.768122
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:20:50.006745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.65
 ---- batch: 020 ----
mean loss: 193.38
 ---- batch: 030 ----
mean loss: 185.91
 ---- batch: 040 ----
mean loss: 199.13
 ---- batch: 050 ----
mean loss: 199.81
 ---- batch: 060 ----
mean loss: 193.30
 ---- batch: 070 ----
mean loss: 199.86
 ---- batch: 080 ----
mean loss: 197.77
 ---- batch: 090 ----
mean loss: 207.00
 ---- batch: 100 ----
mean loss: 200.73
 ---- batch: 110 ----
mean loss: 205.38
train mean loss: 198.42
epoch train time: 0:00:00.547062
elapsed time: 0:01:10.315311
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:20:50.553939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.01
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 187.69
 ---- batch: 040 ----
mean loss: 192.51
 ---- batch: 050 ----
mean loss: 199.27
 ---- batch: 060 ----
mean loss: 202.04
 ---- batch: 070 ----
mean loss: 198.35
 ---- batch: 080 ----
mean loss: 192.95
 ---- batch: 090 ----
mean loss: 203.72
 ---- batch: 100 ----
mean loss: 184.18
 ---- batch: 110 ----
mean loss: 199.53
train mean loss: 195.06
epoch train time: 0:00:00.565935
elapsed time: 0:01:10.881389
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:20:51.120013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.11
 ---- batch: 020 ----
mean loss: 194.14
 ---- batch: 030 ----
mean loss: 193.20
 ---- batch: 040 ----
mean loss: 184.29
 ---- batch: 050 ----
mean loss: 195.21
 ---- batch: 060 ----
mean loss: 195.61
 ---- batch: 070 ----
mean loss: 188.40
 ---- batch: 080 ----
mean loss: 188.60
 ---- batch: 090 ----
mean loss: 189.30
 ---- batch: 100 ----
mean loss: 188.32
 ---- batch: 110 ----
mean loss: 197.92
train mean loss: 191.94
epoch train time: 0:00:00.557372
elapsed time: 0:01:11.438896
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:20:51.677519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.99
 ---- batch: 020 ----
mean loss: 191.38
 ---- batch: 030 ----
mean loss: 197.95
 ---- batch: 040 ----
mean loss: 190.17
 ---- batch: 050 ----
mean loss: 186.15
 ---- batch: 060 ----
mean loss: 184.55
 ---- batch: 070 ----
mean loss: 195.02
 ---- batch: 080 ----
mean loss: 190.73
 ---- batch: 090 ----
mean loss: 195.31
 ---- batch: 100 ----
mean loss: 186.59
 ---- batch: 110 ----
mean loss: 190.22
train mean loss: 189.86
epoch train time: 0:00:00.565587
elapsed time: 0:01:12.004647
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:20:52.243282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.79
 ---- batch: 020 ----
mean loss: 179.12
 ---- batch: 030 ----
mean loss: 191.23
 ---- batch: 040 ----
mean loss: 187.25
 ---- batch: 050 ----
mean loss: 175.03
 ---- batch: 060 ----
mean loss: 189.65
 ---- batch: 070 ----
mean loss: 190.77
 ---- batch: 080 ----
mean loss: 197.77
 ---- batch: 090 ----
mean loss: 183.58
 ---- batch: 100 ----
mean loss: 186.72
 ---- batch: 110 ----
mean loss: 189.27
train mean loss: 187.09
epoch train time: 0:00:00.552360
elapsed time: 0:01:12.557152
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:20:52.795782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.34
 ---- batch: 020 ----
mean loss: 183.00
 ---- batch: 030 ----
mean loss: 187.08
 ---- batch: 040 ----
mean loss: 186.82
 ---- batch: 050 ----
mean loss: 190.28
 ---- batch: 060 ----
mean loss: 182.13
 ---- batch: 070 ----
mean loss: 185.85
 ---- batch: 080 ----
mean loss: 182.24
 ---- batch: 090 ----
mean loss: 183.12
 ---- batch: 100 ----
mean loss: 191.25
 ---- batch: 110 ----
mean loss: 182.83
train mean loss: 185.32
epoch train time: 0:00:00.562946
elapsed time: 0:01:13.120241
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:20:53.358868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.59
 ---- batch: 020 ----
mean loss: 186.19
 ---- batch: 030 ----
mean loss: 182.91
 ---- batch: 040 ----
mean loss: 177.15
 ---- batch: 050 ----
mean loss: 178.21
 ---- batch: 060 ----
mean loss: 192.25
 ---- batch: 070 ----
mean loss: 179.75
 ---- batch: 080 ----
mean loss: 189.76
 ---- batch: 090 ----
mean loss: 188.87
 ---- batch: 100 ----
mean loss: 189.46
 ---- batch: 110 ----
mean loss: 180.86
train mean loss: 184.44
epoch train time: 0:00:00.554939
elapsed time: 0:01:13.675327
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:20:53.913951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.04
 ---- batch: 020 ----
mean loss: 180.43
 ---- batch: 030 ----
mean loss: 179.48
 ---- batch: 040 ----
mean loss: 177.35
 ---- batch: 050 ----
mean loss: 181.72
 ---- batch: 060 ----
mean loss: 184.81
 ---- batch: 070 ----
mean loss: 186.06
 ---- batch: 080 ----
mean loss: 186.90
 ---- batch: 090 ----
mean loss: 180.97
 ---- batch: 100 ----
mean loss: 182.59
 ---- batch: 110 ----
mean loss: 184.58
train mean loss: 182.97
epoch train time: 0:00:00.557217
elapsed time: 0:01:14.232679
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:20:54.471304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.74
 ---- batch: 020 ----
mean loss: 177.77
 ---- batch: 030 ----
mean loss: 190.00
 ---- batch: 040 ----
mean loss: 182.88
 ---- batch: 050 ----
mean loss: 185.67
 ---- batch: 060 ----
mean loss: 174.72
 ---- batch: 070 ----
mean loss: 180.52
 ---- batch: 080 ----
mean loss: 183.31
 ---- batch: 090 ----
mean loss: 184.08
 ---- batch: 100 ----
mean loss: 176.46
 ---- batch: 110 ----
mean loss: 187.47
train mean loss: 181.84
epoch train time: 0:00:00.551558
elapsed time: 0:01:14.784386
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:20:55.023009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.41
 ---- batch: 020 ----
mean loss: 173.41
 ---- batch: 030 ----
mean loss: 184.52
 ---- batch: 040 ----
mean loss: 184.22
 ---- batch: 050 ----
mean loss: 185.31
 ---- batch: 060 ----
mean loss: 186.44
 ---- batch: 070 ----
mean loss: 184.48
 ---- batch: 080 ----
mean loss: 178.56
 ---- batch: 090 ----
mean loss: 179.46
 ---- batch: 100 ----
mean loss: 170.09
 ---- batch: 110 ----
mean loss: 184.33
train mean loss: 181.26
epoch train time: 0:00:00.549478
elapsed time: 0:01:15.333992
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:20:55.572629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.87
 ---- batch: 020 ----
mean loss: 182.20
 ---- batch: 030 ----
mean loss: 170.95
 ---- batch: 040 ----
mean loss: 178.00
 ---- batch: 050 ----
mean loss: 185.10
 ---- batch: 060 ----
mean loss: 179.37
 ---- batch: 070 ----
mean loss: 179.28
 ---- batch: 080 ----
mean loss: 178.44
 ---- batch: 090 ----
mean loss: 182.22
 ---- batch: 100 ----
mean loss: 185.49
 ---- batch: 110 ----
mean loss: 174.04
train mean loss: 179.77
epoch train time: 0:00:00.564445
elapsed time: 0:01:15.898586
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:20:56.137232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.12
 ---- batch: 020 ----
mean loss: 174.58
 ---- batch: 030 ----
mean loss: 170.57
 ---- batch: 040 ----
mean loss: 177.39
 ---- batch: 050 ----
mean loss: 180.16
 ---- batch: 060 ----
mean loss: 189.30
 ---- batch: 070 ----
mean loss: 176.78
 ---- batch: 080 ----
mean loss: 182.50
 ---- batch: 090 ----
mean loss: 179.52
 ---- batch: 100 ----
mean loss: 178.33
 ---- batch: 110 ----
mean loss: 180.05
train mean loss: 177.83
epoch train time: 0:00:00.563692
elapsed time: 0:01:16.462431
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:20:56.701055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.88
 ---- batch: 020 ----
mean loss: 177.50
 ---- batch: 030 ----
mean loss: 171.00
 ---- batch: 040 ----
mean loss: 182.53
 ---- batch: 050 ----
mean loss: 179.55
 ---- batch: 060 ----
mean loss: 179.76
 ---- batch: 070 ----
mean loss: 179.56
 ---- batch: 080 ----
mean loss: 171.66
 ---- batch: 090 ----
mean loss: 182.37
 ---- batch: 100 ----
mean loss: 170.89
 ---- batch: 110 ----
mean loss: 194.64
train mean loss: 177.89
epoch train time: 0:00:00.557298
elapsed time: 0:01:17.019862
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:20:57.258487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.70
 ---- batch: 020 ----
mean loss: 180.87
 ---- batch: 030 ----
mean loss: 172.89
 ---- batch: 040 ----
mean loss: 174.98
 ---- batch: 050 ----
mean loss: 175.07
 ---- batch: 060 ----
mean loss: 180.03
 ---- batch: 070 ----
mean loss: 181.96
 ---- batch: 080 ----
mean loss: 173.01
 ---- batch: 090 ----
mean loss: 179.18
 ---- batch: 100 ----
mean loss: 180.65
 ---- batch: 110 ----
mean loss: 174.97
train mean loss: 176.93
epoch train time: 0:00:00.555030
elapsed time: 0:01:17.575046
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:20:57.813679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.15
 ---- batch: 020 ----
mean loss: 173.60
 ---- batch: 030 ----
mean loss: 174.05
 ---- batch: 040 ----
mean loss: 176.71
 ---- batch: 050 ----
mean loss: 178.27
 ---- batch: 060 ----
mean loss: 169.10
 ---- batch: 070 ----
mean loss: 170.63
 ---- batch: 080 ----
mean loss: 190.42
 ---- batch: 090 ----
mean loss: 178.13
 ---- batch: 100 ----
mean loss: 165.08
 ---- batch: 110 ----
mean loss: 183.41
train mean loss: 175.52
epoch train time: 0:00:00.557902
elapsed time: 0:01:18.133092
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:20:58.371718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.07
 ---- batch: 020 ----
mean loss: 180.51
 ---- batch: 030 ----
mean loss: 178.66
 ---- batch: 040 ----
mean loss: 178.99
 ---- batch: 050 ----
mean loss: 167.95
 ---- batch: 060 ----
mean loss: 178.69
 ---- batch: 070 ----
mean loss: 183.77
 ---- batch: 080 ----
mean loss: 175.18
 ---- batch: 090 ----
mean loss: 176.45
 ---- batch: 100 ----
mean loss: 172.23
 ---- batch: 110 ----
mean loss: 174.74
train mean loss: 176.02
epoch train time: 0:00:00.561047
elapsed time: 0:01:18.694271
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:20:58.932894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.39
 ---- batch: 020 ----
mean loss: 174.61
 ---- batch: 030 ----
mean loss: 175.60
 ---- batch: 040 ----
mean loss: 172.01
 ---- batch: 050 ----
mean loss: 176.90
 ---- batch: 060 ----
mean loss: 172.07
 ---- batch: 070 ----
mean loss: 170.25
 ---- batch: 080 ----
mean loss: 172.01
 ---- batch: 090 ----
mean loss: 173.36
 ---- batch: 100 ----
mean loss: 175.12
 ---- batch: 110 ----
mean loss: 177.03
train mean loss: 173.98
epoch train time: 0:00:00.553333
elapsed time: 0:01:19.247733
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:20:59.486411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.53
 ---- batch: 020 ----
mean loss: 176.26
 ---- batch: 030 ----
mean loss: 173.06
 ---- batch: 040 ----
mean loss: 168.15
 ---- batch: 050 ----
mean loss: 167.99
 ---- batch: 060 ----
mean loss: 176.32
 ---- batch: 070 ----
mean loss: 183.92
 ---- batch: 080 ----
mean loss: 180.22
 ---- batch: 090 ----
mean loss: 171.72
 ---- batch: 100 ----
mean loss: 179.17
 ---- batch: 110 ----
mean loss: 173.34
train mean loss: 174.51
epoch train time: 0:00:00.557799
elapsed time: 0:01:19.805721
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:21:00.044346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.59
 ---- batch: 020 ----
mean loss: 181.12
 ---- batch: 030 ----
mean loss: 161.91
 ---- batch: 040 ----
mean loss: 174.19
 ---- batch: 050 ----
mean loss: 173.85
 ---- batch: 060 ----
mean loss: 172.35
 ---- batch: 070 ----
mean loss: 173.90
 ---- batch: 080 ----
mean loss: 179.64
 ---- batch: 090 ----
mean loss: 176.12
 ---- batch: 100 ----
mean loss: 178.23
 ---- batch: 110 ----
mean loss: 177.17
train mean loss: 173.93
epoch train time: 0:00:00.550731
elapsed time: 0:01:20.356601
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:21:00.595228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.45
 ---- batch: 020 ----
mean loss: 182.50
 ---- batch: 030 ----
mean loss: 164.86
 ---- batch: 040 ----
mean loss: 171.04
 ---- batch: 050 ----
mean loss: 176.28
 ---- batch: 060 ----
mean loss: 169.75
 ---- batch: 070 ----
mean loss: 169.31
 ---- batch: 080 ----
mean loss: 167.72
 ---- batch: 090 ----
mean loss: 175.74
 ---- batch: 100 ----
mean loss: 177.17
 ---- batch: 110 ----
mean loss: 178.07
train mean loss: 172.34
epoch train time: 0:00:00.554491
elapsed time: 0:01:20.911228
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:21:01.149864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.44
 ---- batch: 020 ----
mean loss: 173.36
 ---- batch: 030 ----
mean loss: 168.72
 ---- batch: 040 ----
mean loss: 162.23
 ---- batch: 050 ----
mean loss: 169.91
 ---- batch: 060 ----
mean loss: 172.98
 ---- batch: 070 ----
mean loss: 171.94
 ---- batch: 080 ----
mean loss: 180.65
 ---- batch: 090 ----
mean loss: 171.78
 ---- batch: 100 ----
mean loss: 165.37
 ---- batch: 110 ----
mean loss: 177.24
train mean loss: 171.45
epoch train time: 0:00:00.556389
elapsed time: 0:01:21.467766
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:21:01.706395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.09
 ---- batch: 020 ----
mean loss: 161.86
 ---- batch: 030 ----
mean loss: 174.84
 ---- batch: 040 ----
mean loss: 168.89
 ---- batch: 050 ----
mean loss: 172.32
 ---- batch: 060 ----
mean loss: 167.37
 ---- batch: 070 ----
mean loss: 177.30
 ---- batch: 080 ----
mean loss: 173.19
 ---- batch: 090 ----
mean loss: 172.76
 ---- batch: 100 ----
mean loss: 175.06
 ---- batch: 110 ----
mean loss: 170.47
train mean loss: 170.23
epoch train time: 0:00:00.553735
elapsed time: 0:01:22.021692
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:21:02.260316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.83
 ---- batch: 020 ----
mean loss: 167.34
 ---- batch: 030 ----
mean loss: 168.31
 ---- batch: 040 ----
mean loss: 176.65
 ---- batch: 050 ----
mean loss: 161.86
 ---- batch: 060 ----
mean loss: 170.05
 ---- batch: 070 ----
mean loss: 174.74
 ---- batch: 080 ----
mean loss: 174.60
 ---- batch: 090 ----
mean loss: 167.23
 ---- batch: 100 ----
mean loss: 171.24
 ---- batch: 110 ----
mean loss: 171.68
train mean loss: 170.16
epoch train time: 0:00:00.558141
elapsed time: 0:01:22.579966
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:21:02.818593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.59
 ---- batch: 020 ----
mean loss: 163.65
 ---- batch: 030 ----
mean loss: 158.70
 ---- batch: 040 ----
mean loss: 165.86
 ---- batch: 050 ----
mean loss: 164.51
 ---- batch: 060 ----
mean loss: 176.95
 ---- batch: 070 ----
mean loss: 178.84
 ---- batch: 080 ----
mean loss: 174.29
 ---- batch: 090 ----
mean loss: 166.88
 ---- batch: 100 ----
mean loss: 171.80
 ---- batch: 110 ----
mean loss: 172.83
train mean loss: 169.90
epoch train time: 0:00:00.550639
elapsed time: 0:01:23.130753
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:21:03.369424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.32
 ---- batch: 020 ----
mean loss: 170.59
 ---- batch: 030 ----
mean loss: 169.54
 ---- batch: 040 ----
mean loss: 159.62
 ---- batch: 050 ----
mean loss: 166.84
 ---- batch: 060 ----
mean loss: 174.41
 ---- batch: 070 ----
mean loss: 168.05
 ---- batch: 080 ----
mean loss: 170.45
 ---- batch: 090 ----
mean loss: 170.68
 ---- batch: 100 ----
mean loss: 171.15
 ---- batch: 110 ----
mean loss: 173.10
train mean loss: 169.09
epoch train time: 0:00:00.553455
elapsed time: 0:01:23.684399
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:21:03.923044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.64
 ---- batch: 020 ----
mean loss: 172.39
 ---- batch: 030 ----
mean loss: 159.53
 ---- batch: 040 ----
mean loss: 172.81
 ---- batch: 050 ----
mean loss: 158.32
 ---- batch: 060 ----
mean loss: 168.54
 ---- batch: 070 ----
mean loss: 175.59
 ---- batch: 080 ----
mean loss: 179.89
 ---- batch: 090 ----
mean loss: 166.77
 ---- batch: 100 ----
mean loss: 167.86
 ---- batch: 110 ----
mean loss: 172.64
train mean loss: 169.12
epoch train time: 0:00:00.559915
elapsed time: 0:01:24.244471
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:21:04.483097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.24
 ---- batch: 020 ----
mean loss: 172.51
 ---- batch: 030 ----
mean loss: 166.74
 ---- batch: 040 ----
mean loss: 170.21
 ---- batch: 050 ----
mean loss: 164.71
 ---- batch: 060 ----
mean loss: 156.72
 ---- batch: 070 ----
mean loss: 171.50
 ---- batch: 080 ----
mean loss: 157.72
 ---- batch: 090 ----
mean loss: 171.30
 ---- batch: 100 ----
mean loss: 170.19
 ---- batch: 110 ----
mean loss: 173.19
train mean loss: 167.56
epoch train time: 0:00:00.556149
elapsed time: 0:01:24.800751
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:21:05.039388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.90
 ---- batch: 020 ----
mean loss: 170.62
 ---- batch: 030 ----
mean loss: 170.38
 ---- batch: 040 ----
mean loss: 163.90
 ---- batch: 050 ----
mean loss: 162.74
 ---- batch: 060 ----
mean loss: 170.68
 ---- batch: 070 ----
mean loss: 173.41
 ---- batch: 080 ----
mean loss: 174.47
 ---- batch: 090 ----
mean loss: 164.32
 ---- batch: 100 ----
mean loss: 169.12
 ---- batch: 110 ----
mean loss: 166.06
train mean loss: 167.73
epoch train time: 0:00:00.544770
elapsed time: 0:01:25.345664
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:21:05.584288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.36
 ---- batch: 020 ----
mean loss: 168.56
 ---- batch: 030 ----
mean loss: 169.93
 ---- batch: 040 ----
mean loss: 164.79
 ---- batch: 050 ----
mean loss: 167.89
 ---- batch: 060 ----
mean loss: 165.96
 ---- batch: 070 ----
mean loss: 166.55
 ---- batch: 080 ----
mean loss: 167.64
 ---- batch: 090 ----
mean loss: 163.54
 ---- batch: 100 ----
mean loss: 165.53
 ---- batch: 110 ----
mean loss: 168.48
train mean loss: 166.97
epoch train time: 0:00:00.553633
elapsed time: 0:01:25.899429
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:21:06.138074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.83
 ---- batch: 020 ----
mean loss: 165.73
 ---- batch: 030 ----
mean loss: 164.00
 ---- batch: 040 ----
mean loss: 172.58
 ---- batch: 050 ----
mean loss: 172.49
 ---- batch: 060 ----
mean loss: 161.67
 ---- batch: 070 ----
mean loss: 161.52
 ---- batch: 080 ----
mean loss: 162.97
 ---- batch: 090 ----
mean loss: 170.02
 ---- batch: 100 ----
mean loss: 164.55
 ---- batch: 110 ----
mean loss: 168.80
train mean loss: 167.18
epoch train time: 0:00:00.552334
elapsed time: 0:01:26.451926
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:21:06.690559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.83
 ---- batch: 020 ----
mean loss: 155.96
 ---- batch: 030 ----
mean loss: 151.37
 ---- batch: 040 ----
mean loss: 166.87
 ---- batch: 050 ----
mean loss: 175.21
 ---- batch: 060 ----
mean loss: 172.10
 ---- batch: 070 ----
mean loss: 168.75
 ---- batch: 080 ----
mean loss: 165.52
 ---- batch: 090 ----
mean loss: 160.40
 ---- batch: 100 ----
mean loss: 171.92
 ---- batch: 110 ----
mean loss: 170.23
train mean loss: 165.66
epoch train time: 0:00:00.563740
elapsed time: 0:01:27.015812
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:21:07.254455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.67
 ---- batch: 020 ----
mean loss: 163.79
 ---- batch: 030 ----
mean loss: 162.17
 ---- batch: 040 ----
mean loss: 166.60
 ---- batch: 050 ----
mean loss: 174.64
 ---- batch: 060 ----
mean loss: 160.80
 ---- batch: 070 ----
mean loss: 157.90
 ---- batch: 080 ----
mean loss: 171.48
 ---- batch: 090 ----
mean loss: 173.81
 ---- batch: 100 ----
mean loss: 168.28
 ---- batch: 110 ----
mean loss: 163.37
train mean loss: 165.28
epoch train time: 0:00:00.556988
elapsed time: 0:01:27.572950
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:21:07.811578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.87
 ---- batch: 020 ----
mean loss: 164.18
 ---- batch: 030 ----
mean loss: 164.25
 ---- batch: 040 ----
mean loss: 164.31
 ---- batch: 050 ----
mean loss: 166.17
 ---- batch: 060 ----
mean loss: 169.55
 ---- batch: 070 ----
mean loss: 167.91
 ---- batch: 080 ----
mean loss: 172.68
 ---- batch: 090 ----
mean loss: 163.73
 ---- batch: 100 ----
mean loss: 165.13
 ---- batch: 110 ----
mean loss: 161.06
train mean loss: 164.77
epoch train time: 0:00:00.551140
elapsed time: 0:01:28.124233
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:21:08.362855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.38
 ---- batch: 020 ----
mean loss: 170.68
 ---- batch: 030 ----
mean loss: 151.99
 ---- batch: 040 ----
mean loss: 167.86
 ---- batch: 050 ----
mean loss: 158.87
 ---- batch: 060 ----
mean loss: 166.00
 ---- batch: 070 ----
mean loss: 156.13
 ---- batch: 080 ----
mean loss: 155.88
 ---- batch: 090 ----
mean loss: 166.80
 ---- batch: 100 ----
mean loss: 170.58
 ---- batch: 110 ----
mean loss: 174.07
train mean loss: 164.20
epoch train time: 0:00:00.556353
elapsed time: 0:01:28.680717
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:21:08.919341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.13
 ---- batch: 020 ----
mean loss: 160.45
 ---- batch: 030 ----
mean loss: 164.26
 ---- batch: 040 ----
mean loss: 159.19
 ---- batch: 050 ----
mean loss: 163.87
 ---- batch: 060 ----
mean loss: 170.18
 ---- batch: 070 ----
mean loss: 163.30
 ---- batch: 080 ----
mean loss: 164.61
 ---- batch: 090 ----
mean loss: 161.33
 ---- batch: 100 ----
mean loss: 168.64
 ---- batch: 110 ----
mean loss: 166.56
train mean loss: 163.44
epoch train time: 0:00:00.549405
elapsed time: 0:01:29.230250
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:21:09.468872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.21
 ---- batch: 020 ----
mean loss: 159.57
 ---- batch: 030 ----
mean loss: 163.59
 ---- batch: 040 ----
mean loss: 155.28
 ---- batch: 050 ----
mean loss: 169.14
 ---- batch: 060 ----
mean loss: 158.58
 ---- batch: 070 ----
mean loss: 161.21
 ---- batch: 080 ----
mean loss: 168.98
 ---- batch: 090 ----
mean loss: 163.48
 ---- batch: 100 ----
mean loss: 155.02
 ---- batch: 110 ----
mean loss: 169.40
train mean loss: 163.12
epoch train time: 0:00:00.560550
elapsed time: 0:01:29.790930
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:21:10.029555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.74
 ---- batch: 020 ----
mean loss: 161.16
 ---- batch: 030 ----
mean loss: 161.21
 ---- batch: 040 ----
mean loss: 165.20
 ---- batch: 050 ----
mean loss: 162.40
 ---- batch: 060 ----
mean loss: 165.35
 ---- batch: 070 ----
mean loss: 160.25
 ---- batch: 080 ----
mean loss: 171.49
 ---- batch: 090 ----
mean loss: 160.99
 ---- batch: 100 ----
mean loss: 165.99
 ---- batch: 110 ----
mean loss: 158.73
train mean loss: 163.63
epoch train time: 0:00:00.557577
elapsed time: 0:01:30.348642
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:21:10.587287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.90
 ---- batch: 020 ----
mean loss: 155.32
 ---- batch: 030 ----
mean loss: 161.20
 ---- batch: 040 ----
mean loss: 163.61
 ---- batch: 050 ----
mean loss: 153.42
 ---- batch: 060 ----
mean loss: 167.94
 ---- batch: 070 ----
mean loss: 159.80
 ---- batch: 080 ----
mean loss: 161.19
 ---- batch: 090 ----
mean loss: 158.58
 ---- batch: 100 ----
mean loss: 166.54
 ---- batch: 110 ----
mean loss: 168.42
train mean loss: 162.15
epoch train time: 0:00:00.558175
elapsed time: 0:01:30.906969
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:21:11.145594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.09
 ---- batch: 020 ----
mean loss: 156.07
 ---- batch: 030 ----
mean loss: 163.51
 ---- batch: 040 ----
mean loss: 160.37
 ---- batch: 050 ----
mean loss: 160.39
 ---- batch: 060 ----
mean loss: 162.82
 ---- batch: 070 ----
mean loss: 159.70
 ---- batch: 080 ----
mean loss: 159.53
 ---- batch: 090 ----
mean loss: 165.32
 ---- batch: 100 ----
mean loss: 166.72
 ---- batch: 110 ----
mean loss: 172.16
train mean loss: 161.36
epoch train time: 0:00:00.565453
elapsed time: 0:01:31.472572
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:21:11.711199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.76
 ---- batch: 020 ----
mean loss: 157.62
 ---- batch: 030 ----
mean loss: 160.44
 ---- batch: 040 ----
mean loss: 155.68
 ---- batch: 050 ----
mean loss: 156.58
 ---- batch: 060 ----
mean loss: 169.78
 ---- batch: 070 ----
mean loss: 162.10
 ---- batch: 080 ----
mean loss: 159.03
 ---- batch: 090 ----
mean loss: 165.96
 ---- batch: 100 ----
mean loss: 166.19
 ---- batch: 110 ----
mean loss: 159.63
train mean loss: 161.82
epoch train time: 0:00:00.558608
elapsed time: 0:01:32.031343
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:21:12.269990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.31
 ---- batch: 020 ----
mean loss: 168.97
 ---- batch: 030 ----
mean loss: 158.68
 ---- batch: 040 ----
mean loss: 159.38
 ---- batch: 050 ----
mean loss: 159.70
 ---- batch: 060 ----
mean loss: 159.53
 ---- batch: 070 ----
mean loss: 159.01
 ---- batch: 080 ----
mean loss: 164.18
 ---- batch: 090 ----
mean loss: 166.48
 ---- batch: 100 ----
mean loss: 156.94
 ---- batch: 110 ----
mean loss: 164.62
train mean loss: 161.02
epoch train time: 0:00:00.564569
elapsed time: 0:01:32.596069
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:21:12.834714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.07
 ---- batch: 020 ----
mean loss: 164.92
 ---- batch: 030 ----
mean loss: 163.12
 ---- batch: 040 ----
mean loss: 156.41
 ---- batch: 050 ----
mean loss: 165.24
 ---- batch: 060 ----
mean loss: 158.09
 ---- batch: 070 ----
mean loss: 168.47
 ---- batch: 080 ----
mean loss: 166.43
 ---- batch: 090 ----
mean loss: 163.23
 ---- batch: 100 ----
mean loss: 151.61
 ---- batch: 110 ----
mean loss: 162.29
train mean loss: 161.33
epoch train time: 0:00:00.574922
elapsed time: 0:01:33.171153
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:21:13.409779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.85
 ---- batch: 020 ----
mean loss: 162.94
 ---- batch: 030 ----
mean loss: 150.36
 ---- batch: 040 ----
mean loss: 154.51
 ---- batch: 050 ----
mean loss: 162.94
 ---- batch: 060 ----
mean loss: 156.91
 ---- batch: 070 ----
mean loss: 167.38
 ---- batch: 080 ----
mean loss: 162.94
 ---- batch: 090 ----
mean loss: 164.19
 ---- batch: 100 ----
mean loss: 168.96
 ---- batch: 110 ----
mean loss: 160.52
train mean loss: 160.81
epoch train time: 0:00:00.564418
elapsed time: 0:01:33.735710
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:21:13.974336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.15
 ---- batch: 020 ----
mean loss: 151.93
 ---- batch: 030 ----
mean loss: 162.22
 ---- batch: 040 ----
mean loss: 157.20
 ---- batch: 050 ----
mean loss: 161.92
 ---- batch: 060 ----
mean loss: 156.86
 ---- batch: 070 ----
mean loss: 168.04
 ---- batch: 080 ----
mean loss: 164.33
 ---- batch: 090 ----
mean loss: 155.57
 ---- batch: 100 ----
mean loss: 155.37
 ---- batch: 110 ----
mean loss: 159.73
train mean loss: 159.25
epoch train time: 0:00:00.556218
elapsed time: 0:01:34.292062
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:21:14.530709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.46
 ---- batch: 020 ----
mean loss: 156.33
 ---- batch: 030 ----
mean loss: 153.60
 ---- batch: 040 ----
mean loss: 150.92
 ---- batch: 050 ----
mean loss: 167.03
 ---- batch: 060 ----
mean loss: 157.34
 ---- batch: 070 ----
mean loss: 158.23
 ---- batch: 080 ----
mean loss: 164.33
 ---- batch: 090 ----
mean loss: 156.17
 ---- batch: 100 ----
mean loss: 156.82
 ---- batch: 110 ----
mean loss: 161.58
train mean loss: 158.54
epoch train time: 0:00:00.564874
elapsed time: 0:01:34.857091
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:21:15.095716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.07
 ---- batch: 020 ----
mean loss: 155.44
 ---- batch: 030 ----
mean loss: 159.07
 ---- batch: 040 ----
mean loss: 162.94
 ---- batch: 050 ----
mean loss: 158.83
 ---- batch: 060 ----
mean loss: 160.87
 ---- batch: 070 ----
mean loss: 163.69
 ---- batch: 080 ----
mean loss: 167.74
 ---- batch: 090 ----
mean loss: 166.40
 ---- batch: 100 ----
mean loss: 149.07
 ---- batch: 110 ----
mean loss: 159.41
train mean loss: 160.30
epoch train time: 0:00:00.557008
elapsed time: 0:01:35.414233
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:21:15.652863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.83
 ---- batch: 020 ----
mean loss: 152.40
 ---- batch: 030 ----
mean loss: 159.08
 ---- batch: 040 ----
mean loss: 148.76
 ---- batch: 050 ----
mean loss: 157.58
 ---- batch: 060 ----
mean loss: 157.52
 ---- batch: 070 ----
mean loss: 153.02
 ---- batch: 080 ----
mean loss: 165.96
 ---- batch: 090 ----
mean loss: 160.03
 ---- batch: 100 ----
mean loss: 161.40
 ---- batch: 110 ----
mean loss: 165.99
train mean loss: 158.67
epoch train time: 0:00:00.564556
elapsed time: 0:01:35.978926
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:21:16.217552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.57
 ---- batch: 020 ----
mean loss: 165.34
 ---- batch: 030 ----
mean loss: 153.15
 ---- batch: 040 ----
mean loss: 157.76
 ---- batch: 050 ----
mean loss: 159.42
 ---- batch: 060 ----
mean loss: 167.81
 ---- batch: 070 ----
mean loss: 154.43
 ---- batch: 080 ----
mean loss: 153.25
 ---- batch: 090 ----
mean loss: 158.85
 ---- batch: 100 ----
mean loss: 160.35
 ---- batch: 110 ----
mean loss: 160.05
train mean loss: 158.61
epoch train time: 0:00:00.557329
elapsed time: 0:01:36.536395
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:21:16.775022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.34
 ---- batch: 020 ----
mean loss: 163.93
 ---- batch: 030 ----
mean loss: 152.76
 ---- batch: 040 ----
mean loss: 150.94
 ---- batch: 050 ----
mean loss: 159.30
 ---- batch: 060 ----
mean loss: 158.44
 ---- batch: 070 ----
mean loss: 161.72
 ---- batch: 080 ----
mean loss: 161.71
 ---- batch: 090 ----
mean loss: 159.31
 ---- batch: 100 ----
mean loss: 148.62
 ---- batch: 110 ----
mean loss: 146.97
train mean loss: 156.89
epoch train time: 0:00:00.561195
elapsed time: 0:01:37.097722
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:21:17.336346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.15
 ---- batch: 020 ----
mean loss: 157.85
 ---- batch: 030 ----
mean loss: 156.38
 ---- batch: 040 ----
mean loss: 159.85
 ---- batch: 050 ----
mean loss: 157.87
 ---- batch: 060 ----
mean loss: 161.34
 ---- batch: 070 ----
mean loss: 163.55
 ---- batch: 080 ----
mean loss: 162.44
 ---- batch: 090 ----
mean loss: 155.49
 ---- batch: 100 ----
mean loss: 159.46
 ---- batch: 110 ----
mean loss: 154.48
train mean loss: 158.16
epoch train time: 0:00:00.562455
elapsed time: 0:01:37.660350
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:21:17.899001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.68
 ---- batch: 020 ----
mean loss: 156.71
 ---- batch: 030 ----
mean loss: 157.63
 ---- batch: 040 ----
mean loss: 151.49
 ---- batch: 050 ----
mean loss: 152.89
 ---- batch: 060 ----
mean loss: 157.75
 ---- batch: 070 ----
mean loss: 160.24
 ---- batch: 080 ----
mean loss: 161.53
 ---- batch: 090 ----
mean loss: 158.91
 ---- batch: 100 ----
mean loss: 160.32
 ---- batch: 110 ----
mean loss: 151.77
train mean loss: 157.21
epoch train time: 0:00:00.555955
elapsed time: 0:01:38.216467
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:21:18.455092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.19
 ---- batch: 020 ----
mean loss: 152.23
 ---- batch: 030 ----
mean loss: 155.08
 ---- batch: 040 ----
mean loss: 155.99
 ---- batch: 050 ----
mean loss: 156.63
 ---- batch: 060 ----
mean loss: 156.25
 ---- batch: 070 ----
mean loss: 156.11
 ---- batch: 080 ----
mean loss: 166.50
 ---- batch: 090 ----
mean loss: 159.18
 ---- batch: 100 ----
mean loss: 156.16
 ---- batch: 110 ----
mean loss: 160.41
train mean loss: 156.83
epoch train time: 0:00:00.570467
elapsed time: 0:01:38.787089
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:21:19.025719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.94
 ---- batch: 020 ----
mean loss: 149.71
 ---- batch: 030 ----
mean loss: 151.69
 ---- batch: 040 ----
mean loss: 155.82
 ---- batch: 050 ----
mean loss: 159.23
 ---- batch: 060 ----
mean loss: 160.86
 ---- batch: 070 ----
mean loss: 159.35
 ---- batch: 080 ----
mean loss: 156.71
 ---- batch: 090 ----
mean loss: 162.48
 ---- batch: 100 ----
mean loss: 158.46
 ---- batch: 110 ----
mean loss: 161.45
train mean loss: 157.27
epoch train time: 0:00:00.563567
elapsed time: 0:01:39.350793
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:21:19.589417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.66
 ---- batch: 020 ----
mean loss: 146.53
 ---- batch: 030 ----
mean loss: 162.19
 ---- batch: 040 ----
mean loss: 156.04
 ---- batch: 050 ----
mean loss: 154.41
 ---- batch: 060 ----
mean loss: 149.51
 ---- batch: 070 ----
mean loss: 161.96
 ---- batch: 080 ----
mean loss: 153.60
 ---- batch: 090 ----
mean loss: 163.89
 ---- batch: 100 ----
mean loss: 156.54
 ---- batch: 110 ----
mean loss: 154.40
train mean loss: 156.28
epoch train time: 0:00:00.563638
elapsed time: 0:01:39.914612
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:21:20.153253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.15
 ---- batch: 020 ----
mean loss: 148.81
 ---- batch: 030 ----
mean loss: 153.28
 ---- batch: 040 ----
mean loss: 152.53
 ---- batch: 050 ----
mean loss: 145.01
 ---- batch: 060 ----
mean loss: 150.23
 ---- batch: 070 ----
mean loss: 159.54
 ---- batch: 080 ----
mean loss: 160.18
 ---- batch: 090 ----
mean loss: 158.38
 ---- batch: 100 ----
mean loss: 162.60
 ---- batch: 110 ----
mean loss: 160.22
train mean loss: 155.26
epoch train time: 0:00:00.567054
elapsed time: 0:01:40.481815
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:21:20.720445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.17
 ---- batch: 020 ----
mean loss: 161.92
 ---- batch: 030 ----
mean loss: 151.02
 ---- batch: 040 ----
mean loss: 153.94
 ---- batch: 050 ----
mean loss: 161.71
 ---- batch: 060 ----
mean loss: 160.17
 ---- batch: 070 ----
mean loss: 163.44
 ---- batch: 080 ----
mean loss: 159.25
 ---- batch: 090 ----
mean loss: 153.48
 ---- batch: 100 ----
mean loss: 160.41
 ---- batch: 110 ----
mean loss: 150.04
train mean loss: 157.10
epoch train time: 0:00:00.559209
elapsed time: 0:01:41.041158
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:21:21.279782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.78
 ---- batch: 020 ----
mean loss: 149.86
 ---- batch: 030 ----
mean loss: 152.54
 ---- batch: 040 ----
mean loss: 159.24
 ---- batch: 050 ----
mean loss: 153.23
 ---- batch: 060 ----
mean loss: 154.39
 ---- batch: 070 ----
mean loss: 155.80
 ---- batch: 080 ----
mean loss: 158.22
 ---- batch: 090 ----
mean loss: 161.23
 ---- batch: 100 ----
mean loss: 158.07
 ---- batch: 110 ----
mean loss: 149.74
train mean loss: 154.82
epoch train time: 0:00:00.551561
elapsed time: 0:01:41.592863
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:21:21.831536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.23
 ---- batch: 020 ----
mean loss: 143.95
 ---- batch: 030 ----
mean loss: 151.98
 ---- batch: 040 ----
mean loss: 153.63
 ---- batch: 050 ----
mean loss: 160.19
 ---- batch: 060 ----
mean loss: 158.76
 ---- batch: 070 ----
mean loss: 163.34
 ---- batch: 080 ----
mean loss: 149.43
 ---- batch: 090 ----
mean loss: 155.82
 ---- batch: 100 ----
mean loss: 152.81
 ---- batch: 110 ----
mean loss: 154.24
train mean loss: 154.26
epoch train time: 0:00:00.565502
elapsed time: 0:01:42.158548
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:21:22.397174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.64
 ---- batch: 020 ----
mean loss: 146.68
 ---- batch: 030 ----
mean loss: 159.35
 ---- batch: 040 ----
mean loss: 159.09
 ---- batch: 050 ----
mean loss: 148.08
 ---- batch: 060 ----
mean loss: 162.58
 ---- batch: 070 ----
mean loss: 158.30
 ---- batch: 080 ----
mean loss: 157.65
 ---- batch: 090 ----
mean loss: 154.58
 ---- batch: 100 ----
mean loss: 150.19
 ---- batch: 110 ----
mean loss: 154.75
train mean loss: 154.94
epoch train time: 0:00:00.561593
elapsed time: 0:01:42.720276
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:21:22.958905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.24
 ---- batch: 020 ----
mean loss: 147.54
 ---- batch: 030 ----
mean loss: 154.25
 ---- batch: 040 ----
mean loss: 155.33
 ---- batch: 050 ----
mean loss: 154.48
 ---- batch: 060 ----
mean loss: 151.73
 ---- batch: 070 ----
mean loss: 156.33
 ---- batch: 080 ----
mean loss: 158.46
 ---- batch: 090 ----
mean loss: 152.89
 ---- batch: 100 ----
mean loss: 154.29
 ---- batch: 110 ----
mean loss: 152.37
train mean loss: 153.56
epoch train time: 0:00:00.547310
elapsed time: 0:01:43.267735
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:21:23.506351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.49
 ---- batch: 020 ----
mean loss: 153.48
 ---- batch: 030 ----
mean loss: 151.62
 ---- batch: 040 ----
mean loss: 136.79
 ---- batch: 050 ----
mean loss: 165.72
 ---- batch: 060 ----
mean loss: 158.10
 ---- batch: 070 ----
mean loss: 153.36
 ---- batch: 080 ----
mean loss: 154.54
 ---- batch: 090 ----
mean loss: 160.04
 ---- batch: 100 ----
mean loss: 152.47
 ---- batch: 110 ----
mean loss: 156.33
train mean loss: 154.32
epoch train time: 0:00:00.553100
elapsed time: 0:01:43.820959
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:21:24.059583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.78
 ---- batch: 020 ----
mean loss: 139.03
 ---- batch: 030 ----
mean loss: 152.86
 ---- batch: 040 ----
mean loss: 155.33
 ---- batch: 050 ----
mean loss: 157.90
 ---- batch: 060 ----
mean loss: 154.69
 ---- batch: 070 ----
mean loss: 157.85
 ---- batch: 080 ----
mean loss: 158.15
 ---- batch: 090 ----
mean loss: 151.68
 ---- batch: 100 ----
mean loss: 164.64
 ---- batch: 110 ----
mean loss: 145.06
train mean loss: 154.12
epoch train time: 0:00:00.552093
elapsed time: 0:01:44.373202
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:21:24.611828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.63
 ---- batch: 020 ----
mean loss: 157.27
 ---- batch: 030 ----
mean loss: 149.48
 ---- batch: 040 ----
mean loss: 152.92
 ---- batch: 050 ----
mean loss: 154.57
 ---- batch: 060 ----
mean loss: 155.50
 ---- batch: 070 ----
mean loss: 147.88
 ---- batch: 080 ----
mean loss: 152.42
 ---- batch: 090 ----
mean loss: 149.75
 ---- batch: 100 ----
mean loss: 154.86
 ---- batch: 110 ----
mean loss: 158.86
train mean loss: 153.13
epoch train time: 0:00:00.552203
elapsed time: 0:01:44.925538
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:21:25.164182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.46
 ---- batch: 020 ----
mean loss: 149.45
 ---- batch: 030 ----
mean loss: 150.84
 ---- batch: 040 ----
mean loss: 157.94
 ---- batch: 050 ----
mean loss: 144.66
 ---- batch: 060 ----
mean loss: 149.82
 ---- batch: 070 ----
mean loss: 158.85
 ---- batch: 080 ----
mean loss: 157.19
 ---- batch: 090 ----
mean loss: 161.59
 ---- batch: 100 ----
mean loss: 147.65
 ---- batch: 110 ----
mean loss: 153.87
train mean loss: 153.48
epoch train time: 0:00:00.551688
elapsed time: 0:01:45.477397
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:21:25.716036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.22
 ---- batch: 020 ----
mean loss: 152.25
 ---- batch: 030 ----
mean loss: 151.24
 ---- batch: 040 ----
mean loss: 152.29
 ---- batch: 050 ----
mean loss: 153.36
 ---- batch: 060 ----
mean loss: 153.41
 ---- batch: 070 ----
mean loss: 156.56
 ---- batch: 080 ----
mean loss: 150.65
 ---- batch: 090 ----
mean loss: 147.55
 ---- batch: 100 ----
mean loss: 156.02
 ---- batch: 110 ----
mean loss: 158.29
train mean loss: 153.07
epoch train time: 0:00:00.554328
elapsed time: 0:01:46.031871
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:21:26.270495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.41
 ---- batch: 020 ----
mean loss: 153.62
 ---- batch: 030 ----
mean loss: 146.45
 ---- batch: 040 ----
mean loss: 155.68
 ---- batch: 050 ----
mean loss: 156.07
 ---- batch: 060 ----
mean loss: 155.16
 ---- batch: 070 ----
mean loss: 154.40
 ---- batch: 080 ----
mean loss: 146.69
 ---- batch: 090 ----
mean loss: 144.87
 ---- batch: 100 ----
mean loss: 151.29
 ---- batch: 110 ----
mean loss: 153.40
train mean loss: 151.79
epoch train time: 0:00:00.549909
elapsed time: 0:01:46.581914
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:21:26.820539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.73
 ---- batch: 020 ----
mean loss: 146.16
 ---- batch: 030 ----
mean loss: 147.00
 ---- batch: 040 ----
mean loss: 149.42
 ---- batch: 050 ----
mean loss: 149.42
 ---- batch: 060 ----
mean loss: 145.13
 ---- batch: 070 ----
mean loss: 154.89
 ---- batch: 080 ----
mean loss: 150.87
 ---- batch: 090 ----
mean loss: 168.35
 ---- batch: 100 ----
mean loss: 145.74
 ---- batch: 110 ----
mean loss: 159.17
train mean loss: 152.89
epoch train time: 0:00:00.558272
elapsed time: 0:01:47.140320
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:21:27.378953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.03
 ---- batch: 020 ----
mean loss: 157.12
 ---- batch: 030 ----
mean loss: 156.76
 ---- batch: 040 ----
mean loss: 154.13
 ---- batch: 050 ----
mean loss: 155.41
 ---- batch: 060 ----
mean loss: 151.06
 ---- batch: 070 ----
mean loss: 152.38
 ---- batch: 080 ----
mean loss: 145.50
 ---- batch: 090 ----
mean loss: 148.36
 ---- batch: 100 ----
mean loss: 158.49
 ---- batch: 110 ----
mean loss: 150.78
train mean loss: 152.78
epoch train time: 0:00:00.552213
elapsed time: 0:01:47.692670
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:21:27.931297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.72
 ---- batch: 020 ----
mean loss: 155.17
 ---- batch: 030 ----
mean loss: 155.60
 ---- batch: 040 ----
mean loss: 160.31
 ---- batch: 050 ----
mean loss: 147.75
 ---- batch: 060 ----
mean loss: 152.47
 ---- batch: 070 ----
mean loss: 155.58
 ---- batch: 080 ----
mean loss: 153.92
 ---- batch: 090 ----
mean loss: 153.82
 ---- batch: 100 ----
mean loss: 146.37
 ---- batch: 110 ----
mean loss: 153.29
train mean loss: 152.97
epoch train time: 0:00:00.557236
elapsed time: 0:01:48.250040
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:21:28.488666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.75
 ---- batch: 020 ----
mean loss: 153.29
 ---- batch: 030 ----
mean loss: 152.38
 ---- batch: 040 ----
mean loss: 147.92
 ---- batch: 050 ----
mean loss: 144.93
 ---- batch: 060 ----
mean loss: 148.98
 ---- batch: 070 ----
mean loss: 156.87
 ---- batch: 080 ----
mean loss: 152.29
 ---- batch: 090 ----
mean loss: 154.68
 ---- batch: 100 ----
mean loss: 146.53
 ---- batch: 110 ----
mean loss: 147.80
train mean loss: 151.44
epoch train time: 0:00:00.557172
elapsed time: 0:01:48.807348
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:21:29.046005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.80
 ---- batch: 020 ----
mean loss: 152.51
 ---- batch: 030 ----
mean loss: 144.14
 ---- batch: 040 ----
mean loss: 153.03
 ---- batch: 050 ----
mean loss: 150.82
 ---- batch: 060 ----
mean loss: 154.42
 ---- batch: 070 ----
mean loss: 141.98
 ---- batch: 080 ----
mean loss: 151.29
 ---- batch: 090 ----
mean loss: 140.53
 ---- batch: 100 ----
mean loss: 162.75
 ---- batch: 110 ----
mean loss: 157.85
train mean loss: 150.18
epoch train time: 0:00:00.559827
elapsed time: 0:01:49.367347
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:21:29.605974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.18
 ---- batch: 020 ----
mean loss: 157.70
 ---- batch: 030 ----
mean loss: 141.86
 ---- batch: 040 ----
mean loss: 158.79
 ---- batch: 050 ----
mean loss: 147.23
 ---- batch: 060 ----
mean loss: 153.44
 ---- batch: 070 ----
mean loss: 146.43
 ---- batch: 080 ----
mean loss: 150.10
 ---- batch: 090 ----
mean loss: 147.21
 ---- batch: 100 ----
mean loss: 154.56
 ---- batch: 110 ----
mean loss: 149.63
train mean loss: 151.24
epoch train time: 0:00:00.559292
elapsed time: 0:01:49.926777
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:21:30.165403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.87
 ---- batch: 020 ----
mean loss: 149.92
 ---- batch: 030 ----
mean loss: 150.86
 ---- batch: 040 ----
mean loss: 155.96
 ---- batch: 050 ----
mean loss: 157.80
 ---- batch: 060 ----
mean loss: 153.55
 ---- batch: 070 ----
mean loss: 148.83
 ---- batch: 080 ----
mean loss: 143.95
 ---- batch: 090 ----
mean loss: 153.43
 ---- batch: 100 ----
mean loss: 143.68
 ---- batch: 110 ----
mean loss: 157.38
train mean loss: 151.01
epoch train time: 0:00:00.567335
elapsed time: 0:01:50.494249
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:21:30.732875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.88
 ---- batch: 020 ----
mean loss: 144.05
 ---- batch: 030 ----
mean loss: 152.16
 ---- batch: 040 ----
mean loss: 155.28
 ---- batch: 050 ----
mean loss: 153.31
 ---- batch: 060 ----
mean loss: 144.73
 ---- batch: 070 ----
mean loss: 150.56
 ---- batch: 080 ----
mean loss: 158.13
 ---- batch: 090 ----
mean loss: 159.03
 ---- batch: 100 ----
mean loss: 148.28
 ---- batch: 110 ----
mean loss: 147.64
train mean loss: 150.52
epoch train time: 0:00:00.561103
elapsed time: 0:01:51.055537
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:21:31.294168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.81
 ---- batch: 020 ----
mean loss: 150.17
 ---- batch: 030 ----
mean loss: 152.25
 ---- batch: 040 ----
mean loss: 143.12
 ---- batch: 050 ----
mean loss: 150.32
 ---- batch: 060 ----
mean loss: 150.43
 ---- batch: 070 ----
mean loss: 150.37
 ---- batch: 080 ----
mean loss: 149.11
 ---- batch: 090 ----
mean loss: 152.65
 ---- batch: 100 ----
mean loss: 148.87
 ---- batch: 110 ----
mean loss: 145.77
train mean loss: 149.92
epoch train time: 0:00:00.562093
elapsed time: 0:01:51.617834
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:21:31.856454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.02
 ---- batch: 020 ----
mean loss: 155.33
 ---- batch: 030 ----
mean loss: 145.10
 ---- batch: 040 ----
mean loss: 148.57
 ---- batch: 050 ----
mean loss: 143.04
 ---- batch: 060 ----
mean loss: 151.42
 ---- batch: 070 ----
mean loss: 146.82
 ---- batch: 080 ----
mean loss: 147.78
 ---- batch: 090 ----
mean loss: 143.02
 ---- batch: 100 ----
mean loss: 158.35
 ---- batch: 110 ----
mean loss: 159.73
train mean loss: 149.49
epoch train time: 0:00:00.547678
elapsed time: 0:01:52.165655
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:21:32.404298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.10
 ---- batch: 020 ----
mean loss: 140.53
 ---- batch: 030 ----
mean loss: 145.23
 ---- batch: 040 ----
mean loss: 151.82
 ---- batch: 050 ----
mean loss: 151.85
 ---- batch: 060 ----
mean loss: 143.52
 ---- batch: 070 ----
mean loss: 155.66
 ---- batch: 080 ----
mean loss: 155.67
 ---- batch: 090 ----
mean loss: 159.57
 ---- batch: 100 ----
mean loss: 150.56
 ---- batch: 110 ----
mean loss: 141.64
train mean loss: 149.42
epoch train time: 0:00:00.556750
elapsed time: 0:01:52.722555
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:21:32.961199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.93
 ---- batch: 020 ----
mean loss: 143.74
 ---- batch: 030 ----
mean loss: 151.33
 ---- batch: 040 ----
mean loss: 156.23
 ---- batch: 050 ----
mean loss: 147.27
 ---- batch: 060 ----
mean loss: 149.26
 ---- batch: 070 ----
mean loss: 142.12
 ---- batch: 080 ----
mean loss: 149.10
 ---- batch: 090 ----
mean loss: 149.96
 ---- batch: 100 ----
mean loss: 151.86
 ---- batch: 110 ----
mean loss: 153.00
train mean loss: 149.36
epoch train time: 0:00:00.558556
elapsed time: 0:01:53.281263
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:21:33.519887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.61
 ---- batch: 020 ----
mean loss: 153.61
 ---- batch: 030 ----
mean loss: 151.52
 ---- batch: 040 ----
mean loss: 147.88
 ---- batch: 050 ----
mean loss: 154.26
 ---- batch: 060 ----
mean loss: 153.31
 ---- batch: 070 ----
mean loss: 146.74
 ---- batch: 080 ----
mean loss: 150.90
 ---- batch: 090 ----
mean loss: 151.96
 ---- batch: 100 ----
mean loss: 146.07
 ---- batch: 110 ----
mean loss: 149.89
train mean loss: 149.44
epoch train time: 0:00:00.559176
elapsed time: 0:01:53.840597
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:21:34.079223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.98
 ---- batch: 020 ----
mean loss: 150.50
 ---- batch: 030 ----
mean loss: 142.02
 ---- batch: 040 ----
mean loss: 148.74
 ---- batch: 050 ----
mean loss: 141.54
 ---- batch: 060 ----
mean loss: 147.05
 ---- batch: 070 ----
mean loss: 150.41
 ---- batch: 080 ----
mean loss: 147.15
 ---- batch: 090 ----
mean loss: 149.58
 ---- batch: 100 ----
mean loss: 147.14
 ---- batch: 110 ----
mean loss: 152.03
train mean loss: 148.29
epoch train time: 0:00:00.563705
elapsed time: 0:01:54.404454
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:21:34.643079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.57
 ---- batch: 020 ----
mean loss: 151.20
 ---- batch: 030 ----
mean loss: 147.62
 ---- batch: 040 ----
mean loss: 145.42
 ---- batch: 050 ----
mean loss: 150.13
 ---- batch: 060 ----
mean loss: 147.82
 ---- batch: 070 ----
mean loss: 148.20
 ---- batch: 080 ----
mean loss: 147.92
 ---- batch: 090 ----
mean loss: 152.91
 ---- batch: 100 ----
mean loss: 160.45
 ---- batch: 110 ----
mean loss: 142.69
train mean loss: 149.02
epoch train time: 0:00:00.552798
elapsed time: 0:01:54.957384
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:21:35.196010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.82
 ---- batch: 020 ----
mean loss: 153.48
 ---- batch: 030 ----
mean loss: 149.40
 ---- batch: 040 ----
mean loss: 144.72
 ---- batch: 050 ----
mean loss: 144.61
 ---- batch: 060 ----
mean loss: 141.43
 ---- batch: 070 ----
mean loss: 147.93
 ---- batch: 080 ----
mean loss: 156.24
 ---- batch: 090 ----
mean loss: 152.48
 ---- batch: 100 ----
mean loss: 153.29
 ---- batch: 110 ----
mean loss: 144.68
train mean loss: 147.79
epoch train time: 0:00:00.551010
elapsed time: 0:01:55.508540
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:21:35.747155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.30
 ---- batch: 020 ----
mean loss: 146.97
 ---- batch: 030 ----
mean loss: 139.15
 ---- batch: 040 ----
mean loss: 139.37
 ---- batch: 050 ----
mean loss: 141.48
 ---- batch: 060 ----
mean loss: 149.68
 ---- batch: 070 ----
mean loss: 153.84
 ---- batch: 080 ----
mean loss: 151.89
 ---- batch: 090 ----
mean loss: 151.18
 ---- batch: 100 ----
mean loss: 154.93
 ---- batch: 110 ----
mean loss: 149.10
train mean loss: 148.15
epoch train time: 0:00:00.555640
elapsed time: 0:01:56.064305
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:21:36.302948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.86
 ---- batch: 020 ----
mean loss: 138.02
 ---- batch: 030 ----
mean loss: 151.81
 ---- batch: 040 ----
mean loss: 146.90
 ---- batch: 050 ----
mean loss: 146.19
 ---- batch: 060 ----
mean loss: 153.72
 ---- batch: 070 ----
mean loss: 145.18
 ---- batch: 080 ----
mean loss: 156.48
 ---- batch: 090 ----
mean loss: 153.90
 ---- batch: 100 ----
mean loss: 144.33
 ---- batch: 110 ----
mean loss: 143.09
train mean loss: 148.14
epoch train time: 0:00:00.561784
elapsed time: 0:01:56.626241
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:21:36.864866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.74
 ---- batch: 020 ----
mean loss: 142.89
 ---- batch: 030 ----
mean loss: 150.72
 ---- batch: 040 ----
mean loss: 146.14
 ---- batch: 050 ----
mean loss: 141.96
 ---- batch: 060 ----
mean loss: 151.47
 ---- batch: 070 ----
mean loss: 150.36
 ---- batch: 080 ----
mean loss: 145.93
 ---- batch: 090 ----
mean loss: 150.29
 ---- batch: 100 ----
mean loss: 152.47
 ---- batch: 110 ----
mean loss: 146.41
train mean loss: 148.28
epoch train time: 0:00:00.548040
elapsed time: 0:01:57.174411
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:21:37.413034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.29
 ---- batch: 020 ----
mean loss: 142.01
 ---- batch: 030 ----
mean loss: 144.51
 ---- batch: 040 ----
mean loss: 142.28
 ---- batch: 050 ----
mean loss: 147.50
 ---- batch: 060 ----
mean loss: 148.01
 ---- batch: 070 ----
mean loss: 148.07
 ---- batch: 080 ----
mean loss: 155.66
 ---- batch: 090 ----
mean loss: 147.04
 ---- batch: 100 ----
mean loss: 150.45
 ---- batch: 110 ----
mean loss: 143.91
train mean loss: 147.86
epoch train time: 0:00:00.554588
elapsed time: 0:01:57.729132
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:21:37.967758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.66
 ---- batch: 020 ----
mean loss: 134.85
 ---- batch: 030 ----
mean loss: 147.15
 ---- batch: 040 ----
mean loss: 145.01
 ---- batch: 050 ----
mean loss: 153.99
 ---- batch: 060 ----
mean loss: 154.59
 ---- batch: 070 ----
mean loss: 153.96
 ---- batch: 080 ----
mean loss: 145.40
 ---- batch: 090 ----
mean loss: 138.66
 ---- batch: 100 ----
mean loss: 145.70
 ---- batch: 110 ----
mean loss: 150.78
train mean loss: 147.37
epoch train time: 0:00:00.556454
elapsed time: 0:01:58.285717
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:21:38.524340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.14
 ---- batch: 020 ----
mean loss: 142.25
 ---- batch: 030 ----
mean loss: 144.83
 ---- batch: 040 ----
mean loss: 147.12
 ---- batch: 050 ----
mean loss: 146.31
 ---- batch: 060 ----
mean loss: 147.64
 ---- batch: 070 ----
mean loss: 142.39
 ---- batch: 080 ----
mean loss: 142.95
 ---- batch: 090 ----
mean loss: 150.79
 ---- batch: 100 ----
mean loss: 143.06
 ---- batch: 110 ----
mean loss: 156.94
train mean loss: 146.49
epoch train time: 0:00:00.556378
elapsed time: 0:01:58.842243
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:21:39.080870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.82
 ---- batch: 020 ----
mean loss: 138.94
 ---- batch: 030 ----
mean loss: 147.95
 ---- batch: 040 ----
mean loss: 144.04
 ---- batch: 050 ----
mean loss: 144.49
 ---- batch: 060 ----
mean loss: 146.01
 ---- batch: 070 ----
mean loss: 156.01
 ---- batch: 080 ----
mean loss: 145.61
 ---- batch: 090 ----
mean loss: 149.74
 ---- batch: 100 ----
mean loss: 146.72
 ---- batch: 110 ----
mean loss: 157.08
train mean loss: 147.49
epoch train time: 0:00:00.555032
elapsed time: 0:01:59.397456
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:21:39.636097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.81
 ---- batch: 020 ----
mean loss: 143.33
 ---- batch: 030 ----
mean loss: 155.62
 ---- batch: 040 ----
mean loss: 156.99
 ---- batch: 050 ----
mean loss: 147.53
 ---- batch: 060 ----
mean loss: 145.56
 ---- batch: 070 ----
mean loss: 149.51
 ---- batch: 080 ----
mean loss: 143.02
 ---- batch: 090 ----
mean loss: 142.70
 ---- batch: 100 ----
mean loss: 154.50
 ---- batch: 110 ----
mean loss: 148.42
train mean loss: 148.16
epoch train time: 0:00:00.554356
elapsed time: 0:01:59.951959
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:21:40.190602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.82
 ---- batch: 020 ----
mean loss: 143.66
 ---- batch: 030 ----
mean loss: 150.94
 ---- batch: 040 ----
mean loss: 142.51
 ---- batch: 050 ----
mean loss: 137.86
 ---- batch: 060 ----
mean loss: 150.13
 ---- batch: 070 ----
mean loss: 138.25
 ---- batch: 080 ----
mean loss: 146.64
 ---- batch: 090 ----
mean loss: 157.26
 ---- batch: 100 ----
mean loss: 145.59
 ---- batch: 110 ----
mean loss: 155.26
train mean loss: 146.00
epoch train time: 0:00:00.548326
elapsed time: 0:02:00.500436
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:21:40.739060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.03
 ---- batch: 020 ----
mean loss: 140.37
 ---- batch: 030 ----
mean loss: 143.32
 ---- batch: 040 ----
mean loss: 142.58
 ---- batch: 050 ----
mean loss: 146.77
 ---- batch: 060 ----
mean loss: 141.11
 ---- batch: 070 ----
mean loss: 144.56
 ---- batch: 080 ----
mean loss: 147.40
 ---- batch: 090 ----
mean loss: 137.02
 ---- batch: 100 ----
mean loss: 153.11
 ---- batch: 110 ----
mean loss: 154.07
train mean loss: 145.64
epoch train time: 0:00:00.559652
elapsed time: 0:02:01.060236
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:21:41.298860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.04
 ---- batch: 020 ----
mean loss: 147.48
 ---- batch: 030 ----
mean loss: 137.71
 ---- batch: 040 ----
mean loss: 151.66
 ---- batch: 050 ----
mean loss: 146.28
 ---- batch: 060 ----
mean loss: 143.59
 ---- batch: 070 ----
mean loss: 146.16
 ---- batch: 080 ----
mean loss: 145.50
 ---- batch: 090 ----
mean loss: 144.83
 ---- batch: 100 ----
mean loss: 153.46
 ---- batch: 110 ----
mean loss: 140.42
train mean loss: 146.21
epoch train time: 0:00:00.554911
elapsed time: 0:02:01.615288
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:21:41.853914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.52
 ---- batch: 020 ----
mean loss: 142.93
 ---- batch: 030 ----
mean loss: 140.03
 ---- batch: 040 ----
mean loss: 151.11
 ---- batch: 050 ----
mean loss: 152.31
 ---- batch: 060 ----
mean loss: 141.90
 ---- batch: 070 ----
mean loss: 140.45
 ---- batch: 080 ----
mean loss: 157.05
 ---- batch: 090 ----
mean loss: 149.13
 ---- batch: 100 ----
mean loss: 136.18
 ---- batch: 110 ----
mean loss: 140.50
train mean loss: 144.97
epoch train time: 0:00:00.548668
elapsed time: 0:02:02.164088
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:21:42.402709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.25
 ---- batch: 020 ----
mean loss: 144.25
 ---- batch: 030 ----
mean loss: 140.84
 ---- batch: 040 ----
mean loss: 144.97
 ---- batch: 050 ----
mean loss: 154.98
 ---- batch: 060 ----
mean loss: 145.65
 ---- batch: 070 ----
mean loss: 144.14
 ---- batch: 080 ----
mean loss: 147.70
 ---- batch: 090 ----
mean loss: 144.84
 ---- batch: 100 ----
mean loss: 150.85
 ---- batch: 110 ----
mean loss: 147.88
train mean loss: 145.93
epoch train time: 0:00:00.572086
elapsed time: 0:02:02.736308
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:21:42.974938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.98
 ---- batch: 020 ----
mean loss: 149.41
 ---- batch: 030 ----
mean loss: 144.17
 ---- batch: 040 ----
mean loss: 134.79
 ---- batch: 050 ----
mean loss: 156.39
 ---- batch: 060 ----
mean loss: 148.00
 ---- batch: 070 ----
mean loss: 144.35
 ---- batch: 080 ----
mean loss: 130.36
 ---- batch: 090 ----
mean loss: 146.49
 ---- batch: 100 ----
mean loss: 154.59
 ---- batch: 110 ----
mean loss: 151.53
train mean loss: 145.80
epoch train time: 0:00:00.558325
elapsed time: 0:02:03.294771
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:21:43.533422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.42
 ---- batch: 020 ----
mean loss: 143.34
 ---- batch: 030 ----
mean loss: 150.11
 ---- batch: 040 ----
mean loss: 146.67
 ---- batch: 050 ----
mean loss: 147.75
 ---- batch: 060 ----
mean loss: 144.98
 ---- batch: 070 ----
mean loss: 144.41
 ---- batch: 080 ----
mean loss: 145.91
 ---- batch: 090 ----
mean loss: 144.52
 ---- batch: 100 ----
mean loss: 136.26
 ---- batch: 110 ----
mean loss: 144.99
train mean loss: 145.19
epoch train time: 0:00:00.557162
elapsed time: 0:02:03.852095
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:21:44.090720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.95
 ---- batch: 020 ----
mean loss: 145.49
 ---- batch: 030 ----
mean loss: 143.85
 ---- batch: 040 ----
mean loss: 148.01
 ---- batch: 050 ----
mean loss: 144.45
 ---- batch: 060 ----
mean loss: 141.07
 ---- batch: 070 ----
mean loss: 153.86
 ---- batch: 080 ----
mean loss: 145.16
 ---- batch: 090 ----
mean loss: 134.84
 ---- batch: 100 ----
mean loss: 149.03
 ---- batch: 110 ----
mean loss: 144.88
train mean loss: 144.70
epoch train time: 0:00:00.555355
elapsed time: 0:02:04.407586
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:21:44.646211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.68
 ---- batch: 020 ----
mean loss: 137.94
 ---- batch: 030 ----
mean loss: 145.66
 ---- batch: 040 ----
mean loss: 140.28
 ---- batch: 050 ----
mean loss: 156.29
 ---- batch: 060 ----
mean loss: 141.15
 ---- batch: 070 ----
mean loss: 147.85
 ---- batch: 080 ----
mean loss: 145.78
 ---- batch: 090 ----
mean loss: 152.53
 ---- batch: 100 ----
mean loss: 139.08
 ---- batch: 110 ----
mean loss: 135.91
train mean loss: 144.97
epoch train time: 0:00:00.550401
elapsed time: 0:02:04.958119
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:21:45.196761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.28
 ---- batch: 020 ----
mean loss: 142.61
 ---- batch: 030 ----
mean loss: 140.74
 ---- batch: 040 ----
mean loss: 140.97
 ---- batch: 050 ----
mean loss: 143.15
 ---- batch: 060 ----
mean loss: 142.25
 ---- batch: 070 ----
mean loss: 132.39
 ---- batch: 080 ----
mean loss: 159.05
 ---- batch: 090 ----
mean loss: 149.40
 ---- batch: 100 ----
mean loss: 158.57
 ---- batch: 110 ----
mean loss: 141.92
train mean loss: 144.75
epoch train time: 0:00:00.550706
elapsed time: 0:02:05.508975
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:21:45.747600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.31
 ---- batch: 020 ----
mean loss: 142.59
 ---- batch: 030 ----
mean loss: 141.12
 ---- batch: 040 ----
mean loss: 138.25
 ---- batch: 050 ----
mean loss: 143.86
 ---- batch: 060 ----
mean loss: 140.71
 ---- batch: 070 ----
mean loss: 144.48
 ---- batch: 080 ----
mean loss: 139.99
 ---- batch: 090 ----
mean loss: 154.93
 ---- batch: 100 ----
mean loss: 150.56
 ---- batch: 110 ----
mean loss: 146.67
train mean loss: 144.06
epoch train time: 0:00:00.556715
elapsed time: 0:02:06.065825
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:21:46.304452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.56
 ---- batch: 020 ----
mean loss: 145.77
 ---- batch: 030 ----
mean loss: 139.12
 ---- batch: 040 ----
mean loss: 152.58
 ---- batch: 050 ----
mean loss: 151.77
 ---- batch: 060 ----
mean loss: 139.28
 ---- batch: 070 ----
mean loss: 132.85
 ---- batch: 080 ----
mean loss: 148.63
 ---- batch: 090 ----
mean loss: 147.74
 ---- batch: 100 ----
mean loss: 143.30
 ---- batch: 110 ----
mean loss: 144.96
train mean loss: 144.10
epoch train time: 0:00:00.558254
elapsed time: 0:02:06.624218
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:21:46.862844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.42
 ---- batch: 020 ----
mean loss: 142.75
 ---- batch: 030 ----
mean loss: 134.28
 ---- batch: 040 ----
mean loss: 150.75
 ---- batch: 050 ----
mean loss: 152.18
 ---- batch: 060 ----
mean loss: 144.69
 ---- batch: 070 ----
mean loss: 144.41
 ---- batch: 080 ----
mean loss: 152.87
 ---- batch: 090 ----
mean loss: 143.33
 ---- batch: 100 ----
mean loss: 149.26
 ---- batch: 110 ----
mean loss: 139.51
train mean loss: 144.49
epoch train time: 0:00:00.552113
elapsed time: 0:02:07.176494
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:21:47.415165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.64
 ---- batch: 020 ----
mean loss: 142.58
 ---- batch: 030 ----
mean loss: 142.51
 ---- batch: 040 ----
mean loss: 143.01
 ---- batch: 050 ----
mean loss: 142.90
 ---- batch: 060 ----
mean loss: 152.37
 ---- batch: 070 ----
mean loss: 140.23
 ---- batch: 080 ----
mean loss: 147.68
 ---- batch: 090 ----
mean loss: 147.67
 ---- batch: 100 ----
mean loss: 136.52
 ---- batch: 110 ----
mean loss: 142.14
train mean loss: 143.89
epoch train time: 0:00:00.569190
elapsed time: 0:02:07.745863
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:21:47.984489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.59
 ---- batch: 020 ----
mean loss: 153.60
 ---- batch: 030 ----
mean loss: 151.20
 ---- batch: 040 ----
mean loss: 139.68
 ---- batch: 050 ----
mean loss: 138.82
 ---- batch: 060 ----
mean loss: 138.83
 ---- batch: 070 ----
mean loss: 147.67
 ---- batch: 080 ----
mean loss: 141.14
 ---- batch: 090 ----
mean loss: 143.94
 ---- batch: 100 ----
mean loss: 146.32
 ---- batch: 110 ----
mean loss: 141.28
train mean loss: 143.24
epoch train time: 0:00:00.562305
elapsed time: 0:02:08.308312
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:21:48.546955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.82
 ---- batch: 020 ----
mean loss: 144.82
 ---- batch: 030 ----
mean loss: 144.27
 ---- batch: 040 ----
mean loss: 140.32
 ---- batch: 050 ----
mean loss: 142.04
 ---- batch: 060 ----
mean loss: 141.56
 ---- batch: 070 ----
mean loss: 139.88
 ---- batch: 080 ----
mean loss: 146.21
 ---- batch: 090 ----
mean loss: 147.00
 ---- batch: 100 ----
mean loss: 146.94
 ---- batch: 110 ----
mean loss: 151.31
train mean loss: 143.67
epoch train time: 0:00:00.569711
elapsed time: 0:02:08.878175
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:21:49.116802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.26
 ---- batch: 020 ----
mean loss: 137.06
 ---- batch: 030 ----
mean loss: 141.55
 ---- batch: 040 ----
mean loss: 144.95
 ---- batch: 050 ----
mean loss: 134.08
 ---- batch: 060 ----
mean loss: 141.05
 ---- batch: 070 ----
mean loss: 145.90
 ---- batch: 080 ----
mean loss: 137.27
 ---- batch: 090 ----
mean loss: 148.83
 ---- batch: 100 ----
mean loss: 137.33
 ---- batch: 110 ----
mean loss: 150.16
train mean loss: 142.32
epoch train time: 0:00:00.557507
elapsed time: 0:02:09.435826
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:21:49.674441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.98
 ---- batch: 020 ----
mean loss: 145.21
 ---- batch: 030 ----
mean loss: 139.68
 ---- batch: 040 ----
mean loss: 143.56
 ---- batch: 050 ----
mean loss: 149.01
 ---- batch: 060 ----
mean loss: 137.18
 ---- batch: 070 ----
mean loss: 143.54
 ---- batch: 080 ----
mean loss: 141.53
 ---- batch: 090 ----
mean loss: 147.27
 ---- batch: 100 ----
mean loss: 145.15
 ---- batch: 110 ----
mean loss: 146.47
train mean loss: 143.27
epoch train time: 0:00:00.551524
elapsed time: 0:02:09.987486
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:21:50.226126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.44
 ---- batch: 020 ----
mean loss: 141.02
 ---- batch: 030 ----
mean loss: 139.81
 ---- batch: 040 ----
mean loss: 144.14
 ---- batch: 050 ----
mean loss: 140.38
 ---- batch: 060 ----
mean loss: 142.78
 ---- batch: 070 ----
mean loss: 149.91
 ---- batch: 080 ----
mean loss: 142.36
 ---- batch: 090 ----
mean loss: 148.55
 ---- batch: 100 ----
mean loss: 142.95
 ---- batch: 110 ----
mean loss: 139.58
train mean loss: 143.30
epoch train time: 0:00:00.547330
elapsed time: 0:02:10.534973
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:21:50.773653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.43
 ---- batch: 020 ----
mean loss: 143.38
 ---- batch: 030 ----
mean loss: 132.33
 ---- batch: 040 ----
mean loss: 149.08
 ---- batch: 050 ----
mean loss: 146.14
 ---- batch: 060 ----
mean loss: 142.86
 ---- batch: 070 ----
mean loss: 140.81
 ---- batch: 080 ----
mean loss: 146.24
 ---- batch: 090 ----
mean loss: 137.34
 ---- batch: 100 ----
mean loss: 142.47
 ---- batch: 110 ----
mean loss: 142.14
train mean loss: 142.19
epoch train time: 0:00:00.554143
elapsed time: 0:02:11.089301
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:21:51.327929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.06
 ---- batch: 020 ----
mean loss: 145.05
 ---- batch: 030 ----
mean loss: 144.56
 ---- batch: 040 ----
mean loss: 140.72
 ---- batch: 050 ----
mean loss: 141.06
 ---- batch: 060 ----
mean loss: 142.29
 ---- batch: 070 ----
mean loss: 135.65
 ---- batch: 080 ----
mean loss: 144.02
 ---- batch: 090 ----
mean loss: 144.32
 ---- batch: 100 ----
mean loss: 144.94
 ---- batch: 110 ----
mean loss: 138.60
train mean loss: 142.29
epoch train time: 0:00:00.564031
elapsed time: 0:02:11.653479
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:21:51.892109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.51
 ---- batch: 020 ----
mean loss: 148.90
 ---- batch: 030 ----
mean loss: 145.87
 ---- batch: 040 ----
mean loss: 134.10
 ---- batch: 050 ----
mean loss: 139.06
 ---- batch: 060 ----
mean loss: 142.39
 ---- batch: 070 ----
mean loss: 135.36
 ---- batch: 080 ----
mean loss: 139.57
 ---- batch: 090 ----
mean loss: 146.43
 ---- batch: 100 ----
mean loss: 138.95
 ---- batch: 110 ----
mean loss: 149.45
train mean loss: 142.36
epoch train time: 0:00:00.564198
elapsed time: 0:02:12.217825
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:21:52.456454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.63
 ---- batch: 020 ----
mean loss: 140.22
 ---- batch: 030 ----
mean loss: 138.19
 ---- batch: 040 ----
mean loss: 144.62
 ---- batch: 050 ----
mean loss: 142.83
 ---- batch: 060 ----
mean loss: 135.26
 ---- batch: 070 ----
mean loss: 143.34
 ---- batch: 080 ----
mean loss: 143.41
 ---- batch: 090 ----
mean loss: 141.63
 ---- batch: 100 ----
mean loss: 146.34
 ---- batch: 110 ----
mean loss: 150.33
train mean loss: 142.48
epoch train time: 0:00:00.557600
elapsed time: 0:02:12.775564
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:21:53.014191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.87
 ---- batch: 020 ----
mean loss: 145.45
 ---- batch: 030 ----
mean loss: 131.98
 ---- batch: 040 ----
mean loss: 137.85
 ---- batch: 050 ----
mean loss: 149.86
 ---- batch: 060 ----
mean loss: 143.50
 ---- batch: 070 ----
mean loss: 148.84
 ---- batch: 080 ----
mean loss: 144.73
 ---- batch: 090 ----
mean loss: 138.58
 ---- batch: 100 ----
mean loss: 142.53
 ---- batch: 110 ----
mean loss: 144.47
train mean loss: 142.01
epoch train time: 0:00:00.543217
elapsed time: 0:02:13.318912
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:21:53.557537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.35
 ---- batch: 020 ----
mean loss: 141.30
 ---- batch: 030 ----
mean loss: 149.68
 ---- batch: 040 ----
mean loss: 129.43
 ---- batch: 050 ----
mean loss: 145.06
 ---- batch: 060 ----
mean loss: 135.75
 ---- batch: 070 ----
mean loss: 139.96
 ---- batch: 080 ----
mean loss: 147.14
 ---- batch: 090 ----
mean loss: 146.77
 ---- batch: 100 ----
mean loss: 146.96
 ---- batch: 110 ----
mean loss: 146.30
train mean loss: 141.94
epoch train time: 0:00:00.562470
elapsed time: 0:02:13.881517
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:21:54.120145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.01
 ---- batch: 020 ----
mean loss: 146.44
 ---- batch: 030 ----
mean loss: 143.88
 ---- batch: 040 ----
mean loss: 143.88
 ---- batch: 050 ----
mean loss: 144.00
 ---- batch: 060 ----
mean loss: 144.90
 ---- batch: 070 ----
mean loss: 139.77
 ---- batch: 080 ----
mean loss: 140.05
 ---- batch: 090 ----
mean loss: 142.08
 ---- batch: 100 ----
mean loss: 147.42
 ---- batch: 110 ----
mean loss: 137.39
train mean loss: 143.16
epoch train time: 0:00:00.566865
elapsed time: 0:02:14.448528
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:21:54.687183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.50
 ---- batch: 020 ----
mean loss: 142.35
 ---- batch: 030 ----
mean loss: 129.15
 ---- batch: 040 ----
mean loss: 141.41
 ---- batch: 050 ----
mean loss: 132.37
 ---- batch: 060 ----
mean loss: 136.99
 ---- batch: 070 ----
mean loss: 148.43
 ---- batch: 080 ----
mean loss: 144.78
 ---- batch: 090 ----
mean loss: 153.42
 ---- batch: 100 ----
mean loss: 141.87
 ---- batch: 110 ----
mean loss: 140.77
train mean loss: 141.26
epoch train time: 0:00:00.573306
elapsed time: 0:02:15.021996
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:21:55.260622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.80
 ---- batch: 020 ----
mean loss: 142.79
 ---- batch: 030 ----
mean loss: 140.07
 ---- batch: 040 ----
mean loss: 140.55
 ---- batch: 050 ----
mean loss: 143.27
 ---- batch: 060 ----
mean loss: 139.05
 ---- batch: 070 ----
mean loss: 142.41
 ---- batch: 080 ----
mean loss: 145.34
 ---- batch: 090 ----
mean loss: 135.04
 ---- batch: 100 ----
mean loss: 144.64
 ---- batch: 110 ----
mean loss: 149.34
train mean loss: 141.74
epoch train time: 0:00:00.556316
elapsed time: 0:02:15.578461
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:21:55.817087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.77
 ---- batch: 020 ----
mean loss: 135.44
 ---- batch: 030 ----
mean loss: 146.71
 ---- batch: 040 ----
mean loss: 143.75
 ---- batch: 050 ----
mean loss: 142.89
 ---- batch: 060 ----
mean loss: 142.73
 ---- batch: 070 ----
mean loss: 144.52
 ---- batch: 080 ----
mean loss: 134.80
 ---- batch: 090 ----
mean loss: 139.42
 ---- batch: 100 ----
mean loss: 143.52
 ---- batch: 110 ----
mean loss: 149.06
train mean loss: 141.85
epoch train time: 0:00:00.548185
elapsed time: 0:02:16.126781
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:21:56.365406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.84
 ---- batch: 020 ----
mean loss: 147.59
 ---- batch: 030 ----
mean loss: 152.99
 ---- batch: 040 ----
mean loss: 143.14
 ---- batch: 050 ----
mean loss: 141.32
 ---- batch: 060 ----
mean loss: 138.90
 ---- batch: 070 ----
mean loss: 143.48
 ---- batch: 080 ----
mean loss: 140.11
 ---- batch: 090 ----
mean loss: 136.29
 ---- batch: 100 ----
mean loss: 139.20
 ---- batch: 110 ----
mean loss: 140.22
train mean loss: 141.60
epoch train time: 0:00:00.553660
elapsed time: 0:02:16.680574
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:21:56.919197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.82
 ---- batch: 020 ----
mean loss: 138.01
 ---- batch: 030 ----
mean loss: 137.94
 ---- batch: 040 ----
mean loss: 144.09
 ---- batch: 050 ----
mean loss: 145.02
 ---- batch: 060 ----
mean loss: 153.80
 ---- batch: 070 ----
mean loss: 132.81
 ---- batch: 080 ----
mean loss: 132.59
 ---- batch: 090 ----
mean loss: 145.94
 ---- batch: 100 ----
mean loss: 145.84
 ---- batch: 110 ----
mean loss: 144.11
train mean loss: 141.36
epoch train time: 0:00:00.554512
elapsed time: 0:02:17.235235
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:21:57.473859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.79
 ---- batch: 020 ----
mean loss: 141.27
 ---- batch: 030 ----
mean loss: 147.68
 ---- batch: 040 ----
mean loss: 131.24
 ---- batch: 050 ----
mean loss: 140.24
 ---- batch: 060 ----
mean loss: 137.22
 ---- batch: 070 ----
mean loss: 141.29
 ---- batch: 080 ----
mean loss: 138.89
 ---- batch: 090 ----
mean loss: 147.41
 ---- batch: 100 ----
mean loss: 144.19
 ---- batch: 110 ----
mean loss: 135.19
train mean loss: 140.17
epoch train time: 0:00:00.562251
elapsed time: 0:02:17.797619
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:21:58.036247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.18
 ---- batch: 020 ----
mean loss: 139.37
 ---- batch: 030 ----
mean loss: 155.12
 ---- batch: 040 ----
mean loss: 137.18
 ---- batch: 050 ----
mean loss: 137.39
 ---- batch: 060 ----
mean loss: 140.20
 ---- batch: 070 ----
mean loss: 136.39
 ---- batch: 080 ----
mean loss: 148.70
 ---- batch: 090 ----
mean loss: 140.41
 ---- batch: 100 ----
mean loss: 135.36
 ---- batch: 110 ----
mean loss: 140.54
train mean loss: 140.91
epoch train time: 0:00:00.549879
elapsed time: 0:02:18.347664
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:21:58.586309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.85
 ---- batch: 020 ----
mean loss: 133.59
 ---- batch: 030 ----
mean loss: 139.02
 ---- batch: 040 ----
mean loss: 136.88
 ---- batch: 050 ----
mean loss: 138.19
 ---- batch: 060 ----
mean loss: 143.88
 ---- batch: 070 ----
mean loss: 148.74
 ---- batch: 080 ----
mean loss: 138.52
 ---- batch: 090 ----
mean loss: 138.15
 ---- batch: 100 ----
mean loss: 144.94
 ---- batch: 110 ----
mean loss: 147.66
train mean loss: 140.24
epoch train time: 0:00:00.553183
elapsed time: 0:02:18.900998
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:21:59.139624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.37
 ---- batch: 020 ----
mean loss: 133.22
 ---- batch: 030 ----
mean loss: 139.42
 ---- batch: 040 ----
mean loss: 140.86
 ---- batch: 050 ----
mean loss: 137.66
 ---- batch: 060 ----
mean loss: 148.56
 ---- batch: 070 ----
mean loss: 136.83
 ---- batch: 080 ----
mean loss: 135.77
 ---- batch: 090 ----
mean loss: 141.40
 ---- batch: 100 ----
mean loss: 142.29
 ---- batch: 110 ----
mean loss: 140.19
train mean loss: 139.51
epoch train time: 0:00:00.548737
elapsed time: 0:02:19.449868
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:21:59.688495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.00
 ---- batch: 020 ----
mean loss: 143.55
 ---- batch: 030 ----
mean loss: 140.66
 ---- batch: 040 ----
mean loss: 138.63
 ---- batch: 050 ----
mean loss: 140.13
 ---- batch: 060 ----
mean loss: 144.27
 ---- batch: 070 ----
mean loss: 134.19
 ---- batch: 080 ----
mean loss: 139.53
 ---- batch: 090 ----
mean loss: 137.01
 ---- batch: 100 ----
mean loss: 136.19
 ---- batch: 110 ----
mean loss: 141.13
train mean loss: 139.95
epoch train time: 0:00:00.555604
elapsed time: 0:02:20.005621
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:22:00.244245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.06
 ---- batch: 020 ----
mean loss: 136.02
 ---- batch: 030 ----
mean loss: 134.64
 ---- batch: 040 ----
mean loss: 141.68
 ---- batch: 050 ----
mean loss: 142.86
 ---- batch: 060 ----
mean loss: 141.22
 ---- batch: 070 ----
mean loss: 132.80
 ---- batch: 080 ----
mean loss: 134.92
 ---- batch: 090 ----
mean loss: 135.33
 ---- batch: 100 ----
mean loss: 149.09
 ---- batch: 110 ----
mean loss: 149.07
train mean loss: 139.61
epoch train time: 0:00:00.552367
elapsed time: 0:02:20.558136
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:22:00.796761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.67
 ---- batch: 020 ----
mean loss: 149.57
 ---- batch: 030 ----
mean loss: 140.58
 ---- batch: 040 ----
mean loss: 139.15
 ---- batch: 050 ----
mean loss: 138.33
 ---- batch: 060 ----
mean loss: 131.51
 ---- batch: 070 ----
mean loss: 144.64
 ---- batch: 080 ----
mean loss: 136.70
 ---- batch: 090 ----
mean loss: 140.95
 ---- batch: 100 ----
mean loss: 141.47
 ---- batch: 110 ----
mean loss: 135.59
train mean loss: 139.13
epoch train time: 0:00:00.553272
elapsed time: 0:02:21.111538
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:22:01.350162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.34
 ---- batch: 020 ----
mean loss: 144.57
 ---- batch: 030 ----
mean loss: 145.34
 ---- batch: 040 ----
mean loss: 140.74
 ---- batch: 050 ----
mean loss: 129.61
 ---- batch: 060 ----
mean loss: 137.70
 ---- batch: 070 ----
mean loss: 145.80
 ---- batch: 080 ----
mean loss: 136.72
 ---- batch: 090 ----
mean loss: 141.20
 ---- batch: 100 ----
mean loss: 137.88
 ---- batch: 110 ----
mean loss: 140.39
train mean loss: 139.58
epoch train time: 0:00:00.552763
elapsed time: 0:02:21.664435
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:22:01.903080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.54
 ---- batch: 020 ----
mean loss: 133.31
 ---- batch: 030 ----
mean loss: 134.31
 ---- batch: 040 ----
mean loss: 147.08
 ---- batch: 050 ----
mean loss: 135.07
 ---- batch: 060 ----
mean loss: 135.10
 ---- batch: 070 ----
mean loss: 142.26
 ---- batch: 080 ----
mean loss: 139.71
 ---- batch: 090 ----
mean loss: 139.48
 ---- batch: 100 ----
mean loss: 135.65
 ---- batch: 110 ----
mean loss: 146.17
train mean loss: 139.47
epoch train time: 0:00:00.561814
elapsed time: 0:02:22.226417
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:22:02.465046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.80
 ---- batch: 020 ----
mean loss: 139.08
 ---- batch: 030 ----
mean loss: 138.39
 ---- batch: 040 ----
mean loss: 135.39
 ---- batch: 050 ----
mean loss: 138.37
 ---- batch: 060 ----
mean loss: 132.07
 ---- batch: 070 ----
mean loss: 152.72
 ---- batch: 080 ----
mean loss: 143.20
 ---- batch: 090 ----
mean loss: 141.95
 ---- batch: 100 ----
mean loss: 139.12
 ---- batch: 110 ----
mean loss: 135.52
train mean loss: 139.22
epoch train time: 0:00:00.564552
elapsed time: 0:02:22.791124
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:22:03.029771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.72
 ---- batch: 020 ----
mean loss: 134.09
 ---- batch: 030 ----
mean loss: 137.11
 ---- batch: 040 ----
mean loss: 141.10
 ---- batch: 050 ----
mean loss: 134.60
 ---- batch: 060 ----
mean loss: 146.82
 ---- batch: 070 ----
mean loss: 132.54
 ---- batch: 080 ----
mean loss: 140.70
 ---- batch: 090 ----
mean loss: 134.38
 ---- batch: 100 ----
mean loss: 135.37
 ---- batch: 110 ----
mean loss: 146.13
train mean loss: 138.77
epoch train time: 0:00:00.552678
elapsed time: 0:02:23.343955
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:22:03.582578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.25
 ---- batch: 020 ----
mean loss: 133.06
 ---- batch: 030 ----
mean loss: 145.57
 ---- batch: 040 ----
mean loss: 131.27
 ---- batch: 050 ----
mean loss: 135.21
 ---- batch: 060 ----
mean loss: 149.47
 ---- batch: 070 ----
mean loss: 144.46
 ---- batch: 080 ----
mean loss: 138.56
 ---- batch: 090 ----
mean loss: 135.54
 ---- batch: 100 ----
mean loss: 134.78
 ---- batch: 110 ----
mean loss: 139.88
train mean loss: 138.83
epoch train time: 0:00:00.556453
elapsed time: 0:02:23.900537
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:22:04.139159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.71
 ---- batch: 020 ----
mean loss: 131.17
 ---- batch: 030 ----
mean loss: 131.67
 ---- batch: 040 ----
mean loss: 133.00
 ---- batch: 050 ----
mean loss: 141.65
 ---- batch: 060 ----
mean loss: 141.97
 ---- batch: 070 ----
mean loss: 139.96
 ---- batch: 080 ----
mean loss: 132.88
 ---- batch: 090 ----
mean loss: 146.28
 ---- batch: 100 ----
mean loss: 138.61
 ---- batch: 110 ----
mean loss: 149.73
train mean loss: 138.58
epoch train time: 0:00:00.550142
elapsed time: 0:02:24.450835
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:22:04.689459
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.78
 ---- batch: 020 ----
mean loss: 126.44
 ---- batch: 030 ----
mean loss: 140.69
 ---- batch: 040 ----
mean loss: 134.78
 ---- batch: 050 ----
mean loss: 125.55
 ---- batch: 060 ----
mean loss: 131.77
 ---- batch: 070 ----
mean loss: 127.33
 ---- batch: 080 ----
mean loss: 137.94
 ---- batch: 090 ----
mean loss: 133.69
 ---- batch: 100 ----
mean loss: 134.33
 ---- batch: 110 ----
mean loss: 129.28
train mean loss: 132.35
epoch train time: 0:00:00.554231
elapsed time: 0:02:25.005217
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:22:05.243833
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.90
 ---- batch: 020 ----
mean loss: 128.02
 ---- batch: 030 ----
mean loss: 127.53
 ---- batch: 040 ----
mean loss: 132.43
 ---- batch: 050 ----
mean loss: 134.36
 ---- batch: 060 ----
mean loss: 133.38
 ---- batch: 070 ----
mean loss: 127.40
 ---- batch: 080 ----
mean loss: 139.17
 ---- batch: 090 ----
mean loss: 129.94
 ---- batch: 100 ----
mean loss: 131.39
 ---- batch: 110 ----
mean loss: 126.36
train mean loss: 131.65
epoch train time: 0:00:00.550359
elapsed time: 0:02:25.555704
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:22:05.794349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.86
 ---- batch: 020 ----
mean loss: 132.10
 ---- batch: 030 ----
mean loss: 132.94
 ---- batch: 040 ----
mean loss: 131.93
 ---- batch: 050 ----
mean loss: 132.53
 ---- batch: 060 ----
mean loss: 131.90
 ---- batch: 070 ----
mean loss: 129.53
 ---- batch: 080 ----
mean loss: 137.18
 ---- batch: 090 ----
mean loss: 133.70
 ---- batch: 100 ----
mean loss: 128.36
 ---- batch: 110 ----
mean loss: 127.07
train mean loss: 131.41
epoch train time: 0:00:00.562417
elapsed time: 0:02:26.118273
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:22:06.356896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.30
 ---- batch: 020 ----
mean loss: 126.13
 ---- batch: 030 ----
mean loss: 127.88
 ---- batch: 040 ----
mean loss: 125.56
 ---- batch: 050 ----
mean loss: 137.13
 ---- batch: 060 ----
mean loss: 129.75
 ---- batch: 070 ----
mean loss: 137.98
 ---- batch: 080 ----
mean loss: 130.43
 ---- batch: 090 ----
mean loss: 136.44
 ---- batch: 100 ----
mean loss: 126.70
 ---- batch: 110 ----
mean loss: 130.36
train mean loss: 131.19
epoch train time: 0:00:00.555948
elapsed time: 0:02:26.674389
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:22:06.913017
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.99
 ---- batch: 020 ----
mean loss: 126.82
 ---- batch: 030 ----
mean loss: 128.73
 ---- batch: 040 ----
mean loss: 130.78
 ---- batch: 050 ----
mean loss: 128.00
 ---- batch: 060 ----
mean loss: 132.97
 ---- batch: 070 ----
mean loss: 135.12
 ---- batch: 080 ----
mean loss: 137.88
 ---- batch: 090 ----
mean loss: 123.05
 ---- batch: 100 ----
mean loss: 129.52
 ---- batch: 110 ----
mean loss: 133.11
train mean loss: 131.27
epoch train time: 0:00:00.559996
elapsed time: 0:02:27.234520
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:22:07.473144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.98
 ---- batch: 020 ----
mean loss: 129.68
 ---- batch: 030 ----
mean loss: 127.83
 ---- batch: 040 ----
mean loss: 131.48
 ---- batch: 050 ----
mean loss: 125.97
 ---- batch: 060 ----
mean loss: 139.21
 ---- batch: 070 ----
mean loss: 131.39
 ---- batch: 080 ----
mean loss: 134.29
 ---- batch: 090 ----
mean loss: 131.11
 ---- batch: 100 ----
mean loss: 123.99
 ---- batch: 110 ----
mean loss: 133.65
train mean loss: 131.18
epoch train time: 0:00:00.554290
elapsed time: 0:02:27.788940
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:22:08.027565
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.68
 ---- batch: 020 ----
mean loss: 129.80
 ---- batch: 030 ----
mean loss: 128.85
 ---- batch: 040 ----
mean loss: 139.82
 ---- batch: 050 ----
mean loss: 130.63
 ---- batch: 060 ----
mean loss: 132.27
 ---- batch: 070 ----
mean loss: 125.24
 ---- batch: 080 ----
mean loss: 140.62
 ---- batch: 090 ----
mean loss: 129.92
 ---- batch: 100 ----
mean loss: 131.81
 ---- batch: 110 ----
mean loss: 130.60
train mean loss: 131.08
epoch train time: 0:00:00.555950
elapsed time: 0:02:28.345021
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:22:08.583645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.81
 ---- batch: 020 ----
mean loss: 128.19
 ---- batch: 030 ----
mean loss: 127.38
 ---- batch: 040 ----
mean loss: 127.46
 ---- batch: 050 ----
mean loss: 133.83
 ---- batch: 060 ----
mean loss: 141.21
 ---- batch: 070 ----
mean loss: 137.18
 ---- batch: 080 ----
mean loss: 131.56
 ---- batch: 090 ----
mean loss: 132.35
 ---- batch: 100 ----
mean loss: 128.53
 ---- batch: 110 ----
mean loss: 132.20
train mean loss: 131.11
epoch train time: 0:00:00.557894
elapsed time: 0:02:28.903078
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:22:09.141727
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.47
 ---- batch: 020 ----
mean loss: 118.71
 ---- batch: 030 ----
mean loss: 139.45
 ---- batch: 040 ----
mean loss: 131.55
 ---- batch: 050 ----
mean loss: 125.92
 ---- batch: 060 ----
mean loss: 135.45
 ---- batch: 070 ----
mean loss: 129.64
 ---- batch: 080 ----
mean loss: 124.97
 ---- batch: 090 ----
mean loss: 131.32
 ---- batch: 100 ----
mean loss: 128.31
 ---- batch: 110 ----
mean loss: 143.02
train mean loss: 130.94
epoch train time: 0:00:00.566513
elapsed time: 0:02:29.469748
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:22:09.708372
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.06
 ---- batch: 020 ----
mean loss: 126.66
 ---- batch: 030 ----
mean loss: 132.80
 ---- batch: 040 ----
mean loss: 136.91
 ---- batch: 050 ----
mean loss: 131.09
 ---- batch: 060 ----
mean loss: 129.43
 ---- batch: 070 ----
mean loss: 130.86
 ---- batch: 080 ----
mean loss: 127.70
 ---- batch: 090 ----
mean loss: 129.01
 ---- batch: 100 ----
mean loss: 127.95
 ---- batch: 110 ----
mean loss: 126.67
train mean loss: 130.97
epoch train time: 0:00:00.552491
elapsed time: 0:02:30.022394
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:22:10.261020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.88
 ---- batch: 020 ----
mean loss: 134.61
 ---- batch: 030 ----
mean loss: 136.80
 ---- batch: 040 ----
mean loss: 132.25
 ---- batch: 050 ----
mean loss: 134.04
 ---- batch: 060 ----
mean loss: 131.95
 ---- batch: 070 ----
mean loss: 123.40
 ---- batch: 080 ----
mean loss: 129.19
 ---- batch: 090 ----
mean loss: 134.49
 ---- batch: 100 ----
mean loss: 123.04
 ---- batch: 110 ----
mean loss: 129.26
train mean loss: 131.07
epoch train time: 0:00:00.553133
elapsed time: 0:02:30.575663
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:22:10.814287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.29
 ---- batch: 020 ----
mean loss: 125.75
 ---- batch: 030 ----
mean loss: 132.09
 ---- batch: 040 ----
mean loss: 136.31
 ---- batch: 050 ----
mean loss: 130.56
 ---- batch: 060 ----
mean loss: 131.16
 ---- batch: 070 ----
mean loss: 133.17
 ---- batch: 080 ----
mean loss: 132.50
 ---- batch: 090 ----
mean loss: 127.19
 ---- batch: 100 ----
mean loss: 136.35
 ---- batch: 110 ----
mean loss: 127.10
train mean loss: 130.92
epoch train time: 0:00:00.552974
elapsed time: 0:02:31.128766
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:22:11.367389
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.59
 ---- batch: 020 ----
mean loss: 130.04
 ---- batch: 030 ----
mean loss: 122.56
 ---- batch: 040 ----
mean loss: 137.25
 ---- batch: 050 ----
mean loss: 132.82
 ---- batch: 060 ----
mean loss: 135.51
 ---- batch: 070 ----
mean loss: 126.44
 ---- batch: 080 ----
mean loss: 133.64
 ---- batch: 090 ----
mean loss: 123.45
 ---- batch: 100 ----
mean loss: 131.50
 ---- batch: 110 ----
mean loss: 135.03
train mean loss: 130.83
epoch train time: 0:00:00.565416
elapsed time: 0:02:31.694313
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:22:11.932938
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.89
 ---- batch: 020 ----
mean loss: 126.55
 ---- batch: 030 ----
mean loss: 126.63
 ---- batch: 040 ----
mean loss: 126.99
 ---- batch: 050 ----
mean loss: 133.64
 ---- batch: 060 ----
mean loss: 127.17
 ---- batch: 070 ----
mean loss: 140.87
 ---- batch: 080 ----
mean loss: 131.29
 ---- batch: 090 ----
mean loss: 131.70
 ---- batch: 100 ----
mean loss: 134.96
 ---- batch: 110 ----
mean loss: 134.92
train mean loss: 130.97
epoch train time: 0:00:00.561329
elapsed time: 0:02:32.255793
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:22:12.494434
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.90
 ---- batch: 020 ----
mean loss: 130.11
 ---- batch: 030 ----
mean loss: 134.86
 ---- batch: 040 ----
mean loss: 135.60
 ---- batch: 050 ----
mean loss: 125.32
 ---- batch: 060 ----
mean loss: 125.89
 ---- batch: 070 ----
mean loss: 134.83
 ---- batch: 080 ----
mean loss: 126.36
 ---- batch: 090 ----
mean loss: 121.76
 ---- batch: 100 ----
mean loss: 131.86
 ---- batch: 110 ----
mean loss: 133.80
train mean loss: 130.90
epoch train time: 0:00:00.569406
elapsed time: 0:02:32.825360
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:22:13.064006
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 139.37
 ---- batch: 020 ----
mean loss: 130.78
 ---- batch: 030 ----
mean loss: 129.76
 ---- batch: 040 ----
mean loss: 130.92
 ---- batch: 050 ----
mean loss: 126.39
 ---- batch: 060 ----
mean loss: 131.39
 ---- batch: 070 ----
mean loss: 126.62
 ---- batch: 080 ----
mean loss: 132.94
 ---- batch: 090 ----
mean loss: 137.01
 ---- batch: 100 ----
mean loss: 127.54
 ---- batch: 110 ----
mean loss: 128.97
train mean loss: 130.91
epoch train time: 0:00:00.554191
elapsed time: 0:02:33.379718
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:22:13.618343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.52
 ---- batch: 020 ----
mean loss: 130.66
 ---- batch: 030 ----
mean loss: 130.22
 ---- batch: 040 ----
mean loss: 131.94
 ---- batch: 050 ----
mean loss: 129.17
 ---- batch: 060 ----
mean loss: 134.55
 ---- batch: 070 ----
mean loss: 129.86
 ---- batch: 080 ----
mean loss: 133.50
 ---- batch: 090 ----
mean loss: 129.96
 ---- batch: 100 ----
mean loss: 135.92
 ---- batch: 110 ----
mean loss: 127.85
train mean loss: 130.83
epoch train time: 0:00:00.555701
elapsed time: 0:02:33.935550
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:22:14.174174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.04
 ---- batch: 020 ----
mean loss: 135.58
 ---- batch: 030 ----
mean loss: 134.11
 ---- batch: 040 ----
mean loss: 127.52
 ---- batch: 050 ----
mean loss: 124.93
 ---- batch: 060 ----
mean loss: 129.94
 ---- batch: 070 ----
mean loss: 132.96
 ---- batch: 080 ----
mean loss: 128.32
 ---- batch: 090 ----
mean loss: 128.94
 ---- batch: 100 ----
mean loss: 138.01
 ---- batch: 110 ----
mean loss: 127.25
train mean loss: 130.80
epoch train time: 0:00:00.554029
elapsed time: 0:02:34.489708
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:22:14.728330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.33
 ---- batch: 020 ----
mean loss: 127.91
 ---- batch: 030 ----
mean loss: 127.68
 ---- batch: 040 ----
mean loss: 126.64
 ---- batch: 050 ----
mean loss: 130.27
 ---- batch: 060 ----
mean loss: 122.97
 ---- batch: 070 ----
mean loss: 129.89
 ---- batch: 080 ----
mean loss: 135.46
 ---- batch: 090 ----
mean loss: 134.41
 ---- batch: 100 ----
mean loss: 132.29
 ---- batch: 110 ----
mean loss: 136.85
train mean loss: 130.76
epoch train time: 0:00:00.553074
elapsed time: 0:02:35.042915
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:22:15.281541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.99
 ---- batch: 020 ----
mean loss: 132.78
 ---- batch: 030 ----
mean loss: 134.93
 ---- batch: 040 ----
mean loss: 130.68
 ---- batch: 050 ----
mean loss: 137.18
 ---- batch: 060 ----
mean loss: 130.89
 ---- batch: 070 ----
mean loss: 126.99
 ---- batch: 080 ----
mean loss: 128.24
 ---- batch: 090 ----
mean loss: 130.03
 ---- batch: 100 ----
mean loss: 134.32
 ---- batch: 110 ----
mean loss: 132.89
train mean loss: 130.70
epoch train time: 0:00:00.552586
elapsed time: 0:02:35.595662
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:22:15.834287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.42
 ---- batch: 020 ----
mean loss: 125.85
 ---- batch: 030 ----
mean loss: 134.54
 ---- batch: 040 ----
mean loss: 133.52
 ---- batch: 050 ----
mean loss: 127.01
 ---- batch: 060 ----
mean loss: 128.18
 ---- batch: 070 ----
mean loss: 135.47
 ---- batch: 080 ----
mean loss: 137.26
 ---- batch: 090 ----
mean loss: 128.82
 ---- batch: 100 ----
mean loss: 131.76
 ---- batch: 110 ----
mean loss: 131.99
train mean loss: 130.66
epoch train time: 0:00:00.549869
elapsed time: 0:02:36.145664
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:22:16.384321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.26
 ---- batch: 020 ----
mean loss: 132.01
 ---- batch: 030 ----
mean loss: 135.89
 ---- batch: 040 ----
mean loss: 122.32
 ---- batch: 050 ----
mean loss: 131.69
 ---- batch: 060 ----
mean loss: 130.44
 ---- batch: 070 ----
mean loss: 132.40
 ---- batch: 080 ----
mean loss: 126.21
 ---- batch: 090 ----
mean loss: 128.34
 ---- batch: 100 ----
mean loss: 130.22
 ---- batch: 110 ----
mean loss: 130.37
train mean loss: 130.70
epoch train time: 0:00:00.557155
elapsed time: 0:02:36.702983
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:22:16.941628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.72
 ---- batch: 020 ----
mean loss: 132.40
 ---- batch: 030 ----
mean loss: 124.89
 ---- batch: 040 ----
mean loss: 129.52
 ---- batch: 050 ----
mean loss: 133.74
 ---- batch: 060 ----
mean loss: 132.78
 ---- batch: 070 ----
mean loss: 131.46
 ---- batch: 080 ----
mean loss: 132.21
 ---- batch: 090 ----
mean loss: 134.53
 ---- batch: 100 ----
mean loss: 126.84
 ---- batch: 110 ----
mean loss: 125.14
train mean loss: 130.68
epoch train time: 0:00:00.548891
elapsed time: 0:02:37.252029
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:22:17.490667
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.89
 ---- batch: 020 ----
mean loss: 131.20
 ---- batch: 030 ----
mean loss: 126.66
 ---- batch: 040 ----
mean loss: 128.90
 ---- batch: 050 ----
mean loss: 132.39
 ---- batch: 060 ----
mean loss: 133.96
 ---- batch: 070 ----
mean loss: 132.89
 ---- batch: 080 ----
mean loss: 138.92
 ---- batch: 090 ----
mean loss: 128.26
 ---- batch: 100 ----
mean loss: 121.37
 ---- batch: 110 ----
mean loss: 131.51
train mean loss: 130.75
epoch train time: 0:00:00.559655
elapsed time: 0:02:37.811854
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:22:18.050514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.36
 ---- batch: 020 ----
mean loss: 122.77
 ---- batch: 030 ----
mean loss: 130.70
 ---- batch: 040 ----
mean loss: 133.98
 ---- batch: 050 ----
mean loss: 127.74
 ---- batch: 060 ----
mean loss: 137.82
 ---- batch: 070 ----
mean loss: 123.81
 ---- batch: 080 ----
mean loss: 128.01
 ---- batch: 090 ----
mean loss: 133.95
 ---- batch: 100 ----
mean loss: 135.44
 ---- batch: 110 ----
mean loss: 131.48
train mean loss: 130.68
epoch train time: 0:00:00.577288
elapsed time: 0:02:38.389312
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:22:18.627936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.75
 ---- batch: 020 ----
mean loss: 140.85
 ---- batch: 030 ----
mean loss: 131.24
 ---- batch: 040 ----
mean loss: 125.23
 ---- batch: 050 ----
mean loss: 127.01
 ---- batch: 060 ----
mean loss: 120.86
 ---- batch: 070 ----
mean loss: 144.19
 ---- batch: 080 ----
mean loss: 126.75
 ---- batch: 090 ----
mean loss: 133.30
 ---- batch: 100 ----
mean loss: 129.78
 ---- batch: 110 ----
mean loss: 134.39
train mean loss: 130.43
epoch train time: 0:00:00.555373
elapsed time: 0:02:38.944821
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:22:19.183460
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.81
 ---- batch: 020 ----
mean loss: 131.20
 ---- batch: 030 ----
mean loss: 128.53
 ---- batch: 040 ----
mean loss: 123.89
 ---- batch: 050 ----
mean loss: 129.14
 ---- batch: 060 ----
mean loss: 137.77
 ---- batch: 070 ----
mean loss: 136.17
 ---- batch: 080 ----
mean loss: 133.97
 ---- batch: 090 ----
mean loss: 126.50
 ---- batch: 100 ----
mean loss: 134.01
 ---- batch: 110 ----
mean loss: 132.85
train mean loss: 130.51
epoch train time: 0:00:00.553597
elapsed time: 0:02:39.498570
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:22:19.737190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.02
 ---- batch: 020 ----
mean loss: 128.36
 ---- batch: 030 ----
mean loss: 126.28
 ---- batch: 040 ----
mean loss: 135.88
 ---- batch: 050 ----
mean loss: 130.08
 ---- batch: 060 ----
mean loss: 126.22
 ---- batch: 070 ----
mean loss: 135.74
 ---- batch: 080 ----
mean loss: 132.57
 ---- batch: 090 ----
mean loss: 131.50
 ---- batch: 100 ----
mean loss: 126.87
 ---- batch: 110 ----
mean loss: 130.09
train mean loss: 130.45
epoch train time: 0:00:00.553095
elapsed time: 0:02:40.051800
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:22:20.290444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.71
 ---- batch: 020 ----
mean loss: 135.04
 ---- batch: 030 ----
mean loss: 135.49
 ---- batch: 040 ----
mean loss: 135.27
 ---- batch: 050 ----
mean loss: 130.25
 ---- batch: 060 ----
mean loss: 136.37
 ---- batch: 070 ----
mean loss: 127.58
 ---- batch: 080 ----
mean loss: 118.94
 ---- batch: 090 ----
mean loss: 131.45
 ---- batch: 100 ----
mean loss: 132.44
 ---- batch: 110 ----
mean loss: 132.12
train mean loss: 130.45
epoch train time: 0:00:00.566911
elapsed time: 0:02:40.618861
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:22:20.857485
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.07
 ---- batch: 020 ----
mean loss: 131.86
 ---- batch: 030 ----
mean loss: 128.17
 ---- batch: 040 ----
mean loss: 139.96
 ---- batch: 050 ----
mean loss: 127.57
 ---- batch: 060 ----
mean loss: 128.40
 ---- batch: 070 ----
mean loss: 126.35
 ---- batch: 080 ----
mean loss: 130.69
 ---- batch: 090 ----
mean loss: 127.84
 ---- batch: 100 ----
mean loss: 127.88
 ---- batch: 110 ----
mean loss: 132.53
train mean loss: 130.59
epoch train time: 0:00:00.556612
elapsed time: 0:02:41.175610
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:22:21.414236
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.83
 ---- batch: 020 ----
mean loss: 120.03
 ---- batch: 030 ----
mean loss: 126.17
 ---- batch: 040 ----
mean loss: 136.89
 ---- batch: 050 ----
mean loss: 133.11
 ---- batch: 060 ----
mean loss: 121.97
 ---- batch: 070 ----
mean loss: 129.65
 ---- batch: 080 ----
mean loss: 135.13
 ---- batch: 090 ----
mean loss: 133.05
 ---- batch: 100 ----
mean loss: 139.21
 ---- batch: 110 ----
mean loss: 138.08
train mean loss: 130.48
epoch train time: 0:00:00.568678
elapsed time: 0:02:41.744437
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:22:21.983066
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.92
 ---- batch: 020 ----
mean loss: 126.00
 ---- batch: 030 ----
mean loss: 133.79
 ---- batch: 040 ----
mean loss: 130.79
 ---- batch: 050 ----
mean loss: 124.02
 ---- batch: 060 ----
mean loss: 136.68
 ---- batch: 070 ----
mean loss: 132.37
 ---- batch: 080 ----
mean loss: 133.97
 ---- batch: 090 ----
mean loss: 133.87
 ---- batch: 100 ----
mean loss: 125.66
 ---- batch: 110 ----
mean loss: 127.78
train mean loss: 130.44
epoch train time: 0:00:00.556061
elapsed time: 0:02:42.300635
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:22:22.539262
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.44
 ---- batch: 020 ----
mean loss: 122.04
 ---- batch: 030 ----
mean loss: 131.43
 ---- batch: 040 ----
mean loss: 131.44
 ---- batch: 050 ----
mean loss: 127.43
 ---- batch: 060 ----
mean loss: 136.69
 ---- batch: 070 ----
mean loss: 133.10
 ---- batch: 080 ----
mean loss: 132.88
 ---- batch: 090 ----
mean loss: 128.99
 ---- batch: 100 ----
mean loss: 131.56
 ---- batch: 110 ----
mean loss: 130.18
train mean loss: 130.46
epoch train time: 0:00:00.551816
elapsed time: 0:02:42.852634
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:22:23.091272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.53
 ---- batch: 020 ----
mean loss: 133.97
 ---- batch: 030 ----
mean loss: 124.55
 ---- batch: 040 ----
mean loss: 134.10
 ---- batch: 050 ----
mean loss: 128.42
 ---- batch: 060 ----
mean loss: 131.05
 ---- batch: 070 ----
mean loss: 129.47
 ---- batch: 080 ----
mean loss: 132.29
 ---- batch: 090 ----
mean loss: 134.30
 ---- batch: 100 ----
mean loss: 129.23
 ---- batch: 110 ----
mean loss: 130.16
train mean loss: 130.28
epoch train time: 0:00:00.549210
elapsed time: 0:02:43.401988
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:22:23.640612
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.72
 ---- batch: 020 ----
mean loss: 121.03
 ---- batch: 030 ----
mean loss: 123.76
 ---- batch: 040 ----
mean loss: 142.87
 ---- batch: 050 ----
mean loss: 124.36
 ---- batch: 060 ----
mean loss: 136.51
 ---- batch: 070 ----
mean loss: 132.43
 ---- batch: 080 ----
mean loss: 127.17
 ---- batch: 090 ----
mean loss: 129.40
 ---- batch: 100 ----
mean loss: 132.86
 ---- batch: 110 ----
mean loss: 127.93
train mean loss: 130.33
epoch train time: 0:00:00.557897
elapsed time: 0:02:43.960015
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:22:24.198665
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.16
 ---- batch: 020 ----
mean loss: 134.37
 ---- batch: 030 ----
mean loss: 127.15
 ---- batch: 040 ----
mean loss: 133.37
 ---- batch: 050 ----
mean loss: 130.12
 ---- batch: 060 ----
mean loss: 141.66
 ---- batch: 070 ----
mean loss: 130.89
 ---- batch: 080 ----
mean loss: 127.48
 ---- batch: 090 ----
mean loss: 124.86
 ---- batch: 100 ----
mean loss: 130.94
 ---- batch: 110 ----
mean loss: 124.56
train mean loss: 130.43
epoch train time: 0:00:00.559830
elapsed time: 0:02:44.520007
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:22:24.758649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.65
 ---- batch: 020 ----
mean loss: 139.40
 ---- batch: 030 ----
mean loss: 128.52
 ---- batch: 040 ----
mean loss: 127.46
 ---- batch: 050 ----
mean loss: 128.93
 ---- batch: 060 ----
mean loss: 133.55
 ---- batch: 070 ----
mean loss: 123.75
 ---- batch: 080 ----
mean loss: 130.13
 ---- batch: 090 ----
mean loss: 131.26
 ---- batch: 100 ----
mean loss: 129.82
 ---- batch: 110 ----
mean loss: 126.64
train mean loss: 130.30
epoch train time: 0:00:00.557832
elapsed time: 0:02:45.078022
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:22:25.316648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.42
 ---- batch: 020 ----
mean loss: 137.65
 ---- batch: 030 ----
mean loss: 127.53
 ---- batch: 040 ----
mean loss: 132.48
 ---- batch: 050 ----
mean loss: 132.46
 ---- batch: 060 ----
mean loss: 134.40
 ---- batch: 070 ----
mean loss: 134.11
 ---- batch: 080 ----
mean loss: 121.93
 ---- batch: 090 ----
mean loss: 125.76
 ---- batch: 100 ----
mean loss: 133.00
 ---- batch: 110 ----
mean loss: 125.38
train mean loss: 130.24
epoch train time: 0:00:00.559259
elapsed time: 0:02:45.637417
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:22:25.876043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.36
 ---- batch: 020 ----
mean loss: 134.45
 ---- batch: 030 ----
mean loss: 131.82
 ---- batch: 040 ----
mean loss: 132.03
 ---- batch: 050 ----
mean loss: 131.55
 ---- batch: 060 ----
mean loss: 139.17
 ---- batch: 070 ----
mean loss: 134.01
 ---- batch: 080 ----
mean loss: 133.37
 ---- batch: 090 ----
mean loss: 123.91
 ---- batch: 100 ----
mean loss: 126.38
 ---- batch: 110 ----
mean loss: 121.05
train mean loss: 130.18
epoch train time: 0:00:00.546412
elapsed time: 0:02:46.183964
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:22:26.422589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.22
 ---- batch: 020 ----
mean loss: 126.85
 ---- batch: 030 ----
mean loss: 134.95
 ---- batch: 040 ----
mean loss: 125.74
 ---- batch: 050 ----
mean loss: 124.57
 ---- batch: 060 ----
mean loss: 125.21
 ---- batch: 070 ----
mean loss: 132.46
 ---- batch: 080 ----
mean loss: 128.56
 ---- batch: 090 ----
mean loss: 144.83
 ---- batch: 100 ----
mean loss: 126.77
 ---- batch: 110 ----
mean loss: 134.55
train mean loss: 130.29
epoch train time: 0:00:00.561365
elapsed time: 0:02:46.745465
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:22:26.984092
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.76
 ---- batch: 020 ----
mean loss: 128.11
 ---- batch: 030 ----
mean loss: 123.66
 ---- batch: 040 ----
mean loss: 128.78
 ---- batch: 050 ----
mean loss: 133.32
 ---- batch: 060 ----
mean loss: 134.45
 ---- batch: 070 ----
mean loss: 132.87
 ---- batch: 080 ----
mean loss: 131.81
 ---- batch: 090 ----
mean loss: 133.24
 ---- batch: 100 ----
mean loss: 135.71
 ---- batch: 110 ----
mean loss: 125.76
train mean loss: 130.12
epoch train time: 0:00:00.557369
elapsed time: 0:02:47.302969
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:22:27.541634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.38
 ---- batch: 020 ----
mean loss: 136.81
 ---- batch: 030 ----
mean loss: 135.48
 ---- batch: 040 ----
mean loss: 121.30
 ---- batch: 050 ----
mean loss: 130.93
 ---- batch: 060 ----
mean loss: 136.32
 ---- batch: 070 ----
mean loss: 129.49
 ---- batch: 080 ----
mean loss: 119.84
 ---- batch: 090 ----
mean loss: 122.99
 ---- batch: 100 ----
mean loss: 137.38
 ---- batch: 110 ----
mean loss: 134.92
train mean loss: 130.17
epoch train time: 0:00:00.556362
elapsed time: 0:02:47.859502
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:22:28.098127
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.73
 ---- batch: 020 ----
mean loss: 137.60
 ---- batch: 030 ----
mean loss: 124.80
 ---- batch: 040 ----
mean loss: 136.03
 ---- batch: 050 ----
mean loss: 134.92
 ---- batch: 060 ----
mean loss: 126.92
 ---- batch: 070 ----
mean loss: 132.38
 ---- batch: 080 ----
mean loss: 126.43
 ---- batch: 090 ----
mean loss: 123.56
 ---- batch: 100 ----
mean loss: 135.73
 ---- batch: 110 ----
mean loss: 129.89
train mean loss: 130.23
epoch train time: 0:00:00.551653
elapsed time: 0:02:48.411286
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:22:28.649924
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.07
 ---- batch: 020 ----
mean loss: 121.76
 ---- batch: 030 ----
mean loss: 128.82
 ---- batch: 040 ----
mean loss: 132.21
 ---- batch: 050 ----
mean loss: 129.98
 ---- batch: 060 ----
mean loss: 130.97
 ---- batch: 070 ----
mean loss: 134.90
 ---- batch: 080 ----
mean loss: 136.49
 ---- batch: 090 ----
mean loss: 122.32
 ---- batch: 100 ----
mean loss: 133.44
 ---- batch: 110 ----
mean loss: 133.59
train mean loss: 130.15
epoch train time: 0:00:00.561100
elapsed time: 0:02:48.972537
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:22:29.211163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.90
 ---- batch: 020 ----
mean loss: 127.06
 ---- batch: 030 ----
mean loss: 131.23
 ---- batch: 040 ----
mean loss: 133.93
 ---- batch: 050 ----
mean loss: 128.23
 ---- batch: 060 ----
mean loss: 130.57
 ---- batch: 070 ----
mean loss: 132.57
 ---- batch: 080 ----
mean loss: 123.20
 ---- batch: 090 ----
mean loss: 129.44
 ---- batch: 100 ----
mean loss: 137.46
 ---- batch: 110 ----
mean loss: 134.35
train mean loss: 130.22
epoch train time: 0:00:00.578084
elapsed time: 0:02:49.550754
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:22:29.789379
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.16
 ---- batch: 020 ----
mean loss: 134.18
 ---- batch: 030 ----
mean loss: 126.91
 ---- batch: 040 ----
mean loss: 124.60
 ---- batch: 050 ----
mean loss: 131.50
 ---- batch: 060 ----
mean loss: 131.56
 ---- batch: 070 ----
mean loss: 130.74
 ---- batch: 080 ----
mean loss: 130.16
 ---- batch: 090 ----
mean loss: 127.33
 ---- batch: 100 ----
mean loss: 132.97
 ---- batch: 110 ----
mean loss: 125.27
train mean loss: 130.10
epoch train time: 0:00:00.578536
elapsed time: 0:02:50.129440
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:22:30.368065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.38
 ---- batch: 020 ----
mean loss: 127.70
 ---- batch: 030 ----
mean loss: 130.57
 ---- batch: 040 ----
mean loss: 125.74
 ---- batch: 050 ----
mean loss: 129.28
 ---- batch: 060 ----
mean loss: 133.18
 ---- batch: 070 ----
mean loss: 131.75
 ---- batch: 080 ----
mean loss: 126.47
 ---- batch: 090 ----
mean loss: 127.94
 ---- batch: 100 ----
mean loss: 135.36
 ---- batch: 110 ----
mean loss: 141.22
train mean loss: 129.94
epoch train time: 0:00:00.580065
elapsed time: 0:02:50.709645
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:22:30.948271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.97
 ---- batch: 020 ----
mean loss: 126.35
 ---- batch: 030 ----
mean loss: 126.60
 ---- batch: 040 ----
mean loss: 129.29
 ---- batch: 050 ----
mean loss: 127.79
 ---- batch: 060 ----
mean loss: 128.48
 ---- batch: 070 ----
mean loss: 126.17
 ---- batch: 080 ----
mean loss: 130.11
 ---- batch: 090 ----
mean loss: 135.77
 ---- batch: 100 ----
mean loss: 136.74
 ---- batch: 110 ----
mean loss: 131.29
train mean loss: 130.01
epoch train time: 0:00:00.574417
elapsed time: 0:02:51.284201
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:22:31.522846
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.46
 ---- batch: 020 ----
mean loss: 127.61
 ---- batch: 030 ----
mean loss: 125.48
 ---- batch: 040 ----
mean loss: 128.00
 ---- batch: 050 ----
mean loss: 126.96
 ---- batch: 060 ----
mean loss: 136.13
 ---- batch: 070 ----
mean loss: 130.37
 ---- batch: 080 ----
mean loss: 129.68
 ---- batch: 090 ----
mean loss: 131.04
 ---- batch: 100 ----
mean loss: 135.86
 ---- batch: 110 ----
mean loss: 127.79
train mean loss: 129.94
epoch train time: 0:00:00.572071
elapsed time: 0:02:51.859647
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_5/checkpoint.pth.tar
**** end time: 2019-09-27 02:22:32.098241 ****
