Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 9311
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 17:14:51.230685 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 17:14:51.248062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2541.52
 ---- batch: 020 ----
mean loss: 1538.20
train mean loss: 1791.79
epoch train time: 0:00:11.460087
elapsed time: 0:00:11.486315
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 17:15:02.717052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1205.58
 ---- batch: 020 ----
mean loss: 1123.52
train mean loss: 1151.95
epoch train time: 0:00:03.928172
elapsed time: 0:00:15.415704
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 17:15:06.646629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1080.88
 ---- batch: 020 ----
mean loss: 1039.06
train mean loss: 1051.32
epoch train time: 0:00:03.946126
elapsed time: 0:00:19.363304
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 17:15:10.594265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1028.77
 ---- batch: 020 ----
mean loss: 1027.97
train mean loss: 1028.64
epoch train time: 0:00:03.943509
elapsed time: 0:00:23.308204
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 17:15:14.539177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 993.95
 ---- batch: 020 ----
mean loss: 1017.54
train mean loss: 991.62
epoch train time: 0:00:03.942461
elapsed time: 0:00:27.251969
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 17:15:18.482902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.06
 ---- batch: 020 ----
mean loss: 969.05
train mean loss: 969.89
epoch train time: 0:00:03.936546
elapsed time: 0:00:31.189847
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 17:15:22.420754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 953.35
 ---- batch: 020 ----
mean loss: 977.25
train mean loss: 961.58
epoch train time: 0:00:03.944232
elapsed time: 0:00:35.136373
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 17:15:26.367310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.50
 ---- batch: 020 ----
mean loss: 964.54
train mean loss: 950.98
epoch train time: 0:00:03.960283
elapsed time: 0:00:39.097882
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 17:15:30.328781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.43
 ---- batch: 020 ----
mean loss: 933.88
train mean loss: 946.77
epoch train time: 0:00:03.934725
elapsed time: 0:00:43.033804
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 17:15:34.264718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.78
 ---- batch: 020 ----
mean loss: 935.63
train mean loss: 939.70
epoch train time: 0:00:03.945026
elapsed time: 0:00:46.980047
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 17:15:38.211040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.14
 ---- batch: 020 ----
mean loss: 941.77
train mean loss: 931.31
epoch train time: 0:00:03.940577
elapsed time: 0:00:50.921961
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 17:15:42.152876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.30
 ---- batch: 020 ----
mean loss: 926.49
train mean loss: 925.59
epoch train time: 0:00:03.952191
elapsed time: 0:00:54.875536
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 17:15:46.106465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.34
 ---- batch: 020 ----
mean loss: 938.63
train mean loss: 926.80
epoch train time: 0:00:03.929697
elapsed time: 0:00:58.806452
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 17:15:50.037377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.73
 ---- batch: 020 ----
mean loss: 928.52
train mean loss: 926.04
epoch train time: 0:00:03.934786
elapsed time: 0:01:02.742624
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 17:15:53.973512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.79
 ---- batch: 020 ----
mean loss: 924.84
train mean loss: 924.19
epoch train time: 0:00:03.945229
elapsed time: 0:01:06.689114
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 17:15:57.920046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.56
 ---- batch: 020 ----
mean loss: 894.45
train mean loss: 909.42
epoch train time: 0:00:03.944435
elapsed time: 0:01:10.634871
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 17:16:01.865886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.28
 ---- batch: 020 ----
mean loss: 906.38
train mean loss: 913.07
epoch train time: 0:00:03.934047
elapsed time: 0:01:14.570288
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 17:16:05.801210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.03
 ---- batch: 020 ----
mean loss: 899.27
train mean loss: 912.33
epoch train time: 0:00:03.927672
elapsed time: 0:01:18.499349
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 17:16:09.730308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.31
 ---- batch: 020 ----
mean loss: 919.42
train mean loss: 922.15
epoch train time: 0:00:03.928418
elapsed time: 0:01:22.429183
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 17:16:13.660126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.83
 ---- batch: 020 ----
mean loss: 908.74
train mean loss: 901.50
epoch train time: 0:00:03.929796
elapsed time: 0:01:26.360370
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 17:16:17.591273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.95
 ---- batch: 020 ----
mean loss: 895.04
train mean loss: 908.92
epoch train time: 0:00:03.919539
elapsed time: 0:01:30.281132
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 17:16:21.512055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.62
 ---- batch: 020 ----
mean loss: 895.38
train mean loss: 901.13
epoch train time: 0:00:03.926885
elapsed time: 0:01:34.209246
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 17:16:25.440207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.58
 ---- batch: 020 ----
mean loss: 893.07
train mean loss: 902.62
epoch train time: 0:00:03.944764
elapsed time: 0:01:38.155590
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 17:16:29.386516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.06
 ---- batch: 020 ----
mean loss: 895.82
train mean loss: 901.16
epoch train time: 0:00:03.949246
elapsed time: 0:01:42.106087
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 17:16:33.337047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.28
 ---- batch: 020 ----
mean loss: 863.73
train mean loss: 892.32
epoch train time: 0:00:03.930862
elapsed time: 0:01:46.038264
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 17:16:37.269179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.66
 ---- batch: 020 ----
mean loss: 914.56
train mean loss: 902.75
epoch train time: 0:00:03.930205
elapsed time: 0:01:49.970132
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 17:16:41.201147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.57
 ---- batch: 020 ----
mean loss: 870.58
train mean loss: 895.60
epoch train time: 0:00:03.936829
elapsed time: 0:01:53.908609
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 17:16:45.139526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.48
 ---- batch: 020 ----
mean loss: 889.01
train mean loss: 895.51
epoch train time: 0:00:03.940724
elapsed time: 0:01:57.850541
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 17:16:49.081483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.79
 ---- batch: 020 ----
mean loss: 897.23
train mean loss: 885.56
epoch train time: 0:00:03.941229
elapsed time: 0:02:01.793013
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 17:16:53.023931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.76
 ---- batch: 020 ----
mean loss: 898.53
train mean loss: 893.88
epoch train time: 0:00:03.925629
elapsed time: 0:02:05.719948
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 17:16:56.950910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.98
 ---- batch: 020 ----
mean loss: 881.82
train mean loss: 878.67
epoch train time: 0:00:03.924669
elapsed time: 0:02:09.646047
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 17:17:00.876975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.47
 ---- batch: 020 ----
mean loss: 899.68
train mean loss: 886.75
epoch train time: 0:00:03.926766
elapsed time: 0:02:13.574097
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 17:17:04.805015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.69
 ---- batch: 020 ----
mean loss: 880.30
train mean loss: 885.35
epoch train time: 0:00:03.926275
elapsed time: 0:02:17.501590
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 17:17:08.732496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.38
 ---- batch: 020 ----
mean loss: 873.93
train mean loss: 866.88
epoch train time: 0:00:03.942415
elapsed time: 0:02:21.445234
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 17:17:12.676173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.47
 ---- batch: 020 ----
mean loss: 883.22
train mean loss: 882.44
epoch train time: 0:00:03.933207
elapsed time: 0:02:25.379739
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 17:17:16.610661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.28
 ---- batch: 020 ----
mean loss: 885.91
train mean loss: 872.13
epoch train time: 0:00:03.928004
elapsed time: 0:02:29.308951
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 17:17:20.539886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.36
 ---- batch: 020 ----
mean loss: 878.95
train mean loss: 873.39
epoch train time: 0:00:03.935954
elapsed time: 0:02:33.246375
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 17:17:24.477327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.97
 ---- batch: 020 ----
mean loss: 867.91
train mean loss: 874.09
epoch train time: 0:00:03.945139
elapsed time: 0:02:37.192798
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 17:17:28.423721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.13
 ---- batch: 020 ----
mean loss: 870.25
train mean loss: 872.68
epoch train time: 0:00:03.937920
elapsed time: 0:02:41.131974
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 17:17:32.362910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.08
 ---- batch: 020 ----
mean loss: 878.50
train mean loss: 877.67
epoch train time: 0:00:03.940787
elapsed time: 0:02:45.074064
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 17:17:36.305010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.51
 ---- batch: 020 ----
mean loss: 869.82
train mean loss: 869.89
epoch train time: 0:00:03.946265
elapsed time: 0:02:49.021635
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 17:17:40.252573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.86
 ---- batch: 020 ----
mean loss: 868.75
train mean loss: 860.00
epoch train time: 0:00:03.941353
elapsed time: 0:02:52.964190
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 17:17:44.195129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.03
 ---- batch: 020 ----
mean loss: 856.50
train mean loss: 858.08
epoch train time: 0:00:03.929083
elapsed time: 0:02:56.894682
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 17:17:48.125596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.92
 ---- batch: 020 ----
mean loss: 864.32
train mean loss: 857.47
epoch train time: 0:00:03.919613
elapsed time: 0:03:00.815573
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 17:17:52.046486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.25
 ---- batch: 020 ----
mean loss: 876.57
train mean loss: 857.48
epoch train time: 0:00:03.937981
elapsed time: 0:03:04.754808
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 17:17:55.985754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.49
 ---- batch: 020 ----
mean loss: 835.76
train mean loss: 848.66
epoch train time: 0:00:03.953993
elapsed time: 0:03:08.710064
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 17:17:59.940974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.26
 ---- batch: 020 ----
mean loss: 821.60
train mean loss: 847.88
epoch train time: 0:00:03.934708
elapsed time: 0:03:12.646075
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 17:18:03.877030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.05
 ---- batch: 020 ----
mean loss: 841.68
train mean loss: 848.54
epoch train time: 0:00:03.929429
elapsed time: 0:03:16.576860
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 17:18:07.807805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.57
 ---- batch: 020 ----
mean loss: 860.96
train mean loss: 839.58
epoch train time: 0:00:03.937559
elapsed time: 0:03:20.515722
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 17:18:11.746669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.11
 ---- batch: 020 ----
mean loss: 841.53
train mean loss: 833.99
epoch train time: 0:00:03.960251
elapsed time: 0:03:24.477315
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 17:18:15.708227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.89
 ---- batch: 020 ----
mean loss: 828.83
train mean loss: 829.92
epoch train time: 0:00:03.914971
elapsed time: 0:03:28.393516
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 17:18:19.624439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.01
 ---- batch: 020 ----
mean loss: 811.17
train mean loss: 824.81
epoch train time: 0:00:03.935209
elapsed time: 0:03:32.329986
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 17:18:23.560966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.20
 ---- batch: 020 ----
mean loss: 813.03
train mean loss: 821.33
epoch train time: 0:00:03.931048
elapsed time: 0:03:36.262427
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 17:18:27.493370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 814.02
 ---- batch: 020 ----
mean loss: 817.69
train mean loss: 820.34
epoch train time: 0:00:03.944518
elapsed time: 0:03:40.208216
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 17:18:31.439179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 788.42
 ---- batch: 020 ----
mean loss: 797.95
train mean loss: 799.56
epoch train time: 0:00:03.933635
elapsed time: 0:03:44.143183
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 17:18:35.374140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.77
 ---- batch: 020 ----
mean loss: 802.18
train mean loss: 791.32
epoch train time: 0:00:03.915057
elapsed time: 0:03:48.059583
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 17:18:39.290493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.15
 ---- batch: 020 ----
mean loss: 778.56
train mean loss: 770.74
epoch train time: 0:00:03.939881
elapsed time: 0:03:52.000658
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 17:18:43.231564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 748.86
 ---- batch: 020 ----
mean loss: 760.44
train mean loss: 751.31
epoch train time: 0:00:03.958974
elapsed time: 0:03:55.960817
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 17:18:47.191738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 753.31
 ---- batch: 020 ----
mean loss: 727.42
train mean loss: 736.84
epoch train time: 0:00:03.966861
elapsed time: 0:03:59.928969
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 17:18:51.159891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 721.22
 ---- batch: 020 ----
mean loss: 706.47
train mean loss: 714.34
epoch train time: 0:00:03.947953
elapsed time: 0:04:03.878297
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 17:18:55.109201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.06
 ---- batch: 020 ----
mean loss: 697.28
train mean loss: 697.71
epoch train time: 0:00:03.924047
elapsed time: 0:04:07.803561
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 17:18:59.034499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.32
 ---- batch: 020 ----
mean loss: 694.12
train mean loss: 684.64
epoch train time: 0:00:03.936175
elapsed time: 0:04:11.741057
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 17:19:02.971979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.58
 ---- batch: 020 ----
mean loss: 674.33
train mean loss: 668.23
epoch train time: 0:00:03.932749
elapsed time: 0:04:15.675247
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 17:19:06.906225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.31
 ---- batch: 020 ----
mean loss: 652.97
train mean loss: 653.36
epoch train time: 0:00:03.930365
elapsed time: 0:04:19.606883
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 17:19:10.837843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.48
 ---- batch: 020 ----
mean loss: 636.83
train mean loss: 650.73
epoch train time: 0:00:03.932119
elapsed time: 0:04:23.540299
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 17:19:14.771295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 642.00
 ---- batch: 020 ----
mean loss: 635.37
train mean loss: 640.29
epoch train time: 0:00:03.942095
elapsed time: 0:04:27.483733
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 17:19:18.714695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 640.38
 ---- batch: 020 ----
mean loss: 626.98
train mean loss: 630.10
epoch train time: 0:00:03.935941
elapsed time: 0:04:31.420972
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 17:19:22.651880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 618.42
 ---- batch: 020 ----
mean loss: 606.33
train mean loss: 605.51
epoch train time: 0:00:03.939417
elapsed time: 0:04:35.361557
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 17:19:26.592499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.23
 ---- batch: 020 ----
mean loss: 613.77
train mean loss: 597.03
epoch train time: 0:00:03.933629
elapsed time: 0:04:39.296549
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 17:19:30.527499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.88
 ---- batch: 020 ----
mean loss: 593.93
train mean loss: 585.50
epoch train time: 0:00:03.935179
elapsed time: 0:04:43.232957
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 17:19:34.463906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 581.47
 ---- batch: 020 ----
mean loss: 578.80
train mean loss: 582.41
epoch train time: 0:00:03.946555
elapsed time: 0:04:47.180796
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 17:19:38.411716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.93
 ---- batch: 020 ----
mean loss: 546.30
train mean loss: 564.82
epoch train time: 0:00:03.937195
elapsed time: 0:04:51.119186
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 17:19:42.350130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.10
 ---- batch: 020 ----
mean loss: 575.45
train mean loss: 558.78
epoch train time: 0:00:03.952793
elapsed time: 0:04:55.073338
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 17:19:46.304328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 540.22
 ---- batch: 020 ----
mean loss: 565.03
train mean loss: 552.45
epoch train time: 0:00:03.974105
elapsed time: 0:04:59.048772
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 17:19:50.279691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.34
 ---- batch: 020 ----
mean loss: 547.42
train mean loss: 540.14
epoch train time: 0:00:03.940373
elapsed time: 0:05:02.990443
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 17:19:54.221374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 534.07
 ---- batch: 020 ----
mean loss: 523.08
train mean loss: 523.42
epoch train time: 0:00:03.969896
elapsed time: 0:05:06.961609
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 17:19:58.192527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.34
 ---- batch: 020 ----
mean loss: 514.97
train mean loss: 513.94
epoch train time: 0:00:03.926751
elapsed time: 0:05:10.889586
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 17:20:02.120502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.02
 ---- batch: 020 ----
mean loss: 521.55
train mean loss: 512.49
epoch train time: 0:00:03.914083
elapsed time: 0:05:14.804881
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 17:20:06.035816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.13
 ---- batch: 020 ----
mean loss: 515.26
train mean loss: 503.72
epoch train time: 0:00:03.926577
elapsed time: 0:05:18.732765
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 17:20:09.963677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.77
 ---- batch: 020 ----
mean loss: 517.28
train mean loss: 497.82
epoch train time: 0:00:03.933086
elapsed time: 0:05:22.667291
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 17:20:13.898206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.87
 ---- batch: 020 ----
mean loss: 490.80
train mean loss: 486.64
epoch train time: 0:00:03.944348
elapsed time: 0:05:26.612887
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 17:20:17.843892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.25
 ---- batch: 020 ----
mean loss: 494.61
train mean loss: 482.26
epoch train time: 0:00:03.929214
elapsed time: 0:05:30.543431
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 17:20:21.774327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.60
 ---- batch: 020 ----
mean loss: 475.22
train mean loss: 477.56
epoch train time: 0:00:03.927820
elapsed time: 0:05:34.472587
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 17:20:25.703515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.06
 ---- batch: 020 ----
mean loss: 455.88
train mean loss: 463.63
epoch train time: 0:00:03.926229
elapsed time: 0:05:38.400007
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 17:20:29.630918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.69
 ---- batch: 020 ----
mean loss: 458.61
train mean loss: 459.19
epoch train time: 0:00:03.935675
elapsed time: 0:05:42.336988
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 17:20:33.567895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.98
 ---- batch: 020 ----
mean loss: 451.85
train mean loss: 452.61
epoch train time: 0:00:03.918370
elapsed time: 0:05:46.256517
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 17:20:37.487452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.86
 ---- batch: 020 ----
mean loss: 465.27
train mean loss: 448.95
epoch train time: 0:00:03.931247
elapsed time: 0:05:50.189042
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 17:20:41.419949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.91
 ---- batch: 020 ----
mean loss: 450.10
train mean loss: 446.39
epoch train time: 0:00:03.936132
elapsed time: 0:05:54.126368
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 17:20:45.357309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.39
 ---- batch: 020 ----
mean loss: 431.84
train mean loss: 443.76
epoch train time: 0:00:03.925111
elapsed time: 0:05:58.052774
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 17:20:49.283703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.64
 ---- batch: 020 ----
mean loss: 437.95
train mean loss: 439.37
epoch train time: 0:00:03.924689
elapsed time: 0:06:01.978697
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 17:20:53.209637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.74
 ---- batch: 020 ----
mean loss: 424.84
train mean loss: 421.75
epoch train time: 0:00:03.922527
elapsed time: 0:06:05.902574
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 17:20:57.133487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.15
 ---- batch: 020 ----
mean loss: 427.47
train mean loss: 425.98
epoch train time: 0:00:03.918487
elapsed time: 0:06:09.822286
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 17:21:01.053247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.78
 ---- batch: 020 ----
mean loss: 430.22
train mean loss: 427.20
epoch train time: 0:00:03.923354
elapsed time: 0:06:13.746944
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 17:21:04.977871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.95
 ---- batch: 020 ----
mean loss: 409.80
train mean loss: 411.55
epoch train time: 0:00:03.915304
elapsed time: 0:06:17.663605
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 17:21:08.894525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.17
 ---- batch: 020 ----
mean loss: 403.03
train mean loss: 408.81
epoch train time: 0:00:03.914432
elapsed time: 0:06:21.579331
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 17:21:12.810251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.72
 ---- batch: 020 ----
mean loss: 391.21
train mean loss: 406.83
epoch train time: 0:00:03.916585
elapsed time: 0:06:25.497135
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 17:21:16.728073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.93
 ---- batch: 020 ----
mean loss: 400.96
train mean loss: 402.27
epoch train time: 0:00:03.895578
elapsed time: 0:06:29.393935
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 17:21:20.624839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.54
 ---- batch: 020 ----
mean loss: 394.67
train mean loss: 393.09
epoch train time: 0:00:03.919718
elapsed time: 0:06:33.314954
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 17:21:24.545881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.53
 ---- batch: 020 ----
mean loss: 401.19
train mean loss: 395.74
epoch train time: 0:00:03.907747
elapsed time: 0:06:37.224252
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 17:21:28.455220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.59
 ---- batch: 020 ----
mean loss: 383.79
train mean loss: 387.66
epoch train time: 0:00:03.922312
elapsed time: 0:06:41.147853
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 17:21:32.378805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.30
 ---- batch: 020 ----
mean loss: 388.49
train mean loss: 386.20
epoch train time: 0:00:03.919122
elapsed time: 0:06:45.068290
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 17:21:36.299198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.12
 ---- batch: 020 ----
mean loss: 380.93
train mean loss: 392.44
epoch train time: 0:00:03.907428
elapsed time: 0:06:48.976994
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 17:21:40.207911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.36
 ---- batch: 020 ----
mean loss: 377.48
train mean loss: 381.19
epoch train time: 0:00:03.904794
elapsed time: 0:06:52.883101
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 17:21:44.114056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.53
 ---- batch: 020 ----
mean loss: 383.61
train mean loss: 372.15
epoch train time: 0:00:03.916007
elapsed time: 0:06:56.800476
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 17:21:48.031380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.81
 ---- batch: 020 ----
mean loss: 369.72
train mean loss: 380.40
epoch train time: 0:00:03.924142
elapsed time: 0:07:00.725836
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 17:21:51.956754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.59
 ---- batch: 020 ----
mean loss: 371.58
train mean loss: 376.93
epoch train time: 0:00:03.922760
elapsed time: 0:07:04.649816
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 17:21:55.880735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.29
 ---- batch: 020 ----
mean loss: 373.99
train mean loss: 369.75
epoch train time: 0:00:03.928683
elapsed time: 0:07:08.580382
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 17:21:59.811128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.24
 ---- batch: 020 ----
mean loss: 374.68
train mean loss: 366.84
epoch train time: 0:00:03.911959
elapsed time: 0:07:12.493404
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 17:22:03.724307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.95
 ---- batch: 020 ----
mean loss: 375.57
train mean loss: 366.41
epoch train time: 0:00:03.916041
elapsed time: 0:07:16.410598
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 17:22:07.641527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.09
 ---- batch: 020 ----
mean loss: 363.25
train mean loss: 362.25
epoch train time: 0:00:03.917658
elapsed time: 0:07:20.329488
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 17:22:11.560394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.38
 ---- batch: 020 ----
mean loss: 360.92
train mean loss: 360.64
epoch train time: 0:00:03.914804
elapsed time: 0:07:24.245552
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 17:22:15.476450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.15
 ---- batch: 020 ----
mean loss: 350.09
train mean loss: 354.66
epoch train time: 0:00:03.933139
elapsed time: 0:07:28.179919
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 17:22:19.410844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.97
 ---- batch: 020 ----
mean loss: 355.93
train mean loss: 353.36
epoch train time: 0:00:03.950200
elapsed time: 0:07:32.131333
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 17:22:23.362240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.19
 ---- batch: 020 ----
mean loss: 353.31
train mean loss: 353.39
epoch train time: 0:00:03.936363
elapsed time: 0:07:36.069096
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 17:22:27.300021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.96
 ---- batch: 020 ----
mean loss: 344.68
train mean loss: 347.17
epoch train time: 0:00:03.925792
elapsed time: 0:07:39.996359
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 17:22:31.227289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.95
 ---- batch: 020 ----
mean loss: 360.33
train mean loss: 348.36
epoch train time: 0:00:03.916767
elapsed time: 0:07:43.914441
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 17:22:35.145387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.68
 ---- batch: 020 ----
mean loss: 349.83
train mean loss: 350.71
epoch train time: 0:00:03.926766
elapsed time: 0:07:47.842515
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 17:22:39.073549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.42
 ---- batch: 020 ----
mean loss: 350.16
train mean loss: 344.90
epoch train time: 0:00:03.919866
elapsed time: 0:07:51.764016
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 17:22:42.994935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.45
 ---- batch: 020 ----
mean loss: 347.02
train mean loss: 343.09
epoch train time: 0:00:03.914371
elapsed time: 0:07:55.679621
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 17:22:46.910525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.62
 ---- batch: 020 ----
mean loss: 347.46
train mean loss: 343.63
epoch train time: 0:00:03.916106
elapsed time: 0:07:59.597002
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 17:22:50.827912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.78
 ---- batch: 020 ----
mean loss: 337.86
train mean loss: 337.73
epoch train time: 0:00:03.912716
elapsed time: 0:08:03.510971
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 17:22:54.741869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.17
 ---- batch: 020 ----
mean loss: 342.75
train mean loss: 336.41
epoch train time: 0:00:03.923802
elapsed time: 0:08:07.436137
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 17:22:58.667160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.09
 ---- batch: 020 ----
mean loss: 335.48
train mean loss: 329.55
epoch train time: 0:00:03.919145
elapsed time: 0:08:11.356675
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 17:23:02.587570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.26
 ---- batch: 020 ----
mean loss: 344.49
train mean loss: 329.74
epoch train time: 0:00:03.920539
elapsed time: 0:08:15.278396
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 17:23:06.509315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.40
 ---- batch: 020 ----
mean loss: 334.15
train mean loss: 330.54
epoch train time: 0:00:03.919167
elapsed time: 0:08:19.198705
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 17:23:10.429598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.42
 ---- batch: 020 ----
mean loss: 324.94
train mean loss: 332.28
epoch train time: 0:00:03.928105
elapsed time: 0:08:23.128131
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 17:23:14.359081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.78
 ---- batch: 020 ----
mean loss: 332.75
train mean loss: 325.29
epoch train time: 0:00:03.944828
elapsed time: 0:08:27.074668
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 17:23:18.305444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.63
 ---- batch: 020 ----
mean loss: 322.15
train mean loss: 324.64
epoch train time: 0:00:03.936376
elapsed time: 0:08:31.012070
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 17:23:22.242986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.37
 ---- batch: 020 ----
mean loss: 321.79
train mean loss: 323.30
epoch train time: 0:00:03.933396
elapsed time: 0:08:34.946706
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 17:23:26.177640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.81
 ---- batch: 020 ----
mean loss: 321.73
train mean loss: 320.74
epoch train time: 0:00:03.917993
elapsed time: 0:08:38.865982
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 17:23:30.096915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.64
 ---- batch: 020 ----
mean loss: 306.73
train mean loss: 317.87
epoch train time: 0:00:03.919698
elapsed time: 0:08:42.786962
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 17:23:34.017895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.37
 ---- batch: 020 ----
mean loss: 318.06
train mean loss: 313.95
epoch train time: 0:00:03.915903
elapsed time: 0:08:46.704129
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 17:23:37.935090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.80
 ---- batch: 020 ----
mean loss: 315.73
train mean loss: 316.88
epoch train time: 0:00:03.904562
elapsed time: 0:08:50.609945
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 17:23:41.840868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.71
 ---- batch: 020 ----
mean loss: 317.46
train mean loss: 315.49
epoch train time: 0:00:03.924039
elapsed time: 0:08:54.535403
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 17:23:45.766331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.08
 ---- batch: 020 ----
mean loss: 304.13
train mean loss: 314.53
epoch train time: 0:00:03.915385
elapsed time: 0:08:58.452023
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 17:23:49.682928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.92
 ---- batch: 020 ----
mean loss: 306.27
train mean loss: 314.40
epoch train time: 0:00:03.956821
elapsed time: 0:09:02.410021
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 17:23:53.640944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.17
 ---- batch: 020 ----
mean loss: 315.02
train mean loss: 312.91
epoch train time: 0:00:03.969148
elapsed time: 0:09:06.380462
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 17:23:57.611394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.88
 ---- batch: 020 ----
mean loss: 312.96
train mean loss: 311.24
epoch train time: 0:00:03.960812
elapsed time: 0:09:10.342857
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 17:24:01.573809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.58
 ---- batch: 020 ----
mean loss: 308.23
train mean loss: 309.26
epoch train time: 0:00:03.921625
elapsed time: 0:09:14.265698
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 17:24:05.496616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.88
 ---- batch: 020 ----
mean loss: 314.83
train mean loss: 313.21
epoch train time: 0:00:03.914601
elapsed time: 0:09:18.181564
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 17:24:09.412486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.19
 ---- batch: 020 ----
mean loss: 312.81
train mean loss: 305.02
epoch train time: 0:00:03.912853
elapsed time: 0:09:22.095778
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 17:24:13.326689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.70
 ---- batch: 020 ----
mean loss: 315.41
train mean loss: 309.93
epoch train time: 0:00:03.934675
elapsed time: 0:09:26.031912
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 17:24:17.262920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.53
 ---- batch: 020 ----
mean loss: 301.89
train mean loss: 305.04
epoch train time: 0:00:03.917309
elapsed time: 0:09:29.950553
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 17:24:21.181503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.22
 ---- batch: 020 ----
mean loss: 298.04
train mean loss: 302.32
epoch train time: 0:00:03.933874
elapsed time: 0:09:33.885697
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 17:24:25.116607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.71
 ---- batch: 020 ----
mean loss: 303.04
train mean loss: 304.40
epoch train time: 0:00:03.924622
elapsed time: 0:09:37.811524
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 17:24:29.042453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.30
 ---- batch: 020 ----
mean loss: 309.55
train mean loss: 303.45
epoch train time: 0:00:03.915590
elapsed time: 0:09:41.728527
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 17:24:32.959463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.40
 ---- batch: 020 ----
mean loss: 306.43
train mean loss: 299.18
epoch train time: 0:00:03.906720
elapsed time: 0:09:45.636537
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 17:24:36.867463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.10
 ---- batch: 020 ----
mean loss: 307.31
train mean loss: 297.62
epoch train time: 0:00:03.917161
elapsed time: 0:09:49.554888
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 17:24:40.785861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.10
 ---- batch: 020 ----
mean loss: 305.98
train mean loss: 296.09
epoch train time: 0:00:03.913515
elapsed time: 0:09:53.470097
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 17:24:44.700865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.39
 ---- batch: 020 ----
mean loss: 295.96
train mean loss: 300.35
epoch train time: 0:00:03.931443
elapsed time: 0:09:57.402763
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 17:24:48.633698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.42
 ---- batch: 020 ----
mean loss: 290.89
train mean loss: 296.27
epoch train time: 0:00:03.944456
elapsed time: 0:10:01.348580
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 17:24:52.579503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.57
 ---- batch: 020 ----
mean loss: 301.31
train mean loss: 293.06
epoch train time: 0:00:03.953704
elapsed time: 0:10:05.303567
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 17:24:56.534490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.00
 ---- batch: 020 ----
mean loss: 296.65
train mean loss: 294.34
epoch train time: 0:00:03.929837
elapsed time: 0:10:09.234752
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 17:25:00.465774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.93
 ---- batch: 020 ----
mean loss: 281.90
train mean loss: 293.18
epoch train time: 0:00:03.920885
elapsed time: 0:10:13.157061
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 17:25:04.387987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.37
 ---- batch: 020 ----
mean loss: 284.70
train mean loss: 290.41
epoch train time: 0:00:03.939173
elapsed time: 0:10:17.097490
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 17:25:08.328419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.81
 ---- batch: 020 ----
mean loss: 295.92
train mean loss: 290.29
epoch train time: 0:00:03.924857
elapsed time: 0:10:21.023702
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 17:25:12.254629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.54
 ---- batch: 020 ----
mean loss: 297.80
train mean loss: 288.54
epoch train time: 0:00:03.932571
elapsed time: 0:10:24.957474
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 17:25:16.188421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.43
 ---- batch: 020 ----
mean loss: 287.99
train mean loss: 287.55
epoch train time: 0:00:03.930996
elapsed time: 0:10:28.889746
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 17:25:20.120677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.69
 ---- batch: 020 ----
mean loss: 297.72
train mean loss: 290.65
epoch train time: 0:00:03.930528
elapsed time: 0:10:32.821560
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 17:25:24.052486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.87
 ---- batch: 020 ----
mean loss: 284.35
train mean loss: 288.65
epoch train time: 0:00:03.933210
elapsed time: 0:10:36.756031
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 17:25:27.986966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.44
 ---- batch: 020 ----
mean loss: 279.58
train mean loss: 286.94
epoch train time: 0:00:03.913073
elapsed time: 0:10:40.670372
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 17:25:31.901284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.16
 ---- batch: 020 ----
mean loss: 278.88
train mean loss: 283.76
epoch train time: 0:00:03.915173
elapsed time: 0:10:44.586845
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 17:25:35.817821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.18
 ---- batch: 020 ----
mean loss: 287.28
train mean loss: 285.82
epoch train time: 0:00:03.919766
elapsed time: 0:10:48.507931
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 17:25:39.738855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.80
 ---- batch: 020 ----
mean loss: 287.49
train mean loss: 287.75
epoch train time: 0:00:03.938375
elapsed time: 0:10:52.447599
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 17:25:43.678547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.92
 ---- batch: 020 ----
mean loss: 266.36
train mean loss: 281.53
epoch train time: 0:00:03.931911
elapsed time: 0:10:56.380728
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 17:25:47.611649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.80
 ---- batch: 020 ----
mean loss: 277.26
train mean loss: 280.39
epoch train time: 0:00:03.918662
elapsed time: 0:11:00.300639
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 17:25:51.531544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.15
 ---- batch: 020 ----
mean loss: 282.42
train mean loss: 280.28
epoch train time: 0:00:03.915410
elapsed time: 0:11:04.217461
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 17:25:55.448407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.40
 ---- batch: 020 ----
mean loss: 290.20
train mean loss: 281.42
epoch train time: 0:00:03.948186
elapsed time: 0:11:08.166966
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 17:25:59.397869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.87
 ---- batch: 020 ----
mean loss: 276.55
train mean loss: 285.09
epoch train time: 0:00:03.918839
elapsed time: 0:11:12.087061
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 17:26:03.318024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.33
 ---- batch: 020 ----
mean loss: 276.82
train mean loss: 282.31
epoch train time: 0:00:03.907908
elapsed time: 0:11:15.996377
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 17:26:07.227320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.41
 ---- batch: 020 ----
mean loss: 269.53
train mean loss: 275.39
epoch train time: 0:00:03.915529
elapsed time: 0:11:19.913181
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 17:26:11.144136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.13
 ---- batch: 020 ----
mean loss: 281.07
train mean loss: 280.85
epoch train time: 0:00:03.917369
elapsed time: 0:11:23.831866
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 17:26:15.062792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.52
 ---- batch: 020 ----
mean loss: 275.72
train mean loss: 273.58
epoch train time: 0:00:03.931912
elapsed time: 0:11:27.765057
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 17:26:18.995985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.27
 ---- batch: 020 ----
mean loss: 276.47
train mean loss: 274.14
epoch train time: 0:00:03.932227
elapsed time: 0:11:31.698891
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 17:26:22.929669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.59
 ---- batch: 020 ----
mean loss: 283.51
train mean loss: 275.89
epoch train time: 0:00:03.938524
elapsed time: 0:11:35.638484
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 17:26:26.869397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.43
 ---- batch: 020 ----
mean loss: 273.71
train mean loss: 274.51
epoch train time: 0:00:03.929774
elapsed time: 0:11:39.569485
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 17:26:30.800390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.25
 ---- batch: 020 ----
mean loss: 282.34
train mean loss: 277.67
epoch train time: 0:00:03.912067
elapsed time: 0:11:43.482731
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 17:26:34.713713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.05
 ---- batch: 020 ----
mean loss: 271.94
train mean loss: 279.05
epoch train time: 0:00:03.911522
elapsed time: 0:11:47.395566
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 17:26:38.626475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.43
 ---- batch: 020 ----
mean loss: 275.52
train mean loss: 275.60
epoch train time: 0:00:03.920620
elapsed time: 0:11:51.317449
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 17:26:42.548393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.92
 ---- batch: 020 ----
mean loss: 261.06
train mean loss: 273.38
epoch train time: 0:00:03.933339
elapsed time: 0:11:55.252097
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 17:26:46.483014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.41
 ---- batch: 020 ----
mean loss: 263.47
train mean loss: 271.82
epoch train time: 0:00:03.931740
elapsed time: 0:11:59.185022
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 17:26:50.415946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.78
 ---- batch: 020 ----
mean loss: 282.55
train mean loss: 271.24
epoch train time: 0:00:03.941076
elapsed time: 0:12:03.127387
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 17:26:54.358316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.95
 ---- batch: 020 ----
mean loss: 273.00
train mean loss: 266.97
epoch train time: 0:00:03.944827
elapsed time: 0:12:07.073388
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 17:26:58.304311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.51
 ---- batch: 020 ----
mean loss: 265.84
train mean loss: 268.89
epoch train time: 0:00:03.932625
elapsed time: 0:12:11.007380
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 17:27:02.238313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.31
 ---- batch: 020 ----
mean loss: 280.73
train mean loss: 267.74
epoch train time: 0:00:03.936475
elapsed time: 0:12:14.945104
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 17:27:06.176031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.05
 ---- batch: 020 ----
mean loss: 269.69
train mean loss: 268.09
epoch train time: 0:00:03.935395
elapsed time: 0:12:18.881768
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 17:27:10.112693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.96
 ---- batch: 020 ----
mean loss: 261.30
train mean loss: 261.10
epoch train time: 0:00:03.938734
elapsed time: 0:12:22.821734
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 17:27:14.052674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.33
 ---- batch: 020 ----
mean loss: 271.05
train mean loss: 268.53
epoch train time: 0:00:03.944107
elapsed time: 0:12:26.767114
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 17:27:17.998036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.30
 ---- batch: 020 ----
mean loss: 270.17
train mean loss: 265.44
epoch train time: 0:00:03.940540
elapsed time: 0:12:30.709227
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 17:27:21.940149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.55
 ---- batch: 020 ----
mean loss: 263.81
train mean loss: 264.46
epoch train time: 0:00:03.942698
elapsed time: 0:12:34.653295
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 17:27:25.884231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.77
 ---- batch: 020 ----
mean loss: 264.45
train mean loss: 264.28
epoch train time: 0:00:03.929925
elapsed time: 0:12:38.584455
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 17:27:29.815404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.82
 ---- batch: 020 ----
mean loss: 254.49
train mean loss: 261.09
epoch train time: 0:00:03.942994
elapsed time: 0:12:42.528886
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 17:27:33.759827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.87
 ---- batch: 020 ----
mean loss: 259.88
train mean loss: 259.31
epoch train time: 0:00:03.931557
elapsed time: 0:12:46.461738
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 17:27:37.692695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.56
 ---- batch: 020 ----
mean loss: 256.07
train mean loss: 260.75
epoch train time: 0:00:03.926885
elapsed time: 0:12:50.389962
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 17:27:41.620873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.53
 ---- batch: 020 ----
mean loss: 261.12
train mean loss: 259.79
epoch train time: 0:00:03.921690
elapsed time: 0:12:54.312967
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 17:27:45.543887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.10
 ---- batch: 020 ----
mean loss: 256.14
train mean loss: 258.48
epoch train time: 0:00:03.920702
elapsed time: 0:12:58.234937
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 17:27:49.465850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.71
 ---- batch: 020 ----
mean loss: 277.33
train mean loss: 271.33
epoch train time: 0:00:03.919577
elapsed time: 0:13:02.155756
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 17:27:53.386694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.72
 ---- batch: 020 ----
mean loss: 258.50
train mean loss: 259.51
epoch train time: 0:00:03.931584
elapsed time: 0:13:06.088688
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 17:27:57.319611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.40
 ---- batch: 020 ----
mean loss: 256.48
train mean loss: 259.56
epoch train time: 0:00:03.933668
elapsed time: 0:13:10.023607
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 17:28:01.254526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.05
 ---- batch: 020 ----
mean loss: 259.68
train mean loss: 260.83
epoch train time: 0:00:03.924463
elapsed time: 0:13:13.949285
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 17:28:05.180186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.37
 ---- batch: 020 ----
mean loss: 249.36
train mean loss: 257.08
epoch train time: 0:00:03.923564
elapsed time: 0:13:17.874114
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 17:28:09.105048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.50
 ---- batch: 020 ----
mean loss: 259.16
train mean loss: 254.20
epoch train time: 0:00:03.943282
elapsed time: 0:13:21.818962
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 17:28:13.049703
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.45
 ---- batch: 020 ----
mean loss: 255.70
train mean loss: 251.48
epoch train time: 0:00:03.959645
elapsed time: 0:13:25.779791
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 17:28:17.010713
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.55
 ---- batch: 020 ----
mean loss: 264.92
train mean loss: 254.07
epoch train time: 0:00:03.931684
elapsed time: 0:13:29.712665
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 17:28:20.943575
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.40
 ---- batch: 020 ----
mean loss: 243.88
train mean loss: 253.50
epoch train time: 0:00:03.929720
elapsed time: 0:13:33.643701
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 17:28:24.874666
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.25
 ---- batch: 020 ----
mean loss: 251.92
train mean loss: 254.49
epoch train time: 0:00:03.923243
elapsed time: 0:13:37.568224
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 17:28:28.799140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.64
 ---- batch: 020 ----
mean loss: 252.73
train mean loss: 251.71
epoch train time: 0:00:03.938781
elapsed time: 0:13:41.508217
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 17:28:32.739143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.66
 ---- batch: 020 ----
mean loss: 251.66
train mean loss: 253.48
epoch train time: 0:00:03.929329
elapsed time: 0:13:45.438821
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 17:28:36.669774
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.86
 ---- batch: 020 ----
mean loss: 260.05
train mean loss: 253.83
epoch train time: 0:00:03.923365
elapsed time: 0:13:49.363544
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 17:28:40.594461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.35
 ---- batch: 020 ----
mean loss: 250.85
train mean loss: 256.29
epoch train time: 0:00:03.926521
elapsed time: 0:13:53.291324
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 17:28:44.522218
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.92
 ---- batch: 020 ----
mean loss: 258.71
train mean loss: 254.70
epoch train time: 0:00:03.921936
elapsed time: 0:13:57.214384
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 17:28:48.445323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.27
 ---- batch: 020 ----
mean loss: 250.59
train mean loss: 251.45
epoch train time: 0:00:03.925503
elapsed time: 0:14:01.141158
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 17:28:52.372102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 248.54
 ---- batch: 020 ----
mean loss: 253.46
train mean loss: 253.51
epoch train time: 0:00:03.923138
elapsed time: 0:14:05.065533
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 17:28:56.296445
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.01
 ---- batch: 020 ----
mean loss: 245.80
train mean loss: 252.64
epoch train time: 0:00:03.926512
elapsed time: 0:14:08.993248
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 17:29:00.224154
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.12
 ---- batch: 020 ----
mean loss: 251.57
train mean loss: 251.31
epoch train time: 0:00:03.933174
elapsed time: 0:14:12.927747
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 17:29:04.158686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.90
 ---- batch: 020 ----
mean loss: 254.67
train mean loss: 255.01
epoch train time: 0:00:03.959743
elapsed time: 0:14:16.888742
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 17:29:08.119690
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.51
 ---- batch: 020 ----
mean loss: 251.39
train mean loss: 252.14
epoch train time: 0:00:03.985121
elapsed time: 0:14:20.875155
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 17:29:12.106095
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.43
 ---- batch: 020 ----
mean loss: 250.50
train mean loss: 249.98
epoch train time: 0:00:03.938010
elapsed time: 0:14:24.814623
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 17:29:16.045591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.39
 ---- batch: 020 ----
mean loss: 249.20
train mean loss: 251.99
epoch train time: 0:00:03.929319
elapsed time: 0:14:28.745383
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 17:29:19.976298
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.10
 ---- batch: 020 ----
mean loss: 257.16
train mean loss: 252.88
epoch train time: 0:00:03.941518
elapsed time: 0:14:32.688184
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 17:29:23.919110
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.58
 ---- batch: 020 ----
mean loss: 257.18
train mean loss: 252.91
epoch train time: 0:00:03.928935
elapsed time: 0:14:36.618392
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 17:29:27.849361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.98
 ---- batch: 020 ----
mean loss: 263.54
train mean loss: 251.82
epoch train time: 0:00:03.938222
elapsed time: 0:14:40.558098
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 17:29:31.789068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.88
 ---- batch: 020 ----
mean loss: 242.15
train mean loss: 251.21
epoch train time: 0:00:03.919195
elapsed time: 0:14:44.478578
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 17:29:35.709480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.32
 ---- batch: 020 ----
mean loss: 252.92
train mean loss: 253.14
epoch train time: 0:00:03.919634
elapsed time: 0:14:48.399406
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 17:29:39.630319
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 246.23
 ---- batch: 020 ----
mean loss: 253.22
train mean loss: 251.97
epoch train time: 0:00:03.919604
elapsed time: 0:14:52.320280
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 17:29:43.551188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.84
 ---- batch: 020 ----
mean loss: 255.54
train mean loss: 252.94
epoch train time: 0:00:03.918078
elapsed time: 0:14:56.239619
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 17:29:47.470538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.45
 ---- batch: 020 ----
mean loss: 251.91
train mean loss: 251.70
epoch train time: 0:00:03.916381
elapsed time: 0:15:00.157206
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 17:29:51.388106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.21
 ---- batch: 020 ----
mean loss: 250.44
train mean loss: 250.15
epoch train time: 0:00:03.911076
elapsed time: 0:15:04.069525
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 17:29:55.300427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.11
 ---- batch: 020 ----
mean loss: 247.64
train mean loss: 250.46
epoch train time: 0:00:03.951473
elapsed time: 0:15:08.022264
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 17:29:59.253178
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.64
 ---- batch: 020 ----
mean loss: 252.95
train mean loss: 250.77
epoch train time: 0:00:03.939647
elapsed time: 0:15:11.963342
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 17:30:03.194283
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.82
 ---- batch: 020 ----
mean loss: 259.04
train mean loss: 253.66
epoch train time: 0:00:03.920602
elapsed time: 0:15:15.885175
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 17:30:07.116091
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.91
 ---- batch: 020 ----
mean loss: 250.02
train mean loss: 252.97
epoch train time: 0:00:03.907935
elapsed time: 0:15:19.794356
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 17:30:11.025313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.63
 ---- batch: 020 ----
mean loss: 244.92
train mean loss: 251.83
epoch train time: 0:00:03.921015
elapsed time: 0:15:23.716707
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 17:30:14.947649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 243.11
 ---- batch: 020 ----
mean loss: 257.08
train mean loss: 255.24
epoch train time: 0:00:03.923261
elapsed time: 0:15:27.641806
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 17:30:18.872558
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.73
 ---- batch: 020 ----
mean loss: 253.08
train mean loss: 251.24
epoch train time: 0:00:03.918933
elapsed time: 0:15:31.561836
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 17:30:22.792748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.19
 ---- batch: 020 ----
mean loss: 258.99
train mean loss: 250.41
epoch train time: 0:00:03.906891
elapsed time: 0:15:35.470031
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 17:30:26.700993
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.70
 ---- batch: 020 ----
mean loss: 246.18
train mean loss: 253.65
epoch train time: 0:00:03.920534
elapsed time: 0:15:39.391884
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 17:30:30.622794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.47
 ---- batch: 020 ----
mean loss: 256.63
train mean loss: 254.63
epoch train time: 0:00:03.927349
elapsed time: 0:15:43.320564
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 17:30:34.551488
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.56
 ---- batch: 020 ----
mean loss: 250.48
train mean loss: 250.94
epoch train time: 0:00:03.928214
elapsed time: 0:15:47.250200
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 17:30:38.481107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.97
 ---- batch: 020 ----
mean loss: 250.95
train mean loss: 249.48
epoch train time: 0:00:03.928983
elapsed time: 0:15:51.180409
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 17:30:42.411331
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.48
 ---- batch: 020 ----
mean loss: 249.19
train mean loss: 252.85
epoch train time: 0:00:03.952113
elapsed time: 0:15:55.133846
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 17:30:46.364908
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 246.66
 ---- batch: 020 ----
mean loss: 255.67
train mean loss: 253.23
epoch train time: 0:00:03.936615
elapsed time: 0:15:59.072258
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 17:30:50.303227
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.53
 ---- batch: 020 ----
mean loss: 243.57
train mean loss: 252.71
epoch train time: 0:00:03.919646
elapsed time: 0:16:02.993149
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 17:30:54.224056
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.20
 ---- batch: 020 ----
mean loss: 250.22
train mean loss: 249.58
epoch train time: 0:00:03.918864
elapsed time: 0:16:06.913294
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 17:30:58.144206
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.39
 ---- batch: 020 ----
mean loss: 247.07
train mean loss: 250.43
epoch train time: 0:00:03.902397
elapsed time: 0:16:10.816877
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 17:31:02.047835
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.99
 ---- batch: 020 ----
mean loss: 250.87
train mean loss: 251.57
epoch train time: 0:00:03.912419
elapsed time: 0:16:14.730639
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 17:31:05.961550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.52
 ---- batch: 020 ----
mean loss: 246.21
train mean loss: 251.25
epoch train time: 0:00:03.915272
elapsed time: 0:16:18.647144
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 17:31:09.878056
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.95
 ---- batch: 020 ----
mean loss: 243.70
train mean loss: 250.64
epoch train time: 0:00:03.930019
elapsed time: 0:16:22.578449
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 17:31:13.809377
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.77
 ---- batch: 020 ----
mean loss: 245.99
train mean loss: 248.07
epoch train time: 0:00:03.939616
elapsed time: 0:16:26.519333
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 17:31:17.750254
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.49
 ---- batch: 020 ----
mean loss: 255.80
train mean loss: 250.39
epoch train time: 0:00:03.923446
elapsed time: 0:16:30.452823
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_4/checkpoint.pth.tar
**** end time: 2019-09-26 17:31:21.683524 ****
